{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python implementation: CPython\n",
      "Python version       : 3.11.8\n",
      "IPython version      : 8.22.2\n",
      "\n",
      "numpy    : 1.26.4\n",
      "pandas   : 2.2.1\n",
      "polars   : 0.20.18\n",
      "omegaconf: 2.3.0\n",
      "\n",
      "conda environment: torch_p11\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -v -p numpy,pandas,polars,omegaconf --conda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in library\n",
    "from pathlib import Path\n",
    "import re\n",
    "import json\n",
    "from typing import Any, Optional, Union\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "# Standard imports\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from rich.console import Console\n",
    "from rich.theme import Theme\n",
    "\n",
    "custom_theme = Theme(\n",
    "    {\n",
    "        \"info\": \"#76FF7B\",\n",
    "        \"warning\": \"#FBDDFE\",\n",
    "        \"error\": \"#FF0000\",\n",
    "    }\n",
    ")\n",
    "console = Console(theme=custom_theme)\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# NumPy settings\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "# Pandas settings\n",
    "pd.options.display.max_rows = 1_000\n",
    "pd.options.display.max_columns = 1_000\n",
    "pd.options.display.max_colwidth = 600\n",
    "\n",
    "# Polars settings\n",
    "pl.Config.set_fmt_str_lengths(1_000)\n",
    "pl.Config.set_tbl_cols(n=1_000)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# Black code formatter (Optional)\n",
    "%load_ext lab_black\n",
    "\n",
    "# auto reload imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "\n",
    "def create_id_text_mapping(filepath: str) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Create a mapping DataFrame from a Parquet file.\n",
    "\n",
    "    This function reads a Parquet file, processes the data, and creates a mapping\n",
    "    DataFrame with 'id', 'text', and 'label' columns.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filepath : str\n",
    "        The path to the Parquet file.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pl.DataFrame\n",
    "        A DataFrame with shape (1, 3) containing 'id', 'text', and 'label' columns.\n",
    "    \"\"\"\n",
    "    pattern: str = r\"salary|gigworker\"\n",
    "    delimiter: str = \"|\"\n",
    "\n",
    "    df: pl.DataFrame = (\n",
    "        pl.scan_parquet(filepath)\n",
    "        .with_columns(tags=pl.col(\"tags\").map_elements(lambda x: \"\".join(x)))\n",
    "        .filter(pl.col(\"tags\").str.to_lowercase().str.contains(pattern))\n",
    "        .with_columns(\n",
    "            label=pl.col(\"tags\")\n",
    "            .str.extract_all(pattern)\n",
    "            .map_elements(lambda x: \"\".join(set(x)))\n",
    "        )\n",
    "        .drop(\"tags\")\n",
    "        .collect()\n",
    "    )\n",
    "    try:\n",
    "        df = df.rename({\"analysisId\": \"id\"})\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    df_grpby: pl.DataFrame = df.group_by(\"id\").agg(\n",
    "        text=(pl.struct([\"date\", \"description\", \"amount\"]))\n",
    "    )\n",
    "    body: list[str] = []\n",
    "    for row in df_grpby.select(\"text\").to_dicts():\n",
    "        for data_ in row[\"text\"]:\n",
    "            date: str = data_[\"date\"]\n",
    "            description: str = data_[\"description\"]\n",
    "            amount: float = data_[\"amount\"]\n",
    "            b_str: str = f\"{date} {delimiter} {description} {delimiter} {amount} \"\n",
    "            value: str = f\"{b_str}\\n\"\n",
    "            body.append(value)\n",
    "\n",
    "    id: str = str(df.select(\"id\").unique().to_numpy().squeeze())\n",
    "    label: str = str(df.select(\"label\").unique().to_numpy().squeeze())\n",
    "    data: dict[str, Any] = {\"id\": id, \"text\": body, \"label\": label}\n",
    "    mapping_df: pl.DataFrame = pl.DataFrame([data])\n",
    "\n",
    "    return mapping_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_dataset(\n",
    "    filepath: str = \"./data/*.parquet\", output_path: str | None = None\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Create a dataset by combining multiple Parquet files and save as JSONL.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filepath : str, optional\n",
    "        Glob pattern for input Parquet files, by default \"./data/*.parquet\"\n",
    "    output_path : str | None, optional\n",
    "        Path to save the output JSONL file, by default None\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pl.DataFrame\n",
    "        Combined DataFrame from all input files\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    If output_path is None, the function will use \"output.jsonl\" as default.\n",
    "    \"\"\"\n",
    "\n",
    "    files: list[str] = glob(filepath)\n",
    "    all_df: pl.DataFrame = pl.DataFrame()\n",
    "\n",
    "    for f in files:\n",
    "        df: pl.DataFrame = create_id_text_mapping(filepath=f)\n",
    "        all_df = all_df.vstack(df)\n",
    "\n",
    "    # Convert the DataFrame to JSONL\n",
    "    if output_path is None:\n",
    "        output_path = \"output.jsonl\"\n",
    "    all_df.write_ndjson(output_path)\n",
    "    return all_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch p_311",
   "language": "python",
   "name": "torch_p11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
