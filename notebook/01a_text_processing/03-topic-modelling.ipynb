{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modelling\n",
    "\n",
    "- Topic modeling in NLP discovers abstract themes in document collections using algorithms like LDA (Latent Dirichlet Allocation), LSA (Latent Semantic Analysis), etc. \n",
    "- It's unsupervised, clusters similar expressions, and aids in organizing, summarizing, and analyzing large text datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python implementation: CPython\n",
      "Python version       : 3.11.8\n",
      "IPython version      : 8.22.2\n",
      "\n",
      "numpy    : 1.26.4\n",
      "pandas   : 2.2.1\n",
      "polars   : 0.20.18\n",
      "omegaconf: 2.3.0\n",
      "\n",
      "conda environment: torch_p11\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -v -p numpy,pandas,polars,omegaconf --conda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in library\n",
    "from pathlib import Path\n",
    "import re\n",
    "import json\n",
    "from typing import Any, Optional, Union\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "# Standard imports\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from rich.console import Console\n",
    "from rich.theme import Theme\n",
    "\n",
    "custom_theme = Theme(\n",
    "    {\n",
    "        \"info\": \"#76FF7B\",\n",
    "        \"warning\": \"#FBDDFE\",\n",
    "        \"error\": \"#FF0000\",\n",
    "    }\n",
    ")\n",
    "console = Console(theme=custom_theme)\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# NumPy settings\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "# Pandas settings\n",
    "pd.options.display.max_rows = 1_000\n",
    "pd.options.display.max_columns = 1_000\n",
    "pd.options.display.max_colwidth = 600\n",
    "\n",
    "# Polars settings\n",
    "pl.Config.set_fmt_str_lengths(1_000)\n",
    "pl.Config.set_tbl_cols(n=1_000)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# Black code formatter (Optional)\n",
    "%load_ext lab_black\n",
    "\n",
    "# auto reload imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Dirichlet Allocation (LDA)\n",
    "\n",
    "- LDA is a generative probabilistic model that tries to find groups of words that appear frequently together across different documents.\n",
    "- It assumes that each document is a mixture of topics, and each topic is a distribution over words.\n",
    "- LDA assumes documents are \"bags of words.\" i.e. the order of words doesn't matter, and only the frequency of word occurrences is considered.\n",
    "  - This is a simplification, as word order can be important for understanding meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>review</th><th>sentiment</th></tr><tr><td>str</td><td>i64</td></tr></thead><tbody><tr><td>&quot;I felt this movie was as much about human sexuality as anything else, whether intentionally or not. We are also shown how absurd and paradoxical it is for women not to be allowed to such a nationally important event, meanwhile forgetting the pasts of our respective &quot;advanced&quot; nations. I write from Japan, where women merely got the right to vote 60 years ago, and female technical engineers are a recent phenomenon. Pubs in England were once all-male, the business world was totally off-limits for women in America until rather recently, and women in China had their feet bound so they couldn&#x27;t develop feet strong enough to escape their husbands. Iran is conveniently going through this stage in our time, and we get a good look at how ridiculous we have all looked at one time or another. Back to the issue of sexuality, we are made to wonder what it may be intrinsically about women that make them unfit for a soccer game (the official reason is that the men are bad). Especially such boyish gir…</td><td>1</td></tr><tr><td>&quot;Let&#x27;s face it, a truly awful movie, no...I mean a &quot;truly&quot; awful movie, is a rare, strange, and beautiful thing to behold. I admite that there is a special place in my heart for films like Plan 9 From Outer Space, Half Caste, Species, etc. And although I&#x27;m giving this film a 1, I highly urge anyone who enjoys a bad film for what it truly is (a bad film) to find a friend, snacks, something to drink, and make the special occasion it deserves out of: Aussie Park Boyz. &lt;br /&gt;&lt;br /&gt;From the very first moments of the lead actor&#x27;s side to side eye-rolling performance as he attempts to inject intensity directly into the film without ever looking at a camera (a slice of ham straight out of silent pictures--eat your heart out Rudolph Valentino) to the sudden hey-we&#x27;re-out-of-film conclusion, you...will...not...stop...laughing. &lt;br /&gt;&lt;br /&gt;To sum the film up, its a poor man&#x27;s Warriors down under, complete--and that description alone should be enough, but then comes the wonders of &quot;the spaghetti e…</td><td>0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 2)\n",
       "┌──────────────────────────────────────────────────────────────────────────────────────┬───────────┐\n",
       "│ review                                                                               ┆ sentiment │\n",
       "│ ---                                                                                  ┆ ---       │\n",
       "│ str                                                                                  ┆ i64       │\n",
       "╞══════════════════════════════════════════════════════════════════════════════════════╪═══════════╡\n",
       "│ I felt this movie was as much about human sexuality as anything else, whether        ┆ 1         │\n",
       "│ intentionally or not. We are also shown how absurd and paradoxical it is for women   ┆           │\n",
       "│ not to be allowed to such a nationally important event, meanwhile forgetting the     ┆           │\n",
       "│ pasts of our respective \"advanced\" nations. I write from Japan, where women merely   ┆           │\n",
       "│ got the right to vote 60 years ago, and female technical engineers are a recent      ┆           │\n",
       "│ phenomenon. Pubs in England were once all-male, the business world was totally       ┆           │\n",
       "│ off-limits for women in America until rather recently, and women in China had their  ┆           │\n",
       "│ feet bound so they couldn't develop feet strong enough to escape their husbands.     ┆           │\n",
       "│ Iran is conveniently going through this stage in our time, and we get a good look at ┆           │\n",
       "│ how ridiculous we have all looked at one time or another. Back to the issue of       ┆           │\n",
       "│ sexuality, we are made to wonder what it may be intrinsically about women that make  ┆           │\n",
       "│ them unfit for a soccer game (the official reason is that the men are bad).          ┆           │\n",
       "│ Especially such boyish girl…                                                         ┆           │\n",
       "│ Let's face it, a truly awful movie, no...I mean a \"truly\" awful movie, is a rare,    ┆ 0         │\n",
       "│ strange, and beautiful thing to behold. I admite that there is a special place in my ┆           │\n",
       "│ heart for films like Plan 9 From Outer Space, Half Caste, Species, etc. And although ┆           │\n",
       "│ I'm giving this film a 1, I highly urge anyone who enjoys a bad film for what it     ┆           │\n",
       "│ truly is (a bad film) to find a friend, snacks, something to drink, and make the     ┆           │\n",
       "│ special occasion it deserves out of: Aussie Park Boyz. <br /><br />From the very     ┆           │\n",
       "│ first moments of the lead actor's side to side eye-rolling performance as he         ┆           │\n",
       "│ attempts to inject intensity directly into the film without ever looking at a camera ┆           │\n",
       "│ (a slice of ham straight out of silent pictures--eat your heart out Rudolph          ┆           │\n",
       "│ Valentino) to the sudden hey-we're-out-of-film conclusion,                           ┆           │\n",
       "│ you...will...not...stop...laughing. <br /><br />To sum the film up, its a poor man's ┆           │\n",
       "│ Warriors down under, complete--and that description alone should be enough, but then ┆           │\n",
       "│ comes the wonders of \"the spaghetti ea…                                              ┆           │\n",
       "└──────────────────────────────────────────────────────────────────────────────────────┴───────────┘"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp: str = \"../../data/ImDB_data.parquet\"\n",
    "df: pl.DataFrame = pl.read_parquet(fp)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "\n",
    "count: CountVectorizer = CountVectorizer(\n",
    "    stop_words=\"english\", max_df=0.1, max_features=10_000\n",
    ")\n",
    "X: csr_matrix = count.fit_transform(df.select(\"review\").to_numpy().squeeze().tolist())\n",
    "\n",
    "\n",
    "lda = LatentDirichletAllocation(\n",
    "    n_components=10, random_state=123, learning_method=\"online\"\n",
    ")\n",
    "X_topics = lda.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5_000 features per topic. i.e. 10 topics, 5000 features.\n",
    "lda.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m       \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCall signature:\u001b[0m  \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mType:\u001b[0m            _ArrayFunctionDispatcher\n",
      "\u001b[0;31mString form:\u001b[0m     <function argsort at 0x104b36d40>\n",
      "\u001b[0;31mFile:\u001b[0m            ~/miniconda3/envs/torch_p11/lib/python3.11/site-packages/numpy/core/fromnumeric.py\n",
      "\u001b[0;31mDocstring:\u001b[0m      \n",
      "Returns the indices that would sort an array.\n",
      "\n",
      "Perform an indirect sort along the given axis using the algorithm specified\n",
      "by the `kind` keyword. It returns an array of indices of the same shape as\n",
      "`a` that index data along the given axis in sorted order.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "a : array_like\n",
      "    Array to sort.\n",
      "axis : int or None, optional\n",
      "    Axis along which to sort.  The default is -1 (the last axis). If None,\n",
      "    the flattened array is used.\n",
      "kind : {'quicksort', 'mergesort', 'heapsort', 'stable'}, optional\n",
      "    Sorting algorithm. The default is 'quicksort'. Note that both 'stable'\n",
      "    and 'mergesort' use timsort under the covers and, in general, the\n",
      "    actual implementation will vary with data type. The 'mergesort' option\n",
      "    is retained for backwards compatibility.\n",
      "\n",
      "    .. versionchanged:: 1.15.0.\n",
      "       The 'stable' option was added.\n",
      "order : str or list of str, optional\n",
      "    When `a` is an array with fields defined, this argument specifies\n",
      "    which fields to compare first, second, etc.  A single field can\n",
      "    be specified as a string, and not all fields need be specified,\n",
      "    but unspecified fields will still be used, in the order in which\n",
      "    they come up in the dtype, to break ties.\n",
      "\n",
      "Returns\n",
      "-------\n",
      "index_array : ndarray, int\n",
      "    Array of indices that sort `a` along the specified `axis`.\n",
      "    If `a` is one-dimensional, ``a[index_array]`` yields a sorted `a`.\n",
      "    More generally, ``np.take_along_axis(a, index_array, axis=axis)``\n",
      "    always yields the sorted `a`, irrespective of dimensionality.\n",
      "\n",
      "See Also\n",
      "--------\n",
      "sort : Describes sorting algorithms used.\n",
      "lexsort : Indirect stable sort with multiple keys.\n",
      "ndarray.sort : Inplace sort.\n",
      "argpartition : Indirect partial sort.\n",
      "take_along_axis : Apply ``index_array`` from argsort\n",
      "                  to an array as if by calling sort.\n",
      "\n",
      "Notes\n",
      "-----\n",
      "See `sort` for notes on the different sorting algorithms.\n",
      "\n",
      "As of NumPy 1.4.0 `argsort` works with real/complex arrays containing\n",
      "nan values. The enhanced sort order is documented in `sort`.\n",
      "\n",
      "Examples\n",
      "--------\n",
      "One dimensional array:\n",
      "\n",
      ">>> x = np.array([3, 1, 2])\n",
      ">>> np.argsort(x)\n",
      "array([1, 2, 0])\n",
      "\n",
      "Two-dimensional array:\n",
      "\n",
      ">>> x = np.array([[0, 3], [2, 2]])\n",
      ">>> x\n",
      "array([[0, 3],\n",
      "       [2, 2]])\n",
      "\n",
      ">>> ind = np.argsort(x, axis=0)  # sorts along first axis (down)\n",
      ">>> ind\n",
      "array([[0, 1],\n",
      "       [1, 0]])\n",
      ">>> np.take_along_axis(x, ind, axis=0)  # same as np.sort(x, axis=0)\n",
      "array([[0, 2],\n",
      "       [2, 3]])\n",
      "\n",
      ">>> ind = np.argsort(x, axis=1)  # sorts along last axis (across)\n",
      ">>> ind\n",
      "array([[0, 1],\n",
      "       [0, 1]])\n",
      ">>> np.take_along_axis(x, ind, axis=1)  # same as np.sort(x, axis=1)\n",
      "array([[0, 3],\n",
      "       [2, 2]])\n",
      "\n",
      "Indices of the sorted elements of a N-dimensional array:\n",
      "\n",
      ">>> ind = np.unravel_index(np.argsort(x, axis=None), x.shape)\n",
      ">>> ind\n",
      "(array([0, 1, 1, 0]), array([0, 0, 1, 1]))\n",
      ">>> x[ind]  # same as np.sort(x, axis=None)\n",
      "array([0, 2, 2, 3])\n",
      "\n",
      "Sorting with keys:\n",
      "\n",
      ">>> x = np.array([(1, 0), (0, 1)], dtype=[('x', '<i4'), ('y', '<i4')])\n",
      ">>> x\n",
      "array([(1, 0), (0, 1)],\n",
      "      dtype=[('x', '<i4'), ('y', '<i4')])\n",
      "\n",
      ">>> np.argsort(x, order=('x','y'))\n",
      "array([1, 0])\n",
      "\n",
      ">>> np.argsort(x, order=('y','x'))\n",
      "array([0, 1])\n",
      "\u001b[0;31mClass docstring:\u001b[0m\n",
      "Class to wrap functions with checks for __array_function__ overrides.\n",
      "\n",
      "All arguments are required, and can only be passed by position.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "dispatcher : function or None\n",
      "    The dispatcher function that returns a single sequence-like object\n",
      "    of all arguments relevant.  It must have the same signature (except\n",
      "    the default values) as the actual implementation.\n",
      "    If ``None``, this is a ``like=`` dispatcher and the\n",
      "    ``_ArrayFunctionDispatcher`` must be called with ``like`` as the\n",
      "    first (additional and positional) argument.\n",
      "implementation : function\n",
      "    Function that implements the operation on NumPy arrays without\n",
      "    overrides.  Arguments passed calling the ``_ArrayFunctionDispatcher``\n",
      "    will be forwarded to this (and the ``dispatcher``) as if using\n",
      "    ``*args, **kwargs``.\n",
      "\n",
      "Attributes\n",
      "----------\n",
      "_implementation : function\n",
      "    The original implementation passed in."
     ]
    }
   ],
   "source": [
    "np.argsort?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1:\n",
      "girl sex guy women woman\n",
      "Topic 2:\n",
      "horror series original episode tv\n",
      "Topic 3:\n",
      "role performance john comedy played\n",
      "Topic 4:\n",
      "american war english french history\n",
      "Topic 5:\n",
      "worst minutes watched ll maybe\n",
      "Topic 6:\n",
      "action game space fight effects\n",
      "Topic 7:\n",
      "art documentary human reality subject\n",
      "Topic 8:\n",
      "book kids comedy read children\n",
      "Topic 9:\n",
      "family father mother beautiful novel\n",
      "Topic 10:\n",
      "killer murder death police dead\n"
     ]
    }
   ],
   "source": [
    "n_top_words: int = 5\n",
    "feature_names: np.ndarray = count.get_feature_names_out()\n",
    "\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print(f\"Topic {topic_idx + 1}:\")\n",
    "    print(\n",
    "        \" \".join([feature_names[i] for i in topic.argsort()[: -n_top_words - 1 : -1]])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "\n",
    "def create_id_text_mapping(filepath: str) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Create a mapping DataFrame from a Parquet file.\n",
    "\n",
    "    This function reads a Parquet file, processes the data, and creates a mapping\n",
    "    DataFrame with 'id', 'text', and 'label' columns.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filepath : str\n",
    "        The path to the Parquet file.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pl.DataFrame\n",
    "        A DataFrame with shape (1, 3) containing 'id', 'text', and 'label' columns.\n",
    "    \"\"\"\n",
    "    pattern: str = r\"salary|gigworker\"\n",
    "    delimiter: str = \"|\"\n",
    "\n",
    "    df: pl.DataFrame = (\n",
    "        pl.scan_parquet(filepath)\n",
    "        .with_columns(tags=pl.col(\"tags\").map_elements(lambda x: \"\".join(x)))\n",
    "        .filter(pl.col(\"tags\").str.to_lowercase().str.contains(pattern))\n",
    "        .with_columns(\n",
    "            label=pl.col(\"tags\")\n",
    "            .str.extract_all(pattern)\n",
    "            .map_elements(lambda x: \"\".join(set(x)))\n",
    "        )\n",
    "        .drop(\"tags\")\n",
    "        .collect()\n",
    "    )\n",
    "    try:\n",
    "        df = df.rename({\"analysisId\": \"id\"})\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    df_grpby: pl.DataFrame = df.group_by(\"id\").agg(\n",
    "        text=(pl.struct([\"date\", \"description\", \"amount\"]))\n",
    "    )\n",
    "    body: list[str] = []\n",
    "    for row in df_grpby.select(\"text\").to_dicts():\n",
    "        for data_ in row[\"text\"]:\n",
    "            date: str = data_[\"date\"]\n",
    "            description: str = data_[\"description\"]\n",
    "            amount: float = data_[\"amount\"]\n",
    "            b_str: str = f\"{date} {delimiter} {description} {delimiter} {amount} \"\n",
    "            value: str = f\"{b_str}\\n\"\n",
    "            body.append(value)\n",
    "\n",
    "    id: str = str(df.select(\"id\").unique().to_numpy().squeeze())\n",
    "    label: str = str(df.select(\"label\").unique().to_numpy().squeeze())\n",
    "    data: dict[str, Any] = {\"id\": id, \"text\": body, \"label\": label}\n",
    "    mapping_df: pl.DataFrame = pl.DataFrame([data])\n",
    "\n",
    "    return mapping_df\n",
    "\n",
    "\n",
    "def create_dataset(\n",
    "    filepath: str = \"./data/*.parquet\", output_path: str | None = None\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Create a dataset by combining multiple Parquet files and save as JSONL.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filepath : str, optional\n",
    "        Glob pattern for input Parquet files, by default \"./data/*.parquet\"\n",
    "    output_path : str | None, optional\n",
    "        Path to save the output JSONL file, by default None\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pl.DataFrame\n",
    "        Combined DataFrame from all input files\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    If output_path is None, the function will use \"output.jsonl\" as default.\n",
    "    \"\"\"\n",
    "\n",
    "    files: list[str] = glob(filepath)\n",
    "    all_df: pl.DataFrame = pl.DataFrame()\n",
    "\n",
    "    for f in files:\n",
    "        df: pl.DataFrame = create_id_text_mapping(filepath=f)\n",
    "        all_df = all_df.vstack(df)\n",
    "\n",
    "    # Convert the DataFrame to JSONL\n",
    "    if output_path is None:\n",
    "        output_path = \"output.jsonl\"\n",
    "    all_df.write_ndjson(output_path)\n",
    "    return all_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch p_311",
   "language": "python",
   "name": "torch_p11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
