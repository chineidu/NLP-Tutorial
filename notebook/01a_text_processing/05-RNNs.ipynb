{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chineidu/NLP-Tutorial/blob/main/notebook/01a_text_processing/05-RNNs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9RZg7ZPA4cd"
      },
      "source": [
        "# RNNs (Recurrent Neural Networks)\n",
        "\n",
        "## Basic RNN\n",
        "## Long Short-Term Memory (LSTM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJosLFceFQi_",
        "outputId": "24d4364e-3470-4da0-8a35-729f4116bcf6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m890.1/890.1 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m71.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.4.0+cu121 requires torch==2.4.0, but you have torch 1.13.0 which is incompatible.\n",
            "torchvision 0.19.0+cu121 requires torch==2.4.0, but you have torch 1.13.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip -q install torch==1.13.0 torchtext==0.14.0 \\\n",
        "  torchdata==0.5.0 portalocker==2.5.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "DnGvNbkYA4cf"
      },
      "outputs": [],
      "source": [
        "# %load_ext watermark\n",
        "# %watermark -v -p numpy,pandas,polars,mlxtend,omegaconf --conda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Vmc-scv-A4cg"
      },
      "outputs": [],
      "source": [
        "# Built-in library\n",
        "from pathlib import Path\n",
        "import re\n",
        "import json\n",
        "from typing import Any, Literal, Optional, TypedDict, Union\n",
        "import logging\n",
        "import warnings\n",
        "\n",
        "# Standard imports\n",
        "import numpy as np\n",
        "import numpy.typing as npt\n",
        "from pprint import pprint\n",
        "import pandas as pd\n",
        "import polars as pl\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# NumPy settings\n",
        "np.set_printoptions(precision=4)\n",
        "\n",
        "# Pandas settings\n",
        "pd.options.display.max_rows = 1_000\n",
        "pd.options.display.max_columns = 1_000\n",
        "pd.options.display.max_colwidth = 600\n",
        "\n",
        "# Polars settings\n",
        "pl.Config.set_fmt_str_lengths(1_000)\n",
        "pl.Config.set_tbl_cols(n=1_000)\n",
        "pl.Config.set_tbl_rows(n=200)\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# # Black code formatter (Optional)\n",
        "# %load_ext lab_black\n",
        "\n",
        "# # auto reload imports\n",
        "# %load_ext autoreload\n",
        "# %autoreload 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxrJSJiAA4ch"
      },
      "source": [
        "## Sequential Data\n",
        "\n",
        "- `Sequential data` refers to data that is ordered in a particular sequence or time series.\n",
        "- Each data point in a sequential dataset is `dependent` on the `previous data points`, and the `order` of the data is crucial for understanding the information it contains.\n",
        "- Time series data is sequential data where the order of examples is determined by time. Examples include stock prices and audio recordings.\n",
        "- Not all sequential data is time series. e.g. Text and DNA sequences are ordered but not time-based.\n",
        "\n",
        "<hr>\n",
        "\n",
        "## Recurrent Neural Networks (RNNs)\n",
        "\n",
        "- `RNN` is a type of neural network designed to process sequential data.\n",
        "- Unlike feedforward neural networks that process data in a single pass, RNNs can process data across multiple time steps, making them ideal for tasks involving sequences like text, speech, and time series data.\n",
        "\n",
        "### Key characteristics of RNNs\n",
        "\n",
        "- **Sequential Processing**: RNNs process data sequentially, allowing them to consider the context of previous inputs when making predictions.\n",
        "- **Internal Memory**: RNNs have an internal memory, often represented as a hidden state, that stores information about past inputs. This memory allows the network to maintain context and make predictions based on the entire sequence.\n",
        "- **Feedback Loop**: RNNs have a feedback loop that connects the output of a layer back to its input, creating a cyclic structure. This allows the network to learn long-term dependencies and capture patterns in sequential data.\n",
        "\n",
        "[![image.png](https://i.postimg.cc/3wSPP3Qn/image.png)](https://postimg.cc/cK393yHn)\n",
        "\n",
        "<br>\n",
        "\n",
        "[![image.png](https://i.postimg.cc/dtPzMWgP/image.png)](https://postimg.cc/Dm6CLc2B)\n",
        "\n",
        "### Common Types of Sequencing Tasks\n",
        "\n",
        "- **Many-to-one**: Input is a sequence, output is a fixed-size vector or scalar. Example: Sentiment analysis.\n",
        "- **One-to-many**: Input is standard, output is a sequence. e.g. in image captioning, a single image is given as input to a model, which then produces a sequence of words to describe the image (image caption).\n",
        "- **Many-to-many**: Both input and output are sequences. Can be further divided based on alignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0bjrodQjA4ch"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zbzFIViA4ci"
      },
      "source": [
        "### Hidden Recurrence vs Output Recurrence\n",
        "\n",
        "- **Hidden recurrence**: The recurrent connection is from the hidden layer to itself.\n",
        "- **Output recurrence**: The recurrent connection is from the output layer to either the hidden layer or itself.\n",
        "\n",
        "#### Two Types of Output Recurrence\n",
        "\n",
        "- **Output-to-hidden**: Connection from output layer at previous time step to hidden layer at current time step.\n",
        "- **Output-to-output**: Connection from output layer at previous time step to output layer at current time step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t5Mj1CJoA4ci"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vaPYxYgtA4ci"
      },
      "source": [
        "### Load Data\n",
        "\n",
        "#### Dependencies\n",
        "\n",
        "```sh\n",
        "pip install torchtext\n",
        "pip install torchdata\n",
        "pip install portalocker\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "V19vOUaUA4ci"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data.dataset import random_split\n",
        "\n",
        "# from torchtext.datasets import IMDB\n",
        "\n",
        "from torch.utils.data.dataset import Subset\n",
        "\n",
        "\n",
        "if not torch.cuda.is_available():\n",
        "    print(\"[Warning]: This code may be very slow on CPU\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "LErFBly7A4cj"
      },
      "outputs": [],
      "source": [
        "def tokenizer(text: str) -> list[str]:\n",
        "    \"\"\"\n",
        "    Tokenize the input text by removing HTML tags, extracting emoticons,\n",
        "    and splitting the text into individual tokens.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    text : str\n",
        "        The input text to be tokenized.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    list[str]\n",
        "        A list of tokens extracted from the input text.\n",
        "    \"\"\"\n",
        "    text = re.sub(\"<[^>]*>\", \"\", text)\n",
        "    emoticons: list[str] = re.findall(\"(?::|;|=)(?:-)?(?:\\)|\\(|D|P)\", text.lower())\n",
        "    text = re.sub(\"[\\W]+\", \" \", text.lower()) + \" \".join(emoticons).replace(\"-\", \"\")\n",
        "    tokenized: list[str] = text.split()\n",
        "    return tokenized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FHZcnqntA4cj"
      },
      "outputs": [],
      "source": [
        "# # Load data\n",
        "# train_dataset: list[tuple[int, str]] = list(IMDB(split=\"train\"))\n",
        "# test_dataset: list[tuple[int, str]] = list(IMDB(split=\"test\"))\n",
        "\n",
        "\n",
        "# print(f\"Train dataset length: {len(train_dataset):,}\")\n",
        "# print(f\"Test dataset length: {len(test_dataset):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAbe8mL0A4cj"
      },
      "source": [
        "### Save Data To Disk\n",
        "\n",
        "- I'm unable to use `torchtext` on colab so I have to save the data locally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qamoFAMhA4cj"
      },
      "outputs": [],
      "source": [
        "# # Convert the datasets to a JSON-serializable format\n",
        "# train_dataset_json: list[dict[str, Any]] = [\n",
        "#     {\"label\": label, \"text\": text} for label, text in train_dataset\n",
        "# ]\n",
        "# test_dataset_json: list[dict[str, Any]] = [\n",
        "#     {\"label\": label, \"text\": text} for label, text in test_dataset\n",
        "# ]\n",
        "\n",
        "# # Save the datasets to disk\n",
        "# sp: str = \"../../data/IMDB\"\n",
        "\n",
        "# with open(f\"{sp}/train_dataset.json\", \"w\") as train_file:\n",
        "#     json.dump(train_dataset_json, train_file)\n",
        "\n",
        "# with open(f\"{sp}/test_dataset.json\", \"w\") as test_file:\n",
        "#     json.dump(test_dataset_json, test_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4_Wy-wLBRM-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWuQyng0BSCr",
        "outputId": "8b8518da-f996-4893-d871-fc85fca5fdfc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dotZHHpTA4ck",
        "outputId": "fe6f6f20-50f5-481c-8a56-5dcfdf0bd98f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train dataset length: 25,000\n",
            "Test dataset length: 25,000\n"
          ]
        }
      ],
      "source": [
        "# Load the datasets from disk\n",
        "sp: str = \"/content/drive/MyDrive/My doc/Deep Learning/Data/IMDB\"\n",
        "\n",
        "with open(f\"{sp}/train_dataset.json\", \"r\") as train_file:\n",
        "    train_dataset_json = json.load(train_file)\n",
        "\n",
        "with open(f\"{sp}/test_dataset.json\", \"r\") as test_file:\n",
        "    test_dataset_json = json.load(test_file)\n",
        "\n",
        "# Convert the JSON-formatted datasets back to the original format\n",
        "train_dataset: list[tuple[int, str]] = [\n",
        "    (item[\"label\"], item[\"text\"]) for item in train_dataset_json\n",
        "]\n",
        "test_dataset: list[tuple[int, str]] = [\n",
        "    (item[\"label\"], item[\"text\"]) for item in test_dataset_json\n",
        "]\n",
        "\n",
        "# Verify the loaded datasets\n",
        "print(f\"Train dataset length: {len(train_dataset):,}\")\n",
        "print(f\"Test dataset length: {len(test_dataset):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8TQnY__YA4ck",
        "outputId": "80132f3f-cc86-41de-9d20-1c7c7ff43f18"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(1,\n",
              "  'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.'),\n",
              " (1,\n",
              "  '\"I Am Curious: Yellow\" is a risible and pretentious steaming pile. It doesn\\'t matter what one\\'s political views are because this film can hardly be taken seriously on any level. As for the claim that frontal male nudity is an automatic NC-17, that isn\\'t true. I\\'ve seen R-rated films with male nudity. Granted, they only offer some fleeting views, but where are the R-rated films with gaping vulvas and flapping labia? Nowhere, because they don\\'t exist. The same goes for those crappy cable shows: schlongs swinging in the breeze but not a clitoris in sight. And those pretentious indie movies like The Brown Bunny, in which we\\'re treated to the site of Vincent Gallo\\'s throbbing johnson, but not a trace of pink visible on Chloe Sevigny. Before crying (or implying) \"double-standard\" in matters of nudity, the mentally obtuse should take into account one unavoidably obvious anatomical difference between men and women: there are no genitals on display when actresses appears nude, and the same cannot be said for a man. In fact, you generally won\\'t see female genitals in an American film in anything short of porn or explicit erotica. This alleged double-standard is less a double standard than an admittedly depressing ability to come to terms culturally with the insides of women\\'s bodies.')]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "train_dataset[:2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "aC125ADGA4ck"
      },
      "outputs": [],
      "source": [
        "# Split the dataset into train and validation\n",
        "torch.manual_seed(123)\n",
        "\n",
        "train_dataset: Subset\n",
        "valid_dataset: Subset\n",
        "\n",
        "\n",
        "train_dataset, valid_dataset = random_split(dataset=train_dataset, lengths=[0.85, 0.15])\n",
        "# train_dataset, valid_dataset = random_split(\n",
        "#     dataset=train_dataset, lengths=[20000, 5000]\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWWsMdJ3A4ck",
        "outputId": "8b369579-b7e6-4488-f83f-3599aa2408da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab-size: 71,011\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter, OrderedDict\n",
        "\n",
        "\n",
        "device: torch.device | str = torch.device(\n",
        "    \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        ")\n",
        "\n",
        "\n",
        "# Find unique tokens (words)\n",
        "token_counts: Counter = Counter()\n",
        "\n",
        "\n",
        "for label, line in train_dataset:\n",
        "    tokens = tokenizer(line)\n",
        "    token_counts.update(tokens)\n",
        "\n",
        "\n",
        "print(f\"Vocab-size: {len(token_counts):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7aNFLbPYA4cl",
        "outputId": "43685b2d-7617-4f31-d00c-59bd457c4bf3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'seeing': 1801, 'as': 39774, 'i': 74206}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "from itertools import islice\n",
        "\n",
        "\n",
        "dict(islice(token_counts.items(), 0, 3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "I9cmMqQ2A4cl"
      },
      "outputs": [],
      "source": [
        "from torchtext.vocab import vocab\n",
        "from torchtext import __version__ as torchtext_version\n",
        "from pkg_resources import parse_version\n",
        "\n",
        "\n",
        "def create_vocabulary(token_counts: dict[str, int]) -> vocab:\n",
        "    \"\"\"\n",
        "    Create a vocabulary from token counts.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    token_counts : dict[str, int]\n",
        "        A dictionary mapping tokens to their frequency counts.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    vocab\n",
        "        A vocabulary object with special tokens and default index set.\n",
        "\n",
        "    Notes\n",
        "    -----\n",
        "    The vocabulary is created by sorting tokens by frequency in descending order,\n",
        "    inserting special tokens '<pad>' and '<unk>', and setting a default index for\n",
        "    out-of-vocabulary tokens.\n",
        "    \"\"\"\n",
        "    # Sort tokens by frequency in descending order\n",
        "    sorted_tokens: list[tuple[str, int]] = sorted(\n",
        "        token_counts.items(), key=lambda x: x[1], reverse=True\n",
        "    )\n",
        "\n",
        "    # Create vocab object\n",
        "    vocabulary: vocab = vocab(OrderedDict(sorted_tokens))\n",
        "\n",
        "    # Insert special tokens\n",
        "    vocabulary.insert_token(\"<pad>\", 0)\n",
        "    vocabulary.insert_token(\"<unk>\", 1)\n",
        "\n",
        "    # Set default index for OOV tokens\n",
        "    vocabulary.set_default_index(1)\n",
        "\n",
        "    return vocabulary\n",
        "\n",
        "\n",
        "def encode_texts(text: str) -> list[int]:\n",
        "    \"\"\"\n",
        "    Encode a text string into a list of integer token indices.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    text : str\n",
        "        The input text to be encoded.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    list[int]\n",
        "        A list of integer token indices representing the encoded text.\n",
        "\n",
        "    Notes\n",
        "    -----\n",
        "    This function uses a global `vocabulary` object to map tokens to their\n",
        "    corresponding integer indices. The input text is first tokenized using\n",
        "    a global `tokenizer` function before being encoded.\n",
        "    \"\"\"\n",
        "    global vocabulary\n",
        "\n",
        "    enc_text: list[int] = [vocabulary[token] for token in tokenizer(text)]\n",
        "    return enc_text\n",
        "\n",
        "\n",
        "def encode_labels(label: int | str) -> float:\n",
        "    \"\"\"\n",
        "    Encode labels into binary values.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    label : int or str\n",
        "        The input label to be encoded. Can be either an integer or a string.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    float\n",
        "        The encoded label as a float value (0.0 or 1.0).\n",
        "\n",
        "    Notes\n",
        "    -----\n",
        "    For torchtext versions > 0.10:\n",
        "        - 1 represents a negative review\n",
        "        - 2 represents a positive review\n",
        "    For torchtext versions <= 0.10:\n",
        "        - \"neg\" represents a negative review\n",
        "        - \"pos\" represents a positive review\n",
        "    \"\"\"\n",
        "    # Transform labels into 0 or 1\n",
        "    if parse_version(torchtext_version) > parse_version(\"0.10\"):\n",
        "        # 1 ~ negative, 2 ~ positive review\n",
        "        enc_label: float = 1.0 if label == 2 else 0.0\n",
        "    else:\n",
        "        enc_label: float = 1.0 if label == \"pos\" else 0.0\n",
        "\n",
        "    return enc_label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQGwnPMvA4cl",
        "outputId": "9d7d7c30-779b-4a81-927d-93c905fdf046"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[11, 7, 35, 458, 1]\n",
            "[11, 7, 35, 458, 1]\n"
          ]
        }
      ],
      "source": [
        "# Assuming token_counts is defined elsewhere\n",
        "vocabulary: vocab = create_vocabulary(token_counts)\n",
        "\n",
        "# Test the vocabulary\n",
        "test_tokens: list[str] = [\"this\", \"is\", \"an\", \"example\", \"thisTokenDoesNotExist\"]\n",
        "print([vocabulary[token] for token in test_tokens])\n",
        "\n",
        "\n",
        "# Test the encode_texts\n",
        "print(encode_texts(\"this is an example thisTokenDoesNotExist\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "SjYy2RryA4cm"
      },
      "outputs": [],
      "source": [
        "# Define the functions for transformation\n",
        "def collate_fn(\n",
        "    batch: list[tuple[int, str]]\n",
        ") -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "    \"\"\"\n",
        "    Collate function for processing batches of data.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    batch : list[tuple[int, str]]\n",
        "        A list of tuples containing label (int) and text (str) pairs.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    tuple[torch.Tensor, torch.Tensor, torch.Tensor]\n",
        "        A tuple containing:\n",
        "        - padded_texts: torch.Tensor of shape (batch_size, max_seq_length)\n",
        "        - labels: torch.Tensor of shape (batch_size,)\n",
        "        - lengths: torch.Tensor of shape (batch_size,)\n",
        "    \"\"\"\n",
        "    labels: list[int] = []\n",
        "    texts: list[torch.Tensor] = []\n",
        "    lengths: list[int] = []\n",
        "\n",
        "    for _label, _text in batch:\n",
        "        labels.append(encode_labels(_label))\n",
        "        tok_text: torch.Tensor = torch.tensor(encode_texts(_text), dtype=torch.int64)\n",
        "        texts.append(tok_text)\n",
        "        lengths.append(tok_text.size(0))\n",
        "\n",
        "    # Convert to tensor\n",
        "    labels_tensor: torch.Tensor = torch.tensor(labels)\n",
        "    lengths_tensor: torch.Tensor = torch.tensor(lengths)\n",
        "    padded_texts: torch.Tensor = nn.utils.rnn.pad_sequence(\n",
        "        texts, batch_first=True, padding_value=0.0\n",
        "    )\n",
        "\n",
        "    return padded_texts.to(device), labels_tensor.to(device), lengths_tensor.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rk9CnkbPA4cm"
      },
      "source": [
        "#### Test The Collate Function\n",
        "\n",
        "- Test with a small batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3Jok9pjA4cm",
        "outputId": "bd8ab620-4112-460d-ad27-c027634973ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "encoded_batch = tensor([[ 318,   15,   10,  ...,    0,    0,    0],\n",
            "        [  48,   52,   51,  ...,  598,    2, 1591],\n",
            "        [4793, 8188,  127,  ...,    0,    0,    0],\n",
            "        [  15,    4, 9725,  ...,    0,    0,    0]], device='cuda:0')\n",
            "label_batch = tensor([0., 1., 0., 1.], device='cuda:0')\n",
            "length_batch = tensor([156, 864, 835, 541], device='cuda:0')\n",
            "encoded_batch.shape = torch.Size([4, 864])\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "dataloader = DataLoader(\n",
        "    train_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn\n",
        ")\n",
        "encoded_batch, label_batch, length_batch = next(iter(dataloader))\n",
        "print(f\"{encoded_batch = }\")\n",
        "print(f\"{label_batch = }\")\n",
        "print(f\"{length_batch = }\")\n",
        "print(f\"{encoded_batch.shape = }\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tDXYGHlA4cm"
      },
      "source": [
        "<br>\n",
        "\n",
        "#### Batch the Datasets\n",
        "\n",
        "- Create dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "nrxs9wO6A4cn"
      },
      "outputs": [],
      "source": [
        "batch_size: int = 32\n",
        "\n",
        "train_dl: DataLoader = DataLoader(\n",
        "    train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn\n",
        ")\n",
        "valid_dl: DataLoader = DataLoader(\n",
        "    valid_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn\n",
        ")\n",
        "test_dl: DataLoader = DataLoader(\n",
        "    test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1H-A13_A4cn"
      },
      "source": [
        "<br>\n",
        "\n",
        "### Create Embedding Layers for Sentence Encoding\n",
        "\n",
        "- input_dim: number of words, i.e. maximum integer index + 1.\n",
        "\n",
        "- output_dim:\n",
        "\n",
        "- input_length: the length of (padded) sequence\n",
        "\n",
        "<br>\n",
        "\n",
        "```text\n",
        "for example, 'This is an example' -> [0, 0, 0, 0, 0, 0, 3, 1, 8, 9]\n",
        "=> input_lenght is 10\n",
        "When calling the layer, takes integr values as input,\n",
        "the embedding layer convert each interger into float vector of size [output_dim]\n",
        "\n",
        "If input shape is [BATCH_SIZE], output shape will be [BATCH_SIZE, output_dim]\n",
        "If input shape is [BATCH_SIZE, 10], output shape will be [BATCH_SIZE, 10, output_dim]\n",
        "```\n",
        "\n",
        "### Convert The Text To Vectors\n",
        "\n",
        "[![image.png](https://i.postimg.cc/Y066d3ms/image.png)](https://postimg.cc/N202MRD6)\n",
        "\n",
        "<br>\n",
        "\n",
        "- There are two ways of converting the texts to vectors.\n",
        "\n",
        "1. **One-hot encoding**:\n",
        "  - Converts word indices into sparse vectors of zeros and ones.\n",
        "  - Vector size equals the vocabulary size (can be very large).\n",
        "  - Results in high-dimensional, sparse feature space, leading to the curse of dimensionality.\n",
        "\n",
        "2. **Embedding**:\n",
        "  - Maps each word to a fixed-size, real-valued vector.\n",
        "  - Embedding dimension is much smaller than the vocabulary size.\n",
        "  - Reduces feature space dimensionality, mitigating the curse of dimensionality.\n",
        "  - Learns salient word features through model optimization, improving efficiency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h3bnmMEnA4cn",
        "outputId": "dd1f420c-71a7-4c1b-f7b5-4c5b1024ca50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "output = tensor([[[-1.3074,  1.0545, -0.1409],\n",
            "         [-0.8768, -0.1416, -0.5148],\n",
            "         [-1.3336, -0.3185,  0.0877],\n",
            "         [-0.3443,  1.2392,  1.1727]],\n",
            "\n",
            "        [[-1.3336, -0.3185,  0.0877],\n",
            "         [ 0.5286, -0.7448,  0.5654],\n",
            "         [-0.8768, -0.1416, -0.5148],\n",
            "         [ 0.0000,  0.0000,  0.0000]]], grad_fn=<EmbeddingBackward0>)\n",
            "output.shape = torch.Size([2, 4, 3])\n"
          ]
        }
      ],
      "source": [
        "embedding = nn.Embedding(\n",
        "    num_embeddings=10,  # vocab size\n",
        "    embedding_dim=3,  # num of feats per token\n",
        "    padding_idx=0,\n",
        ")\n",
        "\n",
        "# a batch of 2 samples of 4 indices each\n",
        "text_encoded_input = torch.LongTensor([[1, 2, 4, 5], [4, 3, 2, 0]])\n",
        "output: torch.Tensor = embedding(text_encoded_input)\n",
        "print(f\"{output = }\")\n",
        "\n",
        "# batch_size, seq_len, embedding_dim\n",
        "print(f\"{output.shape = }\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avhbE8AUA4cn"
      },
      "source": [
        "<hr><br>\n",
        "\n",
        "### Building An RNN Model\n",
        "\n",
        "- **RNN layers**:\n",
        "  - nn.RNN(input_size, hidden_size, num_layers=1)\n",
        "  - nn.LSTM(..)\n",
        "  - nn.GRU(..)\n",
        "  - nn.RNN(input_size, hidden_size, num_layers=1, bidirectional=True)\n",
        "\n",
        "<br>\n",
        "\n",
        "#### Vanilla RNN\n",
        "\n",
        "- Simplest form of RNN.\n",
        "- Suffers from the vanishing gradient problem, making it difficult to learn long-term dependencies.\n",
        "\n",
        "#### LSTM (Long Short-Term Memory)\n",
        "\n",
        "- Introduces gates (input, forget, output) to control the flow of information, addressing the vanishing gradient problem.\n",
        "  - Input Gate: Controls how much of the new information should be added to the cell state.\n",
        "  - Forget Gate: Controls how much of the previous cell state should be retained.\n",
        "  - Output Gate: Controls how much of the current cell state should be output.\n",
        "  - Cell State: A memory unit that carries information across time steps.\n",
        "- Can learn long-term dependencies effectively.\n",
        "- More complex than vanilla RNN.\n",
        "\n",
        "#### GRU (Gated Recurrent Unit)\n",
        "\n",
        "- A simpler variant of LSTM with fewer gates (reset, update).\n",
        "- Offers similar performance to LSTM but is computationally less expensive.\n",
        "- Also addresses the vanishing gradient problem.\n",
        "\n",
        "<br>\n",
        "\n",
        "### PyTorch Syntax\n",
        "\n",
        "```py\n",
        "# Fully connected neural network with one hidden layer\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.rnn = nn.RNN(\n",
        "            input_size,\n",
        "            hidden_size,\n",
        "            num_layers=2,\n",
        "            batch_first=True,\n",
        "        )\n",
        "        # self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        # self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        _, hidden = self.rnn(x)\n",
        "        # Selects the hidden state from the last layer and the last time step.\n",
        "        out = hidden[-1, :, :]\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "# Usage\n",
        "model = RNN(64, 32)\n",
        "model(torch.randn(5, 3, 64))\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "RznIrJSEA4co"
      },
      "outputs": [],
      "source": [
        "class RNN(nn.Module):\n",
        "    \"\"\"\n",
        "    Recurrent Neural Network with one hidden layer.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    input_size : int\n",
        "        The number of expected features in the input x.\n",
        "    hidden_size : int\n",
        "        The number of features in the hidden state h.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    rnn : nn.RNN\n",
        "        The RNN layer.\n",
        "    fc : nn.Linear\n",
        "        The fully connected output layer.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_size: int, hidden_size: int):\n",
        "        super().__init__()\n",
        "        self.rnn = nn.RNN(\n",
        "            input_size,\n",
        "            hidden_size,\n",
        "            num_layers=2,\n",
        "            batch_first=True,\n",
        "        )\n",
        "        self.fc = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass of the RNN.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : torch.Tensor\n",
        "            Input tensor of shape (batch_size, seq_len, input_size).\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        torch.Tensor\n",
        "            Output tensor of shape (batch_size, 1).\n",
        "        \"\"\"\n",
        "        # hidden state: (num_layers, batch_size, hidden_size)\n",
        "        _, hidden = self.rnn(x)\n",
        "        out: torch.Tensor = hidden[-1, :, :]\n",
        "        out = self.fc(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VgDmcUGaA4co",
        "outputId": "9cb0de7c-a006-436e-aecf-00efc4f09049"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RNN(\n",
            "  (rnn): RNN(64, 32, num_layers=2, batch_first=True)\n",
            "  (fc): Linear(in_features=32, out_features=1, bias=True)\n",
            ")\n",
            "input_tensor.shape = torch.Size([5, 3, 64])\n",
            "\n",
            "tensor([[ 0.5008],\n",
            "        [ 0.1696],\n",
            "        [ 0.4209],\n",
            "        [ 0.1581],\n",
            "        [-0.0279]], grad_fn=<AddmmBackward0>)\n",
            "\n",
            "output.shape = torch.Size([5, 1])\n"
          ]
        }
      ],
      "source": [
        "model = RNN(input_size=64, hidden_size=32)\n",
        "\n",
        "print(model)\n",
        "# shape of hidden state: (num_layers, batch_size, hidden_size)\n",
        "# (2, 5, 32). num_layer=2 (from the model architecture)\n",
        "# hidden[-1, :, :] => (5, 32) (from the model architecture)\n",
        "# output shape: (5, 32) @ (32, 1) => (5, 1)\n",
        "input_tensor: torch.Tensor = torch.randn(5, 3, 64)\n",
        "print(f\"{input_tensor.shape = }\\n\")\n",
        "output: torch.Tensor = model(input_tensor)\n",
        "\n",
        "print(f\"{output}\\n\")\n",
        "print(f\"{output.shape = }\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2KDrJ9QiA4co"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUPPBPrwA4co"
      },
      "source": [
        "### Build An RNN Model For Sentiment Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "FF-BnfjoA4cp"
      },
      "outputs": [],
      "source": [
        "class RNNModelConfig(TypedDict):\n",
        "    vocab_size: int\n",
        "    embed_dim: int\n",
        "    rnn_hidden_size: int\n",
        "    fc_hidden_size: int\n",
        "\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(\n",
        "            num_embeddings=config[\"vocab_size\"],\n",
        "            embedding_dim=config[\"embed_dim\"],\n",
        "            padding_idx=0,\n",
        "        )\n",
        "        self.rnn = nn.LSTM(\n",
        "            input_size=config[\"embed_dim\"],\n",
        "            hidden_size=config[\"rnn_hidden_size\"],\n",
        "            batch_first=True,\n",
        "        )\n",
        "        self.fc1 = nn.Linear(config[\"rnn_hidden_size\"], config[\"fc_hidden_size\"])\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(config[\"fc_hidden_size\"], 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, text, lengths):\n",
        "        out = self.embedding(text)\n",
        "        out = nn.utils.rnn.pack_padded_sequence(\n",
        "            out, lengths.cpu().numpy(), enforce_sorted=False, batch_first=True\n",
        "        )\n",
        "        out, (hidden, cell) = self.rnn(out)\n",
        "        out = hidden[-1, :, :]\n",
        "        out = self.fc1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        out = self.sigmoid(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "# Updated!\n",
        "class RNN(nn.Module):\n",
        "    \"\"\"\n",
        "    Recurrent Neural Network (RNN) model for text classification.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    config : RNNModelConfig\n",
        "        Configuration object containing model parameters.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    embedding : nn.Embedding\n",
        "        Embedding layer for text input.\n",
        "    rnn : nn.LSTM\n",
        "        LSTM layer for sequence processing.\n",
        "    out_layer : nn.Sequential\n",
        "        Output layer for classification.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: RNNModelConfig) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding: nn.Embedding = nn.Embedding(\n",
        "            num_embeddings=config[\"vocab_size\"],\n",
        "            embedding_dim=config[\"embed_dim\"],\n",
        "            padding_idx=0,\n",
        "        )\n",
        "        self.rnn: nn.LSTM = nn.LSTM(\n",
        "            input_size=config[\"embed_dim\"],\n",
        "            hidden_size=config[\"rnn_hidden_size\"],\n",
        "            batch_first=True,\n",
        "        )\n",
        "        self.out_layer: nn.Sequential = nn.Sequential(\n",
        "            nn.Linear(config[\"rnn_hidden_size\"], config[\"fc_hidden_size\"]),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(config[\"fc_hidden_size\"], 1),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "    def forward(self, text: torch.Tensor, lengths: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass of the RNN model.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        text : torch.Tensor\n",
        "            Input tensor of shape (batch_size, seq_length).\n",
        "        lengths : torch.Tensor\n",
        "            Tensor of shape (batch_size,) containing the length of each sequence in the batch.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        torch.Tensor\n",
        "            Output tensor of shape (batch_size, 1) containing classification probabilities.\n",
        "        \"\"\"\n",
        "        out: torch.Tensor = self.embedding(text)\n",
        "        out: torch.nn.utils.rnn.PackedSequence = nn.utils.rnn.pack_padded_sequence(\n",
        "            out, lengths.cpu().numpy(), enforce_sorted=False, batch_first=True\n",
        "        )\n",
        "        out: tuple[\n",
        "            torch.nn.utils.rnn.PackedSequence, tuple[torch.Tensor, torch.Tensor]\n",
        "        ] = self.rnn(out)\n",
        "        out, (hidden, cell) = out\n",
        "        out: torch.Tensor = hidden[-1, :, :]\n",
        "        out: torch.Tensor = self.out_layer(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NkdnkLmKA4cp",
        "outputId": "7fd83dba-a7dc-4743-c730-8cee6095f37f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'vocab_size': 71013,\n",
              " 'embed_dim': 20,\n",
              " 'rnn_hidden_size': 64,\n",
              " 'fc_hidden_size': 64}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "model_config: RNNModelConfig = {\n",
        "    \"vocab_size\": len(vocabulary),\n",
        "    \"embed_dim\": 20,\n",
        "    \"rnn_hidden_size\": 64,\n",
        "    \"fc_hidden_size\": 64,\n",
        "}\n",
        "\n",
        "model_config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "sYTINPFCA4cp"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(1)\n",
        "\n",
        "# Initialize the model\n",
        "model = RNN(model_config)\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jcWgKDk2GW5g",
        "outputId": "26d59f5c-64c8-44f5-e05c-692579fc2883"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_tensor = tensor([[1301,   21, 1847,   16, 1723,   71]], device='cuda:0') | text_length = tensor([1], device='cuda:0')\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.5349]], device='cuda:0', grad_fn=<SigmoidBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "# Test the model\n",
        "text: str = \"Thank you Jesus for loving me\"\n",
        "input_text: list[int] = encode_texts(text)\n",
        "input_tensor: torch.Tensor = torch.tensor([input_text], dtype=torch.int64).to(device)\n",
        "text_length: torch.Tensor = torch.tensor([input_tensor.size(0)]).to(device)\n",
        "print(f\"{input_tensor = } | {text_length = }\\n\")\n",
        "\n",
        "model(input_tensor, text_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "9RVhIh5NA4cp"
      },
      "outputs": [],
      "source": [
        "def train_model(\n",
        "    dataloader: torch.utils.data.DataLoader, model: nn.Module, lr: float = 0.001\n",
        ") -> tuple[float, float]:\n",
        "    \"\"\"\n",
        "    Train the model using the provided dataloader.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    dataloader : torch.utils.data.DataLoader\n",
        "        The dataloader containing the training data.\n",
        "    model : nn.Module\n",
        "        The neural network model to be trained.\n",
        "    lr : float, optional\n",
        "        The learning rate for the optimizer (default is 0.001).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    tuple[float, float]\n",
        "        A tuple containing the average accuracy and average loss.\n",
        "    \"\"\"\n",
        "    loss_fn: nn.BCELoss = nn.BCELoss()\n",
        "    optimizer: torch.optim.Adam = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    model.train()\n",
        "    total_acc: float = 0\n",
        "    total_loss: float = 0\n",
        "\n",
        "    for text_batch, label_batch, lengths in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        pred: torch.Tensor = model(text_batch, lengths)[:, 0]\n",
        "        loss: torch.Tensor = loss_fn(pred, label_batch)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update metrics\n",
        "        total_acc += ((pred >= 0.5).float() == label_batch).float().sum().item()\n",
        "        total_loss += loss.item() * label_batch.size(0)\n",
        "\n",
        "    avg_acc: float = total_acc / len(dataloader.dataset)\n",
        "    avg_loss: float = total_loss / len(dataloader.dataset)\n",
        "\n",
        "    return (avg_acc, avg_loss)\n",
        "\n",
        "\n",
        "def evaluate_model(\n",
        "    dataloader: torch.utils.data.DataLoader, model: nn.Module\n",
        ") -> tuple[float, float]:\n",
        "    \"\"\"\n",
        "    Evaluate the model using the provided dataloader.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    dataloader : torch.utils.data.DataLoader\n",
        "        The dataloader containing the evaluation data.\n",
        "    model : nn.Module\n",
        "        The neural network model to be evaluated.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    tuple[float, float]\n",
        "        A tuple containing the average accuracy and average loss.\n",
        "    \"\"\"\n",
        "    loss_fn: nn.BCELoss = nn.BCELoss()\n",
        "\n",
        "    model.eval()\n",
        "    total_acc: float = 0\n",
        "    total_loss: float = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for text_batch, label_batch, lengths in dataloader:\n",
        "            pred: torch.Tensor = model(text_batch, lengths)[:, 0]\n",
        "            loss: torch.Tensor = loss_fn(pred, label_batch)\n",
        "            total_acc += ((pred >= 0.5).float() == label_batch).float().sum().item()\n",
        "            total_loss += loss.item() * label_batch.size(0)\n",
        "\n",
        "    avg_acc: float = total_acc / len(dataloader.dataset)\n",
        "    avg_loss: float = total_loss / len(dataloader.dataset)\n",
        "    return (avg_acc, avg_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4FuU4ruA4cq",
        "outputId": "38a25bed-e0e0-4fbf-8cfc-8fbe884ede2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | accuracy: 0.5990 | val_accuracy: 0.6864\n",
            "Epoch 2 | accuracy: 0.7187 | val_accuracy: 0.6816\n",
            "Epoch 3 | accuracy: 0.7805 | val_accuracy: 0.7963\n",
            "Epoch 4 | accuracy: 0.8403 | val_accuracy: 0.8293\n",
            "Epoch 5 | accuracy: 0.8710 | val_accuracy: 0.8512\n",
            "Epoch 6 | accuracy: 0.8960 | val_accuracy: 0.8643\n",
            "Epoch 7 | accuracy: 0.9094 | val_accuracy: 0.7643\n",
            "Epoch 8 | accuracy: 0.9272 | val_accuracy: 0.8808\n",
            "Epoch 9 | accuracy: 0.9229 | val_accuracy: 0.8483\n",
            "Epoch 10 | accuracy: 0.9448 | val_accuracy: 0.8805\n"
          ]
        }
      ],
      "source": [
        "num_epochs: int = 10\n",
        "\n",
        "torch.manual_seed(1)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    acc_train, loss_train = train_model(train_dl, model=model)\n",
        "    acc_valid, loss_valid = evaluate_model(valid_dl, model=model)\n",
        "    print(\n",
        "        f\"Epoch {epoch+1} | accuracy: {acc_train:.4f} | val_accuracy: {acc_valid:.4f}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hvJdwWPVA4cq",
        "outputId": "9caa0000-fa39-409c-e2fd-84071c7811cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test_accuracy: 0.8584\n"
          ]
        }
      ],
      "source": [
        "acc_test, _ = evaluate_model(test_dl, model=model)\n",
        "print(f\"test_accuracy: {acc_test:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKP6ndV7Mwjj"
      },
      "source": [
        "<br>\n",
        "\n",
        "#### Try Out Bidirectional Recurrent Layer\n",
        "\n",
        "- The bidirectional RNN layer makes `two passes` over each input sequence: a `forward` pass and a `reverse` or backward pass (note that this is not to be confused with the forward and backward passes in the context of backpropagation).\n",
        "\n",
        "- The resulting hidden states of these forward and backward passes are usually `concatenated` into a single hidden state.\n",
        "- Other merge modes include `summation`, `multiplication` (multiplying the results of the two passes), and `averaging` (taking the average of the two).\n",
        "\n",
        "- We can also try other types of recurrent layers, such as the regular RNN. However, as it turns out, a model built with regular recurrent layers won’t be able to reach a good predictive performance (even on the training data).\n",
        "  - For example, if you try replacing the bidirectional LSTM layer in the previous code with a `unidirectional nn.RNN` (instead of nn.LSTM) layer and train the model on full-length sequences, you may observe that the loss will not even decrease during training.\n",
        "  - The reason is that the sequences in this dataset are too long, so a model with an RNN layer cannot learn the long-term dependencies and may suffer from vanishing or exploding gradient problems."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "8QsbCGGgA4cr"
      },
      "outputs": [],
      "source": [
        "class RNNModelConfig(TypedDict):\n",
        "    vocab_size: int\n",
        "    embed_dim: int\n",
        "    rnn_hidden_size: int\n",
        "    fc_hidden_size: int\n",
        "    merge_strategy: Literal[\"avg\", \"concat\", \"mul\", \"sum\"]\n",
        "\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    \"\"\"\n",
        "    Recurrent Neural Network (RNN) model for text classification.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    config : RNNModelConfig\n",
        "        Configuration object containing model parameters.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    embedding : nn.Embedding\n",
        "        Embedding layer for text input.\n",
        "    rnn : nn.LSTM\n",
        "        LSTM layer for sequence processing.\n",
        "    out_layer : nn.Sequential\n",
        "        Output layer for classification.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: RNNModelConfig) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding: nn.Embedding = nn.Embedding(\n",
        "            num_embeddings=config[\"vocab_size\"],\n",
        "            embedding_dim=config[\"embed_dim\"],\n",
        "            padding_idx=0,\n",
        "        )\n",
        "        self.rnn: nn.LSTM = nn.LSTM(\n",
        "            input_size=config[\"embed_dim\"],\n",
        "            hidden_size=config[\"rnn_hidden_size\"],\n",
        "            batch_first=True,\n",
        "            bidirectional=True,  # NEW!\n",
        "        )\n",
        "        # In a bidirectional LSTM, setting bidirectional=True allows the LSTM to process\n",
        "        # the input sequence in both forward and backward directions, resulting in two\n",
        "        # sets of hidden states. For a bidirectional LSTM, the hidden state size is doubled\n",
        "        #  (rnn_hidden_size * 2) because it concatenates the forward and backward hidden states.\n",
        "        # Consequently, the input size to the output layer must also be rnn_hidden_size * 2 to\n",
        "        # accommodate the concatenated hidden states.\n",
        "        self.out_layer: nn.Sequential = nn.Sequential(\n",
        "            nn.Linear(config[\"rnn_hidden_size\"] * 2, config[\"fc_hidden_size\"]),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(config[\"fc_hidden_size\"], 1),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "        self.merge_strategy: str = config[\"merge_strategy\"]\n",
        "\n",
        "    def forward(self, text: torch.Tensor, lengths: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass of the RNN model.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        text : torch.Tensor\n",
        "            Input tensor of shape (batch_size, seq_length).\n",
        "        lengths : torch.Tensor\n",
        "            Tensor of shape (batch_size,) containing the length of each sequence in the batch.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        torch.Tensor\n",
        "            Output tensor of shape (batch_size, 1) containing classification probabilities.\n",
        "        \"\"\"\n",
        "        out: torch.Tensor = self.embedding(text)\n",
        "        out: torch.nn.utils.rnn.PackedSequence = nn.utils.rnn.pack_padded_sequence(\n",
        "            out, lengths.cpu().numpy(), enforce_sorted=False, batch_first=True\n",
        "        )\n",
        "        _: torch.Tensor\n",
        "        hidden: torch.Tensor\n",
        "        _, (hidden, _) = self.rnn(out)\n",
        "        if self.merge_strategy == \"avg\":\n",
        "            out = (hidden[-2, :, :] + hidden[-1, :, :]) / 2\n",
        "        elif self.merge_strategy == \"concat\":\n",
        "            out = torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1)\n",
        "        elif self.merge_strategy == \"mul\":\n",
        "            out = torch.mul(hidden[-2, :, :], hidden[-1, :, :])\n",
        "        elif self.merge_strategy == \"sum\":\n",
        "            out = torch.add(hidden[-2, :, :], hidden[-1, :, :])\n",
        "        else:\n",
        "            raise ValueError(\"merge_strategy must be one of 'avg', 'concat', 'mul', 'sum'\")\n",
        "\n",
        "        out = self.out_layer(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "CIAxNTKtilER",
        "outputId": "176ea427-299b-4ebe-8b83-33c943e05167",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'vocab_size': 71013,\n",
              " 'embed_dim': 20,\n",
              " 'rnn_hidden_size': 64,\n",
              " 'fc_hidden_size': 64,\n",
              " 'merge_strategy': 'concat'}"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "model_config: RNNModelConfig = {\n",
        "    \"vocab_size\": len(vocabulary),\n",
        "    \"embed_dim\": 20,\n",
        "    \"rnn_hidden_size\": 64,\n",
        "    \"fc_hidden_size\": 64,\n",
        "    \"merge_strategy\": \"concat\",\n",
        "}\n",
        "\n",
        "model_config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dqG83O9rA4cr",
        "outputId": "d325b79f-2bcb-4fbf-8fb4-4001ab41db3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | accuracy: 0.6166 | val_accuracy: 0.7203\n",
            "Epoch 2 | accuracy: 0.7620 | val_accuracy: 0.5272\n",
            "Epoch 3 | accuracy: 0.8078 | val_accuracy: 0.8117\n",
            "Epoch 4 | accuracy: 0.8624 | val_accuracy: 0.8379\n",
            "Epoch 5 | accuracy: 0.8937 | val_accuracy: 0.8531\n",
            "Epoch 6 | accuracy: 0.9106 | val_accuracy: 0.8429\n",
            "Epoch 7 | accuracy: 0.9240 | val_accuracy: 0.8669\n",
            "Epoch 8 | accuracy: 0.9414 | val_accuracy: 0.8760\n",
            "Epoch 9 | accuracy: 0.9534 | val_accuracy: 0.8819\n",
            "Epoch 10 | accuracy: 0.9656 | val_accuracy: 0.8901\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(1)\n",
        "\n",
        "# Initialize the model\n",
        "model = RNN(model_config)\n",
        "model = model.to(device)\n",
        "\n",
        "\n",
        "num_epochs: int = 10\n",
        "\n",
        "torch.manual_seed(1)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    acc_train, loss_train = train_model(train_dl, model=model)\n",
        "    acc_valid, loss_valid = evaluate_model(valid_dl, model=model)\n",
        "    print(\n",
        "        f\"Epoch {epoch+1} | accuracy: {acc_train:.4f} | val_accuracy: {acc_valid:.4f}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vnyqg1tgA4cr",
        "outputId": "1ab03922-1807-4466-9ad5-a27f971c3c3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test_accuracy: 0.8614\n"
          ]
        }
      ],
      "source": [
        "acc_test, _ = evaluate_model(test_dl, model=model)\n",
        "print(f\"test_accuracy: {acc_test:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YfvvBHSvWxTm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0HcRs9iMNa3C"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}