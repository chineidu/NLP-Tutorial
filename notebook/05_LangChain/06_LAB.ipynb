{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab\n",
    "\n",
    "```text\n",
    "Retriever Augmented Generation\n",
    "------------------------------\n",
    "- Create a chatbot using RAG (Retriever Augmented Generation) or Generative Question Answering (GQA)\n",
    "- RAG is a type of language generation model that combines pre-trained parametric and non-parametric (source) memory for language generation.\n",
    "- In this case, source knowledge is used to update the knowledge of the LLM.\n",
    "- Examples of source knowledge include data from external sources like PDFs, URLs, CSVs, etc.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in library\n",
    "import re\n",
    "import json\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "# Standard imports\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# pandas settings\n",
    "pd.options.display.max_rows = 1_000\n",
    "pd.options.display.max_columns = 1_000\n",
    "pd.options.display.max_colwidth = 600\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Black code formatter (Optional)\n",
    "%load_ext lab_black\n",
    "# auto reload imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())  # read local .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = \"../../data/res.json\"\n",
    "\n",
    "with open(fp, \"r\") as f:\n",
    "    json_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(json_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retriever Augmented Generation\n",
    "\n",
    "- [Example notebook](https://github.com/pinecone-io/examples/blob/master/generation/langchain/handbook/05-langchain-retrieval-augmentation.ipynb)\n",
    "\n",
    "<br>\n",
    "\n",
    "[![image.png](https://i.postimg.cc/7PWxf0dh/image.png)](https://postimg.cc/YjQcPGpB)\n",
    "\n",
    "<br>\n",
    "\n",
    "```text\n",
    "1. Using the embedding model create vector embeddings for the context to be indexed.\n",
    "2. Insert the veector embeddings into the vector DB with some reference to the original context the embeddings were created from.\n",
    "3. When a query is made using the application, embed the query using the same embedding model and query the vector DB for similar vector embeddings.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Encoding 'cl100k_base'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "\n",
    "# Get the tokenizer name\n",
    "model_name = \"gpt-3.5-turbo\"\n",
    "encoding_name = tiktoken.encoding_for_model(model_name=model_name)\n",
    "encoding_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "\n",
    "def count_tokens(*, chain: Any, query: str) -> Any:\n",
    "    \"\"\"This is used to count the number of tokens sent to the LLM.\"\"\"\n",
    "    with get_openai_callback() as cb:\n",
    "        result = chain.run(query)\n",
    "        print(f\"Spent a total of {cb.total_tokens!r} tokens\")\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def calculate_number_of_tokens(*, text: str, model_name: str = \"gpt-3.5-turbo\") -> int:\n",
    "    \"\"\"This is usd to count the number of tokens.\"\"\"\n",
    "    import tiktoken\n",
    "\n",
    "    encoding_name = tiktoken.encoding_for_model(model_name=model_name)\n",
    "    encoding_name = re.compile(pattern=r\"(\\<|\\>|[\\'\\\"\\s]|Encoding)\").sub(\n",
    "        repl=\"\", string=str(encoding_name)\n",
    "    )\n",
    "    tokenizer = tiktoken.get_encoding(encoding_name)\n",
    "    tokens = tokenizer.encode(text, disallowed_special=())\n",
    "    return len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = (\n",
    "    \"hello I am a chunk of text and using the calculate_number_of_tokens function \"\n",
    "    \"we can find the length of this chunk of text in tokens\"\n",
    ")\n",
    "calculate_number_of_tokens(text=text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps\n",
    "\n",
    "```text\n",
    "- Load data\n",
    "- Split data\n",
    "  * To improve the accuracy of the embeddings.\n",
    "  * To improve the efficiency of the embeddings. Embedding a large piece of text can be computationally expensive. By splitting the text into smaller chunks, we can reduce the amount of computation required to embed the text.\n",
    "  * To make the embeddings more interpretable. \n",
    "\n",
    "- Embed the data. i.e. convert to numerical representations w/o losing info about the data.\n",
    "- Store the embeddings in a vector store.\n",
    "- Build the chatbot.\n",
    "\n",
    "Using the chatbot\n",
    "-----------------\n",
    "- Send a query.\n",
    "- Embed the query using the same embedding model.\n",
    "- Compare the embeddings of the query with the embeddings of the original source document.\n",
    "- Retrieve the docs that are similar to the query.\n",
    "- Summarize the retrieved doc using an LLM.\n",
    "- Return the summarized result as the query result.\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "```sh\n",
    "pip install jq\n",
    "pip install tiktoken\n",
    "```\n",
    "\n",
    "<br><hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Load Source Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1425"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import JSONLoader\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import (\n",
    "    CharacterTextSplitter,\n",
    "    RecursiveCharacterTextSplitter,\n",
    ")\n",
    "\n",
    "\n",
    "# Load JSON data\n",
    "loader = JSONLoader(\n",
    "    file_path=fp,\n",
    "    jq_schema=\".data\",\n",
    "    text_content=False,\n",
    ")\n",
    "\n",
    "data = loader.load()\n",
    "\n",
    "# Embedding model\n",
    "embedding = OpenAIEmbeddings()\n",
    "persist_directory = \"../../data/doc_db\"\n",
    "\n",
    "\n",
    "# Count the number tokens\n",
    "# LLMs struggle when the number of tokens are too many\n",
    "calculate_number_of_tokens(text=data[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Preprocess\n",
    "\n",
    "```text\n",
    "- Split the data into chunks (docs).\n",
    "- Embed the docs.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CHUNK_SIZE = 2_000\n",
    "CHUNK_OVERLAP = 50\n",
    "\n",
    "# Split the doc(s)\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    "    separators=[\"\\n\\n\", \"\\n\", r\"\\{\\}\", \"\"],\n",
    ")\n",
    "splits = text_splitter.split_documents(documents=data)\n",
    "len(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run once!\n",
    "# Create and save vectorstore to disk\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embedding,\n",
    "    persist_directory=persist_directory,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "vectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding)\n",
    "\n",
    "# Number of docs\n",
    "print(vectordb._collection.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Create AI Bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import (\n",
    "    ConversationBufferWindowMemory,\n",
    "    ConversationTokenBufferMemory,\n",
    ")\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "#Init LLM\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "# Create memory\n",
    "memory = ConversationBufferWindowMemory(\n",
    "    k=5,  # window size\n",
    "    input_key=\"question\",\n",
    "    memory_key=\"chat_history\",\n",
    ")\n",
    "retriever = vectordb.as_retriever()\n",
    "\n",
    "# Init Chatbot\n",
    "decide_bot = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,  # retrieve data from data source\n",
    "    verbose=False,\n",
    "    chain_type_kwargs={\n",
    "        \"document_separator\": \"<<<<>>>>>\",\n",
    "        \"memory\": memory,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Send Queris To The Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 1521 tokens\n",
      "'The maximum EMI eligibility of the customer is 2898.'\n"
     ]
    }
   ],
   "source": [
    "question = \"What is the maxEMIEligibility of the customer?\"\n",
    "result = count_tokens(chain=decide_bot, query={\"query\": question})\n",
    "\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'What is the average monthly debit?',\n",
      " 'result': 'The average monthly debit is as follows:\\n'\n",
      "           '\\n'\n",
      "           '- February: ₦3,065.48\\n'\n",
      "           '- March: ₦1,599.84\\n'\n",
      "           '- April: ₦5,705.34\\n'\n",
      "           '- May: ₦5,401.95\\n'\n",
      "           '\\n'\n",
      "           'Please note that these values are in Nigerian Naira (NGN).'}\n"
     ]
    }
   ],
   "source": [
    "question = \"What is the average monthly debit?\"\n",
    "result = decide_bot({\"query\": question})\n",
    "\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'What is the averageMonthlyDebit?',\n",
      " 'result': 'The averageMonthlyDebit is [{\"month\": \"2022-04\", \"amount\": '\n",
      "           '5705.34}, {\"month\": \"2022-05\", \"amount\": 5401.95}, {\"month\": '\n",
      "           '\"2022-02\", \"amount\": 383.19}, {\"month\": \"2022-03\", \"amount\": '\n",
      "           '1599.84}].'}\n"
     ]
    }
   ],
   "source": [
    "# It's better to use the exact variable name(s)\n",
    "question = \"What is the averageMonthlyDebit?\"\n",
    "result = decide_bot({\"query\": question})\n",
    "\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'What is the mean of the averageMonthlyCredit in the data?',\n",
      " 'result': 'The mean of the averageMonthlyCredit in the data is 8,070.71.'}\n"
     ]
    }
   ],
   "source": [
    "# It's better to use the exact variable name(s)\n",
    "question = \"What is the mean of the averageMonthlyCredit in the data?\"\n",
    "result = decide_bot({\"query\": question})\n",
    "\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'What is the customer ID and is the customer a salary earner?',\n",
      " 'result': 'The customer ID is not provided in the given context. However, '\n",
      "           'based on the information provided, the customer is a salary '\n",
      "           'earner.'}\n"
     ]
    }
   ],
   "source": [
    "question = \"What is the customer ID and is the customer a salary earner?\"\n",
    "result = decide_bot({\"query\": question})\n",
    "\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'Are there any recurringExpenses in the transaction data? If yes, '\n",
      "          'list them',\n",
      " 'result': 'Yes, there are recurring expenses in the transaction data. The '\n",
      "           'recurring expenses are as follows:\\n'\n",
      "           '\\n'\n",
      "           '1. Amount: 10026.88, Description: NEXTGENUSR TRF/Payment of '\n",
      "           'money/FRM FAITH OLUWASEYI OLODU TO AMEENAH OYINKANSOLA OYESOJI- '\n",
      "           '033\\n'\n",
      "           '2. Amount: 2010.75, Description: NEXTGENUSR TRF/From Benson/FRM '\n",
      "           'FAITH OLUWASEYI OLODU TO CHINENYE JENNIFER MADU- 057\\n'\n",
      "           '3. Amount: 2500, Description: FLEXSWITCH WT|KENNETH UGOCHI UNACHI '\n",
      "           'ABAKALIKI NG'}\n"
     ]
    }
   ],
   "source": [
    "question = \"Are there any recurringExpenses in the transaction data? If yes, list them\"\n",
    "result = decide_bot({\"query\": question})\n",
    "\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'Which month had the highest totalMonthlyDebit and what was the '\n",
      "          'amount?',\n",
      " 'result': 'The month with the highest totalMonthlyDebit was April, and the '\n",
      "           'amount was 114106.87 NGN.'}\n"
     ]
    }
   ],
   "source": [
    "question = \"Which month had the highest totalMonthlyDebit and what was the amount?\"\n",
    "result = decide_bot({\"query\": question})\n",
    "\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'chat_history': 'Human: What is the average monthly debit?\\n'\n",
      "                 'AI: The average monthly debit is as follows:\\n'\n",
      "                 '\\n'\n",
      "                 '- February: ₦3,065.48\\n'\n",
      "                 '- March: ₦1,599.84\\n'\n",
      "                 '- April: ₦5,705.34\\n'\n",
      "                 '- May: ₦5,401.95\\n'\n",
      "                 '\\n'\n",
      "                 'Please note that these values are in Nigerian Naira (NGN).\\n'\n",
      "                 'Human: What is the averageMonthlyDebit?\\n'\n",
      "                 'AI: The averageMonthlyDebit is [{\"month\": \"2022-04\", '\n",
      "                 '\"amount\": 5705.34}, {\"month\": \"2022-05\", \"amount\": 5401.95}, '\n",
      "                 '{\"month\": \"2022-02\", \"amount\": 383.19}, {\"month\": \"2022-03\", '\n",
      "                 '\"amount\": 1599.84}].\\n'\n",
      "                 'Human: What is the customer ID and is the customer a salary '\n",
      "                 'earner?\\n'\n",
      "                 'AI: The customer ID is not provided in the given context. '\n",
      "                 'However, based on the information provided, the customer is '\n",
      "                 'a salary earner.\\n'\n",
      "                 'Human: Are there any recurringExpenses in the transaction '\n",
      "                 'data? If yes, list them\\n'\n",
      "                 'AI: Yes, there are recurring expenses in the transaction '\n",
      "                 'data. The recurring expenses are as follows:\\n'\n",
      "                 '\\n'\n",
      "                 '1. Amount: 10026.88, Description: NEXTGENUSR TRF/Payment of '\n",
      "                 'money/FRM FAITH OLUWASEYI OLODU TO AMEENAH OYINKANSOLA '\n",
      "                 'OYESOJI- 033\\n'\n",
      "                 '2. Amount: 2010.75, Description: NEXTGENUSR TRF/From '\n",
      "                 'Benson/FRM FAITH OLUWASEYI OLODU TO CHINENYE JENNIFER MADU- '\n",
      "                 '057\\n'\n",
      "                 '3. Amount: 2500, Description: FLEXSWITCH WT|KENNETH UGOCHI '\n",
      "                 'UNACHI ABAKALIKI NG\\n'\n",
      "                 'Human: Which month had the highest totalMonthlyDebit and '\n",
      "                 'what was the amount?\\n'\n",
      "                 'AI: The month with the highest totalMonthlyDebit was April, '\n",
      "                 'and the amount was 114106.87 NGN.'}\n"
     ]
    }
   ],
   "source": [
    "# Most recent chat history.\n",
    "pprint(memory.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "```text\n",
    "1. Data source\n",
    "\n",
    "2. Vector store/database\n",
    "\n",
    "3. Create an API and expose the endpoints\n",
    "   - Upload source data\n",
    "   - Create and use chatbot\n",
    "     - Preprocess the source data\n",
    "     - Send queries to the bot\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedback\n",
    "\n",
    "- Break it down into pieces.\n",
    "  - Start with a specific use case.\n",
    "\n",
    "\n",
    "- MVP and 1st use case\n",
    "  - MVP that allows first time users to interact with the Decide UI easily.\n",
    "  - How can conversational chatbots be useful to the users?\n",
    "  - Interpretability of the Decide variables. e.g. what is the gamblingRate and what insights can gotten from the variable. \n",
    "  - Users should be able to interact with Decide docs using the chatbot.\n",
    "- What is the best way for the LLM to retrieve the info? text, json, csv??\n",
    "  - Transform the source data?\n",
    "- How do you improve the performance the chatbot? \n",
    "  - Latency\n",
    "  - reduce hallucination\n",
    "  - reduce token usage\n",
    "\n",
    "- Use cases:\n",
    "  1. Connect to the Decide Docs.\n",
    "     1. What are the Decide variables?\n",
    "     2. How are they calculated?\n",
    "     3. What is a good value for this variable?\n",
    "  2. Add validation to the query response using an LLM to make the response very succint. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "import click\n",
    "from langchain.document_loaders import JSONLoader\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class DecideBot(BaseModel):\n",
    "    \"\"\"This is used to create the Decide AI Assistant.\"\"\"\n",
    "\n",
    "    # Constants\n",
    "    PERSIST_DIRECTORY: str = \"./data/doc_db\"\n",
    "    CHUNK_SIZE: int = 1_000\n",
    "    CHUNK_OVERLAP: int = 50\n",
    "    TEMPERATURE: float = 0.0\n",
    "\n",
    "    # Variables\n",
    "    filepath: str\n",
    "\n",
    "    def _load_data(self) -> List[Any]:\n",
    "        \"\"\"This is used to load the JSON data.\"\"\"\n",
    "        loader = JSONLoader(\n",
    "            file_path=self.filepath,\n",
    "            jq_schema=\".data\",\n",
    "            text_content=False,\n",
    "        )\n",
    "        data = loader.load()\n",
    "        return data\n",
    "\n",
    "    def _preprocess_data(self) -> Chroma:\n",
    "        \"\"\"This is used to split and store the embedded data in a vector store.\"\"\"\n",
    "        data = self._load_data()\n",
    "\n",
    "        # Embedding model\n",
    "        embedding = OpenAIEmbeddings()\n",
    "\n",
    "        # Split the doc(s)\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=self.CHUNK_SIZE,\n",
    "            chunk_overlap=self.CHUNK_OVERLAP,\n",
    "            separators=[\"\\n\\n\", \"\\n\", r\"\\{\\}\", \"\"],\n",
    "        )\n",
    "        splits = text_splitter.split_documents(documents=data)\n",
    "\n",
    "        # Create and save vectors tore to disk\n",
    "        vector_db = Chroma.from_documents(\n",
    "            documents=splits,\n",
    "            embedding=embedding,\n",
    "            persist_directory=self.PERSIST_DIRECTORY,\n",
    "        )\n",
    "        return vector_db\n",
    "\n",
    "    # Create bot\n",
    "    def _create_RAG_model(self) -> RetrievalQA:\n",
    "        \"\"\"This is used to create a RAG model for question and answering.\"\"\"\n",
    "        llm = ChatOpenAI(temperature=self.TEMPERATURE)\n",
    "        # Create memory\n",
    "        memory = ConversationBufferWindowMemory(\n",
    "            k=5,  # window size\n",
    "            input_key=\"question\",\n",
    "            memory_key=\"chat_history\",\n",
    "        )\n",
    "        vector_db = self._preprocess_data()\n",
    "\n",
    "        # Init Chatbot\n",
    "        decide_bot = RetrievalQA.from_chain_type(\n",
    "            llm=llm,\n",
    "            chain_type=\"stuff\",\n",
    "            retriever=vector_db.as_retriever(),  # retrieve data from data source\n",
    "            verbose=False,\n",
    "            chain_type_kwargs={\n",
    "                \"document_separator\": \"<<<<>>>>>\",\n",
    "                \"memory\": memory,\n",
    "            },\n",
    "        )\n",
    "        return decide_bot\n",
    "\n",
    "    # Generate answers\n",
    "    def generate_response(self, *, question: str) -> Dict[str, Any]:\n",
    "        \"\"\"This is a chat bot used for question and answering.\"\"\"\n",
    "        decide_bot = self._create_RAG_model()\n",
    "        result = decide_bot({\"query\": question})\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_p310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
