{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "\n",
    "import spacy\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "# Built-in library\n",
    "import itertools\n",
    "import re\n",
    "import json\n",
    "from typing import Union, Optional, Any\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# pandas settings\n",
    "pd.options.display.max_rows = 1_000\n",
    "pd.options.display.max_columns = 1_000\n",
    "pd.options.display.max_colwidth = 600\n",
    "\n",
    "# Black code formatter (Optional)\n",
    "%load_ext lab_black\n",
    "# auto reload imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_data(*, filepath: Path, label: int) -> pd.DataFrame:\n",
    "    \"\"\"This loads the text data, assigns a label and returns a DF.\"\"\"\n",
    "    with open(filepath, \"r\") as file:\n",
    "        # Remove empty lines: using len(line) > 5\n",
    "        data = [(line.strip(), label) for line in file.readlines() if len(line) > 5]\n",
    "    # Convert to DF\n",
    "    df = pd.DataFrame(data=data, columns=[\"text\", \"label\"])\n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_punctuations(text: str) -> str:\n",
    "    import string\n",
    "\n",
    "    \"\"\"This returns the text without punctuations\"\"\"\n",
    "    cleaned_text = re.compile(pattern=f\"[{re.escape(string.punctuation)}]\").sub(\n",
    "        repl=\"\", string=str(text)\n",
    "    )\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LO! Death hath rear'd himself a throne</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In a strange city, all alone,</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Far down within the dim west</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Where the good, and the bad, and the worst, and the best,</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Have gone to their eternal rest.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        text  label\n",
       "0                     LO! Death hath rear'd himself a throne      0\n",
       "1                              In a strange city, all alone,      0\n",
       "2                               Far down within the dim west      0\n",
       "3  Where the good, and the bad, and the worst, and the best,      0\n",
       "4                           Have gone to their eternal rest.      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp = \"edgar_allan_poe.txt\"\n",
    "fp1 = \"robert_frost.txt\"\n",
    "label_0, label_1 = 0, 1\n",
    "\n",
    "edgar_allan_poe = extract_text_data(filepath=fp, label=label_0)\n",
    "robert_frost = extract_text_data(filepath=fp1, label=label_1)\n",
    "\n",
    "edgar_allan_poe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Two roads diverged in a yellow wood,</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>And sorry I could not travel both</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>And be one traveler, long I stood</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>And looked down one as far as I could</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>To where it bent in the undergrowth;</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    text  label\n",
       "0   Two roads diverged in a yellow wood,      1\n",
       "1      And sorry I could not travel both      1\n",
       "2      And be one traveler, long I stood      1\n",
       "3  And looked down one as far as I could      1\n",
       "4   To where it bent in the undergrowth;      1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "robert_frost.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1703</th>\n",
       "      <td>Not bluebells gracing a tunnel mouth</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1275</th>\n",
       "      <td>Of door and headboard Where it wants to get</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>Hath ever told or is it of a thought</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551</th>\n",
       "      <td>To the Lethean peace of the skies</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>Then desolately fall</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>662</th>\n",
       "      <td>That from new fountains overflow</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2055</th>\n",
       "      <td>Whats this</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611</th>\n",
       "      <td>Of its own fervour  what had oer it power</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1303</th>\n",
       "      <td>The way a man with one leg and a crutch</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1641</th>\n",
       "      <td>How shall we</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             text  label\n",
       "1703         Not bluebells gracing a tunnel mouth      1\n",
       "1275  Of door and headboard Where it wants to get      1\n",
       "294          Hath ever told or is it of a thought      0\n",
       "551             To the Lethean peace of the skies      0\n",
       "426                          Then desolately fall      0\n",
       "662              That from new fountains overflow      0\n",
       "2055                                   Whats this      1\n",
       "611     Of its own fervour  what had oer it power      0\n",
       "1303      The way a man with one leg and a crutch      1\n",
       "1641                                 How shall we      1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the data\n",
    "\n",
    "RANDOM_STATE = 2\n",
    "TEST_SIZE = 0.1\n",
    "\n",
    "data = pd.concat([edgar_allan_poe, robert_frost], axis=\"rows\").reset_index(drop=True)\n",
    "# Remove punctuations\n",
    "data = data.assign(text=data[\"text\"].apply(remove_punctuations))\n",
    "\n",
    "data.sample(n=10, random_state=RANDOM_STATE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split The Data Into Train And Validation Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1936,), (216,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = data[\"text\"]\n",
    "y = data[\"label\"]\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "X_train.shape, X_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "\n",
    "class BaseModel(ABC):\n",
    "    \"\"\"This is the blueprint used for building ML models.\"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def __repr__(self) -> str:\n",
    "        \"\"\"This is the string representation of the model\"\"\"\n",
    "        pass\n",
    "\n",
    "    def fit(\n",
    "        self, X: Union[np.ndarray, pd.DataFrame], y: Union[np.ndarray, pd.Series]\n",
    "    ) -> None:\n",
    "        \"\"\"This is used to train the model.\"\"\"\n",
    "        return self\n",
    "\n",
    "    def predict(self, X: Union[np.ndarray, pd.DataFrame]) -> None:\n",
    "        \"\"\"This is used to make predictions using new data.\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                     LO Death hath reard himself a throne\n",
       "1                              In a strange city all alone\n",
       "2                             Far down within the dim west\n",
       "3    Where the good and the bad and the worst and the best\n",
       "4                          Have gone to their eternal rest\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"text\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessText:\n",
    "    \"\"\"this is used to convert the corpus to a list of\n",
    "    tokenized documents.\"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.X = None\n",
    "        self._vocab = {}\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return (\n",
    "            f\"{__class__.__name__}(corpus_size: {len(self.X)}, \"\n",
    "            f\"vocab_size: {len(self._vocab)})\"\n",
    "        )\n",
    "\n",
    "    def fit(self, X: pd.Series) -> dict:\n",
    "        \"\"\"This is used to create the vocabulary of the corpus.\"\"\"\n",
    "        # Create a dict that'll be used to store unique words and their integer\n",
    "        # mappings as key-value pairs. Add a custom token `unk` with a value of 0\n",
    "        # which will be used for tokens that are not present in the training data.\n",
    "        vocab = {\"unk\": 0}\n",
    "        count = 1\n",
    "\n",
    "        for doc in X:\n",
    "            # Tokenize the document\n",
    "            tokenized_doc = [x.lower() for x in doc.split()]\n",
    "            for term in tokenized_doc:\n",
    "                if term not in vocab:\n",
    "                    vocab[term] = count\n",
    "                    count += 1\n",
    "\n",
    "        self.X, self._vocab = X, vocab\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: pd.Series) -> list[int]:\n",
    "        \"\"\"This is used to tokenize the documents.\n",
    "        It returns the tokenized documents as list of integers.\"\"\"\n",
    "        tokenized_documents = []\n",
    "        for doc in X:\n",
    "            # Tokenize the document\n",
    "            tokenized_doc = [x.lower() for x in doc.split()]\n",
    "            # Get the integer values of the tokens\n",
    "            sent_int = [self._vocab.get(term, 0) for term in tokenized_doc]\n",
    "            tokenized_documents.append(sent_int)\n",
    "\n",
    "        return tokenized_documents\n",
    "\n",
    "    def fit_transform(self, X: pd.Series) -> list[int]:\n",
    "        \"\"\"this is used to create the vocabulary and tokenize the corpus.\"\"\"\n",
    "        self.fit(X=X)\n",
    "        return self.transform(X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Nomial Naive Bayes\n",
    "\n",
    "* **Transition Matrix** $\\mathbf{A_{ij}}$: This is the probability of a state, `t`, given a previous state, `t-1`. This is a matrix (2-D array).\n",
    "  \n",
    "$$\n",
    "\\mathbf{A_{ij}} = p(s_{t} = j | s_{t-1} = i)\n",
    "$$\n",
    "\n",
    "* Predicted/Estimate Transition Matrix\n",
    "\n",
    "$$\n",
    "\\mathbf{\\hat{A}_{ij}} = \\frac{count(i \\rightarrow j)}{count(i)}\n",
    "$$\n",
    "\n",
    "* **Initial State Distribution** $\\mathbf{\\pi_{i}} $: This is the probability of an initial state in a sequence. This is a vector (1-D array).\n",
    "\n",
    "$$\n",
    "\\mathbf{\\pi_{i}} = p(s_{1} = i)\n",
    "$$\n",
    "\n",
    "* Estimated Initial State Distribution\n",
    "\n",
    "$$\n",
    "\\mathbf{\\hat{\\pi_{i}}} = \\frac{count(s_{1} = i)}{N}\n",
    "$$\n",
    "\n",
    "\n",
    "```text\n",
    "where\n",
    "N = Number of sequences.\n",
    "```\n",
    "\n",
    "$$\n",
    "\\mathbf{p(y|X)} = \\frac{p(X|y).p(y)}{p(X)}\n",
    "$$\n",
    "\n",
    "```text\n",
    "where\n",
    "p(y|X) = Posterior probability\n",
    "p(X|y) = Class conditional probability or likelihood\n",
    "p(y) = Prior probability of y\n",
    "p(X) = Marginal probability of X\n",
    "```\n",
    "\n",
    "* Since $p(X)$ does not depend on `y`, we can ignore it.\n",
    "\n",
    "$$\n",
    "\\mathbf{p(y|X)} = {p(X|y).p(y)}\n",
    "$$\n",
    "\n",
    "* If we have `n` features, it becomes:\n",
    "\n",
    "$$\n",
    "\\mathbf{p(y|X)} = {p(x_{1}|y).p(x_{2}|y)...p(x_{n}|y).p(y)}\n",
    "$$\n",
    "\n",
    "* Taking the log, we have:\n",
    "\n",
    "$$\n",
    "\\mathbf{p(y|X)} = {log(p(x_{1}|y))+log(p(x_{2}|y))+...+log(p(x_{n}|y))+log(p(y)})\n",
    "$$\n",
    "\n",
    "\n",
    "**Note**: $\\hat{A}_{ij}$ and $\\hat{\\pi_{i}}$ will be used to model the `class conditional probability`.\n",
    "\n",
    "\n",
    "<hr>\n",
    "\n",
    "## Steps Required To Build The Model From Scratch\n",
    "\n",
    "### Training:\n",
    "\n",
    "1. Determine the vocabulary and tokenize the documents.\n",
    "2. Initialize the parameters: $\\hat{A}_{ij}$, $\\hat{\\pi_{i}}$, and $p(y)$ for each class label. \n",
    "   * i.e we need to initialize two variables per parameters since we have two class labels. i.e. $\\hat{A}0_{ij}$,  $\\hat{A}1_{ij}$, etc.\n",
    "3. Compute the count of A and Pi. i.e A_hat and Pi_hat.\n",
    "4. Calculate the log probabilities of A_hat and Pi_hat.\n",
    "\n",
    "\n",
    "### Making Predictions\n",
    "\n",
    "$$\n",
    "\\mathbf{p(y|X)} = argmax({log(p(x_{1}|y))+log(p(x_{2}|y))+...+log(p(x_{n}|y))+log(p(y)}))\n",
    "$$\n",
    "\n",
    "1. Calculate the posteriors. i.e (log_likelihood + log_priors) for each class.\n",
    "2. Find `y` by calculating the `argmax` of the posteriors given the input over all classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiNomial_NB(BaseModel):\n",
    "    \"\"\"This classifier uses MArkov model for classifiication.\\n\n",
    "    It's trained using two models. i.e for 2 classes (labels).\n",
    "\n",
    "    Params:\n",
    "        vocab (dict): A dictionary containing the vocabulary.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *, vocab: dict) -> None:\n",
    "        self.vocab = vocab\n",
    "        self.transition_matrix = None\n",
    "        self.initial_state_distr = None\n",
    "        self.log_priors = [None, None]\n",
    "        self.K = [None, None]\n",
    "        self.priors = [None, None]\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        priors_dict = {\n",
    "            self.K[0]: round(self.priors[0], 2),\n",
    "            self.K[1]: round(self.priors[1], 2),\n",
    "        }\n",
    "        return (\n",
    "            f\"{__class__.__name__}(log_priors={self.log_priors[0], self.log_priors[1]}, \"\n",
    "            f\"priors={priors_dict})\"\n",
    "        )\n",
    "\n",
    "    def fit(\n",
    "        self, X: Union[np.ndarray, pd.DataFrame], y: Union[np.ndarray, pd.Series]\n",
    "    ) -> None:\n",
    "        # Since we have 2 class labels, we need to initialize and\n",
    "        # create 2 models. Compute count and log probs for each model.\n",
    "        # Retrieve the probs\n",
    "        A_0, Pi_0 = self._init_A_and_Pi()\n",
    "        A_1, Pi_1 = self._init_A_and_Pi()\n",
    "        k_0, k_1 = np.unique(y)\n",
    "        log_y0, log_y1 = self._log_priors(y=y)\n",
    "\n",
    "        # Model 0\n",
    "        input_0 = self._get_input(tokenized_doc=X, y=y, k_=k_0)\n",
    "        A_hat_0, Pi_hat_0 = self._count_state_transitions(\n",
    "            X=input_0, A_hat=A_0, Pi_hat=Pi_0\n",
    "        )\n",
    "        log_A_hat_0, log_Pi_hat_0 = self._convert_counts_to_log_prob(\n",
    "            X=input_0, A_hat=A_0, Pi_hat=Pi_0\n",
    "        )\n",
    "\n",
    "        # Model 1\n",
    "        input_1 = self._get_input(tokenized_doc=X, y=y, k_=k_1)\n",
    "        A_hat_1, Pi_hat_1 = self._count_state_transitions(\n",
    "            X=input_1, A_hat=A_1, Pi_hat=Pi_1\n",
    "        )\n",
    "        log_A_hat_1, log_Pi_hat_1 = self._convert_counts_to_log_prob(\n",
    "            X=input_1, A_hat=A_1, Pi_hat=Pi_1\n",
    "        )\n",
    "\n",
    "        self.transition_matrix = (log_A_hat_0, log_A_hat_1)\n",
    "        self.initial_state_distr = (log_Pi_hat_0, log_Pi_hat_1)\n",
    "        self.log_priors = (log_y0, log_y1)\n",
    "        self.K = k_0, k_1\n",
    "\n",
    "        return self\n",
    "\n",
    "    def _init_A_and_Pi(self) -> tuple:\n",
    "        \"\"\"This is used to initialize the state transition matrix\n",
    "        and the initial state distribution.\"\"\"\n",
    "        V = len(self.vocab)\n",
    "        # Add add-one smoothering\n",
    "        # A is a matrix and Pi is a vector\n",
    "        A = np.ones(shape=(V, V), dtype=float)\n",
    "        Pi = np.ones(shape=(V), dtype=float)\n",
    "        return (A, Pi)\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_input(*, tokenized_doc: list[int], y: np.ndarray, k_: int) -> list[int]:\n",
    "        \"\"\"This returns an input given a specific class label.\n",
    "        i.e the input given a specific class label.\n",
    "\n",
    "        Params:\n",
    "            tokenized_doc (list[int]): The tokenized documents (corpus).\n",
    "            y (np.ndarray): The labels for the data.\n",
    "            k_ (int): The class label.\n",
    "\n",
    "        Returns:\n",
    "            tokenized_data: The tokenized documents belonging to the\n",
    "                specified class label.\n",
    "        \"\"\"\n",
    "        tokenized_data = [txt for txt, label in zip(tokenized_doc, y) if label == k_]\n",
    "        return tokenized_data\n",
    "\n",
    "    def _log_priors(self, *, y: np.ndarray) -> list[float]:\n",
    "        \"\"\"This returns the log priors of y.\n",
    "\n",
    "        Params:\n",
    "            y (np.ndarray): The labels for the data.\n",
    "\n",
    "        Returns:\n",
    "            log_probs: A list containing the log probabilities\n",
    "            of the class labels.\n",
    "        \"\"\"\n",
    "        # Get the counts; calculate the log probabilities\n",
    "        # using the probabilities obtained from the counts.\n",
    "        counts = np.bincount(y)\n",
    "        probs = counts / len(y)\n",
    "        self.priors = [p_i for p_i in probs if p_i > 0]\n",
    "        log_probs = [(np.log(p_i)) for p_i in self.priors]\n",
    "        return log_probs\n",
    "\n",
    "    def _count_state_transitions(\n",
    "        self, *, X: list[int], A_hat: np.ndarray, Pi_hat: np.ndarray\n",
    "    ) -> tuple[np.ndarray]:\n",
    "        \"\"\"This is used to count the occurrences of transitions.\n",
    "        i.e calculate the counts of A_hat and Pi_hat.\n",
    "\n",
    "        Returns:\n",
    "            (A_hat, Pi_hat)\n",
    "        \"\"\"\n",
    "\n",
    "        # To calculate the Pi_hat, we need to count the number of times\n",
    "        # the initial state was `i` divided by the number of state sequences.\n",
    "        # Pi_hat = (count(state = i) / N)\n",
    "        # A_hat: count of the number of times we transitioned from the prev state `i`\n",
    "        # to the current state `j` divided by the count of the prev state `i`.\n",
    "        # i.e. A_hat = ( count(state_i to state_j) / (count(state_i)) )\n",
    "        # Note: Update the prev_state after each transition.\n",
    "        for tokenized_doc in X:\n",
    "            prev_token = None\n",
    "            for curr_token in tokenized_doc:\n",
    "                if prev_token is None:\n",
    "                    Pi_hat[curr_token] += 1\n",
    "                else:\n",
    "                    A_hat[prev_token, curr_token] += 1\n",
    "                # Update the prev_token\n",
    "                prev_token = curr_token\n",
    "        return (A_hat, Pi_hat)\n",
    "\n",
    "    def _convert_counts_to_log_prob(\n",
    "        self,\n",
    "        *,\n",
    "        X: Union[np.ndarray, pd.Series],\n",
    "        A_hat: np.ndarray,\n",
    "        Pi_hat: np.ndarray,\n",
    "    ) -> tuple[np.ndarray]:\n",
    "        \"\"\"This is used to calculate the log of the class conditional\n",
    "        probability given a specific class label. It returns a tuple\n",
    "        of arrays containing the log probabilities.\n",
    "\n",
    "        Returns:\n",
    "            (log(A_hat), log(Pi_hat))\n",
    "        \"\"\"\n",
    "        # Calculate the probabilities\n",
    "        A_hat /= A_hat.sum(axis=1, keepdims=True)  # OR A_hat/ A_hat.shape[0]\n",
    "        Pi_hat /= Pi_hat.sum(axis=0)\n",
    "        return (np.log(A_hat), np.log(Pi_hat))\n",
    "\n",
    "    def _calculate_log_likelihoods(self, *, x: list[int], k_: int) -> tuple[np.ndarray]:\n",
    "        \"\"\"This is used to extract the log probability given the log\n",
    "        probabililty and class label of the tokenized document.\n",
    "\n",
    "        Returns:\n",
    "            (log(A_hat), log(Pi_hat))\n",
    "        \"\"\"\n",
    "        log_A_hat, log_Pi_hat = self.transition_matrix[k_], self.initial_state_distr[k_]\n",
    "\n",
    "        # Calculate the probability:\n",
    "        # if it's an initial state (prev_idx is None), retrieve the probability\n",
    "        # otherwise, transition to a new state and retrieve the probability using\n",
    "        # the prev_idx and the curr_idx.\n",
    "        prev_idx, log_prob = None, 0\n",
    "\n",
    "        for curr_idx in x:\n",
    "            if prev_idx is None:\n",
    "                log_prob += log_Pi_hat[curr_idx]\n",
    "            else:\n",
    "                log_prob += log_A_hat[prev_idx, curr_idx]\n",
    "\n",
    "            # Update the value (for the next iteration)\n",
    "            prev_idx = curr_idx\n",
    "        return log_prob\n",
    "\n",
    "    def predict(self, X: list[int]) -> None:\n",
    "        # Instantiate\n",
    "        predictions = np.zeros(shape=(len(X)))\n",
    "\n",
    "        # For each sentence/tokenized_doc, make a prediction of the class label.\n",
    "        # This is done by calculating the argmax of the posteriors over all classes.\n",
    "        # posterior = likelihood + log_prior\n",
    "        # i.e compute the prob that an input/sentence belongs to a specific class label.\n",
    "        # The argmax of the posterior returns the index (class label) with the highest probability\n",
    "        # i.e if an input has a prob of [0.05, 0.001], the argmax returns 0 (index 0) which means\n",
    "        # that the input belongs to class 0 since 0.05 > 0.001 and it has an index of 0.\n",
    "        for idx, sentence in enumerate(X):\n",
    "            # Posteriors = posterior_k_0 and posterior_k_1\n",
    "            posteriors = [\n",
    "                (\n",
    "                    self._calculate_log_likelihoods(x=sentence, k_=k_)\n",
    "                    + self.log_priors[k_]\n",
    "                )\n",
    "                for k_ in self.K\n",
    "            ]\n",
    "            pred = np.argmax(posteriors)\n",
    "            predictions[idx] = pred\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreprocessText(corpus_size: 1936, vocab_size: 2812)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor = PreprocessText()\n",
    "X_vec = preprocessor.fit_transform(X=X_train)\n",
    "vocab = preprocessor._vocab\n",
    "\n",
    "preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0], [181, 505, 21], [0]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the preprocessor\n",
    "new = pd.Series(data=[\"Adonai\", \"we worship you\", \"neidu\"])\n",
    "preprocessor.transform(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiNomial_NB(log_priors=(-1.0944885714842476, -0.4075333611722234), priors={0: 0.33, 1: 0.67})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate\n",
    "m_nb = MultiNomial_NB(vocab=vocab)\n",
    "\n",
    "# Train the model\n",
    "m_nb.fit(X=X_vec, y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9974173553719008"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions\n",
    "y_pred = m_nb.predict(X=X_vec)\n",
    "\n",
    "# Calculate the accuracy\n",
    "# Note: Since the class labels are imbalanced, we'll need to use\n",
    "# a better metric e.g f1 score\n",
    "np.mean(y_pred == y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8611111111111112"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocess the validation data\n",
    "X_tr_vec = preprocessor.transform(X=X_valid)\n",
    "\n",
    "y_pred = m_nb.predict(X=X_tr_vec)\n",
    "\n",
    "np.mean(y_pred == y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_p310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
