{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification Using Probalistic Models\n",
    "\n",
    "* Markok Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "\n",
    "import spacy\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "# Custom imports\n",
    "from src.text_summarizer import Tokenizer\n",
    "\n",
    "\n",
    "# Built-in library\n",
    "import itertools\n",
    "import re\n",
    "import json\n",
    "from typing import Union, Optional, Any\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# pandas settings\n",
    "pd.options.display.max_rows = 1_000\n",
    "pd.options.display.max_columns = 1_000\n",
    "pd.options.display.max_colwidth = 600\n",
    "\n",
    "# Black code formatter (Optional)\n",
    "%load_ext lab_black\n",
    "# auto reload imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download The Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ‘edgar_allan_poe.txt’ already there; not retrieving.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -nc https://raw.githubusercontent.com/lazyprogrammer/machine_learning_examples/master/hmm_class/edgar_allan_poe.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ‘robert_frost.txt’ already there; not retrieving.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -nc https://raw.githubusercontent.com/lazyprogrammer/machine_learning_examples/master/hmm_class/robert_frost.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_data(*, filepath: Path, label: int) -> pd.DataFrame:\n",
    "    \"\"\"This loads the text data, assigns a label and returns a DF.\"\"\"\n",
    "    with open(filepath, \"r\") as file:\n",
    "        # Remove empty lines: using len(line) > 5\n",
    "        data = [(line.strip(), label) for line in file.readlines() if len(line) > 5]\n",
    "    # Convert to DF\n",
    "    df = pd.DataFrame(data=data, columns=[\"text\", \"label\"])\n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_punctuations(text: str) -> str:\n",
    "    import string\n",
    "\n",
    "    \"\"\"This returns the text without punctuations\"\"\"\n",
    "    cleaned_text = re.compile(pattern=f\"[{re.escape(string.punctuation)}]\").sub(\n",
    "        repl=\"\", string=str(text)\n",
    "    )\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LO! Death hath rear'd himself a throne</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In a strange city, all alone,</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Far down within the dim west</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Where the good, and the bad, and the worst, and the best,</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Have gone to their eternal rest.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        text  label\n",
       "0                     LO! Death hath rear'd himself a throne      0\n",
       "1                              In a strange city, all alone,      0\n",
       "2                               Far down within the dim west      0\n",
       "3  Where the good, and the bad, and the worst, and the best,      0\n",
       "4                           Have gone to their eternal rest.      0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp = \"edgar_allan_poe.txt\"\n",
    "fp1 = \"robert_frost.txt\"\n",
    "label_0, label_1 = 0, 1\n",
    "\n",
    "edgar_allan_poe = extract_text_data(filepath=fp, label=label_0)\n",
    "robert_frost = extract_text_data(filepath=fp1, label=label_1)\n",
    "\n",
    "edgar_allan_poe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Two roads diverged in a yellow wood,</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>And sorry I could not travel both</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>And be one traveler, long I stood</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>And looked down one as far as I could</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>To where it bent in the undergrowth;</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    text  label\n",
       "0   Two roads diverged in a yellow wood,      1\n",
       "1      And sorry I could not travel both      1\n",
       "2      And be one traveler, long I stood      1\n",
       "3  And looked down one as far as I could      1\n",
       "4   To where it bent in the undergrowth;      1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "robert_frost.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1703</th>\n",
       "      <td>Not bluebells gracing a tunnel mouth-</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1275</th>\n",
       "      <td>Of door and headboard. Where it wants to get</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>Hath ever told- or is it of a thought</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551</th>\n",
       "      <td>To the Lethean peace of the skies-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>Then desolately fall,</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>662</th>\n",
       "      <td>That from new fountains overflow,</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2055</th>\n",
       "      <td>'What's this?'</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611</th>\n",
       "      <td>Of its own fervour - what had o'er it power.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1303</th>\n",
       "      <td>The way a man with one leg and a crutch,</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1641</th>\n",
       "      <td>'How shall we?'</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              text  label\n",
       "1703         Not bluebells gracing a tunnel mouth-      1\n",
       "1275  Of door and headboard. Where it wants to get      1\n",
       "294          Hath ever told- or is it of a thought      0\n",
       "551             To the Lethean peace of the skies-      0\n",
       "426                          Then desolately fall,      0\n",
       "662              That from new fountains overflow,      0\n",
       "2055                                'What's this?'      1\n",
       "611   Of its own fervour - what had o'er it power.      0\n",
       "1303      The way a man with one leg and a crutch,      1\n",
       "1641                               'How shall we?'      1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RANDOM_STATE = 2\n",
    "TEST_SIZE = 0.1\n",
    "\n",
    "data = pd.concat([edgar_allan_poe, robert_frost], axis=\"rows\").reset_index(drop=True)\n",
    "data.sample(n=10, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1703</th>\n",
       "      <td>Not bluebells gracing a tunnel mouth</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1275</th>\n",
       "      <td>Of door and headboard Where it wants to get</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>Hath ever told or is it of a thought</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551</th>\n",
       "      <td>To the Lethean peace of the skies</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>Then desolately fall</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>662</th>\n",
       "      <td>That from new fountains overflow</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2055</th>\n",
       "      <td>Whats this</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611</th>\n",
       "      <td>Of its own fervour  what had oer it power</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1303</th>\n",
       "      <td>The way a man with one leg and a crutch</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1641</th>\n",
       "      <td>How shall we</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             text  label\n",
       "1703         Not bluebells gracing a tunnel mouth      1\n",
       "1275  Of door and headboard Where it wants to get      1\n",
       "294          Hath ever told or is it of a thought      0\n",
       "551             To the Lethean peace of the skies      0\n",
       "426                          Then desolately fall      0\n",
       "662              That from new fountains overflow      0\n",
       "2055                                   Whats this      1\n",
       "611     Of its own fervour  what had oer it power      0\n",
       "1303      The way a man with one leg and a crutch      1\n",
       "1641                                 How shall we      1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove punctuations\n",
    "data = data.assign(text=data[\"text\"].apply(remove_punctuations))\n",
    "\n",
    "data.sample(n=10, random_state=RANDOM_STATE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split The Data Into Train And Validation Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1936,), (216,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = data[\"text\"]\n",
    "y = data[\"label\"]\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "X_train.shape, X_valid.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Bag of Words (word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add an index for unknown words. This will be used if the word\n",
    "# is present in the validation set but NOT in the train set.\n",
    "word2idx = {\"<unk>\": 0}\n",
    "idx = 1  # Initialize the index\n",
    "\n",
    "# Populate word2idx using the train set. Tokenize each sentence,\n",
    "# if the token isn't in word2idx, add it and increment the idx.\n",
    "for txt_ in X_train:\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenized_text = tokenizer(doc=txt_)\n",
    "    for token in tokenized_text:\n",
    "        if token not in word2idx:\n",
    "            word2idx[token] = idx\n",
    "            idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<unk>': 0,\n",
       " 'in': 1,\n",
       " 'the': 2,\n",
       " 'realms': 3,\n",
       " 'of': 4,\n",
       " 'boreal': 5,\n",
       " 'pole': 6,\n",
       " 'i': 7,\n",
       " 'had': 8,\n",
       " 'a': 9}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Slice the dictionary\n",
    "dict(itertools.islice(word2idx.items(), 10))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize The Train And Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(*, word_2_idx: dict, data: pd.Series) -> list[list[int]]:\n",
    "    \"\"\"This is used to convert text to vectors.\"\"\"\n",
    "    vectorized_doc = []\n",
    "    data = data.copy()\n",
    "\n",
    "    for txt_ in data:\n",
    "        tokenizer = Tokenizer()\n",
    "        tokenized_text = tokenizer(doc=txt_)\n",
    "        # Extract the index from the dict. i.e convert each token to int\n",
    "        sent_idx = [word_2_idx.get(token, 0) for token in tokenized_text]\n",
    "        vectorized_doc.append(sent_idx)\n",
    "    return vectorized_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3, 4, 2, 5, 6],\n",
       " [7, 8, 9, 10, 4, 11, 12, 13],\n",
       " [14, 15, 16],\n",
       " [17, 18, 19, 20]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Vectorize the data\n",
    "X_train_vec = vectorize_text(word_2_idx=word2idx, data=X_train)\n",
    "X_valid_vec = vectorize_text(word_2_idx=word2idx, data=X_valid)\n",
    "\n",
    "X_train_vec[:4]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize State Transition Matrix (A) and Initial State Distribution (Pi)\n",
    "\n",
    "1. **A**: This is a matrix (2-D array) that's used to store the probability that a state at time **`t`** is **`j`**, given that the state at time **`t-1`** was **`i`**. i.e the probability of a **state** given the **previous state**.\n",
    "\n",
    "$$\n",
    "A_{ij} = p(s_{t} = j | s_{t-1} = i) \n",
    "$$\n",
    "\n",
    ">Estimated **`A`** is the number of times we transition from state **`i`** to state **`j`** divided by the total number of times we were in state **`i`**.\n",
    "\n",
    "$$\n",
    "\\hat{A}_{ij} = \\frac{count(i \\rightarrow j)}{count(i)}\n",
    "$$\n",
    "\n",
    "2. **Pi**: This is a vector that's used to store the probability of the **inital state** in a **sequence**.\n",
    "\n",
    "$$\n",
    "Pi_{i} = p(s_{1} = i) \n",
    "$$\n",
    "\n",
    ">Estimated **`Pi`** is the number of times the sequence started at state **`i`** divided by the number of sequences in the dataset, **`N`**.\n",
    "\n",
    "$$\n",
    "\\hat{Pi}_{i} = \\frac{count(s_{1} = i)}{N}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we have 2 classes (edgar_allan_poe, robert_frost),\n",
    "# we need to build 2 models. i.e A_0, Pi_0 and A_1, Pi_1\n",
    "V = len(word2idx)  # Vocabulary or number of states\n",
    "\n",
    "A_0 = np.ones((V, V))  # Matrix with add-one smoothering\n",
    "Pi_0 = np.ones(V)  # Vector with add-one smoothering\n",
    "\n",
    "A_1 = np.ones((V, V))  # Matrix with add-one smoothering\n",
    "Pi_1 = np.ones(V)  # Vector with add-one smoothering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_counts(\n",
    "    *, vectorize_doc: list[int], A: np.ndarray, Pi: np.ndarray\n",
    ") -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"This is used to populate A and Pi, normalize the values and\n",
    "    calculate log probabilities. i.e count the number of occurrences\n",
    "    of each state and divide by the total number of posible occurrences\n",
    "    and find the log probability to prevent overflow error.\n",
    "\n",
    "    Params:\n",
    "        vectorize_doc: The vectorized doc. i.e List of int.\n",
    "        A: The state transition matrix.\n",
    "        Pi: The initial state distibution.\n",
    "\n",
    "    Returns:\n",
    "        (log_A, log_Pi): Tuple containing the log prob of A and Pi.\n",
    "    \"\"\"\n",
    "    for tokenized_doc in vectorize_doc:\n",
    "        prev_idx = None  # previous state/first word\n",
    "        for idx in tokenized_doc:\n",
    "            # If there's no prev state/word. i.e it's the first word.\n",
    "            if prev_idx is None:\n",
    "                Pi[idx] += 1\n",
    "            else:\n",
    "                # A prev word exists, count a transition\n",
    "                A[prev_idx, idx] += 1\n",
    "\n",
    "            # Update the prev idx with the current idx\n",
    "            prev_idx = idx\n",
    "\n",
    "    return (A, Pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.60750361e-04, 3.60750361e-04, 3.60750361e-04, ...,\n",
       "        3.60750361e-04, 3.60750361e-04, 3.60750361e-04],\n",
       "       [3.37812730e-06, 3.37812730e-06, 3.49639554e-01, ...,\n",
       "        3.37812730e-06, 3.37812730e-06, 3.37812730e-06],\n",
       "       [1.13941156e-06, 1.13941156e-06, 1.13941156e-06, ...,\n",
       "        1.13941156e-06, 1.13941156e-06, 1.13941156e-06],\n",
       "       [1.20163422e-04, 1.20163422e-04, 1.20163422e-04, ...,\n",
       "        1.20163422e-04, 1.20163422e-04, 1.20163422e-04],\n",
       "       [2.01263938e-06, 2.01263938e-06, 1.59819668e-01, ...,\n",
       "        2.01263938e-06, 2.01263938e-06, 2.01263938e-06]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1st model (edgar_allan_poe)\n",
    "A_0, Pi_0 = compute_counts(\n",
    "    vectorize_doc=[_data for _data, y in zip(X_train_vec, y_train) if y == 0],\n",
    "    A=A_0,\n",
    "    Pi=Pi_0,\n",
    ")\n",
    "\n",
    "# 2nd model (robert_frost)\n",
    "A_1, Pi_1 = compute_counts(\n",
    "    vectorize_doc=[_data for _data, y in zip(X_train_vec, y_train) if y == 1],\n",
    "    A=A_1,\n",
    "    Pi=Pi_1,\n",
    ")\n",
    "\n",
    "# Normalize i.e calculate probabilities for 1st model\n",
    "A_0 /= A_0.sum(axis=1, keepdims=True)  # Returns in 2-D array\n",
    "Pi_0 /= Pi_0.sum()  # Returns in 1-D array\n",
    "\n",
    "A_0[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00036075, 0.00036075, 0.00036075, ..., 0.00036075, 0.00036075,\n",
       "        0.00036075],\n",
       "       [0.00034165, 0.00034165, 0.01469081, ..., 0.00034165, 0.00034165,\n",
       "        0.00068329],\n",
       "       [0.00030628, 0.00030628, 0.00030628, ..., 0.00030628, 0.00030628,\n",
       "        0.00030628],\n",
       "       [0.00036075, 0.00036075, 0.00036075, ..., 0.00036075, 0.00036075,\n",
       "        0.00036075],\n",
       "       [0.00033852, 0.00033852, 0.00947867, ..., 0.00033852, 0.00033852,\n",
       "        0.00033852]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize i.e calculate probabilities for 2nd model\n",
    "A_1 /= A_1.sum(axis=1, keepdims=True)  # Returns in 2-D array\n",
    "Pi_1 /= Pi_1.sum()  # Returns in 1-D array\n",
    "\n",
    "A_1[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find log A and pi since we don't need the actual probs\n",
    "log_A_0 = np.log(A_0)\n",
    "log_Pi_0 = np.log(Pi_0)\n",
    "\n",
    "log_A_1 = np.log(A_1)\n",
    "log_Pi_1 = np.log(Pi_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-7.92732436, -7.92732436, -7.92732436, ..., -7.92732436,\n",
       "        -7.92732436, -7.92732436],\n",
       "       [-7.96346007, -7.96346007, -4.35254215, ..., -7.96346007,\n",
       "        -7.96346007, -7.96346007],\n",
       "       [-8.02551639, -8.02551639, -8.02551639, ..., -8.02551639,\n",
       "        -8.02551639, -8.02551639],\n",
       "       [-7.9280456 , -7.9280456 , -7.9280456 , ..., -7.9280456 ,\n",
       "        -7.9280456 , -7.9280456 ],\n",
       "       [-7.98616486, -7.98616486, -4.65396035, ..., -7.98616486,\n",
       "        -7.98616486, -7.98616486]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_A_0[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Priors: \n",
      "p_0: 0.3347107438016529, p_1: 0.6652892561983471\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute priors\n",
    "count_0 = sum(y == 0 for y in y_train)\n",
    "count_1 = sum(y == 1 for y in y_train)\n",
    "total = len(y_train)\n",
    "p_0 = count_0 / total\n",
    "p_1 = count_1 / total\n",
    "print(f\"Priors: \\np_0: {p_0}, p_1: {p_1}\\n\\n\")\n",
    "\n",
    "log_p_0 = np.log(p_0)\n",
    "log_p_1 = np.log(p_1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment\n",
    "\n",
    "* Since the priors are are not uniformly distributed. i.e **prob** of **`p_0`** and **`p_1`** are not equal, we can't use the Maximum Likelihood equation which is:\n",
    "\n",
    "$$\n",
    "log(p(class = k| input)) = k^*\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "k^* = argmax_{k} (log(p(input | class = k))\n",
    "$$\n",
    "\n",
    "* **Maximum a Posteriori (MAP)** which includes the priors will be used instead. This is the `argmax` of the probability of class = `k`, given the `input` over all classes `k`\n",
    "* This means that if we're trying to predict the class (label) `k`, the probability with the highest value belongs to class `k` since each model was trained using only inputs corresponding to the desired class.\n",
    "\n",
    "$$\n",
    "k^* = argmax_{k} (log(p(input | class = k)) + log(p(class = k)))\n",
    "$$\n",
    "\n",
    "#### Note:\n",
    ">`k` is the ***class*** (or label). In this example, `k=0` or `k=1`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting It All Together\n",
    "\n",
    "* Build a `Text Classifier` that has a similar API to sklearn. i.e it has a fit (for model training) and predict for making predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarkovClassifier:\n",
    "    \"\"\"This classifier uses MArkov model for classifiication.\n",
    "    It's trained using two models. i.e for 2 classes (labels).\"\"\"\n",
    "\n",
    "    def __init__(self, log_A_s: list, log_Pi_s: list, log_priors: list) -> None:\n",
    "        self.log_A_s = log_A_s\n",
    "        self.log_Pi_s = log_Pi_s\n",
    "        self.log_priors = log_priors\n",
    "        self.K = len(log_priors)\n",
    "\n",
    "    def _compute_log_likelihood(self, input_: list[int], class_: int) -> None:\n",
    "        \"\"\"This returns the log of the probabilities.\"\"\"\n",
    "        # Extract the log of A and Pi for the given class (label)\n",
    "        log_A = self.log_A_s[class_]\n",
    "        log_Pi = self.log_Pi_s[class_]\n",
    "\n",
    "        # Initialize variables\n",
    "        prev_idx = None\n",
    "        log_prob = 0\n",
    "\n",
    "        for idx in input_:\n",
    "            # If it's the first token, replace\n",
    "            # it with the probability from log_Pi.\n",
    "            if prev_idx is None:\n",
    "                log_prob += log_Pi[idx]\n",
    "\n",
    "            # replace it with the probability from log_A.\n",
    "            else:\n",
    "                log_prob += log_A[prev_idx, idx]\n",
    "\n",
    "            # Update the value (for the next iteration)\n",
    "            prev_idx = idx\n",
    "        return log_prob\n",
    "\n",
    "    def predict(self, inputs: list[list[int]]) -> list[int]:\n",
    "        \"\"\"This is used to make predictions using the trained Markov model.\"\"\"\n",
    "        predictions = np.zeros(len(inputs))  # Initialize\n",
    "        # Make predictions for every sentence in inputs\n",
    "        for idx, input_ in enumerate(inputs):\n",
    "            posteriors = [\n",
    "                self._compute_log_likelihood(input_=input_, class_=class_)\n",
    "                + self.log_priors[class_]\n",
    "                for class_ in range(self.K)\n",
    "            ]\n",
    "            pred = np.argmax(posteriors)\n",
    "            predictions[idx] = pred\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_A_s = [log_A_0, log_A_1]\n",
    "log_Pi_s = [log_Pi_0, log_Pi_1]\n",
    "log_priors = [log_p_0, log_p_1]\n",
    "\n",
    "clf = MarkovClassifier(log_A_s=log_A_s, log_Pi_s=log_Pi_s, log_priors=log_priors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 1., ..., 0., 1., 1.])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_train = clf.predict(inputs=X_train_vec)\n",
    "y_pred_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9963842975206612"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the accuracy\n",
    "np.mean(y_pred_train == y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8564814814814815"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_valid = clf.predict(inputs=X_valid_vec)\n",
    "\n",
    "# Calculate the accuracy\n",
    "np.mean(y_pred_valid == y_valid)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment\n",
    "\n",
    "* The classes are imbalanced (p0 = 0.34) and (p1 = 0.66) so using **`accuracy`** might not be the best evaluation metric.\n",
    "* **`ROCAUC`** and **`F1 score`** are better metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 641,    7],\n",
       "       [   0, 1288]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# Training Set\n",
    "confusion_matrix_train = metrics.confusion_matrix(y_true=y_train, y_pred=y_pred_train)\n",
    "confusion_matrix_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 44,  26],\n",
       "       [  5, 141]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validation Set\n",
    "confusion_matrix_valid = metrics.confusion_matrix(y_true=y_valid, y_pred=y_pred_valid)\n",
    "confusion_matrix_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.997289972899729, 0.9009584664536742)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score_train = metrics.f1_score(y_true=y_train, y_pred=y_pred_train)\n",
    "f1_score_valid = metrics.f1_score(y_true=y_valid, y_pred=y_pred_valid)\n",
    "\n",
    "f1_score_train, f1_score_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class MarkovClassifier:\n",
    "    \"\"\"This classifier uses MArkov model for classifiication.\\n\n",
    "    It's trained using two models. i.e for 2 classes (labels).\n",
    "\n",
    "    Params:\n",
    "        word2idx (dict): A dictionary containing the vocabulary. i.e Bag of Words.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, word2idx: dict) -> None:\n",
    "        self.log_A_s = None\n",
    "        self.log_Pi_s = None\n",
    "        self.log_priors = None\n",
    "        self.K = None\n",
    "        self.word2idx = word2idx\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{__class__.__name__}()\"\n",
    "\n",
    "    def _initialize_A_n_Pi(self) -> tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"This is used to initialise the matrices.\"\"\"\n",
    "        # Since we have 2 classes (edgar_allan_poe, robert_frost),\n",
    "        # we need to build 2 models. i.e A_0, Pi_0 and A_1, Pi_1\n",
    "        V = len(self.word2idx)  # Vocabulary or number of states\n",
    "\n",
    "        A = np.ones((V, V))  # Matrix with add-one smoothering\n",
    "        Pi = np.ones(V)  # Vector with add-one smoothering\n",
    "        return (A, Pi)\n",
    "\n",
    "    def _compute_counts(\n",
    "        self, X: list[int], A: np.ndarray, Pi: np.ndarray\n",
    "    ) -> tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"This is used to populate A and Pi, normalize the values and\n",
    "        calculate log probabilities. i.e count the number of occurrences\n",
    "        of each state and divide by the total number of posible occurrences\n",
    "        and find the log probability to prevent overflow error.\n",
    "\n",
    "        Params:\n",
    "            X: The vectorized doc. i.e List of int.\n",
    "            A: The state transition matrix.\n",
    "            Pi: The initial state distibution.\n",
    "\n",
    "        Returns:\n",
    "            (log_A, log_Pi): Tuple containing the log prob of A and Pi.\n",
    "        \"\"\"\n",
    "        for tokenized_doc in X:\n",
    "            prev_idx = None  # previous state/first word\n",
    "            for idx in tokenized_doc:\n",
    "                # If there's no prev state/word. i.e it's the first word.\n",
    "                if prev_idx is None:\n",
    "                    Pi[idx] += 1\n",
    "                else:\n",
    "                    # A prev word exists, count a transition\n",
    "                    A[prev_idx, idx] += 1\n",
    "\n",
    "                # Update the prev idx with the current idx\n",
    "                prev_idx = idx\n",
    "        return (A, Pi)\n",
    "\n",
    "    def _calculate_probabilities(\n",
    "        self, A: np.ndarray, Pi: np.ndarray\n",
    "    ) -> tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"This is used to normalize the arrays. i.e calculate\n",
    "        log probabilities.\"\"\"\n",
    "        A /= A.sum(axis=1, keepdims=True)  # Returns in 2-D array\n",
    "        Pi /= Pi.sum()  # Returns in 1-D array\n",
    "        return (np.log(A), np.log(Pi))\n",
    "\n",
    "    def _compute_log_priors(self, y: np.ndarray) -> tuple[float, float]:\n",
    "        count_0 = sum(_y == 0 for _y in y)\n",
    "        count_1 = sum(_y == 1 for _y in y)\n",
    "        total = len(y)\n",
    "        p_0 = count_0 / total\n",
    "        p_1 = count_1 / total\n",
    "        log_p_0, log_p_1 = np.log(p_0), np.log(p_1)\n",
    "        return (log_p_0, log_p_1)\n",
    "\n",
    "    def _calculate_log_probs_n_priors(self, X: np.ndarray, y: np.ndarray) -> None:\n",
    "        \"\"\"This calculates the log probabilities for the two (2) models (classes) and log priors.\n",
    "        It returns (log_A_0, log_A_1), (log_Pi_0, and log_Pi_1) and (log_p_0, log_p_1).\"\"\"\n",
    "        # 1st model\n",
    "        A_0, Pi_0 = self._initialize_A_n_Pi()\n",
    "        A_0, Pi_0 = self._compute_counts(\n",
    "            X=[_data for _data, _y in zip(X, y) if _y == 0],\n",
    "            A=A_0,\n",
    "            Pi=Pi_0,\n",
    "        )\n",
    "        log_A_0, log_Pi_0 = self._calculate_probabilities(A_0, Pi_0)\n",
    "\n",
    "        # 2nd model\n",
    "        A_1, Pi_1 = self._initialize_A_n_Pi()\n",
    "        A_1, Pi_1 = self._compute_counts(\n",
    "            X=[_data for _data, _y in zip(X, y) if _y == 1],\n",
    "            A=A_1,\n",
    "            Pi=Pi_1,\n",
    "        )\n",
    "        log_A_1, log_Pi_1 = self._calculate_probabilities(A_1, Pi_1)\n",
    "        log_p_0, log_p_1 = self._compute_log_priors(y)\n",
    "        self.log_A_s = [log_A_0, log_A_1]\n",
    "        self.log_Pi_s = [log_Pi_0, log_Pi_1]\n",
    "        self.log_priors = [log_p_0, log_p_1]\n",
    "        self.K = len(self.log_priors)\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray) -> None:\n",
    "        # log_A_s, log_Pi_s, log_priors = self._calculate_log_probs_n_priors(X, y)\n",
    "        self._calculate_log_probs_n_priors(X, y)\n",
    "        return self\n",
    "\n",
    "    def _compute_log_likelihood(self, input_: list[int], class_: int) -> None:\n",
    "        \"\"\"This returns the log of the probabilities.\"\"\"\n",
    "        # Extract the log of A and Pi for the given class (label)\n",
    "        log_A = self.log_A_s[class_]\n",
    "        log_Pi = self.log_Pi_s[class_]\n",
    "\n",
    "        # Initialize variables\n",
    "        prev_idx = None\n",
    "        log_prob = 0\n",
    "\n",
    "        for idx in input_:\n",
    "            # If it's the first token, replace\n",
    "            # it with the probability from log_Pi.\n",
    "            if prev_idx is None:\n",
    "                log_prob += log_Pi[idx]\n",
    "\n",
    "            # replace it with the probability from log_A.\n",
    "            else:\n",
    "                log_prob += log_A[prev_idx, idx]\n",
    "\n",
    "            # Update the value (for the next iteration)\n",
    "            prev_idx = idx\n",
    "        return log_prob\n",
    "\n",
    "    def predict(self, X: list[list[int]]) -> np.ndarray:\n",
    "        \"\"\"This is used to make predictions using the trained Markov model.\"\"\"\n",
    "        # Initialize\n",
    "        predictions = np.zeros(len(X))\n",
    "        # Make predictions for every sentence in X\n",
    "        for idx, input_ in enumerate(X):\n",
    "            posteriors = [\n",
    "                self._compute_log_likelihood(input_=input_, class_=class_)\n",
    "                + self.log_priors[class_]\n",
    "                for class_ in range(self.K)\n",
    "            ]\n",
    "            pred = np.argmax(posteriors)\n",
    "            predictions[idx] = pred\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MarkovClassifier()"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MarkovClassifier(word2idx=word2idx)\n",
    "clf.fit(X=X_train_vec, y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9963842975206612"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_train = clf.predict(X=X_train_vec)\n",
    "# Calculate the accuracy\n",
    "np.mean(y_pred_train == y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_p310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5c160606400bd63443fe4361c23f8347e54b6f9986e7c6d27e878f1970943f47"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
