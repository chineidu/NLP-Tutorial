{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create A Multi-Layer Perceptron (MLP) For Predicting The Next Character In A Sequence\n",
    "\n",
    "- [Andrej Karpathy YouTube Tutorial](https://www.youtube.com/watch?v=TCH_1BHY58I&t=541s&ab_channel=AndrejKarpathy)\n",
    "- [A Neural Probabilistic Language Model (Paper)](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python implementation: CPython\n",
      "Python version       : 3.11.8\n",
      "IPython version      : 8.22.2\n",
      "\n",
      "numpy    : 1.26.4\n",
      "pandas   : 2.2.1\n",
      "polars   : 0.20.18\n",
      "torch    : 2.2.2\n",
      "lightning: 2.2.1\n",
      "\n",
      "conda environment: torch_p11\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -v -p numpy,pandas,polars,torch,lightning --conda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in library\n",
    "from pathlib import Path\n",
    "import re\n",
    "import json\n",
    "from typing import Any, Optional, Union\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "# Standard imports\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from rich.console import Console\n",
    "from rich.theme import Theme\n",
    "\n",
    "custom_theme = Theme(\n",
    "    {\n",
    "        \"info\": \"#76FF7B\",\n",
    "        \"warning\": \"#FBDDFE\",\n",
    "        \"error\": \"#FF0000\",\n",
    "    }\n",
    ")\n",
    "console = Console(theme=custom_theme)\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Pandas settings\n",
    "pd.options.display.max_rows = 1_000\n",
    "pd.options.display.max_columns = 1_000\n",
    "pd.options.display.max_colwidth = 600\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# Black code formatter (Optional)\n",
    "%load_ext lab_black\n",
    "\n",
    "# auto reload imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path: str) -> list[str]:\n",
    "    \"\"\"Load text data from a file and return as a list of strings.\"\"\"\n",
    "\n",
    "    with open(file_path, \"r\") as f:\n",
    "        # Read all the lines as a list\n",
    "        data: list[str] = f.read().splitlines()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Data\n",
    "fp: str = \"../../../data/names.txt\"\n",
    "names: list[str] = load_data(file_path=fp)\n",
    "\n",
    "names[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Vocabulary Of Characters And Mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(\"\".join(names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'.'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'a'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'b'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'c'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'d'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'e'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'f'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'g'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'h'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'i'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'j'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'k'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'l'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'m'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'n'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'o'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'p'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'q'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'r'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">18</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'s'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">19</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'t'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'u'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">21</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'v'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">22</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'w'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">23</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'x'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">24</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'y'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">25</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'z'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'.'</span>,\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'a'</span>,\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'b'</span>,\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'c'</span>,\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'d'</span>,\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'e'</span>,\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'f'</span>,\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'g'</span>,\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'h'</span>,\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'i'</span>,\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'j'</span>,\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'k'</span>,\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'l'</span>,\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'m'</span>,\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'n'</span>,\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'o'</span>,\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'p'</span>,\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'q'</span>,\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">18</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'r'</span>,\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">19</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'s'</span>,\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'t'</span>,\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">21</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'u'</span>,\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">22</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'v'</span>,\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">23</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'w'</span>,\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">24</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'x'</span>,\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">25</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'y'</span>,\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'z'</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'.'\u001b[0m: \u001b[1;36m0\u001b[0m,\n",
       "    \u001b[32m'a'\u001b[0m: \u001b[1;36m1\u001b[0m,\n",
       "    \u001b[32m'b'\u001b[0m: \u001b[1;36m2\u001b[0m,\n",
       "    \u001b[32m'c'\u001b[0m: \u001b[1;36m3\u001b[0m,\n",
       "    \u001b[32m'd'\u001b[0m: \u001b[1;36m4\u001b[0m,\n",
       "    \u001b[32m'e'\u001b[0m: \u001b[1;36m5\u001b[0m,\n",
       "    \u001b[32m'f'\u001b[0m: \u001b[1;36m6\u001b[0m,\n",
       "    \u001b[32m'g'\u001b[0m: \u001b[1;36m7\u001b[0m,\n",
       "    \u001b[32m'h'\u001b[0m: \u001b[1;36m8\u001b[0m,\n",
       "    \u001b[32m'i'\u001b[0m: \u001b[1;36m9\u001b[0m,\n",
       "    \u001b[32m'j'\u001b[0m: \u001b[1;36m10\u001b[0m,\n",
       "    \u001b[32m'k'\u001b[0m: \u001b[1;36m11\u001b[0m,\n",
       "    \u001b[32m'l'\u001b[0m: \u001b[1;36m12\u001b[0m,\n",
       "    \u001b[32m'm'\u001b[0m: \u001b[1;36m13\u001b[0m,\n",
       "    \u001b[32m'n'\u001b[0m: \u001b[1;36m14\u001b[0m,\n",
       "    \u001b[32m'o'\u001b[0m: \u001b[1;36m15\u001b[0m,\n",
       "    \u001b[32m'p'\u001b[0m: \u001b[1;36m16\u001b[0m,\n",
       "    \u001b[32m'q'\u001b[0m: \u001b[1;36m17\u001b[0m,\n",
       "    \u001b[32m'r'\u001b[0m: \u001b[1;36m18\u001b[0m,\n",
       "    \u001b[32m's'\u001b[0m: \u001b[1;36m19\u001b[0m,\n",
       "    \u001b[32m't'\u001b[0m: \u001b[1;36m20\u001b[0m,\n",
       "    \u001b[32m'u'\u001b[0m: \u001b[1;36m21\u001b[0m,\n",
       "    \u001b[32m'v'\u001b[0m: \u001b[1;36m22\u001b[0m,\n",
       "    \u001b[32m'w'\u001b[0m: \u001b[1;36m23\u001b[0m,\n",
       "    \u001b[32m'x'\u001b[0m: \u001b[1;36m24\u001b[0m,\n",
       "    \u001b[32m'y'\u001b[0m: \u001b[1;36m25\u001b[0m,\n",
       "    \u001b[32m'z'\u001b[0m: \u001b[1;36m26\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n",
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[1;36m0\u001b[0m: \u001b[32m'.'\u001b[0m,\n",
       "    \u001b[1;36m1\u001b[0m: \u001b[32m'a'\u001b[0m,\n",
       "    \u001b[1;36m2\u001b[0m: \u001b[32m'b'\u001b[0m,\n",
       "    \u001b[1;36m3\u001b[0m: \u001b[32m'c'\u001b[0m,\n",
       "    \u001b[1;36m4\u001b[0m: \u001b[32m'd'\u001b[0m,\n",
       "    \u001b[1;36m5\u001b[0m: \u001b[32m'e'\u001b[0m,\n",
       "    \u001b[1;36m6\u001b[0m: \u001b[32m'f'\u001b[0m,\n",
       "    \u001b[1;36m7\u001b[0m: \u001b[32m'g'\u001b[0m,\n",
       "    \u001b[1;36m8\u001b[0m: \u001b[32m'h'\u001b[0m,\n",
       "    \u001b[1;36m9\u001b[0m: \u001b[32m'i'\u001b[0m,\n",
       "    \u001b[1;36m10\u001b[0m: \u001b[32m'j'\u001b[0m,\n",
       "    \u001b[1;36m11\u001b[0m: \u001b[32m'k'\u001b[0m,\n",
       "    \u001b[1;36m12\u001b[0m: \u001b[32m'l'\u001b[0m,\n",
       "    \u001b[1;36m13\u001b[0m: \u001b[32m'm'\u001b[0m,\n",
       "    \u001b[1;36m14\u001b[0m: \u001b[32m'n'\u001b[0m,\n",
       "    \u001b[1;36m15\u001b[0m: \u001b[32m'o'\u001b[0m,\n",
       "    \u001b[1;36m16\u001b[0m: \u001b[32m'p'\u001b[0m,\n",
       "    \u001b[1;36m17\u001b[0m: \u001b[32m'q'\u001b[0m,\n",
       "    \u001b[1;36m18\u001b[0m: \u001b[32m'r'\u001b[0m,\n",
       "    \u001b[1;36m19\u001b[0m: \u001b[32m's'\u001b[0m,\n",
       "    \u001b[1;36m20\u001b[0m: \u001b[32m't'\u001b[0m,\n",
       "    \u001b[1;36m21\u001b[0m: \u001b[32m'u'\u001b[0m,\n",
       "    \u001b[1;36m22\u001b[0m: \u001b[32m'v'\u001b[0m,\n",
       "    \u001b[1;36m23\u001b[0m: \u001b[32m'w'\u001b[0m,\n",
       "    \u001b[1;36m24\u001b[0m: \u001b[32m'x'\u001b[0m,\n",
       "    \u001b[1;36m25\u001b[0m: \u001b[32m'y'\u001b[0m,\n",
       "    \u001b[1;36m26\u001b[0m: \u001b[32m'z'\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "special_token: str = \".\"\n",
    "characters: list[str] = sorted(set(\"\".join(names)))\n",
    "# Add the special token to the beginning of the list.\n",
    "characters.insert(0, special_token)\n",
    "n_chars: int = len(characters)\n",
    "\n",
    "# Convert text to numbers.\n",
    "text_to_num: dict[str, int] = {text: idx for idx, text in enumerate(characters)}\n",
    "# Convert numbers to text\n",
    "num_to_text: dict[int, str] = {idx: text for text, idx in text_to_num.items()}\n",
    "\n",
    "\n",
    "console.print(text_to_num, num_to_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(\n",
    "    names: list[str],\n",
    "    special_token: str = \".\",\n",
    "    block_size: int = 3,\n",
    "    print_info: bool = False,\n",
    ") -> tuple[Tensor, Tensor]:\n",
    "    \"\"\"\n",
    "    Builds a dataset of name sequences and their corresponding character indices.\n",
    "\n",
    "    Args:\n",
    "        names (list[str]): A list of names to build the dataset from.\n",
    "        special_token (str, optional): A special token to append to the end of each name. Defaults to \".\".\n",
    "        block_size (int, optional): The size of the context window for each input sequence. Defaults to 3.\n",
    "        print_info (bool, optional): Whether to print information about the dataset generation. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        tuple[Tensor, Tensor]: A tuple containing the input sequences (X) and their corresponding target indices (Y).\n",
    "    \"\"\"\n",
    "    X, Y = [], []\n",
    "\n",
    "    for w in names:\n",
    "        if print_info:\n",
    "            print(w)\n",
    "        context: list[str] = [0] * block_size\n",
    "\n",
    "        for ch in w + special_token:\n",
    "            ix: int = text_to_num.get(ch)\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "\n",
    "            if print_info:\n",
    "                print(\n",
    "                    f\"{''.join([num_to_text.get(i) for i in context])} ---> {num_to_text.get(ix)}\"\n",
    "                )\n",
    "\n",
    "            # Crop and append, like a rolling window\n",
    "            context = context[1:] + [ix]\n",
    "\n",
    "    X: Tensor = torch.tensor(X)\n",
    "    Y: Tensor = torch.tensor(Y)\n",
    "    print(f\"\\n{X.shape=}, {Y.shape=}\")\n",
    "    return (X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emma\n",
      "... ---> e\n",
      "..e ---> m\n",
      ".em ---> m\n",
      "emm ---> a\n",
      "mma ---> .\n",
      "olivia\n",
      "... ---> o\n",
      "..o ---> l\n",
      ".ol ---> i\n",
      "oli ---> v\n",
      "liv ---> i\n",
      "ivi ---> a\n",
      "via ---> .\n",
      "ava\n",
      "... ---> a\n",
      "..a ---> v\n",
      ".av ---> a\n",
      "ava ---> .\n",
      "isabella\n",
      "... ---> i\n",
      "..i ---> s\n",
      ".is ---> a\n",
      "isa ---> b\n",
      "sab ---> e\n",
      "abe ---> l\n",
      "bel ---> l\n",
      "ell ---> a\n",
      "lla ---> .\n",
      "sophia\n",
      "... ---> s\n",
      "..s ---> o\n",
      ".so ---> p\n",
      "sop ---> h\n",
      "oph ---> i\n",
      "phi ---> a\n",
      "hia ---> .\n",
      "\n",
      "X.shape=torch.Size([32, 3]), Y.shape=torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "X, y = build_dataset(names=names[:5], block_size=3, print_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3]), torch.Size([32]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/mlp 1.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C.shape = torch.Size([27, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3686, -0.5562],\n",
       "        [ 1.2637, -0.5048],\n",
       "        [-1.4296, -0.3214],\n",
       "        [-0.6920,  0.1593],\n",
       "        [ 0.1617, -0.0502],\n",
       "        [ 0.8174,  0.4805],\n",
       "        [-1.3794,  0.1881],\n",
       "        [-1.2635, -0.3200],\n",
       "        [ 1.3065,  0.3617],\n",
       "        [ 0.6837, -0.6760],\n",
       "        [-2.2507, -0.0455],\n",
       "        [ 1.1029, -0.2650],\n",
       "        [-1.6356,  0.4328],\n",
       "        [ 1.3866, -1.3432],\n",
       "        [-0.1160, -1.3923],\n",
       "        [ 1.7940, -1.0061],\n",
       "        [-1.0558,  0.4111],\n",
       "        [-0.3326, -1.3829],\n",
       "        [ 1.3733, -1.3214],\n",
       "        [-0.1643,  0.7629],\n",
       "        [-0.2303, -0.4835],\n",
       "        [-2.6855, -1.3601],\n",
       "        [ 1.3371,  0.6430],\n",
       "        [-0.2928,  2.2157],\n",
       "        [ 1.7291, -1.1297],\n",
       "        [-0.7569, -1.0828],\n",
       "        [-0.7454, -1.3146]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Build an embedding lookup table\n",
    "emb_dim: int = 2  # embedding dimension\n",
    "C: Tensor = torch.randn((n_chars, emb_dim))\n",
    "\n",
    "print(f\"{C.shape = }\")\n",
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8174, 0.4805])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Embed the inputs\n",
    "# Method 1\n",
    "C[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8174, 0.4805])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Method 2\n",
    "F.one_hot(torch.tensor(5), num_classes=n_chars).float() @ C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emb.shape=torch.Size([32, 3, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3686, -0.5562],\n",
       "         [ 0.3686, -0.5562],\n",
       "         [ 0.3686, -0.5562]],\n",
       "\n",
       "        [[ 0.3686, -0.5562],\n",
       "         [ 0.3686, -0.5562],\n",
       "         [ 0.8174,  0.4805]],\n",
       "\n",
       "        [[ 0.3686, -0.5562],\n",
       "         [ 0.8174,  0.4805],\n",
       "         [ 1.3866, -1.3432]],\n",
       "\n",
       "        [[ 0.8174,  0.4805],\n",
       "         [ 1.3866, -1.3432],\n",
       "         [ 1.3866, -1.3432]],\n",
       "\n",
       "        [[ 1.3866, -1.3432],\n",
       "         [ 1.3866, -1.3432],\n",
       "         [ 1.2637, -0.5048]],\n",
       "\n",
       "        [[ 0.3686, -0.5562],\n",
       "         [ 0.3686, -0.5562],\n",
       "         [ 0.3686, -0.5562]],\n",
       "\n",
       "        [[ 0.3686, -0.5562],\n",
       "         [ 0.3686, -0.5562],\n",
       "         [ 1.7940, -1.0061]],\n",
       "\n",
       "        [[ 0.3686, -0.5562],\n",
       "         [ 1.7940, -1.0061],\n",
       "         [-1.6356,  0.4328]],\n",
       "\n",
       "        [[ 1.7940, -1.0061],\n",
       "         [-1.6356,  0.4328],\n",
       "         [ 0.6837, -0.6760]],\n",
       "\n",
       "        [[-1.6356,  0.4328],\n",
       "         [ 0.6837, -0.6760],\n",
       "         [ 1.3371,  0.6430]],\n",
       "\n",
       "        [[ 0.6837, -0.6760],\n",
       "         [ 1.3371,  0.6430],\n",
       "         [ 0.6837, -0.6760]],\n",
       "\n",
       "        [[ 1.3371,  0.6430],\n",
       "         [ 0.6837, -0.6760],\n",
       "         [ 1.2637, -0.5048]],\n",
       "\n",
       "        [[ 0.3686, -0.5562],\n",
       "         [ 0.3686, -0.5562],\n",
       "         [ 0.3686, -0.5562]],\n",
       "\n",
       "        [[ 0.3686, -0.5562],\n",
       "         [ 0.3686, -0.5562],\n",
       "         [ 1.2637, -0.5048]],\n",
       "\n",
       "        [[ 0.3686, -0.5562],\n",
       "         [ 1.2637, -0.5048],\n",
       "         [ 1.3371,  0.6430]],\n",
       "\n",
       "        [[ 1.2637, -0.5048],\n",
       "         [ 1.3371,  0.6430],\n",
       "         [ 1.2637, -0.5048]],\n",
       "\n",
       "        [[ 0.3686, -0.5562],\n",
       "         [ 0.3686, -0.5562],\n",
       "         [ 0.3686, -0.5562]],\n",
       "\n",
       "        [[ 0.3686, -0.5562],\n",
       "         [ 0.3686, -0.5562],\n",
       "         [ 0.6837, -0.6760]],\n",
       "\n",
       "        [[ 0.3686, -0.5562],\n",
       "         [ 0.6837, -0.6760],\n",
       "         [-0.1643,  0.7629]],\n",
       "\n",
       "        [[ 0.6837, -0.6760],\n",
       "         [-0.1643,  0.7629],\n",
       "         [ 1.2637, -0.5048]],\n",
       "\n",
       "        [[-0.1643,  0.7629],\n",
       "         [ 1.2637, -0.5048],\n",
       "         [-1.4296, -0.3214]],\n",
       "\n",
       "        [[ 1.2637, -0.5048],\n",
       "         [-1.4296, -0.3214],\n",
       "         [ 0.8174,  0.4805]],\n",
       "\n",
       "        [[-1.4296, -0.3214],\n",
       "         [ 0.8174,  0.4805],\n",
       "         [-1.6356,  0.4328]],\n",
       "\n",
       "        [[ 0.8174,  0.4805],\n",
       "         [-1.6356,  0.4328],\n",
       "         [-1.6356,  0.4328]],\n",
       "\n",
       "        [[-1.6356,  0.4328],\n",
       "         [-1.6356,  0.4328],\n",
       "         [ 1.2637, -0.5048]],\n",
       "\n",
       "        [[ 0.3686, -0.5562],\n",
       "         [ 0.3686, -0.5562],\n",
       "         [ 0.3686, -0.5562]],\n",
       "\n",
       "        [[ 0.3686, -0.5562],\n",
       "         [ 0.3686, -0.5562],\n",
       "         [-0.1643,  0.7629]],\n",
       "\n",
       "        [[ 0.3686, -0.5562],\n",
       "         [-0.1643,  0.7629],\n",
       "         [ 1.7940, -1.0061]],\n",
       "\n",
       "        [[-0.1643,  0.7629],\n",
       "         [ 1.7940, -1.0061],\n",
       "         [-1.0558,  0.4111]],\n",
       "\n",
       "        [[ 1.7940, -1.0061],\n",
       "         [-1.0558,  0.4111],\n",
       "         [ 1.3065,  0.3617]],\n",
       "\n",
       "        [[-1.0558,  0.4111],\n",
       "         [ 1.3065,  0.3617],\n",
       "         [ 0.6837, -0.6760]],\n",
       "\n",
       "        [[ 1.3065,  0.3617],\n",
       "         [ 0.6837, -0.6760],\n",
       "         [ 1.2637, -0.5048]]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Embed the entire input\n",
    "emb: Tensor = C[X]\n",
    "print(f\"{emb.shape=}\")\n",
    "emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emb1.shape=torch.Size([32, 3, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3686, -0.5562],\n",
       "         [ 0.3686, -0.5562],\n",
       "         [ 0.3686, -0.5562]],\n",
       "\n",
       "        [[ 0.3686, -0.5562],\n",
       "         [ 0.3686, -0.5562],\n",
       "         [ 0.8174,  0.4805]],\n",
       "\n",
       "        [[ 0.3686, -0.5562],\n",
       "         [ 0.8174,  0.4805],\n",
       "         [ 1.3866, -1.3432]],\n",
       "\n",
       "        [[ 0.8174,  0.4805],\n",
       "         [ 1.3866, -1.3432],\n",
       "         [ 1.3866, -1.3432]],\n",
       "\n",
       "        [[ 1.3866, -1.3432],\n",
       "         [ 1.3866, -1.3432],\n",
       "         [ 1.2637, -0.5048]],\n",
       "\n",
       "        [[ 0.3686, -0.5562],\n",
       "         [ 0.3686, -0.5562],\n",
       "         [ 0.3686, -0.5562]],\n",
       "\n",
       "        [[ 0.3686, -0.5562],\n",
       "         [ 0.3686, -0.5562],\n",
       "         [ 1.7940, -1.0061]],\n",
       "\n",
       "        [[ 0.3686, -0.5562],\n",
       "         [ 1.7940, -1.0061],\n",
       "         [-1.6356,  0.4328]],\n",
       "\n",
       "        [[ 1.7940, -1.0061],\n",
       "         [-1.6356,  0.4328],\n",
       "         [ 0.6837, -0.6760]],\n",
       "\n",
       "        [[-1.6356,  0.4328],\n",
       "         [ 0.6837, -0.6760],\n",
       "         [ 1.3371,  0.6430]],\n",
       "\n",
       "        [[ 0.6837, -0.6760],\n",
       "         [ 1.3371,  0.6430],\n",
       "         [ 0.6837, -0.6760]],\n",
       "\n",
       "        [[ 1.3371,  0.6430],\n",
       "         [ 0.6837, -0.6760],\n",
       "         [ 1.2637, -0.5048]],\n",
       "\n",
       "        [[ 0.3686, -0.5562],\n",
       "         [ 0.3686, -0.5562],\n",
       "         [ 0.3686, -0.5562]],\n",
       "\n",
       "        [[ 0.3686, -0.5562],\n",
       "         [ 0.3686, -0.5562],\n",
       "         [ 1.2637, -0.5048]],\n",
       "\n",
       "        [[ 0.3686, -0.5562],\n",
       "         [ 1.2637, -0.5048],\n",
       "         [ 1.3371,  0.6430]],\n",
       "\n",
       "        [[ 1.2637, -0.5048],\n",
       "         [ 1.3371,  0.6430],\n",
       "         [ 1.2637, -0.5048]],\n",
       "\n",
       "        [[ 0.3686, -0.5562],\n",
       "         [ 0.3686, -0.5562],\n",
       "         [ 0.3686, -0.5562]],\n",
       "\n",
       "        [[ 0.3686, -0.5562],\n",
       "         [ 0.3686, -0.5562],\n",
       "         [ 0.6837, -0.6760]],\n",
       "\n",
       "        [[ 0.3686, -0.5562],\n",
       "         [ 0.6837, -0.6760],\n",
       "         [-0.1643,  0.7629]],\n",
       "\n",
       "        [[ 0.6837, -0.6760],\n",
       "         [-0.1643,  0.7629],\n",
       "         [ 1.2637, -0.5048]],\n",
       "\n",
       "        [[-0.1643,  0.7629],\n",
       "         [ 1.2637, -0.5048],\n",
       "         [-1.4296, -0.3214]],\n",
       "\n",
       "        [[ 1.2637, -0.5048],\n",
       "         [-1.4296, -0.3214],\n",
       "         [ 0.8174,  0.4805]],\n",
       "\n",
       "        [[-1.4296, -0.3214],\n",
       "         [ 0.8174,  0.4805],\n",
       "         [-1.6356,  0.4328]],\n",
       "\n",
       "        [[ 0.8174,  0.4805],\n",
       "         [-1.6356,  0.4328],\n",
       "         [-1.6356,  0.4328]],\n",
       "\n",
       "        [[-1.6356,  0.4328],\n",
       "         [-1.6356,  0.4328],\n",
       "         [ 1.2637, -0.5048]],\n",
       "\n",
       "        [[ 0.3686, -0.5562],\n",
       "         [ 0.3686, -0.5562],\n",
       "         [ 0.3686, -0.5562]],\n",
       "\n",
       "        [[ 0.3686, -0.5562],\n",
       "         [ 0.3686, -0.5562],\n",
       "         [-0.1643,  0.7629]],\n",
       "\n",
       "        [[ 0.3686, -0.5562],\n",
       "         [-0.1643,  0.7629],\n",
       "         [ 1.7940, -1.0061]],\n",
       "\n",
       "        [[-0.1643,  0.7629],\n",
       "         [ 1.7940, -1.0061],\n",
       "         [-1.0558,  0.4111]],\n",
       "\n",
       "        [[ 1.7940, -1.0061],\n",
       "         [-1.0558,  0.4111],\n",
       "         [ 1.3065,  0.3617]],\n",
       "\n",
       "        [[-1.0558,  0.4111],\n",
       "         [ 1.3065,  0.3617],\n",
       "         [ 0.6837, -0.6760]],\n",
       "\n",
       "        [[ 1.3065,  0.3617],\n",
       "         [ 0.6837, -0.6760],\n",
       "         [ 1.2637, -0.5048]]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OR\n",
    "# Method 2\n",
    "emb1: Tensor = F.one_hot(X, num_classes=n_chars).float() @ C\n",
    "print(f\"{emb1.shape=}\")\n",
    "emb1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.equal(emb1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 2])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.3686, -0.5562],\n",
       "         [ 0.3686, -0.5562],\n",
       "         [ 0.8174,  0.4805]]),\n",
       " torch.Size([3, 2]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb[1], emb[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.2637, -0.5048])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb[13, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2: Build The Hidden Layer\n",
    "W1: Tensor = torch.randn((6, 100))\n",
    "b1: Tensor = torch.randn(100)\n",
    "\n",
    "# Forward pass\n",
    "h: Tensor = emb.view(-1, 6) @ W1 + b1\n",
    "# Apply a non-linearity\n",
    "h = torch.tanh(h)\n",
    "h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 3: Output Layer\n",
    "W2: Tensor = torch.randn(h.shape[-1], n_chars, requires_grad=True)  # (100, 27)\n",
    "b2: Tensor = torch.zeros(n_chars, requires_grad=True)  # (27,)\n",
    "\n",
    "logits: Tensor = torch.matmul(h, W2) + b2  # h @ W2 + b2\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1., grad_fn=<SumBackward0>), tensor(1., grad_fn=<SumBackward0>))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert logits to `fake` counts\n",
    "counts: Tensor = logits.exp()\n",
    "# Normalize: Apply Softmax\n",
    "probs: Tensor = counts / counts.sum(dim=-1, keepdim=True)\n",
    "\n",
    "probs[0].sum(), probs[15].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(14.3851, grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Negative log likelihood loss\n",
    "loss: Tensor = -1 * probs[torch.arange(logits.shape[0]), y].log().mean()\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine The Previous Cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "X.shape=torch.Size([228152, 3]), Y.shape=torch.Size([228152])\n"
     ]
    }
   ],
   "source": [
    "X, y = build_dataset(names=names, block_size=3, print_info=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 63473,  63199,   2996, 138850, 144712, 196523,  68133, 123124, 140752,\n",
       "         11876,  48565,  55557, 193809, 134738, 142102,  71058,  52869,  56823,\n",
       "        109774,  43877, 138688, 223131, 156774, 144497, 214777, 147728,   2755,\n",
       "         43798,  97593,  56453,  69142, 166838])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randint(0, X.shape[0], size=(32,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C.shape = torch.Size([27, 2])\n",
      "n_parameters = 3,481\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(5)\n",
    "\n",
    "# Parameters\n",
    "emb_dim: int = 2  # embedding dimension\n",
    "n_nodes: int = 100  # number of hidden nodes\n",
    "learning_rate: float = 0.1  # learning rate\n",
    "batch_size: int = 32  # batch size\n",
    "epochs: int = 30  # number of epochs\n",
    "C: Tensor = torch.randn((n_chars, emb_dim), generator=g)\n",
    "W1: Tensor = torch.randn((6, n_nodes), generator=g)\n",
    "b1: Tensor = torch.randn(n_nodes, generator=g)\n",
    "W2: Tensor = torch.randn(n_nodes, n_chars, generator=g)  # (100, 27)\n",
    "b2: Tensor = torch.randn(n_chars, generator=g)  # (27,)\n",
    "print(f\"{C.shape = }\")\n",
    "parameters: list[Tensor] = [C, W1, b1, W2, b2]\n",
    "n_parameters: int = sum([p.nelement() for p in parameters])\n",
    "print(f\"{n_parameters = :,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Require gradients to be true\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0011,\n",
       "        0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011,\n",
       "        0.0011, 0.0011, 0.0011, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012,\n",
       "        0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0013, 0.0013, 0.0013,\n",
       "        0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0014,\n",
       "        0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014,\n",
       "        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n",
       "        0.0015, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016,\n",
       "        0.0016, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017,\n",
       "        0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0019,\n",
       "        0.0019, 0.0019, 0.0019, 0.0019, 0.0019, 0.0019, 0.0019, 0.0020, 0.0020,\n",
       "        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0021, 0.0021, 0.0021, 0.0021,\n",
       "        0.0021, 0.0021, 0.0021, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022,\n",
       "        0.0022, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0024, 0.0024,\n",
       "        0.0024, 0.0024, 0.0024, 0.0024, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
       "        0.0025, 0.0026, 0.0026, 0.0026, 0.0026, 0.0026, 0.0027, 0.0027, 0.0027,\n",
       "        0.0027, 0.0027, 0.0027, 0.0028, 0.0028, 0.0028, 0.0028, 0.0028, 0.0029,\n",
       "        0.0029, 0.0029, 0.0029, 0.0029, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030,\n",
       "        0.0031, 0.0031, 0.0031, 0.0031, 0.0032, 0.0032, 0.0032, 0.0032, 0.0032,\n",
       "        0.0033, 0.0033, 0.0033, 0.0033, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034,\n",
       "        0.0035, 0.0035, 0.0035, 0.0035, 0.0036, 0.0036, 0.0036, 0.0036, 0.0037,\n",
       "        0.0037, 0.0037, 0.0037, 0.0038, 0.0038, 0.0038, 0.0039, 0.0039, 0.0039,\n",
       "        0.0039, 0.0040, 0.0040, 0.0040, 0.0040, 0.0041, 0.0041, 0.0041, 0.0042,\n",
       "        0.0042, 0.0042, 0.0042, 0.0043, 0.0043, 0.0043, 0.0044, 0.0044, 0.0044,\n",
       "        0.0045, 0.0045, 0.0045, 0.0045, 0.0046, 0.0046, 0.0046, 0.0047, 0.0047,\n",
       "        0.0047, 0.0048, 0.0048, 0.0048, 0.0049, 0.0049, 0.0049, 0.0050, 0.0050,\n",
       "        0.0050, 0.0051, 0.0051, 0.0051, 0.0052, 0.0052, 0.0053, 0.0053, 0.0053,\n",
       "        0.0054, 0.0054, 0.0054, 0.0055, 0.0055, 0.0056, 0.0056, 0.0056, 0.0057,\n",
       "        0.0057, 0.0058, 0.0058, 0.0058, 0.0059, 0.0059, 0.0060, 0.0060, 0.0060,\n",
       "        0.0061, 0.0061, 0.0062, 0.0062, 0.0062, 0.0063, 0.0063, 0.0064, 0.0064,\n",
       "        0.0065, 0.0065, 0.0066, 0.0066, 0.0067, 0.0067, 0.0067, 0.0068, 0.0068,\n",
       "        0.0069, 0.0069, 0.0070, 0.0070, 0.0071, 0.0071, 0.0072, 0.0072, 0.0073,\n",
       "        0.0073, 0.0074, 0.0074, 0.0075, 0.0075, 0.0076, 0.0076, 0.0077, 0.0077,\n",
       "        0.0078, 0.0079, 0.0079, 0.0080, 0.0080, 0.0081, 0.0081, 0.0082, 0.0082,\n",
       "        0.0083, 0.0084, 0.0084, 0.0085, 0.0085, 0.0086, 0.0086, 0.0087, 0.0088,\n",
       "        0.0088, 0.0089, 0.0090, 0.0090, 0.0091, 0.0091, 0.0092, 0.0093, 0.0093,\n",
       "        0.0094, 0.0095, 0.0095, 0.0096, 0.0097, 0.0097, 0.0098, 0.0099, 0.0099,\n",
       "        0.0100, 0.0101, 0.0101, 0.0102, 0.0103, 0.0104, 0.0104, 0.0105, 0.0106,\n",
       "        0.0106, 0.0107, 0.0108, 0.0109, 0.0109, 0.0110, 0.0111, 0.0112, 0.0112,\n",
       "        0.0113, 0.0114, 0.0115, 0.0116, 0.0116, 0.0117, 0.0118, 0.0119, 0.0120,\n",
       "        0.0121, 0.0121, 0.0122, 0.0123, 0.0124, 0.0125, 0.0126, 0.0127, 0.0127,\n",
       "        0.0128, 0.0129, 0.0130, 0.0131, 0.0132, 0.0133, 0.0134, 0.0135, 0.0136,\n",
       "        0.0137, 0.0137, 0.0138, 0.0139, 0.0140, 0.0141, 0.0142, 0.0143, 0.0144,\n",
       "        0.0145, 0.0146, 0.0147, 0.0148, 0.0149, 0.0150, 0.0151, 0.0152, 0.0154,\n",
       "        0.0155, 0.0156, 0.0157, 0.0158, 0.0159, 0.0160, 0.0161, 0.0162, 0.0163,\n",
       "        0.0165, 0.0166, 0.0167, 0.0168, 0.0169, 0.0170, 0.0171, 0.0173, 0.0174,\n",
       "        0.0175, 0.0176, 0.0178, 0.0179, 0.0180, 0.0181, 0.0182, 0.0184, 0.0185,\n",
       "        0.0186, 0.0188, 0.0189, 0.0190, 0.0192, 0.0193, 0.0194, 0.0196, 0.0197,\n",
       "        0.0198, 0.0200, 0.0201, 0.0202, 0.0204, 0.0205, 0.0207, 0.0208, 0.0210,\n",
       "        0.0211, 0.0212, 0.0214, 0.0215, 0.0217, 0.0218, 0.0220, 0.0221, 0.0223,\n",
       "        0.0225, 0.0226, 0.0228, 0.0229, 0.0231, 0.0232, 0.0234, 0.0236, 0.0237,\n",
       "        0.0239, 0.0241, 0.0242, 0.0244, 0.0246, 0.0247, 0.0249, 0.0251, 0.0253,\n",
       "        0.0254, 0.0256, 0.0258, 0.0260, 0.0261, 0.0263, 0.0265, 0.0267, 0.0269,\n",
       "        0.0271, 0.0273, 0.0274, 0.0276, 0.0278, 0.0280, 0.0282, 0.0284, 0.0286,\n",
       "        0.0288, 0.0290, 0.0292, 0.0294, 0.0296, 0.0298, 0.0300, 0.0302, 0.0304,\n",
       "        0.0307, 0.0309, 0.0311, 0.0313, 0.0315, 0.0317, 0.0320, 0.0322, 0.0324,\n",
       "        0.0326, 0.0328, 0.0331, 0.0333, 0.0335, 0.0338, 0.0340, 0.0342, 0.0345,\n",
       "        0.0347, 0.0350, 0.0352, 0.0354, 0.0357, 0.0359, 0.0362, 0.0364, 0.0367,\n",
       "        0.0369, 0.0372, 0.0375, 0.0377, 0.0380, 0.0382, 0.0385, 0.0388, 0.0390,\n",
       "        0.0393, 0.0396, 0.0399, 0.0401, 0.0404, 0.0407, 0.0410, 0.0413, 0.0416,\n",
       "        0.0418, 0.0421, 0.0424, 0.0427, 0.0430, 0.0433, 0.0436, 0.0439, 0.0442,\n",
       "        0.0445, 0.0448, 0.0451, 0.0455, 0.0458, 0.0461, 0.0464, 0.0467, 0.0471,\n",
       "        0.0474, 0.0477, 0.0480, 0.0484, 0.0487, 0.0491, 0.0494, 0.0497, 0.0501,\n",
       "        0.0504, 0.0508, 0.0511, 0.0515, 0.0518, 0.0522, 0.0526, 0.0529, 0.0533,\n",
       "        0.0537, 0.0540, 0.0544, 0.0548, 0.0552, 0.0556, 0.0559, 0.0563, 0.0567,\n",
       "        0.0571, 0.0575, 0.0579, 0.0583, 0.0587, 0.0591, 0.0595, 0.0599, 0.0604,\n",
       "        0.0608, 0.0612, 0.0616, 0.0621, 0.0625, 0.0629, 0.0634, 0.0638, 0.0642,\n",
       "        0.0647, 0.0651, 0.0656, 0.0660, 0.0665, 0.0670, 0.0674, 0.0679, 0.0684,\n",
       "        0.0688, 0.0693, 0.0698, 0.0703, 0.0708, 0.0713, 0.0718, 0.0723, 0.0728,\n",
       "        0.0733, 0.0738, 0.0743, 0.0748, 0.0753, 0.0758, 0.0764, 0.0769, 0.0774,\n",
       "        0.0780, 0.0785, 0.0790, 0.0796, 0.0802, 0.0807, 0.0813, 0.0818, 0.0824,\n",
       "        0.0830, 0.0835, 0.0841, 0.0847, 0.0853, 0.0859, 0.0865, 0.0871, 0.0877,\n",
       "        0.0883, 0.0889, 0.0895, 0.0901, 0.0908, 0.0914, 0.0920, 0.0927, 0.0933,\n",
       "        0.0940, 0.0946, 0.0953, 0.0959, 0.0966, 0.0973, 0.0979, 0.0986, 0.0993,\n",
       "        0.1000, 0.1007, 0.1014, 0.1021, 0.1028, 0.1035, 0.1042, 0.1050, 0.1057,\n",
       "        0.1064, 0.1072, 0.1079, 0.1087, 0.1094, 0.1102, 0.1109, 0.1117, 0.1125,\n",
       "        0.1133, 0.1140, 0.1148, 0.1156, 0.1164, 0.1172, 0.1181, 0.1189, 0.1197,\n",
       "        0.1205, 0.1214, 0.1222, 0.1231, 0.1239, 0.1248, 0.1256, 0.1265, 0.1274,\n",
       "        0.1283, 0.1292, 0.1301, 0.1310, 0.1319, 0.1328, 0.1337, 0.1346, 0.1356,\n",
       "        0.1365, 0.1374, 0.1384, 0.1394, 0.1403, 0.1413, 0.1423, 0.1433, 0.1443,\n",
       "        0.1453, 0.1463, 0.1473, 0.1483, 0.1493, 0.1504, 0.1514, 0.1525, 0.1535,\n",
       "        0.1546, 0.1557, 0.1567, 0.1578, 0.1589, 0.1600, 0.1611, 0.1623, 0.1634,\n",
       "        0.1645, 0.1657, 0.1668, 0.1680, 0.1691, 0.1703, 0.1715, 0.1727, 0.1739,\n",
       "        0.1751, 0.1763, 0.1775, 0.1788, 0.1800, 0.1812, 0.1825, 0.1838, 0.1850,\n",
       "        0.1863, 0.1876, 0.1889, 0.1902, 0.1916, 0.1929, 0.1942, 0.1956, 0.1969,\n",
       "        0.1983, 0.1997, 0.2010, 0.2024, 0.2038, 0.2053, 0.2067, 0.2081, 0.2096,\n",
       "        0.2110, 0.2125, 0.2140, 0.2154, 0.2169, 0.2184, 0.2200, 0.2215, 0.2230,\n",
       "        0.2246, 0.2261, 0.2277, 0.2293, 0.2309, 0.2325, 0.2341, 0.2357, 0.2373,\n",
       "        0.2390, 0.2406, 0.2423, 0.2440, 0.2457, 0.2474, 0.2491, 0.2508, 0.2526,\n",
       "        0.2543, 0.2561, 0.2579, 0.2597, 0.2615, 0.2633, 0.2651, 0.2669, 0.2688,\n",
       "        0.2707, 0.2725, 0.2744, 0.2763, 0.2783, 0.2802, 0.2821, 0.2841, 0.2861,\n",
       "        0.2880, 0.2900, 0.2921, 0.2941, 0.2961, 0.2982, 0.3002, 0.3023, 0.3044,\n",
       "        0.3065, 0.3087, 0.3108, 0.3130, 0.3151, 0.3173, 0.3195, 0.3217, 0.3240,\n",
       "        0.3262, 0.3285, 0.3308, 0.3331, 0.3354, 0.3377, 0.3400, 0.3424, 0.3448,\n",
       "        0.3472, 0.3496, 0.3520, 0.3544, 0.3569, 0.3594, 0.3619, 0.3644, 0.3669,\n",
       "        0.3695, 0.3720, 0.3746, 0.3772, 0.3798, 0.3825, 0.3851, 0.3878, 0.3905,\n",
       "        0.3932, 0.3959, 0.3987, 0.4014, 0.4042, 0.4070, 0.4098, 0.4127, 0.4155,\n",
       "        0.4184, 0.4213, 0.4243, 0.4272, 0.4302, 0.4331, 0.4362, 0.4392, 0.4422,\n",
       "        0.4453, 0.4484, 0.4515, 0.4546, 0.4578, 0.4610, 0.4642, 0.4674, 0.4706,\n",
       "        0.4739, 0.4772, 0.4805, 0.4838, 0.4872, 0.4906, 0.4940, 0.4974, 0.5008,\n",
       "        0.5043, 0.5078, 0.5113, 0.5149, 0.5185, 0.5221, 0.5257, 0.5293, 0.5330,\n",
       "        0.5367, 0.5404, 0.5442, 0.5479, 0.5517, 0.5556, 0.5594, 0.5633, 0.5672,\n",
       "        0.5712, 0.5751, 0.5791, 0.5831, 0.5872, 0.5913, 0.5954, 0.5995, 0.6036,\n",
       "        0.6078, 0.6120, 0.6163, 0.6206, 0.6249, 0.6292, 0.6336, 0.6380, 0.6424,\n",
       "        0.6469, 0.6513, 0.6559, 0.6604, 0.6650, 0.6696, 0.6743, 0.6789, 0.6837,\n",
       "        0.6884, 0.6932, 0.6980, 0.7028, 0.7077, 0.7126, 0.7176, 0.7225, 0.7275,\n",
       "        0.7326, 0.7377, 0.7428, 0.7480, 0.7531, 0.7584, 0.7636, 0.7689, 0.7743,\n",
       "        0.7796, 0.7850, 0.7905, 0.7960, 0.8015, 0.8071, 0.8127, 0.8183, 0.8240,\n",
       "        0.8297, 0.8355, 0.8412, 0.8471, 0.8530, 0.8589, 0.8648, 0.8708, 0.8769,\n",
       "        0.8830, 0.8891, 0.8953, 0.9015, 0.9077, 0.9140, 0.9204, 0.9268, 0.9332,\n",
       "        0.9397, 0.9462, 0.9528, 0.9594, 0.9660, 0.9727, 0.9795, 0.9863, 0.9931,\n",
       "        1.0000])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the optimal learning rate\n",
    "lr: Tensor = torch.linspace(-3, 0, 1000)\n",
    "lr = 10**lr  # 10^-3, 10^-2, ..., 10^0\n",
    "lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/30 | Loss: 16.4787\n",
      "Epoch: 2/30 | Loss: 14.6044\n",
      "Epoch: 3/30 | Loss: 12.8515\n",
      "Epoch: 4/30 | Loss: 12.8096\n",
      "Epoch: 5/30 | Loss: 12.0617\n",
      "Epoch: 6/30 | Loss: 10.9320\n",
      "Epoch: 7/30 | Loss: 11.5245\n",
      "Epoch: 8/30 | Loss: 9.9370\n",
      "Epoch: 9/30 | Loss: 9.8197\n",
      "Epoch: 10/30 | Loss: 9.3507\n",
      "Epoch: 11/30 | Loss: 7.6521\n",
      "Epoch: 12/30 | Loss: 8.3687\n",
      "Epoch: 13/30 | Loss: 9.6624\n",
      "Epoch: 14/30 | Loss: 7.0304\n",
      "Epoch: 15/30 | Loss: 5.6843\n",
      "Epoch: 16/30 | Loss: 7.0956\n",
      "Epoch: 17/30 | Loss: 7.3071\n",
      "Epoch: 18/30 | Loss: 7.2287\n",
      "Epoch: 19/30 | Loss: 7.8393\n",
      "Epoch: 20/30 | Loss: 7.2934\n",
      "Epoch: 21/30 | Loss: 7.7770\n",
      "Epoch: 22/30 | Loss: 6.1947\n",
      "Epoch: 23/30 | Loss: 5.7256\n",
      "Epoch: 24/30 | Loss: 6.0679\n",
      "Epoch: 25/30 | Loss: 6.6677\n",
      "Epoch: 26/30 | Loss: 5.1784\n",
      "Epoch: 27/30 | Loss: 5.1976\n",
      "Epoch: 28/30 | Loss: 5.7453\n",
      "Epoch: 29/30 | Loss: 5.4178\n",
      "Epoch: 30/30 | Loss: 6.5563\n"
     ]
    }
   ],
   "source": [
    "lr_all: list[float] = []\n",
    "losses_all: list[float] = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # Add mini-batches\n",
    "    idx: Tensor = torch.randint(0, X.shape[0], size=(batch_size,))\n",
    "\n",
    "    # Step 1: Build an embedding lookup table\n",
    "    # Embed the input\n",
    "    emb: Tensor = F.one_hot(X[idx], num_classes=n_chars).float() @ C\n",
    "\n",
    "    # Forward pass\n",
    "    # Step 2: Build The Hidden Layer\n",
    "    # Reshape the input to match the shape of the weight matrix\n",
    "    h: Tensor = emb.view(-1, 6) @ W1 + b1\n",
    "    # Apply a non-linearity\n",
    "    h = torch.tanh(h)\n",
    "\n",
    "    # Step 3: Output Layer\n",
    "    logits: Tensor = torch.matmul(h, W2) + b2  # h @ W2 + b2\n",
    "\n",
    "    # Calculate the loss: Negative log likelihood loss\n",
    "    loss: Tensor = F.cross_entropy(logits, y[idx])\n",
    "\n",
    "    # Backward pass\n",
    "    # Reset gradients\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the parameters\n",
    "    for p in parameters:\n",
    "        p.data -= learning_rate * p.grad\n",
    "\n",
    "    print(f\"Epoch: {epoch+1}/{epochs} | Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_p11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
