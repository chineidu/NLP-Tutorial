{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Makemore: Wavenet\n",
    "\n",
    "- [Andrej Karpathy YouTube](https://www.youtube.com/watch?v=t3YJ5hKiMQ0&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&index=6&ab_channel=AndrejKarpathy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python implementation: CPython\n",
      "Python version       : 3.11.8\n",
      "IPython version      : 8.22.2\n",
      "\n",
      "numpy    : 1.26.4\n",
      "pandas   : 2.2.1\n",
      "polars   : 0.20.18\n",
      "torch    : 2.2.2\n",
      "lightning: 2.2.1\n",
      "\n",
      "conda environment: torch_p11\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -v -p numpy,pandas,polars,torch,lightning --conda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in library\n",
    "from pathlib import Path\n",
    "import re\n",
    "import json\n",
    "from typing import Any, Optional, Union\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "# Standard imports\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from rich.console import Console\n",
    "from rich.theme import Theme\n",
    "\n",
    "custom_theme = Theme(\n",
    "    {\n",
    "        \"info\": \"#76FF7B\",\n",
    "        \"warning\": \"#FBDDFE\",\n",
    "        \"error\": \"#FF0000\",\n",
    "    }\n",
    ")\n",
    "console = Console(theme=custom_theme)\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Pandas settings\n",
    "pd.options.display.max_rows = 1_000\n",
    "pd.options.display.max_columns = 1_000\n",
    "pd.options.display.max_colwidth = 600\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# Black code formatter (Optional)\n",
    "%load_ext lab_black\n",
    "\n",
    "# auto reload imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path: str) -> list[str]:\n",
    "    \"\"\"Load text data from a file and return as a list of strings.\"\"\"\n",
    "\n",
    "    with open(file_path, \"r\") as f:\n",
    "        # Read all the lines as a list\n",
    "        data: list[str] = f.read().splitlines()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Data\n",
    "fp: str = \"../../../data/names.txt\"\n",
    "names: list[str] = load_data(file_path=fp)\n",
    "\n",
    "names[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Vocabulary Of Characters And Mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_token: str = \".\"\n",
    "characters: list[str] = sorted(set(\"\".join(names)))\n",
    "# Add the special token to the beginning of the list.\n",
    "characters.insert(0, special_token)\n",
    "n_chars: int = len(characters)\n",
    "\n",
    "# Convert text to numbers.\n",
    "text_to_num: dict[str, int] = {text: idx for idx, text in enumerate(characters)}\n",
    "# Convert numbers to text\n",
    "num_to_text: dict[int, str] = {idx: text for text, idx in text_to_num.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split, TensorDataset, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def build_dataset(\n",
    "    names: list[str],\n",
    "    special_token: str = \".\",\n",
    "    block_size: int = 3,\n",
    "    print_info: bool = False,\n",
    ") -> tuple[Tensor, Tensor]:\n",
    "    \"\"\"\n",
    "    Builds a dataset of name sequences and their corresponding character indices.\n",
    "\n",
    "    Args:\n",
    "        names (list[str]): A list of names to build the dataset from.\n",
    "        special_token (str, optional): A special token to append to the end of each name. Defaults to \".\".\n",
    "        block_size (int, optional): The size of the context window for each input sequence. Defaults to 3.\n",
    "        print_info (bool, optional): Whether to print information about the dataset generation. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        tuple[Tensor, Tensor]: A tuple containing the input sequences (X) and their corresponding target indices (Y).\n",
    "    \"\"\"\n",
    "    X, Y = [], []\n",
    "\n",
    "    for w in names:\n",
    "        if print_info:\n",
    "            print(w)\n",
    "        context: list[str] = [0] * block_size\n",
    "\n",
    "        for ch in w + special_token:\n",
    "            ix: int = text_to_num.get(ch)\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "\n",
    "            if print_info:\n",
    "                print(\n",
    "                    f\"{''.join([num_to_text.get(i) for i in context])} ---> {num_to_text.get(ix)}\"\n",
    "                )\n",
    "\n",
    "            # Crop and append, like a rolling window\n",
    "            context = context[1:] + [ix]\n",
    "\n",
    "    X: Tensor = torch.tensor(X)\n",
    "    Y: Tensor = torch.tensor(Y)\n",
    "    print(f\"\\n{X.shape=}, {Y.shape=}\")\n",
    "    return (X, Y)\n",
    "\n",
    "\n",
    "def split_data_into_train_dev_test(\n",
    "    data: Tensor | Dataset, test_size: float = 0.05, dev_size: float = 0.1, seed=42\n",
    ") -> tuple[Tensor, ...]:\n",
    "    \"\"\"\n",
    "    Splits a given PyTorch tensor `data` into training, development, and test sets.\n",
    "\n",
    "    Params:\n",
    "    -------\n",
    "        data (torch.Tensor): The input tensor to be split.\n",
    "        test_size (float, optional): The fraction of the data to use for the test set. Defaults to 0.2.\n",
    "        dev_size (float, optional): The fraction of the data to use for the development set. Defaults to 0.1.\n",
    "        seed (int, optional): The random seed to use for reproducibility. Defaults to 42.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "        tuple[torch.Tensor, torch.Tensor, torch.Tensor]: The training, development, and test sets as PyTorch tensors.\n",
    "    \"\"\"\n",
    "    if isinstance(data, Tensor):\n",
    "        X_train, X_test = train_test_split(data, test_size=test_size, random_state=seed)\n",
    "        X_train, X_dev = train_test_split(\n",
    "            X_train, test_size=dev_size, random_state=seed\n",
    "        )\n",
    "        result: tuple[Tensor, ...] = (X_train, X_dev, X_test)\n",
    "    if isinstance(data, Dataset):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            data.data,\n",
    "            data.targets,\n",
    "            test_size=test_size,\n",
    "            random_state=seed,\n",
    "            stratify=data.targets,\n",
    "        )\n",
    "        X_train, X_dev, y_train, y_dev = train_test_split(\n",
    "            X_train, y_train, test_size=dev_size, random_state=seed, stratify=y_train\n",
    "        )\n",
    "        result: tuple[Tensor, ...] = (X_train, X_dev, X_test, y_train, y_dev, y_test)\n",
    "\n",
    "    print(f\"{X_train.shape=}; {X_dev.shape=}; {X_test.shape=}\")\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data: Tensor, targets: Tensor) -> None:\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return (\n",
    "            f\"{self.__class__.__name__}(data.shape={self.data.shape}, \"\n",
    "            f\"target.shape={self.targets.shape=})\"\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx]\n",
    "        y = self.targets[idx]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "\n",
    "class CustomModule(ABC):\n",
    "    @abstractmethod\n",
    "    def __init__(self) -> None:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def __call__(self, x: Tensor) -> Tensor:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def parameters(self) -> list[Tensor]:\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class Linear(CustomModule):\n",
    "    \"\"\"\n",
    "    A linear layer implementation.\n",
    "\n",
    "    This class implements a linear layer, which performs a linear transformation on the input tensor. It takes in the number\n",
    "    of input features and output features, and optionally a bias term. The weights and biases are initialized randomly.\n",
    "\n",
    "    The `__call__` method applies the linear transformation to the input tensor and returns the output tensor.\n",
    "\n",
    "    The `parameters` method returns a list of the learnable parameters (weights and biases) of the layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, in_features: int, out_features: int, bias: bool = True, seed: int = 42\n",
    "    ) -> None:\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        # Kaiming's initialization\n",
    "        self.weight = (\n",
    "            torch.randn(in_features, out_features, generator=torch.manual_seed(seed))\n",
    "            / in_features**0.5\n",
    "        )\n",
    "        self.bias = (\n",
    "            torch.randn(out_features, generator=torch.manual_seed(seed))\n",
    "            if bias\n",
    "            else None\n",
    "        )\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{self.__class__.__name__}({self.in_features}, {self.out_features})\"\n",
    "\n",
    "    def __call__(self, x: Tensor) -> Tensor:\n",
    "        self.output: Tensor = torch.matmul(x, self.weight)\n",
    "        if self.bias is not None:\n",
    "            self.output += self.bias\n",
    "        return self.output\n",
    "\n",
    "    def parameters(self) -> list[Tensor]:\n",
    "        return [self.weight] + ([self.bias] if self.bias is not None else [])\n",
    "\n",
    "\n",
    "class BatchNorm1d(CustomModule):\n",
    "    def __init__(self, dim: int, eps: float = 1e-5, momentum: float = 0.1) -> None:\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self.training = True\n",
    "        # Parameters (trained with backprop)\n",
    "        self.gamma = torch.ones(dim)\n",
    "        self.beta = torch.zeros(dim)\n",
    "        # Buffers (trained with running `momentum update`)\n",
    "        self.running_mean = torch.zeros(dim)\n",
    "        self.running_var = torch.ones(dim)\n",
    "\n",
    "    def __call__(self, x: Tensor) -> Tensor:\n",
    "        if self.training:\n",
    "            # Calculate the batch mean and variance\n",
    "            x_mean: Tensor = x.mean(dim=0, keepdim=True)\n",
    "            x_var: Tensor = x.var(dim=0, keepdim=True)\n",
    "        else:\n",
    "            x_mean = self.running_mean\n",
    "            x_var = self.running_var\n",
    "\n",
    "        # Normalize the input\n",
    "        x_hat: Tensor = (x - x_mean) / (x_var + self.eps).sqrt()\n",
    "        self.output: Tensor = (self.gamma * x_hat) + self.beta\n",
    "\n",
    "        if self.training:\n",
    "            with torch.no_grad():\n",
    "                # Update running mean and variance\n",
    "                self.running_mean = (\n",
    "                    1 - self.momentum\n",
    "                ) * self.running_mean + self.momentum * x_mean\n",
    "                self.running_var = (\n",
    "                    1 - self.momentum\n",
    "                ) * self.running_var + self.momentum * x_var\n",
    "\n",
    "        return self.output\n",
    "\n",
    "    def parameters(self) -> list[Tensor]:\n",
    "        return [self.gamma, self.beta]\n",
    "\n",
    "\n",
    "class Tanh(CustomModule):\n",
    "    \"\"\"A custom module that applies the hyperbolic tangent activation function to the input tensor.\"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{self.__class__.__name__}()\"\n",
    "\n",
    "    def __call__(self, x: Tensor) -> Tensor:\n",
    "        self.output = torch.tanh(x)\n",
    "        return self.output\n",
    "\n",
    "    def parameters(self) -> list[Tensor]:\n",
    "        return []\n",
    "\n",
    "\n",
    "class Embedding(CustomModule):\n",
    "    \"\"\"A custom module that creates an embedding lookup table from a given\n",
    "    vocabulary size and embedding dimension.\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size: int, embedding_dim: int, seed: int = 42) -> None:\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.weight = torch.randn(\n",
    "            (vocab_size, embedding_dim), generator=torch.manual_seed(seed)\n",
    "        )\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{self.__class__.__name__}({self.vocab_size}, {self.embedding_dim})\"\n",
    "\n",
    "    def __call__(self, idx: int) -> Tensor:\n",
    "        self.output = self.weight[idx]\n",
    "        return self.output\n",
    "\n",
    "    def parameters(self) -> list[Tensor]:\n",
    "        \"\"\"Get all the parameters.\"\"\"\n",
    "        return [self.weight]\n",
    "\n",
    "\n",
    "class Flatten(CustomModule):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{__class__.__name__}()\"\n",
    "\n",
    "    def __call__(self, x: Tensor) -> Tensor:\n",
    "        self.output = x.view(x.shape[0], -1)\n",
    "        return self.output\n",
    "\n",
    "    def parameters(self):\n",
    "        \"\"\"Get all the parameters.\"\"\"\n",
    "        return []\n",
    "\n",
    "\n",
    "class FlattenConsecutive(CustomModule):\n",
    "    \"\"\"A custom module that flattens consecutive elements in the input tensor along\n",
    "    the second dimension.\"\"\"\n",
    "\n",
    "    def __init__(self, n_dim: int) -> None:\n",
    "        self.n_dim = n_dim\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{__class__.__name__}({self.n_dim})\"\n",
    "\n",
    "    def __call__(self, x: Tensor) -> Tensor:\n",
    "        B, T, C = x.shape\n",
    "        x = x.view(B, T // self.n_dim, C * self.n_dim)\n",
    "        if x.shape[1] == 1:\n",
    "            x = x.squeeze(1)\n",
    "\n",
    "        self.output = x\n",
    "        return self.output\n",
    "\n",
    "    def parameters(self):\n",
    "        \"\"\"Get all the parameters.\"\"\"\n",
    "        return []\n",
    "\n",
    "\n",
    "class Sequential(CustomModule):\n",
    "    \"\"\"A custom module that applies a sequence of other custom modules to the input tensor.\"\"\"\n",
    "\n",
    "    def __init__(self, layers: list[CustomModule]) -> None:\n",
    "        self.layers = layers\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        self.out = x\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        \"\"\"Get parameters of all layers and stretch them out into one list.\"\"\"\n",
    "        return [p for layer in self.layers for p in layer.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emma\n",
      "... ---> e\n",
      "..e ---> m\n",
      ".em ---> m\n",
      "emm ---> a\n",
      "mma ---> .\n",
      "olivia\n",
      "... ---> o\n",
      "..o ---> l\n",
      ".ol ---> i\n",
      "oli ---> v\n",
      "liv ---> i\n",
      "ivi ---> a\n",
      "via ---> .\n",
      "ava\n",
      "... ---> a\n",
      "..a ---> v\n",
      ".av ---> a\n",
      "ava ---> .\n",
      "isabella\n",
      "... ---> i\n",
      "..i ---> s\n",
      ".is ---> a\n",
      "isa ---> b\n",
      "sab ---> e\n",
      "abe ---> l\n",
      "bel ---> l\n",
      "ell ---> a\n",
      "lla ---> .\n",
      "sophia\n",
      "... ---> s\n",
      "..s ---> o\n",
      ".so ---> p\n",
      "sop ---> h\n",
      "oph ---> i\n",
      "phi ---> a\n",
      "hia ---> .\n",
      "\n",
      "X.shape=torch.Size([32, 3]), Y.shape=torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "X, y = build_dataset(names=names[:5], block_size=3, print_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "X.shape=torch.Size([228152, 3]), Y.shape=torch.Size([228152])\n",
      "X_train.shape=torch.Size([195069, 3]); X_dev.shape=torch.Size([21675, 3]); X_test.shape=torch.Size([11408, 3])\n"
     ]
    }
   ],
   "source": [
    "X, y = build_dataset(names=names, block_size=3, print_info=False)\n",
    "data: Dataset = MyDataset(X, y)\n",
    "\n",
    "X_train, X_dev, X_test, y_train, y_dev, y_test = split_data_into_train_dev_test(\n",
    "    data=data\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment!\n",
    "\n",
    "- 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 18,024\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "g = torch.Generator().manual_seed(5)\n",
    "\n",
    "emb_dim: int = 10  # embedding dimension\n",
    "block_size: int = 3  # size of the context window for each input sequence\n",
    "M: int = emb_dim * block_size  # number of inputs\n",
    "n_nodes: int = 300  # number of hidden nodes\n",
    "learning_rate: float = 0.1  # learning rate\n",
    "batch_size: int = 32  # batch size\n",
    "epochs: int = 140_000  # number of epochs\n",
    "\n",
    "layers: list[Any] = [\n",
    "    Embedding(vocab_size=n_chars, embedding_dim=emb_dim, seed=42),\n",
    "    Flatten(),\n",
    "    Linear(in_features=M, out_features=n_nodes, bias=False),\n",
    "    BatchNorm1d(dim=n_nodes),\n",
    "    Tanh(),\n",
    "    Linear(in_features=n_nodes, out_features=n_chars, bias=False),\n",
    "    BatchNorm1d(dim=n_chars),\n",
    "]\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Make last layer less confident\n",
    "    # layers[-1].weight *= 0.1  # Default: w/o BatchNorm\n",
    "    layers[-1].gamma *= 0.1  # with BatchNorm\n",
    "\n",
    "    # Apply gain to the other layers\n",
    "    for layer in layers[:-1]:\n",
    "        if isinstance(layer, Linear):\n",
    "            # Scale the weights of all other linear layers by a gain factor.\n",
    "            # layer.weight *= 5 / 3 # Default: w/o BatchNorm\n",
    "            layer.weight *= 1.0  # with BatchNorm: No scaling\n",
    "\n",
    "# Parameters Collection\n",
    "parameters: list[Tensor] = [p for layer in layers for p in layer.parameters()]\n",
    "print(f\"Total params: {sum(p.numel() for p in parameters):,}\")\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/140000 | Loss: 3.3159\n"
     ]
    }
   ],
   "source": [
    "# Use The optimal learning rate to train the model\n",
    "losses_all: list[float] = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # Add mini-batches\n",
    "    idx: Tensor = torch.randint(0, X_train.shape[0], size=(batch_size,))\n",
    "    # X, y batch\n",
    "    Xb, yb = X_train[idx], y_train[idx]\n",
    "\n",
    "    # Forward pass\n",
    "    x: Tensor = Xb\n",
    "    for layer in layers:\n",
    "        x: Tensor = layer(x)  # Logits: Apply the linear layer\n",
    "    loss: Tensor = F.cross_entropy(x, yb)\n",
    "\n",
    "    # Backward pass\n",
    "    for p in parameters:\n",
    "        # Reset gradients\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # learning rate decay\n",
    "    learning_rate: float = (\n",
    "        0.1 if epoch < 70_000 else (0.01 if epoch < 85_000 else 0.001)\n",
    "    )\n",
    "\n",
    "    # Update the parameters\n",
    "    for p in parameters:\n",
    "        p.data -= learning_rate * p.grad\n",
    "\n",
    "    # Record the loss\n",
    "    losses_all.append(loss.item())\n",
    "\n",
    "    if (epoch) % 10_000 == 0:\n",
    "        print(f\"Epoch: {epoch}/{epochs} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "    if epoch > 100:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.7592,  1.0786,  0.8008,  1.6781,  1.2758,  1.2908,  0.6107,  1.3340,\n",
       "        -0.2326,  0.0402], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers[0](2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for layer in layers:\n",
    "#     layer.training = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kafyuw.\n",
      ".\n",
      "parales.\n",
      "zfgfixo.\n",
      "erihrbvblxf.\n"
     ]
    }
   ],
   "source": [
    "# sample from the model\n",
    "g = torch.Generator().manual_seed(5)\n",
    "n_names: int = 10\n",
    "\n",
    "for _ in range(n_names):\n",
    "\n",
    "    out: list[str] = []\n",
    "    context: list[int] = [0] * block_size  # initialize with all ...\n",
    "    while True:\n",
    "        # forward pass the neural net\n",
    "        x: Tensor = torch.tensor([context])\n",
    "\n",
    "        for layer in layers:\n",
    "            x = layer(x)\n",
    "            # break\n",
    "\n",
    "        logits: Tensor = x\n",
    "        probs: Tensor = F.softmax(logits, dim=1)\n",
    "        # sample from the distribution\n",
    "        idx: int = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "        # Shift the context window and track the samples\n",
    "        context = context[1:] + [idx]\n",
    "        out.append(idx)\n",
    "        # If we sample the special '.' token, break\n",
    "        if idx == 0:\n",
    "            break\n",
    "\n",
    "    # Decode and print the generated word\n",
    "    print(\"\".join(num_to_text.get(i) for i in out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment!\n",
    "\n",
    "- 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "g = torch.Generator().manual_seed(5)\n",
    "\n",
    "emb_dim: int = 10  # embedding dimension\n",
    "block_size: int = 3  # size of the context window for each input sequence\n",
    "M: int = emb_dim * block_size  # number of inputs\n",
    "n_nodes: int = 300  # number of hidden nodes\n",
    "learning_rate: float = 0.1  # learning rate\n",
    "batch_size: int = 32  # batch size\n",
    "epochs: int = 140_000  # number of epochs\n",
    "\n",
    "# NEW!\n",
    "model: Sequential = Sequential(\n",
    "    layers=[\n",
    "        Embedding(vocab_size=n_chars, embedding_dim=emb_dim, seed=42),\n",
    "        FlattenConsecutive(n_dim=2),\n",
    "        Linear(in_features=M, out_features=n_nodes, bias=False),\n",
    "        BatchNorm1d(dim=n_nodes),\n",
    "        Tanh(),\n",
    "        Linear(in_features=n_nodes, out_features=n_chars, bias=False),\n",
    "        BatchNorm1d(dim=n_chars),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# with torch.no_grad():\n",
    "# Make last layer less confident\n",
    "# model[-1].weight *= 0.1  # Default: w/o BatchNorm\n",
    "\n",
    "# Parameters Collection\n",
    "parameters: list[Tensor] = model.parameters()  # NEW!\n",
    "print(f\"Total params: {sum(p.numel() for p in parameters):,}\")\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use The optimal learning rate to train the model\n",
    "losses_all: list[float] = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # Add mini-batches\n",
    "    idx: Tensor = torch.randint(0, X_train.shape[0], size=(batch_size,))\n",
    "    # X, y batch\n",
    "    Xb, yb = X_train[idx], y_train[idx]\n",
    "\n",
    "    # Forward pass\n",
    "    logits: Tensor = model(Xb)  # NEW!\n",
    "    loss: Tensor = F.cross_entropy(x, yb)\n",
    "\n",
    "    # Backward pass\n",
    "    for p in parameters:\n",
    "        # Reset gradients\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # learning rate decay\n",
    "    learning_rate: float = (\n",
    "        0.1 if epoch < 70_000 else (0.01 if epoch < 85_000 else 0.001)\n",
    "    )\n",
    "\n",
    "    # Update the parameters\n",
    "    for p in parameters:\n",
    "        p.data -= learning_rate * p.grad\n",
    "\n",
    "    # Record the loss\n",
    "    losses_all.append(loss.item())\n",
    "\n",
    "    if (epoch) % 10_000 == 0:\n",
    "        print(f\"Epoch: {epoch}/{epochs} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "    if epoch > 100:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "g = torch.Generator().manual_seed(5)\n",
    "\n",
    "emb_dim: int = 10  # embedding dimension\n",
    "block_size: int = 3  # size of the context window for each input sequence\n",
    "M: int = emb_dim * block_size  # number of inputs\n",
    "n_nodes: int = 300  # number of hidden nodes\n",
    "learning_rate: float = 0.1  # learning rate\n",
    "batch_size: int = 32  # batch size\n",
    "epochs: int = 140_000  # number of epochs\n",
    "# C: Tensor = torch.randn((n_chars, emb_dim), generator=g) * 0.01  # Lookup table\n",
    "W1: Tensor = torch.randn((M, n_nodes), generator=g) * 0.01\n",
    "b1: Tensor = torch.randn(n_nodes, generator=g) * 0.01\n",
    "# Initialize the weights and biases with very small random values.\n",
    "W2: Tensor = torch.randn(n_nodes, n_chars, generator=g) * 0.01  # (100, 27)\n",
    "b2: Tensor = torch.randn(n_chars, generator=g) * 0  # (27,)\n",
    "\n",
    "model: Any = Sequential(\n",
    "    layers=[\n",
    "        Embedding(vocab_size=n_chars, embedding_dim=emb_dim, seed=42),\n",
    "        Linear(in_features=M, out_features=n_nodes, bias=False),\n",
    "        BatchNorm1d(dim=n_nodes),\n",
    "        Tanh(),\n",
    "        Linear(in_features=n_nodes, out_features=n_nodes, bias=False),\n",
    "        BatchNorm1d(dim=n_nodes),\n",
    "        Tanh(),\n",
    "        Linear(in_features=n_nodes, out_features=n_nodes, bias=False),\n",
    "        BatchNorm1d(dim=n_nodes),\n",
    "        Tanh(),\n",
    "        Linear(in_features=n_nodes, out_features=n_nodes, bias=False),\n",
    "        BatchNorm1d(dim=n_nodes),\n",
    "        Tanh(),\n",
    "        Linear(in_features=n_nodes, out_features=n_nodes, bias=False),\n",
    "        BatchNorm1d(dim=n_nodes),\n",
    "        Tanh(),\n",
    "        Linear(in_features=n_nodes, out_features=n_chars, bias=False),\n",
    "        BatchNorm1d(dim=n_chars),\n",
    "    ]\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Make last layer less confident\n",
    "    # layers[-1].weight *= 0.1  # Default: w/o BatchNorm\n",
    "    layers[-1].gamma *= 0.1  # with BatchNorm\n",
    "\n",
    "    # Apply gain to the other layers\n",
    "    for layer in layers[:-1]:\n",
    "        if isinstance(layer, Linear):\n",
    "            # Scale the weights of all other linear layers by a gain factor.\n",
    "            # layer.weight *= 5 / 3 # Default: w/o BatchNorm\n",
    "            layer.weight *= 1.0  # with BatchNorm: No scaling\n",
    "\n",
    "# Parameters Collection\n",
    "parameters: list[Tensor] = [C] + [p for layer in layers for p in layer.parameters()]\n",
    "print(f\"Total params: {sum(p.numel() for p in parameters):,}\")\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_p11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
