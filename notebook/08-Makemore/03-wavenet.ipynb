{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Makemore: Wavenet\n",
    "\n",
    "- [Andrej Karpathy YouTube](https://www.youtube.com/watch?v=t3YJ5hKiMQ0&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&index=6&ab_channel=AndrejKarpathy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark -v -p numpy,pandas,polars,torch,lightning --conda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in library\n",
    "from pathlib import Path\n",
    "import re\n",
    "import json\n",
    "from typing import Any, Optional, Union\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "# Standard imports\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from rich.console import Console\n",
    "from rich.theme import Theme\n",
    "\n",
    "custom_theme = Theme(\n",
    "    {\n",
    "        \"info\": \"#76FF7B\",\n",
    "        \"warning\": \"#FBDDFE\",\n",
    "        \"error\": \"#FF0000\",\n",
    "    }\n",
    ")\n",
    "console = Console(theme=custom_theme)\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Pandas settings\n",
    "pd.options.display.max_rows = 1_000\n",
    "pd.options.display.max_columns = 1_000\n",
    "pd.options.display.max_colwidth = 600\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# Black code formatter (Optional)\n",
    "%load_ext lab_black\n",
    "\n",
    "# auto reload imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path: str) -> list[str]:\n",
    "    \"\"\"Load text data from a file and return as a list of strings.\"\"\"\n",
    "\n",
    "    with open(file_path, \"r\") as f:\n",
    "        # Read all the lines as a list\n",
    "        data: list[str] = f.read().splitlines()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "fp: str = \"../../data/names.txt\"\n",
    "names: list[str] = load_data(file_path=fp)\n",
    "\n",
    "names[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Vocabulary Of Characters And Mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_token: str = \".\"\n",
    "characters: list[str] = sorted(set(\"\".join(names)))\n",
    "# Add the special token to the beginning of the list.\n",
    "characters.insert(0, special_token)\n",
    "n_chars: int = len(characters)\n",
    "\n",
    "# Convert text to numbers.\n",
    "text_to_num: dict[str, int] = {text: idx for idx, text in enumerate(characters)}\n",
    "# Convert numbers to text\n",
    "num_to_text: dict[int, str] = {idx: text for text, idx in text_to_num.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split, TensorDataset, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def build_dataset(\n",
    "    names: list[str],\n",
    "    special_token: str = \".\",\n",
    "    block_size: int = 3,\n",
    "    print_info: bool = False,\n",
    ") -> tuple[Tensor, Tensor]:\n",
    "    \"\"\"\n",
    "    Builds a dataset of name sequences and their corresponding character indices.\n",
    "\n",
    "    Args:\n",
    "        names (list[str]): A list of names to build the dataset from.\n",
    "        special_token (str, optional): A special token to append to the end of each name. Defaults to \".\".\n",
    "        block_size (int, optional): The size of the context window for each input sequence. Defaults to 3.\n",
    "        print_info (bool, optional): Whether to print information about the dataset generation. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        tuple[Tensor, Tensor]: A tuple containing the input sequences (X) and their corresponding target indices (Y).\n",
    "    \"\"\"\n",
    "    X, Y = [], []\n",
    "\n",
    "    for w in names:\n",
    "        if print_info:\n",
    "            print(w)\n",
    "        context: list[int] = [0] * block_size\n",
    "\n",
    "        for ch in w + special_token:\n",
    "            idx: int = text_to_num.get(ch)\n",
    "            X.append(context)\n",
    "            Y.append(idx)\n",
    "\n",
    "            if print_info:\n",
    "                print(\n",
    "                    f\"{''.join([num_to_text.get(i) for i in context])} ---> {num_to_text.get(idx)}\"\n",
    "                )\n",
    "\n",
    "            # Crop and append, like a rolling window\n",
    "            context = context[1:] + [idx]\n",
    "\n",
    "    X: Tensor = torch.tensor(X)\n",
    "    Y: Tensor = torch.tensor(Y)\n",
    "    print(f\"\\n{X.shape=}, {Y.shape=}\")\n",
    "    return (X, Y)\n",
    "\n",
    "\n",
    "def split_data_into_train_dev_test(\n",
    "    data: Tensor | Dataset, test_size: float = 0.05, dev_size: float = 0.1, seed=42\n",
    ") -> tuple[Tensor, ...]:\n",
    "    \"\"\"\n",
    "    Splits a given PyTorch tensor `data` into training, development, and test sets.\n",
    "\n",
    "    Params:\n",
    "    -------\n",
    "        data (torch.Tensor): The input tensor to be split.\n",
    "        test_size (float, optional): The fraction of the data to use for the test set. Defaults to 0.2.\n",
    "        dev_size (float, optional): The fraction of the data to use for the development set. Defaults to 0.1.\n",
    "        seed (int, optional): The random seed to use for reproducibility. Defaults to 42.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "        tuple[torch.Tensor, torch.Tensor, torch.Tensor]: The training, development, and test sets as PyTorch tensors.\n",
    "    \"\"\"\n",
    "    if isinstance(data, Tensor):\n",
    "        X_train, X_test = train_test_split(data, test_size=test_size, random_state=seed)\n",
    "        X_train, X_dev = train_test_split(\n",
    "            X_train, test_size=dev_size, random_state=seed\n",
    "        )\n",
    "        result: tuple[Tensor, ...] = (X_train, X_dev, X_test)\n",
    "    if isinstance(data, Dataset):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            data.data,\n",
    "            data.targets,\n",
    "            test_size=test_size,\n",
    "            random_state=seed,\n",
    "            stratify=data.targets,\n",
    "        )\n",
    "        X_train, X_dev, y_train, y_dev = train_test_split(\n",
    "            X_train, y_train, test_size=dev_size, random_state=seed, stratify=y_train\n",
    "        )\n",
    "        result: tuple[Tensor, ...] = (X_train, X_dev, X_test, y_train, y_dev, y_test)\n",
    "\n",
    "    print(f\"{X_train.shape=}; {X_dev.shape=}; {X_test.shape=}\")\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data: Tensor, targets: Tensor) -> None:\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return (\n",
    "            f\"{self.__class__.__name__}(data.shape={self.data.shape}, \"\n",
    "            f\"target.shape={self.targets.shape=})\"\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx]\n",
    "        y = self.targets[idx]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "\n",
    "class CustomModule(ABC):\n",
    "    @abstractmethod\n",
    "    def __init__(self) -> None:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def __repr__(self) -> str:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def __call__(self, x: Tensor) -> Tensor:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def parameters(self) -> list[Tensor]:\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class Linear(CustomModule):\n",
    "    \"\"\"\n",
    "    A linear layer implementation.\n",
    "\n",
    "    This class implements a linear layer, which performs a linear transformation on the input tensor. It takes in the number\n",
    "    of input features and output features, and optionally a bias term. The weights and biases are initialized randomly.\n",
    "\n",
    "    The `__call__` method applies the linear transformation to the input tensor and returns the output tensor.\n",
    "\n",
    "    The `parameters` method returns a list of the learnable parameters (weights and biases) of the layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, in_features: int, out_features: int, bias: bool = True, seed: int = 42\n",
    "    ) -> None:\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        # Kaiming's initialization\n",
    "        self.weight = (\n",
    "            torch.randn(in_features, out_features, generator=torch.manual_seed(seed))\n",
    "            / in_features**0.5\n",
    "        )\n",
    "        self.bias = (\n",
    "            torch.randn(out_features, generator=torch.manual_seed(seed))\n",
    "            if bias\n",
    "            else None\n",
    "        )\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{self.__class__.__name__}({self.in_features}, {self.out_features})\"\n",
    "\n",
    "    def __call__(self, x: Tensor) -> Tensor:\n",
    "        self.output: Tensor = torch.matmul(x, self.weight)\n",
    "        if self.bias is not None:\n",
    "            self.output += self.bias\n",
    "        return self.output\n",
    "\n",
    "    def parameters(self) -> list[Tensor]:\n",
    "        return [self.weight] + ([self.bias] if self.bias is not None else [])\n",
    "\n",
    "\n",
    "class BatchNorm1d(CustomModule):\n",
    "    def __init__(self, dim: int, eps: float = 1e-5, momentum: float = 0.1) -> None:\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self.training = True\n",
    "        # Parameters (trained with backprop)\n",
    "        self.gamma = torch.ones(dim)\n",
    "        self.beta = torch.zeros(dim)\n",
    "        # Buffers (trained with running `momentum update`)\n",
    "        self.running_mean = torch.zeros(dim)\n",
    "        self.running_var = torch.ones(dim)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{self.__class__.__name__}()\"\n",
    "\n",
    "    def __call__(self, x: Tensor) -> Tensor:\n",
    "        if self.training:\n",
    "            if x.ndim == 2:\n",
    "                dim: tuple[int] | int = 0\n",
    "            elif x.ndim == 3:\n",
    "                dim = (0, 1)\n",
    "            # Calculate the batch mean and variance\n",
    "            x_mean: Tensor = x.mean(dim=dim, keepdim=True)\n",
    "            x_var: Tensor = x.var(dim=dim, keepdim=True)\n",
    "\n",
    "        else:\n",
    "            x_mean = self.running_mean\n",
    "            x_var = self.running_var\n",
    "\n",
    "        # Normalize the input\n",
    "        x_hat: Tensor = (x - x_mean) / (x_var + self.eps).sqrt()\n",
    "        self.output: Tensor = (self.gamma * x_hat) + self.beta\n",
    "\n",
    "        if self.training:\n",
    "            with torch.no_grad():\n",
    "                # Update running mean and variance\n",
    "                self.running_mean = (\n",
    "                    1 - self.momentum\n",
    "                ) * self.running_mean + self.momentum * x_mean\n",
    "                self.running_var = (\n",
    "                    1 - self.momentum\n",
    "                ) * self.running_var + self.momentum * x_var\n",
    "\n",
    "        return self.output\n",
    "\n",
    "    def parameters(self) -> list[Tensor]:\n",
    "        return [self.gamma, self.beta]\n",
    "\n",
    "\n",
    "class Tanh(CustomModule):\n",
    "    \"\"\"A custom module that applies the hyperbolic tangent activation function to the input tensor.\"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{self.__class__.__name__}()\"\n",
    "\n",
    "    def __call__(self, x: Tensor) -> Tensor:\n",
    "        self.output = torch.tanh(x)\n",
    "        return self.output\n",
    "\n",
    "    def parameters(self) -> list[Tensor]:\n",
    "        return []\n",
    "\n",
    "\n",
    "class Embedding(CustomModule):\n",
    "    \"\"\"A custom module that creates an embedding lookup table from a given\n",
    "    vocabulary size and embedding dimension.\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size: int, embedding_dim: int, seed: int = 42) -> None:\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.weight = torch.randn(\n",
    "            (vocab_size, embedding_dim), generator=torch.manual_seed(seed)\n",
    "        )\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{self.__class__.__name__}({self.vocab_size}, {self.embedding_dim})\"\n",
    "\n",
    "    def __call__(self, idx: int) -> Tensor:\n",
    "        self.output = self.weight[idx]\n",
    "        return self.output\n",
    "\n",
    "    def parameters(self) -> list[Tensor]:\n",
    "        \"\"\"Get all the parameters.\"\"\"\n",
    "        return [self.weight]\n",
    "\n",
    "\n",
    "class Flatten(CustomModule):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{__class__.__name__}()\"\n",
    "\n",
    "    def __call__(self, x: Tensor) -> Tensor:\n",
    "        self.output = x.view(x.shape[0], -1)\n",
    "        return self.output\n",
    "\n",
    "    def parameters(self):\n",
    "        \"\"\"Get all the parameters.\"\"\"\n",
    "        return []\n",
    "\n",
    "\n",
    "class Sequential(CustomModule):\n",
    "    \"\"\"A custom module that applies a sequence of other custom modules to the input tensor.\"\"\"\n",
    "\n",
    "    def __init__(self, layers: list[CustomModule]) -> None:\n",
    "        self.layers = layers\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{__class__.__name__}({len(self.layers)})\"\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        self.out = x\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        \"\"\"Get parameters of all layers and stretch them out into one list.\"\"\"\n",
    "        return [p for layer in self.layers for p in layer.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = build_dataset(names=names[:5], block_size=3, print_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = build_dataset(names=names, block_size=3, print_info=False)\n",
    "data: Dataset = MyDataset(X, y)\n",
    "\n",
    "X_train, X_dev, X_test, y_train, y_dev, y_test = split_data_into_train_dev_test(\n",
    "    data=data\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment!\n",
    "\n",
    "- 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "g = torch.Generator().manual_seed(5)\n",
    "\n",
    "emb_dim: int = 10  # embedding dimension\n",
    "block_size: int = 3  # size of the context window for each input sequence\n",
    "M: int = emb_dim * block_size  # number of inputs\n",
    "n_nodes: int = 300  # number of hidden nodes\n",
    "learning_rate: float = 0.1  # learning rate\n",
    "batch_size: int = 32  # batch size\n",
    "epochs: int = 140_000  # number of epochs\n",
    "\n",
    "layers: list[Any] = [\n",
    "    Embedding(vocab_size=n_chars, embedding_dim=emb_dim, seed=42),\n",
    "    Flatten(),\n",
    "    Linear(in_features=M, out_features=n_nodes, bias=False),\n",
    "    BatchNorm1d(dim=n_nodes),\n",
    "    Tanh(),\n",
    "    Linear(in_features=n_nodes, out_features=n_chars, bias=False),\n",
    "    BatchNorm1d(dim=n_chars),\n",
    "]\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Make last layer less confident\n",
    "    # layers[-1].weight *= 0.1  # Default: w/o BatchNorm\n",
    "    layers[-1].gamma *= 0.1  # with BatchNorm\n",
    "\n",
    "    # Apply gain to the other layers\n",
    "    for layer in layers[:-1]:\n",
    "        if isinstance(layer, Linear):\n",
    "            # Scale the weights of all other linear layers by a gain factor.\n",
    "            # layer.weight *= 5 / 3 # Default: w/o BatchNorm\n",
    "            layer.weight *= 1.0  # with BatchNorm: No scaling\n",
    "\n",
    "# Parameters Collection\n",
    "parameters: list[Tensor] = [p for layer in layers for p in layer.parameters()]\n",
    "print(f\"Total params: {sum(p.numel() for p in parameters):,}\")\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use The optimal learning rate to train the model\n",
    "losses_all: list[float] = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # Add mini-batches\n",
    "    idx: Tensor = torch.randint(0, X_train.shape[0], size=(batch_size,))\n",
    "    # X, y batch\n",
    "    Xb, yb = X_train[idx], y_train[idx]\n",
    "\n",
    "    # Forward pass\n",
    "    x: Tensor = Xb\n",
    "    for layer in layers:\n",
    "        x: Tensor = layer(x)  # Logits: Apply the linear layer\n",
    "    loss: Tensor = F.cross_entropy(x, yb)\n",
    "\n",
    "    # Backward pass\n",
    "    for p in parameters:\n",
    "        # Reset gradients\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # learning rate decay\n",
    "    learning_rate: float = (\n",
    "        0.1 if epoch < 70_000 else (0.01 if epoch < 85_000 else 0.001)\n",
    "    )\n",
    "\n",
    "    # Update the parameters\n",
    "    for p in parameters:\n",
    "        p.data -= learning_rate * p.grad\n",
    "\n",
    "    # Record the loss\n",
    "    losses_all.append(loss.item())\n",
    "\n",
    "    if (epoch) % 10_000 == 0:\n",
    "        print(f\"Epoch: {epoch}/{epochs} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "    if epoch > 100:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings of index 2\n",
    "layers[0](2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample from the model\n",
    "\n",
    "for layer in layers:\n",
    "    layer.training = False\n",
    "\n",
    "\n",
    "g = torch.Generator().manual_seed(5)\n",
    "n_names: int = 10\n",
    "\n",
    "for _ in range(n_names):\n",
    "\n",
    "    out: list[str] = []\n",
    "    context: list[int] = [0] * block_size  # initialize with all ...\n",
    "    while True:\n",
    "        # Forward pass the neural net\n",
    "        x: Tensor = torch.tensor([context])\n",
    "\n",
    "        for layer in layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        logits: Tensor = x\n",
    "        probs: Tensor = F.softmax(logits, dim=1)\n",
    "        # Sample from the distribution\n",
    "        idx: int = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "        # Shift the context window and track the samples\n",
    "        context = context[1:] + [idx]\n",
    "        out.append(idx)\n",
    "        # If we sample the special '.' token, break\n",
    "        if idx == 0:\n",
    "            break\n",
    "\n",
    "    # Decode and print the generated word\n",
    "    print(\"\".join(num_to_text.get(i) for i in out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2!\n",
    "\n",
    "- The current inplementation flattens the entire data into a 2-D array which is not optimal when the block_size is large.\n",
    "\n",
    "```py\n",
    "# e.g. If the block_size is 3\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "- Implement the [Wavenet](https://arxiv.org/pdf/1609.03499) architecture shown below:\n",
    "\n",
    "<img src=\"./images/Wavenet.png\" width=\"600\" height=\"300\" alt=\"Wavenet Architecture\">\n",
    "\n",
    "- The input is combined to form several bigrams which are further combined at every layer in the network.\n",
    "  - i.e. if you have 8 characters in the input layer, you will have 4 bigrams (e.g. `ab`, `cd`, `ef`, `gh`).\n",
    "  - `ab` is combined with `cd` to form `abcd`, `ef` is combined with `gh` to form `efgh` and so on.\n",
    "  - finally, `abcd` and `efgh` are combined and used to predict the output.\n",
    "\n",
    "<br><hr>\n",
    "\n",
    "```text\n",
    "Combining each input layer with the next one to form bigrams:\n",
    "\n",
    "FlattenConsecutive(2):   (4, 4, 20)   # i.e. 4 groups (1 2)   (3 4)   (5 6)   (7 8) \n",
    "...\n",
    "FlattenConsecutive(2):   (4, 2, 600)   # i.e. 2 groups (1 2 3 4)   (5 6 7 8) \n",
    "...\n",
    "FlattenConsecutive(2):   (4, 600)   # (4, 1, 600) i.e. 1 group (1 2 3 4 5 6 7 8) \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "g = torch.Generator().manual_seed(5)\n",
    "\n",
    "emb_dim: int = 10  # embedding dimension\n",
    "block_size: int = 8  # size of the context window for each input sequence\n",
    "M: int = emb_dim * block_size  # number of inputs\n",
    "n_nodes: int = 300  # number of hidden nodes\n",
    "learning_rate: float = 0.1  # learning rate\n",
    "batch_size: int = 32  # batch size\n",
    "epochs: int = 140_000  # number of epochs\n",
    "\n",
    "# NEW!\n",
    "model: Sequential = Sequential(\n",
    "    layers=[\n",
    "        Embedding(vocab_size=n_chars, embedding_dim=emb_dim, seed=42),\n",
    "        Flatten(),\n",
    "        Linear(in_features=M, out_features=n_nodes, bias=False),\n",
    "        BatchNorm1d(dim=n_nodes),\n",
    "        Tanh(),\n",
    "        Linear(in_features=n_nodes, out_features=n_nodes, bias=False),\n",
    "        BatchNorm1d(dim=n_nodes),\n",
    "        Tanh(),\n",
    "        Linear(in_features=n_nodes, out_features=n_chars, bias=True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Make last layer less confident\n",
    "    model.layers[-1].weight *= 0.1  # Default: w/o BatchNorm\n",
    "\n",
    "# Parameters Collection\n",
    "parameters: list[Tensor] = model.parameters()  # NEW!\n",
    "print(f\"Total params: {sum(p.numel() for p in parameters):,}\")\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = build_dataset(names=names, block_size=block_size, print_info=False)\n",
    "data: Dataset = MyDataset(X, y)\n",
    "\n",
    "X_train, X_dev, X_test, y_train, y_dev, y_test = split_data_into_train_dev_test(\n",
    "    data=data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8 characters are required to predict the next character\n",
    "build_dataset(names=names[:2], block_size=block_size, print_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size: int = 4\n",
    "idx: Tensor = torch.randint(0, X_train.shape[0], (batch_size,))\n",
    "X_s, y_s = X_train[idx], y_train[idx]\n",
    "logits: Tensor = model(X_s)\n",
    "print(f\"{X_s.shape = }\")\n",
    "\n",
    "# 4 samples each containing a block size of `block_size`\n",
    "X_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><hr>\n",
    "\n",
    "```py\n",
    "M: int = emb_dim * block_size  # number of inputs\n",
    "\n",
    "model: Sequential = Sequential(\n",
    "    layers=[\n",
    "        Embedding(vocab_size=n_chars, embedding_dim=emb_dim, seed=42), # 1st layer\n",
    "        Flatten(),  # 2nd layer\n",
    "        Linear(in_features=M, out_features=n_nodes, bias=False), # 3rd layer\n",
    "        BatchNorm1d(dim=n_nodes), # 4th layer\n",
    "        Tanh(),  \n",
    "        Linear(in_features=n_nodes, out_features=n_chars, bias=False),\n",
    "        BatchNorm1d(dim=n_chars), \n",
    "    ]\n",
    ")\n",
    "```\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st layer: Output of Embedding layer\n",
    "model.layers[0].output.shape  # (Batch_size, block_size, emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2nd layer: Output of Flatten layer\n",
    "# It has been reshaped to ( batch_size, (block_size, emb_dim) ) from (batch_size, block_size, emb_dim)\n",
    "# It was flattened because the next (Linear) layer expects a 2-D input\n",
    "model.layers[1].output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3rd layer: Output of Linear layer\n",
    "# Matrix Multiplication of the input with the weights\n",
    "model.layers[2].output.shape  # (batch_size, block_size) @ (block_size, n_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4th layer: Output of BatchNorm layer\n",
    "# Normalize the output of the previous layer\n",
    "model.layers[3].output.shape  # (batch_size, block_size) @ (block_size, n_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The current implementation of the Linear Layer\n",
    "# (4, 8, 10) is flattened to (4, 80)\n",
    "# (4, 80) @ (80, 300) + (300,) => (4, 300)\n",
    "(\n",
    "    torch.randn((4, block_size * emb_dim))  # (4, 80)\n",
    "    @ torch.randn((block_size * emb_dim, n_nodes))  # (80, 300)\n",
    "    + torch.randn(n_nodes)  # (300,)\n",
    ").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Wavenet\n",
    "#   1       2       3       4     i.e.  n_combined_input = 4 groups\n",
    "# (1 2)   (3 4)   (5 6)   (7 8)\n",
    "# i.e. we want: (batch_size, n_combined_input, -1)\n",
    "# i.e. from (4, 8, 10) to (4, 4, 20) instead of (4, 80)\n",
    "\n",
    "res_1: Tensor = torch.randn((4, block_size * emb_dim)).view(\n",
    "    4, -1, (block_size * emb_dim) // 4\n",
    ")\n",
    "# OR\n",
    "res_2: Tensor = torch.randn((4, block_size * emb_dim)).view(\n",
    "    4, block_size // 2, (emb_dim * 2)\n",
    ")\n",
    "print(f\"{res_1.shape = }\")\n",
    "print(f\"{res_2.shape = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlattenConsecutive(CustomModule):\n",
    "    \"\"\"A custom module that flattens consecutive elements in the input tensor along\n",
    "    the second dimension.\"\"\"\n",
    "\n",
    "    def __init__(self, n_c_elements: int) -> None:\n",
    "        \"\"\"\n",
    "        Note:\n",
    "            n_c_elements: the number of consecutive elements to concatenate.\n",
    "        \"\"\"\n",
    "        self.n_c_elements = n_c_elements\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{__class__.__name__}({self.n_c_elements})\"\n",
    "\n",
    "    def __call__(self, x: Tensor) -> Tensor:\n",
    "        # B: batch size, L: sequence length, C: number of channels\n",
    "        B, L, C = x.shape\n",
    "        assert (\n",
    "            L % self.n_c_elements == 0\n",
    "        ), f\"The sequence length of the input tensor must be a multiple of {self.n_c_elements}.\"\n",
    "\n",
    "        x = x.view(B, L // self.n_c_elements, C * self.n_c_elements)\n",
    "        if x.shape[1] == 1:\n",
    "            x = x.squeeze(1)\n",
    "\n",
    "        self.output = x\n",
    "        return self.output\n",
    "\n",
    "    def parameters(self):\n",
    "        \"\"\"Get all the parameters.\"\"\"\n",
    "        return []\n",
    "\n",
    "\n",
    "def calculate_loss_upd(X: Tensor, y: Tensor, training: True) -> Tensor:\n",
    "    \"\"\"\n",
    "    Calculates the loss for the given input tensors `X` and `y`.\n",
    "\n",
    "    Args:\n",
    "        X (torch.Tensor): The input tensor.\n",
    "        y (torch.Tensor): The target tensor.\n",
    "        training (bool): Indicates whether the model is in training mode or not.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The calculated loss.\n",
    "    \"\"\"\n",
    "\n",
    "    # Set the training mode to False\n",
    "    for layer in model.layers:\n",
    "        if hasattr(layer, \"training\"):\n",
    "            layer.training = False\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Forward pass\n",
    "        logits: Tensor = model(X)\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss: Tensor = F.cross_entropy(logits, y)\n",
    "        result: str = (\n",
    "            f\"Training loss: {loss:.4f}\" if training else f\"Validation loss: {loss:.4f}\"\n",
    "        )\n",
    "        print(result)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_1: Tensor = torch.randn(4, block_size, emb_dim)\n",
    "fl = FlattenConsecutive(n_c_elements=2)\n",
    "# Concatenate 2 consecutive elements. i.e. (1 2), (3 4), (5 6), (7 8)\n",
    "print(f\"{res_1.shape = }\")\n",
    "print(f\"{fl(res_1).shape = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_2: Tensor = torch.randn(4, block_size, emb_dim)\n",
    "fl = FlattenConsecutive(n_c_elements=4)\n",
    "# Concatenate 4 consecutive elements. i.e. (1 2 3 4), (5 6 7 8) etc\n",
    "print(f\"{res_2.shape = }\")\n",
    "print(f\"{fl(res_2).shape = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "g = torch.Generator().manual_seed(5)\n",
    "\n",
    "emb_dim: int = 10  # embedding dimension\n",
    "block_size: int = 8  # size of the context window for each input sequence\n",
    "M: int = emb_dim * block_size  # number of inputs\n",
    "n_nodes: int = 64  # number of hidden nodes\n",
    "learning_rate: float = 0.1  # learning rate\n",
    "batch_size: int = 32  # batch size\n",
    "epochs: int = 140_000  # number of epochs\n",
    "\n",
    "# NEW!\n",
    "n_c_elements: int = 2\n",
    "model: Sequential = Sequential(\n",
    "    layers=[\n",
    "        Embedding(vocab_size=n_chars, embedding_dim=emb_dim, seed=42),\n",
    "        # === Input Layer\n",
    "        FlattenConsecutive(n_c_elements=n_c_elements),\n",
    "        Linear(in_features=emb_dim * n_c_elements, out_features=n_nodes, bias=False),\n",
    "        BatchNorm1d(dim=n_nodes),\n",
    "        Tanh(),\n",
    "        # === layer 1\n",
    "        FlattenConsecutive(n_c_elements=n_c_elements),\n",
    "        Linear(in_features=n_nodes * n_c_elements, out_features=n_nodes, bias=False),\n",
    "        BatchNorm1d(dim=n_nodes),\n",
    "        Tanh(),\n",
    "        # === layer 2\n",
    "        FlattenConsecutive(n_c_elements=n_c_elements),\n",
    "        Linear(in_features=n_nodes * n_c_elements, out_features=n_nodes, bias=False),\n",
    "        BatchNorm1d(dim=n_nodes),\n",
    "        Tanh(),\n",
    "        # === layer 3\n",
    "        Linear(in_features=n_nodes, out_features=n_chars),\n",
    "    ]\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Make last layer less confident\n",
    "    model.layers[-1].weight *= 0.1  # Default: w/o BatchNorm\n",
    "\n",
    "# Parameters Collection\n",
    "parameters: list[Tensor] = model.parameters()  # NEW!\n",
    "print(f\"Total params: {sum(p.numel() for p in parameters):,}\")\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = build_dataset(names=names, block_size=block_size, print_info=False)\n",
    "data: Dataset = MyDataset(X, y)\n",
    "\n",
    "X_train, X_dev, X_test, y_train, y_dev, y_test = split_data_into_train_dev_test(\n",
    "    data=data\n",
    ")\n",
    "\n",
    "print()\n",
    "\n",
    "batch_size: int = 4\n",
    "idx: Tensor = torch.randint(0, X_train.shape[0], (batch_size,))\n",
    "X_s, y_s = X_train[idx], y_train[idx]\n",
    "logits: Tensor = model(X_s)\n",
    "print(f\"{X_s.shape = }\")\n",
    "\n",
    "# 4 samples each containing a block size of `block_size`\n",
    "X_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a constant for the padding width\n",
    "PADDING_WIDTH: int = 25\n",
    "\n",
    "for layer in model.layers:\n",
    "    layer_str: str = f\"{layer}:\"\n",
    "    shape_str: str = f\"{tuple(layer.output.shape)}\"\n",
    "    print(layer_str.ljust(PADDING_WIDTH) + shape_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = build_dataset(names=names, block_size=block_size, print_info=False)\n",
    "data: Dataset = MyDataset(X, y)\n",
    "\n",
    "X_train, X_dev, X_test, y_train, y_dev, y_test = split_data_into_train_dev_test(\n",
    "    data=data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "g = torch.Generator().manual_seed(5)\n",
    "\n",
    "emb_dim: int = 24  # embedding dimension\n",
    "block_size: int = 8  # size of the context window for each input sequence\n",
    "M: int = emb_dim * block_size  # number of inputs\n",
    "n_nodes: int = 256  # number of hidden nodes\n",
    "learning_rate: float = 0.1  # learning rate\n",
    "batch_size: int = 32  # batch size\n",
    "epochs: int = 140_000  # number of epochs\n",
    "\n",
    "# NEW!\n",
    "n_c_elements: int = 2\n",
    "model: Sequential = Sequential(\n",
    "    layers=[\n",
    "        Embedding(vocab_size=n_chars, embedding_dim=emb_dim, seed=42),\n",
    "        # === Input Layer\n",
    "        FlattenConsecutive(n_c_elements=n_c_elements),\n",
    "        Linear(in_features=emb_dim * n_c_elements, out_features=n_nodes, bias=False),\n",
    "        BatchNorm1d(dim=n_nodes),\n",
    "        Tanh(),\n",
    "        # === Layer 1\n",
    "        FlattenConsecutive(n_c_elements=n_c_elements),\n",
    "        Linear(in_features=n_nodes * n_c_elements, out_features=n_nodes, bias=False),\n",
    "        BatchNorm1d(dim=n_nodes),\n",
    "        Tanh(),\n",
    "        # === Layer 2\n",
    "        FlattenConsecutive(n_c_elements=n_c_elements),\n",
    "        Linear(in_features=n_nodes * n_c_elements, out_features=n_nodes, bias=False),\n",
    "        BatchNorm1d(dim=n_nodes),\n",
    "        Tanh(),\n",
    "        # === Layer 3\n",
    "        Linear(in_features=n_nodes, out_features=n_chars),\n",
    "    ]\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Make last layer less confident\n",
    "    model.layers[-1].weight *= 0.1  # Default: w/o BatchNorm\n",
    "\n",
    "# Parameters Collection\n",
    "parameters: list[Tensor] = model.parameters()  # NEW!\n",
    "print(f\"Total params: {sum(p.numel() for p in parameters):,}\")\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Trainning Loop ====\n",
    "losses_all: list[float] = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # Add mini-batches\n",
    "    idx: Tensor = torch.randint(0, X_train.shape[0], size=(batch_size,))\n",
    "    # X, y batch\n",
    "    Xb, yb = X_train[idx], y_train[idx]\n",
    "\n",
    "    # Forward pass\n",
    "    logits: Tensor = model(Xb)\n",
    "    loss: Tensor = F.cross_entropy(logits, yb)\n",
    "\n",
    "    # Backward pass\n",
    "    for p in parameters:\n",
    "        # Reset gradients\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # learning rate decay\n",
    "    learning_rate: float = (\n",
    "        0.1 if epoch < 70_000 else (0.01 if epoch < 90_000 else 0.001)\n",
    "    )\n",
    "\n",
    "    # Update the parameters\n",
    "    for p in parameters:\n",
    "        p.data -= learning_rate * p.grad\n",
    "\n",
    "    # Record the loss\n",
    "    losses_all.append(loss.item())\n",
    "\n",
    "    if (epoch) % 10_000 == 0:\n",
    "        print(f\"Epoch: {epoch}/{epochs} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # if epoch > 100:\n",
    "    #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_loss_upd(X=X_train, y=y_train, training=True)\n",
    "calculate_loss_upd(X=X_dev, y=y_dev, training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_loss: Tensor = (\n",
    "    torch.tensor(losses_all, dtype=torch.float32).view(-1, 1_000).mean(dim=1)\n",
    ")\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))  # Create figure and axes\n",
    "ax.plot(avg_loss)  # Plot the data\n",
    "ax.set(xlabel=\"Epoch\", ylabel=\"Loss\", title=\"Loss vs Epoch\")  # Add labels\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample from the model\n",
    "g = torch.Generator().manual_seed(5)\n",
    "n_names: int = 20\n",
    "\n",
    "# Set the training mode to False\n",
    "for layer in model.layers:\n",
    "    if hasattr(layer, \"training\"):\n",
    "        layer.training = False\n",
    "\n",
    "\n",
    "for _ in range(n_names):\n",
    "\n",
    "    out: list[str] = []\n",
    "    context: list[int] = [0] * block_size  # initialize with all ...\n",
    "    while True:\n",
    "        # Forward pass the neural net\n",
    "        x: Tensor = torch.tensor([context])\n",
    "        logits: Tensor = model(x)\n",
    "        probs: Tensor = F.softmax(logits, dim=1)\n",
    "\n",
    "        # Sample from the distribution\n",
    "        idx: int = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "        # Shift the context window and track the samples\n",
    "        context = context[1:] + [idx]\n",
    "        out.append(idx)\n",
    "        # If we sample the special '.' token, break\n",
    "        if idx == 0:\n",
    "            break\n",
    "\n",
    "    # Decode and print the generated word\n",
    "    print(\"\".join(num_to_text.get(i) for i in out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_p11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
