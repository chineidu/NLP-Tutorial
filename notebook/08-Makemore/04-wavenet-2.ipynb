{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Makemore: Wavenet\n",
    "\n",
    "- [Andrej Karpathy YouTube](https://www.youtube.com/watch?v=t3YJ5hKiMQ0&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&index=6&ab_channel=AndrejKarpathy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark -v -p numpy,pandas,polars,torch,lightning --conda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in library\n",
    "from pathlib import Path\n",
    "import re\n",
    "import json\n",
    "from typing import Any, Optional, Union\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "# Standard imports\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from rich.console import Console\n",
    "from rich.theme import Theme\n",
    "\n",
    "custom_theme = Theme(\n",
    "    {\n",
    "        \"info\": \"#76FF7B\",\n",
    "        \"warning\": \"#FBDDFE\",\n",
    "        \"error\": \"#FF0000\",\n",
    "    }\n",
    ")\n",
    "console = Console(theme=custom_theme)\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Pandas settings\n",
    "pd.options.display.max_rows = 1_000\n",
    "pd.options.display.max_columns = 1_000\n",
    "pd.options.display.max_colwidth = 600\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# Black code formatter (Optional)\n",
    "%load_ext lab_black\n",
    "\n",
    "# auto reload imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path: str) -> list[str]:\n",
    "    \"\"\"Load text data from a file and return as a list of strings.\"\"\"\n",
    "\n",
    "    with open(file_path, \"r\") as f:\n",
    "        # Read all the lines as a list\n",
    "        data: list[str] = f.read().splitlines()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "fp: str = \"../../data/names.txt\"\n",
    "names: list[str] = load_data(file_path=fp)\n",
    "\n",
    "names[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Vocabulary Of Characters And Mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_token: str = \".\"\n",
    "characters: list[str] = sorted(set(\"\".join(names)))\n",
    "# Add the special token to the beginning of the list.\n",
    "characters.insert(0, special_token)\n",
    "n_chars: int = len(characters)\n",
    "\n",
    "# Convert text to numbers.\n",
    "text_to_num: dict[str, int] = {text: idx for idx, text in enumerate(characters)}\n",
    "# Convert numbers to text\n",
    "num_to_text: dict[int, str] = {idx: text for text, idx in text_to_num.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split, TensorDataset, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def build_dataset(\n",
    "    names: list[str],\n",
    "    special_token: str = \".\",\n",
    "    block_size: int = 3,\n",
    "    print_info: bool = False,\n",
    ") -> tuple[Tensor, Tensor]:\n",
    "    \"\"\"\n",
    "    Builds a dataset of name sequences and their corresponding character indices.\n",
    "\n",
    "    Args:\n",
    "        names (list[str]): A list of names to build the dataset from.\n",
    "        special_token (str, optional): A special token to append to the end of each name. Defaults to \".\".\n",
    "        block_size (int, optional): The size of the context window for each input sequence. Defaults to 3.\n",
    "        print_info (bool, optional): Whether to print information about the dataset generation. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        tuple[Tensor, Tensor]: A tuple containing the input sequences (X) and their corresponding target indices (Y).\n",
    "    \"\"\"\n",
    "    X, Y = [], []\n",
    "\n",
    "    for w in names:\n",
    "        if print_info:\n",
    "            print(w)\n",
    "        context: list[int] = [0] * block_size\n",
    "\n",
    "        for ch in w + special_token:\n",
    "            idx: int = text_to_num.get(ch)\n",
    "            X.append(context)\n",
    "            Y.append(idx)\n",
    "\n",
    "            if print_info:\n",
    "                print(\n",
    "                    f\"{''.join([num_to_text.get(i) for i in context])} ---> {num_to_text.get(idx)}\"\n",
    "                )\n",
    "\n",
    "            # Crop and append, like a rolling window\n",
    "            context = context[1:] + [idx]\n",
    "\n",
    "    X: Tensor = torch.tensor(X)\n",
    "    Y: Tensor = torch.tensor(Y)\n",
    "    print(f\"\\n{X.shape=}, {Y.shape=}\")\n",
    "    return (X, Y)\n",
    "\n",
    "\n",
    "def split_data_into_train_dev_test(\n",
    "    data: Tensor | Dataset, test_size: float = 0.05, dev_size: float = 0.1, seed=42\n",
    ") -> tuple[Tensor, ...]:\n",
    "    \"\"\"\n",
    "    Splits a given PyTorch tensor `data` into training, development, and test sets.\n",
    "\n",
    "    Params:\n",
    "    -------\n",
    "        data (torch.Tensor): The input tensor to be split.\n",
    "        test_size (float, optional): The fraction of the data to use for the test set. Defaults to 0.2.\n",
    "        dev_size (float, optional): The fraction of the data to use for the development set. Defaults to 0.1.\n",
    "        seed (int, optional): The random seed to use for reproducibility. Defaults to 42.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "        tuple[torch.Tensor, torch.Tensor, torch.Tensor]: The training, development, and test sets as PyTorch tensors.\n",
    "    \"\"\"\n",
    "    if isinstance(data, Tensor):\n",
    "        X_train, X_test = train_test_split(data, test_size=test_size, random_state=seed)\n",
    "        X_train, X_dev = train_test_split(\n",
    "            X_train, test_size=dev_size, random_state=seed\n",
    "        )\n",
    "        result: tuple[Tensor, ...] = (X_train, X_dev, X_test)\n",
    "    if isinstance(data, Dataset):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            data.data,\n",
    "            data.targets,\n",
    "            test_size=test_size,\n",
    "            random_state=seed,\n",
    "            stratify=data.targets,\n",
    "        )\n",
    "        X_train, X_dev, y_train, y_dev = train_test_split(\n",
    "            X_train, y_train, test_size=dev_size, random_state=seed, stratify=y_train\n",
    "        )\n",
    "        result: tuple[Tensor, ...] = (X_train, X_dev, X_test, y_train, y_dev, y_test)\n",
    "\n",
    "    print(f\"{X_train.shape=}; {X_dev.shape=}; {X_test.shape=}\")\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data: Tensor, targets: Tensor) -> None:\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return (\n",
    "            f\"{self.__class__.__name__}(data.shape={self.data.shape}, \"\n",
    "            f\"target.shape={self.targets.shape=})\"\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx]\n",
    "        y = self.targets[idx]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm1d(nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-5, momentum: float = 0.1) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self.training = True\n",
    "        # Parameters (trained with backprop)\n",
    "        self.gamma = torch.ones(dim)\n",
    "        self.beta = torch.zeros(dim)\n",
    "        # Buffers (trained with running `momentum update`)\n",
    "        self.running_mean = torch.zeros(dim)\n",
    "        self.running_var = torch.ones(dim)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{self.__class__.__name__}()\"\n",
    "\n",
    "    def __call__(self, x: Tensor) -> Tensor:\n",
    "        if self.training:\n",
    "            if x.ndim == 2:\n",
    "                dim: tuple[int] | int = 0\n",
    "            elif x.ndim == 3:\n",
    "                dim = (0, 1)\n",
    "            # Calculate the batch mean and variance\n",
    "            x_mean: Tensor = x.mean(dim=dim, keepdim=True)\n",
    "            x_var: Tensor = x.var(dim=dim, keepdim=True)\n",
    "\n",
    "        else:\n",
    "            x_mean = self.running_mean\n",
    "            x_var = self.running_var\n",
    "\n",
    "        # Normalize the input\n",
    "        x_hat: Tensor = (x - x_mean) / (x_var + self.eps).sqrt()\n",
    "        self.output: Tensor = (self.gamma * x_hat) + self.beta\n",
    "\n",
    "        if self.training:\n",
    "            with torch.no_grad():\n",
    "                # Update running mean and variance\n",
    "                self.running_mean = (\n",
    "                    1 - self.momentum\n",
    "                ) * self.running_mean + self.momentum * x_mean\n",
    "                self.running_var = (\n",
    "                    1 - self.momentum\n",
    "                ) * self.running_var + self.momentum * x_var\n",
    "\n",
    "        return self.output\n",
    "\n",
    "    def parameters(self) -> list[Tensor]:\n",
    "        return [self.gamma, self.beta]\n",
    "\n",
    "\n",
    "class FlattenConsecutive(nn.Module):\n",
    "    \"\"\"A custom module that flattens consecutive elements in the input tensor along\n",
    "    the second dimension.\"\"\"\n",
    "\n",
    "    def __init__(self, n_c_elements: int) -> None:\n",
    "        \"\"\"\n",
    "        Note:\n",
    "            n_c_elements: the number of consecutive elements to concatenate.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.n_c_elements = n_c_elements\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{__class__.__name__}({self.n_c_elements})\"\n",
    "\n",
    "    def __call__(self, x: Tensor) -> Tensor:\n",
    "        # B: batch size, L: sequence length, C: number of channels\n",
    "        B, L, C = x.shape\n",
    "        assert (\n",
    "            L % self.n_c_elements == 0\n",
    "        ), f\"The sequence length of the input tensor must be a multiple of {self.n_c_elements}.\"\n",
    "\n",
    "        x = x.view(B, L // self.n_c_elements, C * self.n_c_elements)\n",
    "        if x.shape[1] == 1:\n",
    "            x = x.squeeze(1)\n",
    "\n",
    "        self.output = x\n",
    "        return self.output\n",
    "\n",
    "    def parameters(self) -> list[Tensor]:\n",
    "        return []\n",
    "\n",
    "\n",
    "class Tanh(nn.Module):\n",
    "    \"\"\"A custom module that applies the hyperbolic tangent activation function to the input tensor.\"\"\"\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{self.__class__.__name__}()\"\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return torch.tanh(x)\n",
    "\n",
    "    def __call__(self, x: Tensor) -> Tensor:\n",
    "        self.output = torch.tanh(x)\n",
    "        return self.output\n",
    "\n",
    "\n",
    "def calculate_loss_upd(model: Any, X: Tensor, y: Tensor, training: True) -> Tensor:\n",
    "    \"\"\"\n",
    "    Calculates the loss for the given input tensors `X` and `y`.\n",
    "\n",
    "    Args:\n",
    "        model (Any): The model to use for the forward pass.\n",
    "        X (torch.Tensor): The input tensor.\n",
    "        y (torch.Tensor): The target tensor.\n",
    "        training (bool): Indicates whether the model is in training mode or not.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The calculated loss.\n",
    "    \"\"\"\n",
    "\n",
    "    # Set the training mode to False\n",
    "    for layer in model.layers:\n",
    "        if hasattr(layer, \"training\"):\n",
    "            layer.training = False\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Forward pass\n",
    "        logits: Tensor = model(X)\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss: Tensor = F.cross_entropy(logits, y)\n",
    "        result: str = (\n",
    "            f\"Training loss: {loss:.4f}\" if training else f\"Validation loss: {loss:.4f}\"\n",
    "        )\n",
    "        print(result)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1!\n",
    "\n",
    "- The current inplementation flattens the entire data into a 2-D array which is not optimal when the block_size is large.\n",
    "- Implement the Wavenet architecture as shown in the diagram below using PyTorch `Linear`, `Embedding` modules and `Sequential` container.\n",
    "\n",
    "```py\n",
    "# e.g. If the block_size is 3\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "- Implement the [Wavenet](https://arxiv.org/pdf/1609.03499) architecture shown below:\n",
    "\n",
    "<img src=\"./images/Wavenet.png\" width=\"600\" height=\"300\" alt=\"Wavenet Architecture\">\n",
    "\n",
    "- The input is combined to form several bigrams which are further combined at every layer in the network.\n",
    "  - i.e. if you have 8 characters in the input layer, you will have 4 bigrams (e.g. `ab`, `cd`, `ef`, `gh`).\n",
    "  - `ab` is combined with `cd` to form `abcd`, `ef` is combined with `gh` to form `efgh` and so on.\n",
    "  - finally, `abcd` and `efgh` are combined and used to predict the output.\n",
    "\n",
    "<br><hr>\n",
    "\n",
    "```text\n",
    "Combining each input layer with the next one to form bigrams:\n",
    "\n",
    "FlattenConsecutive(2):   (4, 4, 20)   # i.e. 4 groups (1 2)   (3 4)   (5 6)   (7 8) \n",
    "...\n",
    "FlattenConsecutive(2):   (4, 2, 600)   # i.e. 2 groups (1 2 3 4)   (5 6 7 8) \n",
    "...\n",
    "FlattenConsecutive(2):   (4, 600)   # (4, 1, 600) i.e. 1 group (1 2 3 4 5 6 7 8) \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleWavenetModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_hidden: int,\n",
    "        n_c_elements: int,\n",
    "        vocab_size: int,\n",
    "        embedding_dim: int,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Embedding(vocab_size, embedding_dim),\n",
    "            # === Input Layer\n",
    "            FlattenConsecutive(n_c_elements),\n",
    "            nn.Linear(embedding_dim * n_c_elements, num_hidden),\n",
    "            BatchNorm1d(num_hidden),\n",
    "            Tanh(),\n",
    "            # === layer 1\n",
    "            FlattenConsecutive(n_c_elements),\n",
    "            nn.Linear(num_hidden * n_c_elements, num_hidden),\n",
    "            BatchNorm1d(num_hidden),\n",
    "            Tanh(),\n",
    "            # === layer 3\n",
    "            FlattenConsecutive(n_c_elements),\n",
    "            nn.Linear(num_hidden * n_c_elements, num_hidden),\n",
    "            BatchNorm1d(num_hidden),\n",
    "            Tanh(),\n",
    "            # === layer 3\n",
    "            nn.Linear(num_hidden, vocab_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x: Tensor = self.layers(x)\n",
    "        return x\n",
    "\n",
    "    def calculate_number_of_parameters(self) -> int:\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size: int = 8  # size of the context window for each input sequence\n",
    "\n",
    "# 8 characters are required to predict the next character\n",
    "build_dataset(names=names[:2], block_size=block_size, print_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size: int = 8  # size of the context window for each input sequence\n",
    "X, y = build_dataset(names=names, block_size=block_size, print_info=False)\n",
    "data: Dataset = MyDataset(X, y)\n",
    "\n",
    "X_train, X_dev, X_test, y_train, y_dev, y_test = split_data_into_train_dev_test(\n",
    "    data=data\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### =============== DEBUG ================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim: int = 10  # embedding dimension\n",
    "batch_size: int = 4\n",
    "n_c_elements: int = 2\n",
    "n_nodes: int = 64  # number of hidden nodes\n",
    "idx: Tensor = torch.randint(0, X_train.shape[0], (batch_size,))\n",
    "X_s, y_s = X_train[idx], y_train[idx]\n",
    "print(f\"{X_s.shape = }\")\n",
    "\n",
    "# 4 samples each containing a block size of `block_size`\n",
    "print(f\"{X_s = }\\n\")\n",
    "\n",
    "\n",
    "model: SimpleWavenetModel = SimpleWavenetModel(\n",
    "    num_hidden=n_nodes,\n",
    "    n_c_elements=n_c_elements,\n",
    "    vocab_size=n_chars,\n",
    "    embedding_dim=emb_dim,\n",
    ")\n",
    "logits: Tensor = model(X_s)\n",
    "print(f\"{logits.shape = }\")\n",
    "n_params: int = model.calculate_number_of_parameters()\n",
    "print(f\"Number of parameters: {n_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============== DEBUG ================\n",
    "# Define a constant for the padding width\n",
    "PADDING_WIDTH: int = 80\n",
    "\n",
    "for layer in model.layers:\n",
    "    layer_str: str = f\"{layer}:\"\n",
    "    try:\n",
    "        shape_str: str = f\"{tuple(layer.weight.shape)}\"\n",
    "    except Exception as e:\n",
    "        # print(f\"Error: {e}\")\n",
    "        shape_str: str = f\"{tuple(layer.output.shape)}\"\n",
    "    print(layer_str.ljust(PADDING_WIDTH) + shape_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell took ~12 minutes to run.\n",
    "\n",
    "emb_dim: int = 32  # embedding dimension\n",
    "batch_size: int = 64\n",
    "n_c_elements: int = 2\n",
    "n_nodes: int = 256  # number of hidden nodes\n",
    "epochs: int = 120_000  # number of epochs\n",
    "learning_rate: float = 0.01  # learning rate\n",
    "\n",
    "\n",
    "model: SimpleWavenetModel = SimpleWavenetModel(\n",
    "    num_hidden=n_nodes,\n",
    "    n_c_elements=n_c_elements,\n",
    "    vocab_size=n_chars,\n",
    "    embedding_dim=emb_dim,\n",
    ")\n",
    "n_params: int = model.calculate_number_of_parameters()\n",
    "print(f\"Number of parameters: {n_params:,}\\n\")\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Define the scheduler\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
    "    optimizer, milestones=[50_000, 100_000], gamma=0.1\n",
    ")\n",
    "\n",
    "# ==== Trainning Loop ====\n",
    "losses_all: list[float] = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # Add mini-batches\n",
    "    idx: Tensor = torch.randint(0, X_train.shape[0], size=(batch_size,))\n",
    "    # X, y batch\n",
    "    Xb, yb = X_train[idx], y_train[idx]\n",
    "\n",
    "    # Forward pass\n",
    "    logits: Tensor = model(Xb)\n",
    "    loss: Tensor = F.cross_entropy(logits, yb)\n",
    "\n",
    "    # Backward pass\n",
    "    # Reset gradients\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the parameters\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    # Record the loss\n",
    "    losses_all.append(loss.item())\n",
    "\n",
    "    if (epoch) % 10_000 == 0:\n",
    "        print(f\"Epoch: {epoch}/{epochs} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # if epoch > 50_000:\n",
    "    #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_loss_upd(model=model, X=X_train, y=y_train, training=True)\n",
    "calculate_loss_upd(model=model, X=X_dev, y=y_dev, training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_loss: Tensor = (\n",
    "    torch.tensor(losses_all, dtype=torch.float32).view(-1, 1_000).mean(dim=1)\n",
    ")\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))  # Create figure and axes\n",
    "ax.plot(avg_loss)  # Plot the data\n",
    "ax.set(xlabel=\"Epoch\", ylabel=\"Loss\", title=\"Loss vs Epoch\")  # Add labels\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample from the model (Untrained model!)\n",
    "g = torch.Generator().manual_seed(5)\n",
    "n_names: int = 20\n",
    "\n",
    "# Set the training mode to False\n",
    "for layer in model.layers:\n",
    "    if hasattr(layer, \"training\"):\n",
    "        layer.training = False\n",
    "\n",
    "\n",
    "for _ in range(n_names):\n",
    "\n",
    "    out: list[str] = []\n",
    "    context: list[int] = [0] * block_size  # initialize with all ...\n",
    "    while True:\n",
    "        # Forward pass the neural net\n",
    "        x: Tensor = torch.tensor([context])\n",
    "        logits: Tensor = model(x)\n",
    "        probs: Tensor = F.softmax(logits, dim=1)\n",
    "\n",
    "        # Sample from the distribution\n",
    "        idx: int = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "        # Shift the context window and track the samples\n",
    "        context = context[1:] + [idx]\n",
    "        out.append(idx)\n",
    "        # If we sample the special '.' token, break\n",
    "        if idx == 0:\n",
    "            break\n",
    "\n",
    "    # Decode and print the generated word\n",
    "    print(\"\".join(num_to_text.get(i) for i in out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_p11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
