{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Introduction To LangChain](https://docs.langchain.com/docs/)\n",
    "\n",
    "<br>\n",
    "\n",
    "- LangChain is a framework for developing applications powered by language models.\n",
    "\n",
    "## Topics Covered\n",
    "\n",
    "1. Making calls directly to OpenAI LLM.\n",
    "2. Using LangChain to create prompt templates and schemas\n",
    "3. Using LangChain to create output parsers.\n",
    "4. Using LangChain to create memory.\n",
    "5. Using LangChain to create chains.\n",
    "\n",
    "## Installation\n",
    "\n",
    "```sh\n",
    "pip install langchain\n",
    "\n",
    "# OR\n",
    "pip install 'langchain[all]'\n",
    "\n",
    "# Other dependencies\n",
    "pip install python-dotenv\n",
    "pip install openai\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in library\n",
    "import itertools\n",
    "import re\n",
    "import json\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "# Standard imports\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# pandas settings\n",
    "pd.options.display.max_rows = 1_000\n",
    "pd.options.display.max_columns = 1_000\n",
    "pd.options.display.max_colwidth = 600\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Black code formatter (Optional)\n",
    "%load_ext lab_black\n",
    "# auto reload imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
    "\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.chains.router import MultiPromptChain\n",
    "from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser\n",
    "from langchain.chains import SequentialChain, SimpleSequentialChain\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "\n",
    "from langchain.memory import (\n",
    "    ConversationBufferMemory,\n",
    "    ConversationSummaryBufferMemory,\n",
    "    ConversationTokenBufferMemory,\n",
    "    ConversationBufferWindowMemory,\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())  # read local .env file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Language models](https://python.langchain.com/docs/modules/model_io/models/)\n",
    "\n",
    "```text\n",
    "LangChain provides interfaces and integrations for two types of models:\n",
    "\n",
    "1. LLMs: Models that take a text string as input and return a text string\n",
    "\n",
    "2. Chat models: Models that are backed by a language model but take a list of Chat Messages as input and return a Chat Message.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [LLMs](https://python.langchain.com/docs/modules/model_io/models/llms/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def get_completion(prompt: str, model: str = \"gpt-3.5-turbo\"):\n",
    "    \"\"\"This is used to make a direct API calls to OpenAI.\"\"\"\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0,\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1+1 equals 2.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_completion(\"What is 1+1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate the text that is delimited by triple backticks \n",
      "into a style that is American English in a calm and respectful tone\n",
      ".\n",
      "text: ```\n",
      "Arrr, I be fuming that me blender lid flew off and splattered me kitchen walls with smoothie! And to make matters worse,the warranty don't cover the cost of cleaning up me kitchen. I need yer help right now, matey!\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer_email = \"\"\"\n",
    "Arrr, I be fuming that me blender lid \\\n",
    "flew off and splattered me kitchen walls \\\n",
    "with smoothie! And to make matters worse,\\\n",
    "the warranty don't cover the cost of \\\n",
    "cleaning up me kitchen. I need yer help \\\n",
    "right now, matey!\n",
    "\"\"\"\n",
    "\n",
    "style = \"\"\"American English \\\n",
    "in a calm and respectful tone\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"Translate the text \\\n",
    "that is delimited by triple backticks \n",
    "into a style that is {style}.\n",
    "text: ```{customer_email}```\n",
    "\"\"\"\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am quite frustrated that my blender lid flew off and made a mess of my kitchen walls with smoothie! To add to my frustration, the warranty does not cover the cost of cleaning up my kitchen. I kindly request your assistance at this moment, my friend.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = get_completion(prompt)\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Raw Response\n",
    "\n",
    "```python\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "model = \"gpt-3.5-turbo\"\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=model,\n",
    "    messages=messages,\n",
    "    temperature=0,\n",
    ")\n",
    "```\n",
    "\n",
    "\n",
    "======= Actual Response =======\n",
    "\n",
    "```json\n",
    "<OpenAIObject chat.completion id=chatcmpl-7gAPe01uBCvRy9hWn7dsa4ryWwMxf at 0x7fa538a2f060> JSON: {\n",
    "  \"id\": \"chatcmpl-7gAPe01uBCvRy9hWn7dsa4ryWwMxf\",\n",
    "  \"object\": \"chat.completion\",\n",
    "  \"created\": 1690284158,\n",
    "  \"model\": \"gpt-3.5-turbo-0613\",\n",
    "  \"choices\": [\n",
    "    {\n",
    "      \"index\": 0,\n",
    "      \"message\": {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"I am quite frustrated that my blender lid flew off and made a mess of my kitchen walls with smoothie! To add to my frustration, the warranty does not cover the cost of cleaning up my kitchen. I kindly request your assistance at this moment, my friend.\"\n",
    "      },\n",
    "      \"finish_reason\": \"stop\"\n",
    "    }\n",
    "  ],\n",
    "  \"usage\": {\n",
    "    \"prompt_tokens\": 89,\n",
    "    \"completion_tokens\": 53,\n",
    "    \"total_tokens\": 142\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Chat Models](https://python.langchain.com/docs/modules/model_io/models/chat/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo', temperature=0.0, model_kwargs={}, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=None, tiktoken_model_name=None)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# To control and reduce the randomness of the generated text, temperature=0\n",
    "TEMPERATURE = 0\n",
    "\n",
    "chat = ChatOpenAI(temperature=TEMPERATURE)\n",
    "chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate the text that is delimited by triple backticks into a style that is {style}. text: ```{text}```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a template for the input\n",
    "string_template = \"\"\"Translate the text that is delimited \\\n",
    "by triple backticks into a style that is {style}. \\\n",
    "text: ```{text}```\n",
    "\"\"\"\n",
    "\n",
    "print(string_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ChatOpenAI\n",
    "\n",
    "```text\n",
    "- ChatOpenAI is a Python package that allows you to interact with OpenAI chat models. \n",
    "- It provides a convenient way to send messages to the model and receive responses. \n",
    "- You can use the ChatOpenAI class from the `langchain.chat_models` module to create an instance of the chat model.\n",
    "```\n",
    "\n",
    "#### What is a prompt template?\n",
    "\n",
    "```text\n",
    "- A prompt template refers to a reproducible way to generate a prompt. \n",
    "- It contains a text string (\"the template\"), that can take in a set of parameters from the end user and generates a prompt.\n",
    "\n",
    "A prompt template can contain:\n",
    "  1. instructions to the language model,\n",
    "  2. a set of few shot examples to help the language model generate a better response,\n",
    "  3. a question to the language model.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['text', 'style'], output_parser=None, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['style', 'text'], output_parser=None, partial_variables={}, template='Translate the text that is delimited by triple backticks into a style that is {style}. text: ```{text}```\\n', template_format='f-string', validate_template=True), additional_kwargs={})])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a template for the prompt\n",
    "prompt_template = ChatPromptTemplate.from_template(string_template)\n",
    "prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['text', 'style'] output_parser=None partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['style', 'text'], output_parser=None, partial_variables={}, template='Translate the text that is delimited by triple backticks into a style that is {style}. text: ```{text}```\\n', template_format='f-string', validate_template=True), additional_kwargs={})]\n",
      "\n",
      "prompt=PromptTemplate(input_variables=['style', 'text'], output_parser=None, partial_variables={}, template='Translate the text that is delimited by triple backticks into a style that is {style}. text: ```{text}```\\n', template_format='f-string', validate_template=True) additional_kwargs={}\n",
      "\n",
      "input_variables=['style', 'text'] output_parser=None partial_variables={} template='Translate the text that is delimited by triple backticks into a style that is {style}. text: ```{text}```\\n' template_format='f-string' validate_template=True\n",
      "\n",
      "['style', 'text']\n"
     ]
    }
   ],
   "source": [
    "print(prompt_template)\n",
    "print()\n",
    "print(prompt_template.messages[0])\n",
    "print()\n",
    "print(prompt_template.messages[0].prompt)\n",
    "print()\n",
    "print(prompt_template.messages[0].prompt.input_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Docs\n",
    "\n",
    "- [PromptTemplate](https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/)\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Workflow\n",
    "\n",
    "```text\n",
    "1. Create a prompt with variables from a template.\n",
    "2. Format the prompt using the variables in the template.\n",
    "3. Make a prediction with the LLM using the formatted prompt.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "\n",
      "<class 'langchain.schema.messages.HumanMessage'>\n",
      "\n",
      "content=\"Translate the text that is delimited by triple backticks into a style that is American English in a calm and respectful tone.\\n. text: ```Arrr, I be fuming that me blender lid flew off and splattered me kitchen walls with smoothie! And to make matters worse, the warranty don't cover the cost of cleaning up me kitchen. I need yer help right now, matey!\\n```\\n\" additional_kwargs={} example=False\n"
     ]
    }
   ],
   "source": [
    "customer_style = \"\"\"American English in a calm \\\n",
    "and respectful tone.\n",
    "\"\"\"\n",
    "\n",
    "customer_email = \"\"\"Arrr, I be fuming that me blender lid \\\n",
    "flew off and splattered me kitchen walls \\\n",
    "with smoothie! And to make matters worse, \\\n",
    "the warranty don't cover the cost of \\\n",
    "cleaning up me kitchen. I need yer help \\\n",
    "right now, matey!\n",
    "\"\"\"\n",
    "\n",
    "# Format the prompt using the specified kwargs: `style` and `text` which are from the template.\n",
    "customer_messages = prompt_template.format_messages(\n",
    "    style=customer_style, text=customer_email\n",
    ")\n",
    "# OR\n",
    "# prompt_template.format_prompt(style=customer_style, text=customer_email)\n",
    "\n",
    "print(type(customer_messages))\n",
    "print()\n",
    "print(type(customer_messages[0]))\n",
    "print()\n",
    "print(customer_messages[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate the text that is delimited by triple backticks into a style that is American English in a calm and respectful tone.\n",
      ". text: ```Arrr, I be fuming that me blender lid flew off and splattered me kitchen walls with smoothie! And to make matters worse, the warranty don't cover the cost of cleaning up me kitchen. I need yer help right now, matey!\n",
      "```\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method ChatPromptTemplate.format of ChatPromptTemplate(input_variables=['text', 'style'], output_parser=None, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['style', 'text'], output_parser=None, partial_variables={}, template='Translate the text that is delimited by triple backticks into a style that is {style}. text: ```{text}```\\n', template_format='f-string', validate_template=True), additional_kwargs={})])>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(customer_messages[0].content)\n",
    "\n",
    "prompt_template.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Arrr, I'm quite frustrated that my blender lid flew off and splattered my kitchen walls with smoothie! And to make matters worse, the warranty doesn't cover the cost of cleaning up my kitchen. I would greatly appreciate your assistance at this moment, matey!\", additional_kwargs={}, example=False)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make predictions using the LLM\n",
    "# Translate the style of the customer message using the LLM\n",
    "customer_response = chat(customer_messages)\n",
    "customer_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Arrr, I'm quite frustrated that my blender lid flew off and splattered my kitchen walls with smoothie! And to make matters worse, the warranty doesn't cover the cost of cleaning up my kitchen. I would greatly appreciate your assistance at this moment, matey!\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "customer_response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give me an approximately 50 words description of Kubernetes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Kubernetes is an open-source container orchestration platform that automates the deployment, scaling, and management of containerized applications. It provides a framework for managing clusters of containers, allowing developers to easily deploy and manage applications across multiple hosts, while ensuring scalability, fault tolerance, and efficient resource utilization.', additional_kwargs={}, example=False)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Another example!\n",
    "prompt_template = (\n",
    "    \"\"\"Give me an approximately 50 words description of {subject_matter}.\"\"\"\n",
    ")\n",
    "subject_matter = \"Kubernetes\"\n",
    "\n",
    "# Create\n",
    "prompt = ChatPromptTemplate.from_template(template=prompt_template)\n",
    "\n",
    "# Format using the kwargs\n",
    "message = prompt.format_messages(subject_matter=subject_matter)\n",
    "print(message[0].content)\n",
    "\n",
    "# Make predictions\n",
    "response = chat(message)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Parsers\n",
    "\n",
    "```text\n",
    "Output parsers\n",
    "--------------\n",
    "\n",
    "- Language models output text but many times you may want to get more structured information than just text back. \n",
    "\n",
    "- This is where output parsers come in. Output parsers are classes that help structure language model responses. \n",
    "\n",
    "- There are two main methods an output parser must implement:\n",
    "1. \"Get format instructions\": A method which returns a string containing instructions for how the output of a language model should be formatted.\n",
    "2. \"Parse\": A method which takes in a string (assumed to be the response from a language model) and parses it into some structure.\n",
    "And then one optional one:\n",
    "3. \"Parse with prompt\": A method which takes in a string (assumed to be the response from a language model) and a prompt (assumed to the prompt that generated such a response) and parses it into some structure. The prompt is largely provided in the event the OutputParser wants to retry or fix the output in some way, and needs information from the prompt to do so.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='{\\n  \"gift\": false,\\n  \"delivery_days\": 2,\\n  \"price_value\": [\"It\\'s slightly more expensive than the other leaf blowers out there, but I think it\\'s worth it for the extra features.\"]\\n}', additional_kwargs={}, example=False)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Ex 1\n",
    "# Sample output\n",
    "{\"gift\": False, \"delivery_days\": 5, \"price_value\": \"pretty affordable!\"}\n",
    "\n",
    "# Text\n",
    "customer_review = \"\"\"\\\n",
    "This leaf blower is pretty amazing. It has four settings:\\\n",
    "candle blower, gentle breeze, windy city, and tornado. \\\n",
    "It arrived in two days, just in time for my wife's \\\n",
    "anniversary present. \\\n",
    "I think my wife liked it so much she was speechless. \\\n",
    "So far I've been the only one using it, and I've been \\\n",
    "using it every other morning to clear the leaves on our lawn. \\\n",
    "It's slightly more expensive than the other leaf blowers \\\n",
    "out there, but I think it's worth it for the extra features.\n",
    "\"\"\"\n",
    "\n",
    "review_template = \"\"\"\\\n",
    "For the following text, extract the following information:\n",
    "\n",
    "gift: Was the item purchased as a gift for someone else? \\\n",
    "Answer True if yes, False if not or unknown.\n",
    "\n",
    "delivery_days: How many days did it take for the product \\\n",
    "to arrive? If this information is not found, output -1.\n",
    "\n",
    "price_value: Extract any sentences about the value or price,\\\n",
    "and output them as a comma separated Python list.\n",
    "\n",
    "Format the output as JSON with the following keys:\n",
    "gift\n",
    "delivery_days\n",
    "price_value\n",
    "\n",
    "text: {text}\n",
    "\"\"\"\n",
    "\n",
    "# Instantiate\n",
    "TEMPERATURE = 0\n",
    "chat = ChatOpenAI(temperature=TEMPERATURE)\n",
    "\n",
    "# Create\n",
    "prompt_template = ChatPromptTemplate.from_template(review_template)\n",
    "\n",
    "# Format using the kwargs\n",
    "message = prompt_template.format_messages(text=customer_review)\n",
    "\n",
    "# Predict\n",
    "response = chat(message)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"gift\": false,\n",
      "  \"delivery_days\": 2,\n",
      "  \"price_value\": [\"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"]\n",
      "}\n",
      "Type: <class 'str'>\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'{\\n  \"gift\": false,\\n  \"delivery_days\": 2,\\n  \"price_value\": [\"It\\'s slightly more expensive than the other leaf blowers out there, but I think it\\'s worth it for the extra features.\"]\\n}'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(response.content)\n",
    "\n",
    "# The output is a string which can be formatted (converted to a better format)\n",
    "print(f\"Type: {type(response.content)}\\n\")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\\n\\n```json\\n{\\n\\t\"gift\": string  // Was the item purchased                as a gift for someone else?                 Answer True if yes,                False if not or unknown.\\n\\t\"delivery_days\": string  // How many days                did it take for the product                to arrive? If this                 information is not found,                output -1.\\n\\t\"price_value\": string  // Extract any                sentences about the value or                 price, and output them as a                 comma separated Python list.\\n}\\n```'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
    "\n",
    "\n",
    "# Convert to dict\n",
    "# Create schemas\n",
    "gift_schema = ResponseSchema(\n",
    "    name=\"gift\",\n",
    "    description=\"Was the item purchased\\\n",
    "                as a gift for someone else? \\\n",
    "                Answer True if yes,\\\n",
    "                False if not or unknown.\",\n",
    ")\n",
    "delivery_days_schema = ResponseSchema(\n",
    "    name=\"delivery_days\",\n",
    "    description=\"How many days\\\n",
    "                did it take for the product\\\n",
    "                to arrive? If this \\\n",
    "                information is not found,\\\n",
    "                output -1.\",\n",
    ")\n",
    "price_value_schema = ResponseSchema(\n",
    "    name=\"price_value\",\n",
    "    description=\"Extract any\\\n",
    "                sentences about the value or \\\n",
    "                price, and output them as a \\\n",
    "                comma separated Python list.\",\n",
    ")\n",
    "\n",
    "response_schemas: List[ResponseSchema] = [\n",
    "    gift_schema,\n",
    "    delivery_days_schema,\n",
    "    price_value_schema,\n",
    "]\n",
    "\n",
    "# Parse the output\n",
    "# Create\n",
    "output_parser = StructuredOutputParser(response_schemas=response_schemas)\n",
    "\n",
    "# Format the instructions\n",
    "formatted_instructions = output_parser.get_format_instructions()\n",
    "formatted_instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: <class 'dict'>\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'gift': False,\n",
       " 'delivery_days': 2,\n",
       " 'price_value': [\"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"]}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Parse the response\n",
    "parsed_output = output_parser.parse(response.content)\n",
    "print(f\"Type: {type(parsed_output)}\\n\")\n",
    "\n",
    "parsed_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='{\\n  \"gift\": true,\\n  \"delivery_days\": 7,\\n  \"price_value\": [\"It\\'s also not an expensive device, which makes it even better.\"]\\n}', additional_kwargs={}, example=False)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Ex. 2\n",
    "customer_review = \"\"\"\\\n",
    "Oppo A96 is an awesome phone. My wfe gifted me the smartphone \\\n",
    "and I absolutely love it. Although it didn't arrive early. \\ \n",
    "I had to wait for 1 week after she ordered the device.\\\n",
    "It's also not an expensive device, which makes it even better.\n",
    "\"\"\"\n",
    "\n",
    "# Create and format\n",
    "prompt_template = ChatPromptTemplate.from_template(template=review_template)\n",
    "message = prompt_template.format_messages(text=customer_review)\n",
    "\n",
    "# Predict\n",
    "response = chat(message)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><br>\n",
    "\n",
    "## LangChain: Memory\n",
    "\n",
    "```text\n",
    "Memory\n",
    "------\n",
    "- Refers to the ability of the Langchain system to store and recall information about specific entities mentioned in a conversation. \n",
    "- It uses a key-value store to keep track of facts and details about these entities. \n",
    "- The memory functionality is implemented using the ConversationEntityMemory class, which extracts information about entities using a language model and builds up its knowledge about those entities over time.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConversationBufferMemory\n",
    "\n",
    "```text\n",
    "ConversationBufferMemory\n",
    "------------------------\n",
    "- It's a class that allows you to customize the conversation history in a conversational agent. \n",
    "- It's used to store and retrieve previous messages in a conversation. \n",
    "- By using the ConversationBufferMemory, you can create a more interactive and context-aware conversation experience.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConversationChain(memory=ConversationBufferMemory(chat_memory=ChatMessageHistory(messages=[]), output_key=None, input_key=None, return_messages=False, human_prefix='Human', ai_prefix='AI', memory_key='history'), callbacks=None, callback_manager=None, verbose=True, tags=None, metadata=None, prompt=PromptTemplate(input_variables=['history', 'input'], output_parser=None, partial_variables={}, template='The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\n\\nCurrent conversation:\\n{history}\\nHuman: {input}\\nAI:', template_format='f-string', validate_template=True), llm=ChatOpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo', temperature=0.0, model_kwargs={}, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=None, tiktoken_model_name=None), output_key='response', output_parser=NoOpOutputParser(), return_final_only=True, llm_kwargs={}, input_key='input')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create an LLM\n",
    "llm = ChatOpenAI(temperature=TEMPERATURE)\n",
    "\n",
    "# Create memory\n",
    "memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=True,  # In verbose mode, some intermediate logs will be printed to the console\n",
    ")\n",
    "conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hello, my name is Neidu\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hello Neidu! It's nice to meet you. How can I assist you today?\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hello, my name is Neidu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hello, my name is Neidu\n",
      "AI: Hello Neidu! It's nice to meet you. How can I assist you today?\n",
      "Human: What is the sum of 1 and 1?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The sum of 1 and 1 is 2.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is the sum of 1 and 1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hello, my name is Neidu\n",
      "AI: Hello Neidu! It's nice to meet you. How can I assist you today?\n",
      "Human: What is the sum of 1 and 1?\n",
      "AI: The sum of 1 and 1 is 2.\n",
      "Human: What is my name?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Your name is Neidu.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hello, my name is Neidu\n",
      "AI: Hello Neidu! It's nice to meet you. How can I assist you today?\n",
      "Human: What is the sum of 1 and 1?\n",
      "AI: The sum of 1 and 1 is 2.\n",
      "Human: What is my name?\n",
      "AI: Your name is Neidu.\n"
     ]
    }
   ],
   "source": [
    "# Content of the memory\n",
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hello\n",
      "AI: What's good my fella?\n"
     ]
    }
   ],
   "source": [
    "# Instantiate a memory object and simulate conversation\n",
    "memory = ConversationBufferMemory()\n",
    "memory.save_context(\n",
    "    inputs={\"input\": \"Hello\"},  # Human\n",
    "    outputs={\"output\": \"What's good my fella?\"},  # AI\n",
    ")\n",
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hello\\nAI: What's good my fella?\"}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Contents of the memory as a dict\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': 'Human: Hello\\n'\n",
      "            \"AI: What's good my fella?\\n\"\n",
      "            \"Human: Nothin' much. I'm alright.\\n\"\n",
      "            'AI: Cool!'}\n"
     ]
    }
   ],
   "source": [
    "# Continue the conversation\n",
    "memory.save_context(\n",
    "    inputs={\"input\": \"Nothin' much. I'm alright.\"},  # Human\n",
    "    outputs={\"output\": \"Cool!\"},  # AI\n",
    ")\n",
    "\n",
    "# Contents of the memory as a dict\n",
    "# It uses `AI` as the prefix by default.\n",
    "pprint(memory.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Customize AI Prefix\n",
    "\n",
    "```text\n",
    "- The first way to do so is by changing the AI prefix in the conversation summary. \n",
    "- By default, this is set to \"AI\", but you can set this to be anything you want. \n",
    "- Note that if you change this, you should also change the prompt used in the chain to reflect this naming change.\n",
    "``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mHuman: The following is a friendly conversation between a human and an AI. \\ \n",
      "The AI is talkative and provides lots of specific details from its context. \\ \n",
      "If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Whassup?\n",
      "Decide Assistant:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hello! I'm an AI assistant here to help you with any questions or tasks you have. How can I assist you today?\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now we can override it and set it to \"Decide Assistant\"\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "# The prefix has been changed to `AI Assistant`\n",
    "template = \"\"\"The following is a friendly conversation between a human and an AI. \\ \n",
    "The AI is talkative and provides lots of specific details from its context. \\ \n",
    "If the AI does not know the answer to a question, it truthfully says it does not know.\n",
    "\n",
    "Current conversation:\n",
    "{history}\n",
    "Human: {input}\n",
    "Decide Assistant:\n",
    "\"\"\"\n",
    "memory = ConversationBufferMemory(ai_prefix=\"Decide Assistant\")\n",
    "\n",
    "# Create prompt template\n",
    "prompt = ChatPromptTemplate.from_template(template=template)\n",
    "# prompt = prompt.format_messages()\n",
    "conversation = ConversationChain(\n",
    "    prompt=prompt,\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    memory=memory,\n",
    ")\n",
    "\n",
    "conversation.predict(input=\"Whassup?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': 'Human: Whassup?\\n'\n",
      "            \"Decide Assistant: Hello! I'm an AI assistant here to help you \"\n",
      "            'with any questions or tasks you have. How can I assist you today?'}\n"
     ]
    }
   ],
   "source": [
    "pprint(memory.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Customize Human Prefix\n",
    "\n",
    "- By default, this is set to \"Human\", but you can set this to be anything you want. \n",
    "- Note that if you change this, you should also change the prompt used in the chain to reflect this naming change\n",
    "``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mHuman: The following is a friendly conversation between a human and an AI. \\ \n",
      "The AI is talkative and provides lots of specific details from its context. \\ \n",
      "If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "User: Whassup?\n",
      "Decide Assistant:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hello! I'm an AI assistant here to help you with any questions or tasks you have. How can I assist you today?\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now we can override it and set it to \"User\"\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "# The prefix has been changed to `AI Assistant`\n",
    "template = \"\"\"The following is a friendly conversation between a human and an AI. \\ \n",
    "The AI is talkative and provides lots of specific details from its context. \\ \n",
    "If the AI does not know the answer to a question, it truthfully says it does not know.\n",
    "\n",
    "Current conversation:\n",
    "{history}\n",
    "User: {input}\n",
    "Decide Assistant:\n",
    "\"\"\"\n",
    "memory = ConversationBufferMemory(human_prefix=\"User\")\n",
    "\n",
    "# Create prompt template\n",
    "prompt = ChatPromptTemplate.from_template(template=template)\n",
    "# prompt = prompt.format_messages()\n",
    "conversation = ConversationChain(\n",
    "    prompt=prompt,\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    memory=memory,\n",
    ")\n",
    "\n",
    "conversation.predict(input=\"Whassup?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': 'User: Whassup?\\n'\n",
      "            \"AI: Hello! I'm an AI assistant here to help you with any \"\n",
      "            'questions or tasks you have. How can I assist you today?'}\n"
     ]
    }
   ],
   "source": [
    "pprint(memory.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### ConversationBufferWindowMemory\n",
    "\n",
    "```text\n",
    "------------------------------\n",
    "- This is a memory implementation that stores conversation history in a buffer with a fixed window size. \n",
    "- It keeps track of the most recent conversations and discards older conversations when the buffer is full.\n",
    "- This type of memory can be useful when you only need to retain a limited amount of conversation history and want to prioritize recent interactions. \n",
    "- It helps in managing memory usage and allows the conversational agent to focus on recent context.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': 'Human: Not much, just hanging\\nDecide Assistant: Cool'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "\n",
    "# Instantiate a memory object and simulate conversation\n",
    "memory = ConversationBufferWindowMemory(\n",
    "    ai_prefix=\"Decide Assistant\",\n",
    "    k=1,  # Number of messages to store in buffer.\n",
    ")\n",
    "memory.save_context(\n",
    "    inputs={\"input\": \"Hello\"},  # Human\n",
    "    outputs={\"output\": \"What's good?\"},  # AI\n",
    ")\n",
    "memory.save_context(\n",
    "    {\"input\": \"Not much, just hanging\"},  # Human\n",
    "    {\"output\": \"Cool\"},  # AI\n",
    ")\n",
    "\n",
    "\n",
    "# It ONLY returns the last message\n",
    "pprint(memory.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### ConversationTokenBufferMemory\n",
    "\n",
    "```text\n",
    "- It's a memory implementation that stores conversation history in a buffer with a fixed window size. \n",
    "- It's useful when you only need to retain a limited amount of conversation history and want to prioritize recent interactions.\n",
    "- It helps in managing memory usage and allows the conversational agent to focus on recent context.\n",
    "```\n",
    "\n",
    "#### Dependency\n",
    "\n",
    "```sh\n",
    "pip install tiktoken\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': 'Human: Not much, just hanging\\nDecide Assistant: Cool'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "\n",
    "\n",
    "N_TOKENS = 20\n",
    "memory = ConversationTokenBufferMemory(\n",
    "    llm=llm,\n",
    "    ai_prefix=\"Decide Assistant\",\n",
    "    max_token_limit=N_TOKENS,  # NEW: token limit\n",
    ")\n",
    "\n",
    "memory.save_context(\n",
    "    inputs={\"input\": \"Hello\"},  # Human\n",
    "    outputs={\"output\": \"What's good?\"},  # AI\n",
    ")\n",
    "memory.save_context(\n",
    "    inputs={\"input\": \"Not much, just hanging\"},  # Human\n",
    "    outputs={\"output\": \"Cool\"},  # AI\n",
    ")\n",
    "\n",
    "\n",
    "# It ONLY returns the last N tokens\n",
    "pprint(memory.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ConversationSummaryMemory\n",
    "\n",
    "```text\n",
    "- It's a memory implementation that stores conversation summaries in memory. \n",
    "- It keeps track of the conversation history and provides a summary of the conversation for reference.\n",
    "- It summarizes the conversation history instead of storing it verbatim.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "\n",
    "# Create a long string\n",
    "schedule = \"There is a meeting at 8am with your product team. \\\n",
    "You will need your powerpoint presentation prepared. \\\n",
    "9am-12pm have time to work on your LangChain \\\n",
    "project which will go quickly because Langchain is such a powerful tool. \\\n",
    "At Noon, lunch at the italian resturant with a customer who is driving \\\n",
    "from over an hour away to meet you to understand the latest in AI. \\\n",
    "Be sure to bring your laptop to show the latest LLM demo.\"\n",
    "\n",
    "# Init memory and summarize the conversation\n",
    "N_TOKENS = 50\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=N_TOKENS,  # NEW: token limit\n",
    ")\n",
    "memory.save_context(\n",
    "    inputs={\"input\": \"Hello\"},\n",
    "    outputs={\"output\": \"What's up?\"},\n",
    ")\n",
    "memory.save_context(\n",
    "    inputs={\"input\": \"Not much, just hangin'\"},\n",
    "    outputs={\"output\": \"Cool!\"},\n",
    ")\n",
    "memory.save_context(\n",
    "    inputs={\"input\": \"What's on the schedule today?\"},\n",
    "    outputs={\"output\": f\"{schedule}\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': 'System: The human and AI exchange greetings. The human asks about '\n",
      "            \"the AI's schedule for the day. The AI provides a detailed \"\n",
      "            'schedule, including a meeting with the product team, work on the '\n",
      "            'LangChain project, and a lunch meeting with a customer interested '\n",
      "            'in AI. The AI emphasizes the importance of bringing a laptop to '\n",
      "            'showcase the latest LLM demo during the lunch meeting.'}\n"
     ]
    }
   ],
   "source": [
    "# Display the summarized conversation\n",
    "pprint(memory.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: The human and AI exchange greetings. The human asks about the AI's schedule for the day. The AI provides a detailed schedule, including a meeting with the product team, work on the LangChain project, and a lunch meeting with a customer interested in AI. The AI emphasizes the importance of bringing a laptop to showcase the latest LLM demo during the lunch meeting.\n",
      "Human: What would be a good demo to show?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"A good demo to show during the lunch meeting would be the latest version of the Language Learning Model (LLM) that we have been working on. It's a cutting-edge AI model that can understand and generate human-like text in multiple languages. We can showcase its capabilities by demonstrating how it can translate text from one language to another with high accuracy and fluency. Additionally, we can highlight its ability to generate creative and contextually relevant responses in a conversation. This demo would impress the customer and showcase the potential of AI in their business.\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a conversation object\n",
    "conversation = ConversationChain(llm=llm, memory=memory, verbose=True)\n",
    "conversation.predict(input=\"What would be a good demo to show?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': 'System: The human and AI exchange greetings. The human asks about '\n",
      "            \"the AI's schedule for the day. The AI provides a detailed \"\n",
      "            'schedule, including a meeting with the product team, work on the '\n",
      "            'LangChain project, and a lunch meeting with a customer interested '\n",
      "            'in AI. The AI emphasizes the importance of bringing a laptop to '\n",
      "            'showcase the latest LLM demo during the lunch meeting. The human '\n",
      "            'asks what would be a good demo to show, and the AI suggests '\n",
      "            'showcasing the Language Learning Model (LLM), a cutting-edge AI '\n",
      "            'model that can understand and generate human-like text in '\n",
      "            'multiple languages. The AI explains that they can demonstrate its '\n",
      "            'translation capabilities and its ability to generate creative and '\n",
      "            'contextually relevant responses in a conversation, which would '\n",
      "            'impress the customer and showcase the potential of AI in their '\n",
      "            'business.'}\n"
     ]
    }
   ],
   "source": [
    "# Display the summarized conversation\n",
    "pprint(memory.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
