{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab\n",
    "\n",
    "```text\n",
    "Retriever Augmented Generation\n",
    "------------------------------\n",
    "- Create a chatbot using RAG (Retriever Augmented Generation)\n",
    "- RAG is a type of language generation model that combines pre-trained parametric and non-parametric (source) memory for language generation.\n",
    "- In this case, source knoledge is used to update the knowledge of the LLM.\n",
    "- Examples of source knowledge include data from external sources like PDFs, URLs, CSVs, etc.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in library\n",
    "import itertools\n",
    "import re\n",
    "import json\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "# Standard imports\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# pandas settings\n",
    "pd.options.display.max_rows = 1_000\n",
    "pd.options.display.max_columns = 1_000\n",
    "pd.options.display.max_colwidth = 600\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Black code formatter (Optional)\n",
    "%load_ext lab_black\n",
    "# auto reload imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())  # read local .env file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps\n",
    "\n",
    "```text\n",
    "- Load data\n",
    "- Split data\n",
    "- Store the data (i.e. embeddings).\n",
    "- Retrieve the data.\n",
    "- Add memory\n",
    "- Engineer prompt template\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "```sh\n",
    "pip install jq\n",
    "pip install tiktoken\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = \"../../data/res.json\"\n",
    "\n",
    "with open(fp, \"r\") as f:\n",
    "    json_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Encoding 'cl100k_base'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "\n",
    "# Get the tokenizer name\n",
    "model_name = \"gpt-3.5-turbo\"\n",
    "encoding_name = tiktoken.encoding_for_model(model_name=model_name)\n",
    "encoding_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "\n",
    "def count_tokens(*, chain: Any, query: str) -> Any:\n",
    "    \"\"\"This is used to count the number of tokens sent to the LLM.\"\"\"\n",
    "    with get_openai_callback() as cb:\n",
    "        result = chain.run(query)\n",
    "        print(f\"Spent a total of {cb.total_tokens!r} tokens\")\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def calculate_number_of_tokens(*, text: str, model_name: str = \"gpt-3.5-turbo\") -> int:\n",
    "    \"\"\"This is usd to count the number of tokens.\"\"\"\n",
    "    import tiktoken\n",
    "\n",
    "    encoding_name = tiktoken.encoding_for_model(model_name=model_name)\n",
    "    encoding_name = re.compile(pattern=r\"(\\<|\\>|[\\'\\\"\\s]|Encoding)\").sub(\n",
    "        repl=\"\", string=str(encoding_name)\n",
    "    )\n",
    "    tokenizer = tiktoken.get_encoding(encoding_name)\n",
    "    tokens = tokenizer.encode(text, disallowed_special=())\n",
    "    return len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = (\n",
    "    \"hello I am a chunk of text and using the calculate_number_of_tokens function \"\n",
    "    \"we can find the length of this chunk of text in tokens\"\n",
    ")\n",
    "calculate_number_of_tokens(text=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1425"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import JSONLoader\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import (\n",
    "    CharacterTextSplitter,\n",
    "    RecursiveCharacterTextSplitter,\n",
    ")\n",
    "\n",
    "\n",
    "# Load JSON data\n",
    "loader = JSONLoader(\n",
    "    file_path=fp,\n",
    "    jq_schema=\".data\",\n",
    "    text_content=False,\n",
    ")\n",
    "\n",
    "data = loader.load()\n",
    "\n",
    "# Embedding model\n",
    "embedding = OpenAIEmbeddings()\n",
    "persist_directory = \"../../data/doc_db\"\n",
    "\n",
    "\n",
    "# Count the number tokens\n",
    "# LLMs struggle when the number of tokens are too many\n",
    "calculate_number_of_tokens(text=data[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CHUNK_SIZE = 1_000\n",
    "CHUNK_OVERLAP = 50\n",
    "\n",
    "# Split the doc(s)\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    "    separators=[\"\\n\\n\", \"\\n\", r\"\\{\\}\", \"\"],\n",
    ")\n",
    "splits = text_splitter.split_documents(documents=data)\n",
    "len(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run once!\n",
    "# Create and save vectorstore to disk\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embedding,\n",
    "    persist_directory=persist_directory,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "vectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding)\n",
    "\n",
    "# Number of docs\n",
    "print(vectordb._collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import (\n",
    "    ConversationBufferWindowMemory,\n",
    "    ConversationTokenBufferMemory,\n",
    ")\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "# Create memory\n",
    "memory = ConversationBufferWindowMemory(\n",
    "    k=5,  # window size\n",
    "    input_key=\"question\",\n",
    "    memory_key=\"chat_history\",\n",
    ")\n",
    "\n",
    "retriever = vectordb.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init Chatbot\n",
    "decide_bot = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,  # retrieve data from data source\n",
    "    verbose=False,\n",
    "    chain_type_kwargs={\n",
    "        \"document_separator\": \"<<<<>>>>>\",\n",
    "        \"memory\": memory,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'What is the maxEMIEligibility of the customer?',\n",
       " 'result': 'The maxEMIEligibility of the customer is 2898.'}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What is the maxEMIEligibility of the customer?\"\n",
    "# result = count_tokens(qa_bot, {\"query\": question})\n",
    "result = decide_bot({\"query\": question})\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'What is the average monthly debit?',\n",
       " 'result': 'The average monthly debit is as follows:\\n- February 2022: ₦3,065.48\\n- March 2022: ₦60,793.96\\n- April 2022: ₦114,106.87\\n- May 2022: ₦86,431.17'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What is the average monthly debit?\"\n",
    "result = decide_bot({\"query\": question})\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'What is the averageMonthlyDebit?',\n",
       " 'result': 'The averageMonthlyDebit is [{\"month\": \"2022-04\", \"amount\": 5705.34}, {\"month\": \"2022-05\", \"amount\": 5401.95}, {\"month\": \"2022-02\", \"amount\": 383.19}, {\"month\": \"2022-03\", \"amount\": 1599.84}].'}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = (\n",
    "    \"What is the averageMonthlyDebit?\"  # It's better to use the exact variable names\n",
    ")\n",
    "result = decide_bot({\"query\": question})\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is the customer ID and is the customer a salary earner?\"\n",
    "result = decide_bot({\"query\": question})\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Are there any recurringExpenses in the transaction data? If yes, list them\"\n",
    "result = decide_bot({\"query\": question})\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Which month had the highest totalMonthlyDebit and what was the amount?\"\n",
    "result = decide_bot({\"query\": question})\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most recent chat history.\n",
    "pprint(memory.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "import click\n",
    "from langchain.document_loaders import JSONLoader\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class DecideBot(BaseModel):\n",
    "    \"\"\"This is used to create the Decide AI Assistant.\"\"\"\n",
    "\n",
    "    # Constants\n",
    "    PERSIST_DIRECTORY: str = \"./data/doc_db\"\n",
    "    CHUNK_SIZE: int = 1_000\n",
    "    CHUNK_OVERLAP: int = 50\n",
    "    TEMPERATURE: float = 0.0\n",
    "\n",
    "    # Variables\n",
    "    filepath: str\n",
    "\n",
    "    def _load_data(self) -> List[Any]:\n",
    "        \"\"\"This is used to load the JSON data.\"\"\"\n",
    "        loader = JSONLoader(\n",
    "            file_path=self.filepath,\n",
    "            jq_schema=\".data\",\n",
    "            text_content=False,\n",
    "        )\n",
    "        data = loader.load()\n",
    "        return data\n",
    "\n",
    "    def _preprocess_data(self) -> Chroma:\n",
    "        \"\"\"This is used to split and store the embedded data in a vector store.\"\"\"\n",
    "        data = self._load_data()\n",
    "\n",
    "        # Embedding model\n",
    "        embedding = OpenAIEmbeddings()\n",
    "\n",
    "        # Split the doc(s)\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=self.CHUNK_SIZE,\n",
    "            chunk_overlap=self.CHUNK_OVERLAP,\n",
    "            separators=[\"\\n\\n\", \"\\n\", r\"\\{\\}\", \"\"],\n",
    "        )\n",
    "        splits = text_splitter.split_documents(documents=data)\n",
    "\n",
    "        # Create and save vectors tore to disk\n",
    "        vector_db = Chroma.from_documents(\n",
    "            documents=splits,\n",
    "            embedding=embedding,\n",
    "            persist_directory=self.PERSIST_DIRECTORY,\n",
    "        )\n",
    "        return vector_db\n",
    "\n",
    "    # Create bot\n",
    "    def _create_RAG_model(self) -> RetrievalQA:\n",
    "        \"\"\"This is used to create a RAG model for question and answering.\"\"\"\n",
    "        llm = ChatOpenAI(temperature=self.TEMPERATURE)\n",
    "        # Create memory\n",
    "        memory = ConversationBufferWindowMemory(\n",
    "            k=5,  # window size\n",
    "            input_key=\"question\",\n",
    "            memory_key=\"chat_history\",\n",
    "        )\n",
    "        vector_db = self._preprocess_data()\n",
    "\n",
    "        # Init Chatbot\n",
    "        decide_bot = RetrievalQA.from_chain_type(\n",
    "            llm=llm,\n",
    "            chain_type=\"stuff\",\n",
    "            retriever=vector_db.as_retriever(),  # retrieve data from data source\n",
    "            verbose=False,\n",
    "            chain_type_kwargs={\n",
    "                \"document_separator\": \"<<<<>>>>>\",\n",
    "                \"memory\": memory,\n",
    "            },\n",
    "        )\n",
    "        return decide_bot\n",
    "\n",
    "    # Generate answers\n",
    "    def generate_response(self, *, question: str) -> Dict[str, Any]:\n",
    "        \"\"\"This is a chat bot used for question and answering.\"\"\"\n",
    "        decide_bot = self._create_RAG_model()\n",
    "        result = decide_bot({\"query\": question})\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecideBot(PERSIST_DIRECTORY='./data/doc_db', CHUNK_SIZE=1000, CHUNK_OVERLAP=50, TEMPERATURE=0.0, filepath='../../data/res.json')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_ai = DecideBot(filepath=fp)\n",
    "\n",
    "d_ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_p310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
