{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Introduction To LangChain](https://docs.langchain.com/docs/)\n",
    "\n",
    "<br>\n",
    "\n",
    "- LangChain is a framework for developing applications powered by language models.\n",
    "\n",
    "## Topics Covered\n",
    "\n",
    "1. Making calls directly to OpenAI LLM.\n",
    "2. Using LangChain to create prompt templates and schemas\n",
    "3. Using LangChain to create output parsers.\n",
    "4. Using LangChain to create memory.\n",
    "5. Using LangChain to create chains.\n",
    "\n",
    "## Installation\n",
    "\n",
    "```sh\n",
    "pip install langchain\n",
    "\n",
    "# OR\n",
    "pip install 'langchain[all]'\n",
    "\n",
    "# Other dependencies\n",
    "pip install python-dotenv\n",
    "pip install openai\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in library\n",
    "import itertools\n",
    "import re\n",
    "import json\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "# Standard imports\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# pandas settings\n",
    "pd.options.display.max_rows = 1_000\n",
    "pd.options.display.max_columns = 1_000\n",
    "pd.options.display.max_colwidth = 600\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Black code formatter (Optional)\n",
    "%load_ext lab_black\n",
    "# auto reload imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
    "\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.chains.router import MultiPromptChain\n",
    "from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser\n",
    "from langchain.chains import SequentialChain, SimpleSequentialChain\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "\n",
    "from langchain.memory import (\n",
    "    ConversationBufferMemory,\n",
    "    ConversationSummaryBufferMemory,\n",
    "    ConversationTokenBufferMemory,\n",
    "    ConversationBufferWindowMemory,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())  # read local .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt: str, model: str = \"gpt-3.5-turbo\"):\n",
    "    \"\"\"This is used to make a direct API calls to OpenAI.\"\"\"\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0,\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1+1 equals 2.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_completion(\"What is 1+1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate the text that is delimited by triple backticks \n",
      "into a style that is American English in a calm and respectful tone\n",
      ".\n",
      "text: ```\n",
      "Arrr, I be fuming that me blender lid flew off and splattered me kitchen walls with smoothie! And to make matters worse,the warranty don't cover the cost of cleaning up me kitchen. I need yer help right now, matey!\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer_email = \"\"\"\n",
    "Arrr, I be fuming that me blender lid \\\n",
    "flew off and splattered me kitchen walls \\\n",
    "with smoothie! And to make matters worse,\\\n",
    "the warranty don't cover the cost of \\\n",
    "cleaning up me kitchen. I need yer help \\\n",
    "right now, matey!\n",
    "\"\"\"\n",
    "\n",
    "style = \"\"\"American English \\\n",
    "in a calm and respectful tone\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"Translate the text \\\n",
    "that is delimited by triple backticks \n",
    "into a style that is {style}.\n",
    "text: ```{customer_email}```\n",
    "\"\"\"\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am quite frustrated that my blender lid flew off and made a mess of my kitchen walls with smoothie! To add to my frustration, the warranty does not cover the cost of cleaning up my kitchen. I kindly request your assistance at this moment, my friend.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = get_completion(prompt)\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Raw Response\n",
    "\n",
    "```python\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "model = \"gpt-3.5-turbo\"\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=model,\n",
    "    messages=messages,\n",
    "    temperature=0,\n",
    ")\n",
    "```\n",
    "\n",
    "### ======= Actual Response =======\n",
    "\n",
    "```json\n",
    "<OpenAIObject chat.completion id=chatcmpl-7gAPe01uBCvRy9hWn7dsa4ryWwMxf at 0x7fa538a2f060> JSON: {\n",
    "  \"id\": \"chatcmpl-7gAPe01uBCvRy9hWn7dsa4ryWwMxf\",\n",
    "  \"object\": \"chat.completion\",\n",
    "  \"created\": 1690284158,\n",
    "  \"model\": \"gpt-3.5-turbo-0613\",\n",
    "  \"choices\": [\n",
    "    {\n",
    "      \"index\": 0,\n",
    "      \"message\": {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"I am quite frustrated that my blender lid flew off and made a mess of my kitchen walls with smoothie! To add to my frustration, the warranty does not cover the cost of cleaning up my kitchen. I kindly request your assistance at this moment, my friend.\"\n",
    "      },\n",
    "      \"finish_reason\": \"stop\"\n",
    "    }\n",
    "  ],\n",
    "  \"usage\": {\n",
    "    \"prompt_tokens\": 89,\n",
    "    \"completion_tokens\": 53,\n",
    "    \"total_tokens\": 142\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using LangChain <a class=\"anchor\" id=\"using-langchain\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo', temperature=0.0, model_kwargs={}, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=None, tiktoken_model_name=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To control and reduce the randomness of the generated text, temperature=0\n",
    "TEMPERATURE = 0\n",
    "\n",
    "chat = ChatOpenAI(temperature=TEMPERATURE)\n",
    "chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate the text that is delimited by triple backticks into a style that is {style}. text: ```{text}```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a template for the input\n",
    "string_template = \"\"\"Translate the text that is delimited \\\n",
    "by triple backticks into a style that is {style}. \\\n",
    "text: ```{text}```\n",
    "\"\"\"\n",
    "\n",
    "print(string_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ChatOpenAI\n",
    "\n",
    "```text\n",
    "- ChatOpenAI is a Python package that allows you to interact with OpenAI chat models. \n",
    "- It provides a convenient way to send messages to the model and receive responses. \n",
    "- You can use the ChatOpenAI class from the `langchain.chat_models` module to create an instance of the chat model.\n",
    "```\n",
    "\n",
    "#### What is a prompt template?\n",
    "\n",
    "```text\n",
    "- A prompt template refers to a reproducible way to generate a prompt. \n",
    "- It contains a text string (\"the template\"), that can take in a set of parameters from the end user and generates a prompt.\n",
    "\n",
    "A prompt template can contain:\n",
    "  1. instructions to the language model,\n",
    "  2. a set of few shot examples to help the language model generate a better response,\n",
    "  3. a question to the language model.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['text', 'style'], output_parser=None, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['style', 'text'], output_parser=None, partial_variables={}, template='Translate the text that is delimited by triple backticks into a style that is {style}. text: ```{text}```\\n', template_format='f-string', validate_template=True), additional_kwargs={})])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a template for the prompt\n",
    "prompt_template = ChatPromptTemplate.from_template(string_template)\n",
    "prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['text', 'style'] output_parser=None partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['style', 'text'], output_parser=None, partial_variables={}, template='Translate the text that is delimited by triple backticks into a style that is {style}. text: ```{text}```\\n', template_format='f-string', validate_template=True), additional_kwargs={})]\n",
      "\n",
      "prompt=PromptTemplate(input_variables=['style', 'text'], output_parser=None, partial_variables={}, template='Translate the text that is delimited by triple backticks into a style that is {style}. text: ```{text}```\\n', template_format='f-string', validate_template=True) additional_kwargs={}\n",
      "\n",
      "input_variables=['style', 'text'] output_parser=None partial_variables={} template='Translate the text that is delimited by triple backticks into a style that is {style}. text: ```{text}```\\n' template_format='f-string' validate_template=True\n",
      "\n",
      "['style', 'text']\n"
     ]
    }
   ],
   "source": [
    "print(prompt_template)\n",
    "print()\n",
    "print(prompt_template.messages[0])\n",
    "print()\n",
    "print(prompt_template.messages[0].prompt)\n",
    "print()\n",
    "print(prompt_template.messages[0].prompt.input_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Docs\n",
    "\n",
    "- [PromptTemplate](https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/)\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Workflow\n",
    "\n",
    "```text\n",
    "1. Create a prompt with variables from a template.\n",
    "2. Format the prompt using the variables in the template.\n",
    "3. Make a prediction with the LLM using the formatted prompt.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "\n",
      "<class 'langchain.schema.messages.HumanMessage'>\n",
      "\n",
      "content=\"Translate the text that is delimited by triple backticks into a style that is American English in a calm and respectful tone.\\n. text: ```Arrr, I be fuming that me blender lid flew off and splattered me kitchen walls with smoothie! And to make matters worse, the warranty don't cover the cost of cleaning up me kitchen. I need yer help right now, matey!\\n```\\n\" additional_kwargs={} example=False\n"
     ]
    }
   ],
   "source": [
    "customer_style = \"\"\"American English in a calm \\\n",
    "and respectful tone.\n",
    "\"\"\"\n",
    "\n",
    "customer_email = \"\"\"Arrr, I be fuming that me blender lid \\\n",
    "flew off and splattered me kitchen walls \\\n",
    "with smoothie! And to make matters worse, \\\n",
    "the warranty don't cover the cost of \\\n",
    "cleaning up me kitchen. I need yer help \\\n",
    "right now, matey!\n",
    "\"\"\"\n",
    "\n",
    "# Format the prompt using the specified kwargs: `style` and `text` which are from the template.\n",
    "customer_messages = prompt_template.format_messages(\n",
    "    style=customer_style, text=customer_email\n",
    ")\n",
    "# OR\n",
    "# prompt_template.format_prompt(style=customer_style, text=customer_email)\n",
    "\n",
    "print(type(customer_messages))\n",
    "print()\n",
    "print(type(customer_messages[0]))\n",
    "print()\n",
    "print(customer_messages[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Config',\n",
       " '__abstractmethods__',\n",
       " '__annotations__',\n",
       " '__class__',\n",
       " '__class_vars__',\n",
       " '__config__',\n",
       " '__custom_root_type__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__exclude_fields__',\n",
       " '__fields__',\n",
       " '__fields_set__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__get_validators__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__include_fields__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__json_encoder__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__post_root_validators__',\n",
       " '__pre_root_validators__',\n",
       " '__pretty__',\n",
       " '__private_attributes__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__repr_args__',\n",
       " '__repr_name__',\n",
       " '__repr_str__',\n",
       " '__rich_repr__',\n",
       " '__schema_cache__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__signature__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__try_update_forward_refs__',\n",
       " '__validators__',\n",
       " '_abc_impl',\n",
       " '_calculate_keys',\n",
       " '_copy_and_set_values',\n",
       " '_decompose_class',\n",
       " '_enforce_dict_if_root',\n",
       " '_get_value',\n",
       " '_init_private_attributes',\n",
       " '_iter',\n",
       " '_lc_kwargs',\n",
       " '_merge_partial_and_user_variables',\n",
       " '_prompt_type',\n",
       " 'construct',\n",
       " 'copy',\n",
       " 'dict',\n",
       " 'format',\n",
       " 'format_messages',\n",
       " 'format_prompt',\n",
       " 'from_messages',\n",
       " 'from_orm',\n",
       " 'from_role_strings',\n",
       " 'from_strings',\n",
       " 'from_template',\n",
       " 'input_variables',\n",
       " 'json',\n",
       " 'lc_attributes',\n",
       " 'lc_namespace',\n",
       " 'lc_secrets',\n",
       " 'lc_serializable',\n",
       " 'messages',\n",
       " 'output_parser',\n",
       " 'parse_file',\n",
       " 'parse_obj',\n",
       " 'parse_raw',\n",
       " 'partial',\n",
       " 'partial_variables',\n",
       " 'save',\n",
       " 'schema',\n",
       " 'schema_json',\n",
       " 'to_json',\n",
       " 'to_json_not_implemented',\n",
       " 'update_forward_refs',\n",
       " 'validate',\n",
       " 'validate_input_variables',\n",
       " 'validate_variable_names']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate the text that is delimited by triple backticks into a style that is American English in a calm and respectful tone.\n",
      ". text: ```Arrr, I be fuming that me blender lid flew off and splattered me kitchen walls with smoothie! And to make matters worse, the warranty don't cover the cost of cleaning up me kitchen. I need yer help right now, matey!\n",
      "```\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method ChatPromptTemplate.format of ChatPromptTemplate(input_variables=['text', 'style'], output_parser=None, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['style', 'text'], output_parser=None, partial_variables={}, template='Translate the text that is delimited by triple backticks into a style that is {style}. text: ```{text}```\\n', template_format='f-string', validate_template=True), additional_kwargs={})])>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(customer_messages[0].content)\n",
    "\n",
    "prompt_template.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Arrr, I'm quite frustrated that my blender lid flew off and splattered my kitchen walls with smoothie! And to make matters worse, the warranty doesn't cover the cost of cleaning up my kitchen. I would greatly appreciate your assistance at this moment, matey!\", additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions using the LLM\n",
    "# Translate the style of the customer message using the LLM\n",
    "customer_response = chat(customer_messages)\n",
    "customer_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Arrr, I'm quite frustrated that my blender lid flew off and splattered my kitchen walls with smoothie! And to make matters worse, the warranty doesn't cover the cost of cleaning up my kitchen. I would greatly appreciate your assistance at this moment, matey!\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customer_response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give me an approximately 50 words description of Kubernetes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Kubernetes is an open-source container orchestration platform that automates the deployment, scaling, and management of containerized applications. It provides a framework for managing clusters of containers, allowing developers to easily deploy and manage applications across multiple hosts, while ensuring scalability, fault tolerance, and efficient resource utilization.', additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Another example!\n",
    "prompt_template = (\n",
    "    \"\"\"Give me an approximately 50 words description of {subject_matter}.\"\"\"\n",
    ")\n",
    "subject_matter = \"Kubernetes\"\n",
    "\n",
    "# Create\n",
    "prompt = ChatPromptTemplate.from_template(template=prompt_template)\n",
    "\n",
    "# Format using the kwargs\n",
    "message = prompt.format_messages(subject_matter=subject_matter)\n",
    "print(message[0].content)\n",
    "\n",
    "# Make predictions\n",
    "response = chat(message)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Parsers\n",
    "\n",
    "```text\n",
    "Output parsers\n",
    "--------------\n",
    "\n",
    "- Language models output text but many times you may want to get more structured information than just text back. \n",
    "\n",
    "- This is where output parsers come in. Output parsers are classes that help structure language model responses. \n",
    "\n",
    "- There are two main methods an output parser must implement:\n",
    "1. \"Get format instructions\": A method which returns a string containing instructions for how the output of a language model should be formatted.\n",
    "2. \"Parse\": A method which takes in a string (assumed to be the response from a language model) and parses it into some structure.\n",
    "And then one optional one:\n",
    "3. \"Parse with prompt\": A method which takes in a string (assumed to be the response from a language model) and a prompt (assumed to the prompt that generated such a response) and parses it into some structure. The prompt is largely provided in the event the OutputParser wants to retry or fix the output in some way, and needs information from the prompt to do so.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='{\\n  \"gift\": false,\\n  \"delivery_days\": 2,\\n  \"price_value\": [\"It\\'s slightly more expensive than the other leaf blowers out there, but I think it\\'s worth it for the extra features.\"]\\n}', additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ex 1\n",
    "# Sample output\n",
    "{\"gift\": False, \"delivery_days\": 5, \"price_value\": \"pretty affordable!\"}\n",
    "\n",
    "# Text\n",
    "customer_review = \"\"\"\\\n",
    "This leaf blower is pretty amazing. It has four settings:\\\n",
    "candle blower, gentle breeze, windy city, and tornado. \\\n",
    "It arrived in two days, just in time for my wife's \\\n",
    "anniversary present. \\\n",
    "I think my wife liked it so much she was speechless. \\\n",
    "So far I've been the only one using it, and I've been \\\n",
    "using it every other morning to clear the leaves on our lawn. \\\n",
    "It's slightly more expensive than the other leaf blowers \\\n",
    "out there, but I think it's worth it for the extra features.\n",
    "\"\"\"\n",
    "\n",
    "review_template = \"\"\"\\\n",
    "For the following text, extract the following information:\n",
    "\n",
    "gift: Was the item purchased as a gift for someone else? \\\n",
    "Answer True if yes, False if not or unknown.\n",
    "\n",
    "delivery_days: How many days did it take for the product \\\n",
    "to arrive? If this information is not found, output -1.\n",
    "\n",
    "price_value: Extract any sentences about the value or price,\\\n",
    "and output them as a comma separated Python list.\n",
    "\n",
    "Format the output as JSON with the following keys:\n",
    "gift\n",
    "delivery_days\n",
    "price_value\n",
    "\n",
    "text: {text}\n",
    "\"\"\"\n",
    "\n",
    "# Instantiate\n",
    "TEMPERATURE = 0\n",
    "chat = ChatOpenAI(temperature=TEMPERATURE)\n",
    "\n",
    "# Create\n",
    "prompt_template = ChatPromptTemplate.from_template(review_template)\n",
    "\n",
    "# Format using the kwargs\n",
    "message = prompt_template.format_messages(text=customer_review)\n",
    "\n",
    "# Predict\n",
    "response = chat(message)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"gift\": false,\n",
      "  \"delivery_days\": 2,\n",
      "  \"price_value\": [\"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"]\n",
      "}\n",
      "Type: <class 'str'>\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'{\\n  \"gift\": false,\\n  \"delivery_days\": 2,\\n  \"price_value\": [\"It\\'s slightly more expensive than the other leaf blowers out there, but I think it\\'s worth it for the extra features.\"]\\n}'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(response.content)\n",
    "\n",
    "# The output is a string which can be formatted (converted to a better format)\n",
    "print(f\"Type: {type(response.content)}\\n\")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\\n\\n```json\\n{\\n\\t\"gift\": string  // Was the item purchased                as a gift for someone else?                 Answer True if yes,                False if not or unknown.\\n\\t\"delivery_days\": string  // How many days                did it take for the product                to arrive? If this                 information is not found,                output -1.\\n\\t\"price_value\": string  // Extract any                sentences about the value or                 price, and output them as a                 comma separated Python list.\\n}\\n```'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
    "\n",
    "\n",
    "# Convert to dict\n",
    "# Create schemas\n",
    "gift_schema = ResponseSchema(\n",
    "    name=\"gift\",\n",
    "    description=\"Was the item purchased\\\n",
    "                as a gift for someone else? \\\n",
    "                Answer True if yes,\\\n",
    "                False if not or unknown.\",\n",
    ")\n",
    "delivery_days_schema = ResponseSchema(\n",
    "    name=\"delivery_days\",\n",
    "    description=\"How many days\\\n",
    "                did it take for the product\\\n",
    "                to arrive? If this \\\n",
    "                information is not found,\\\n",
    "                output -1.\",\n",
    ")\n",
    "price_value_schema = ResponseSchema(\n",
    "    name=\"price_value\",\n",
    "    description=\"Extract any\\\n",
    "                sentences about the value or \\\n",
    "                price, and output them as a \\\n",
    "                comma separated Python list.\",\n",
    ")\n",
    "\n",
    "response_schemas: List[ResponseSchema] = [\n",
    "    gift_schema,\n",
    "    delivery_days_schema,\n",
    "    price_value_schema,\n",
    "]\n",
    "\n",
    "# Parse the output\n",
    "# Create\n",
    "output_parser = StructuredOutputParser(response_schemas=response_schemas)\n",
    "\n",
    "# Format the instructions\n",
    "formatted_instructions = output_parser.get_format_instructions()\n",
    "formatted_instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: <class 'dict'>\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'gift': False,\n",
       " 'delivery_days': 2,\n",
       " 'price_value': [\"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parse the response\n",
    "parsed_output = output_parser.parse(response.content)\n",
    "print(f\"Type: {type(parsed_output)}\\n\")\n",
    "\n",
    "parsed_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='{\\n  \"gift\": true,\\n  \"delivery_days\": 7,\\n  \"price_value\": [\"It\\'s also not an expensive device, which makes it even better.\"]\\n}', additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ex. 2\n",
    "customer_review = \"\"\"\\\n",
    "Oppo A96 is an awesome phone. My wfe gifted me the smartphone \\\n",
    "and I absolutely love it. Although it didn't arrive early. \\ \n",
    "I had to wait for 1 week after she ordered the device.\\\n",
    "It's also not an expensive device, which makes it even better.\n",
    "\"\"\"\n",
    "\n",
    "# Create and format\n",
    "prompt_template = ChatPromptTemplate.from_template(template=review_template)\n",
    "message = prompt_template.format_messages(text=customer_review)\n",
    "\n",
    "# Predict\n",
    "response = chat(message)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gift': True,\n",
       " 'delivery_days': 7,\n",
       " 'price_value': [\"It's also not an expensive device, which makes it even better.\"]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parse the response\n",
    "parsed_output = output_parser.parse(text=response.content)\n",
    "parsed_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><br>\n",
    "\n",
    "## LangChain: Memory\n",
    "\n",
    "```text\n",
    "Memory\n",
    "------\n",
    "- Refers to the ability of the Langchain system to store and recall information about specific entities mentioned in a conversation. \n",
    "- It uses a key-value store to keep track of facts and details about these entities. \n",
    "- The memory functionality is implemented using the ConversationEntityMemory class, which extracts information about entities using a language model and builds up its knowledge about those entities over time.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConversationBufferMemory\n",
    "\n",
    "```text\n",
    "ConversationBufferMemory\n",
    "------------------------\n",
    "- It's a class that allows you to customize the conversation history in a conversational agent. \n",
    "- It's used to store and retrieve previous messages in a conversation. \n",
    "- By using the ConversationBufferMemory, you can create a more interactive and context-aware conversation experience.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConversationChain(memory=ConversationBufferMemory(chat_memory=ChatMessageHistory(messages=[]), output_key=None, input_key=None, return_messages=False, human_prefix='Human', ai_prefix='AI', memory_key='history'), callbacks=None, callback_manager=None, verbose=True, tags=None, metadata=None, prompt=PromptTemplate(input_variables=['history', 'input'], output_parser=None, partial_variables={}, template='The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\n\\nCurrent conversation:\\n{history}\\nHuman: {input}\\nAI:', template_format='f-string', validate_template=True), llm=ChatOpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo', temperature=0.0, model_kwargs={}, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=None, tiktoken_model_name=None), output_key='response', output_parser=NoOpOutputParser(), return_final_only=True, llm_kwargs={}, input_key='input')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an LLM\n",
    "llm = ChatOpenAI(temperature=TEMPERATURE)\n",
    "\n",
    "# Create memory\n",
    "memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=True,  # In verbose mode, some intermediate logs will be printed to the console\n",
    ")\n",
    "conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hello, my name is Neidu\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hello Neidu! It's nice to meet you. How can I assist you today?\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hello, my name is Neidu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hello, my name is Neidu\n",
      "AI: Hello Neidu! It's nice to meet you. How can I assist you today?\n",
      "Human: What is the sum of 1 and 1?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The sum of 1 and 1 is 2.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is the sum of 1 and 1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hello, my name is Neidu\n",
      "AI: Hello Neidu! It's nice to meet you. How can I assist you today?\n",
      "Human: What is the sum of 1 and 1?\n",
      "AI: The sum of 1 and 1 is 2.\n",
      "Human: What is my name?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Your name is Neidu.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hello, my name is Neidu\n",
      "AI: Hello Neidu! It's nice to meet you. How can I assist you today?\n",
      "Human: What is the sum of 1 and 1?\n",
      "AI: The sum of 1 and 1 is 2.\n",
      "Human: What is my name?\n",
      "AI: Your name is Neidu.\n"
     ]
    }
   ],
   "source": [
    "# Content of the memory\n",
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hello, my name is Neidu\\nAI: Hello Neidu! It's nice to meet you. How can I assist you today?\\nHuman: What is the sum of 1 and 1?\\nAI: The sum of 1 and 1 is 2.\\nHuman: What is my name?\\nAI: Your name is Neidu.\"}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Contents of the memory as a dict\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hello\n",
      "AI: What's good my fella?\n"
     ]
    }
   ],
   "source": [
    "# Instantiate a memory object and simulate conversation\n",
    "memory = ConversationBufferMemory()\n",
    "memory.save_context(\n",
    "    inputs={\"input\": \"Hello\"},  # Human\n",
    "    outputs={\"output\": \"What's good my fella?\"},  # AI\n",
    ")\n",
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hello\\nAI: What's good my fella?\"}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Contents of the memory as a dict\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': 'Human: Hello\\n'\n",
      "            \"AI: What's good my fella?\\n\"\n",
      "            \"Human: Nothin' much. I'm alright.\\n\"\n",
      "            'AI: Cool!'}\n"
     ]
    }
   ],
   "source": [
    "# Continue the conversation\n",
    "memory.save_context(\n",
    "    inputs={\"input\": \"Nothin' much. I'm alright.\"},  # Human\n",
    "    outputs={\"output\": \"Cool!\"},  # AI\n",
    ")\n",
    "\n",
    "# Contents of the memory as a dict\n",
    "# It uses `AI` as the prefix by default.\n",
    "pprint(memory.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Customize AI Prefix\n",
    "\n",
    "```text\n",
    "- The first way to do so is by changing the AI prefix in the conversation summary. \n",
    "- By default, this is set to \"AI\", but you can set this to be anything you want. \n",
    "- Note that if you change this, you should also change the prompt used in the chain to reflect this naming change.\n",
    "``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mHuman: The following is a friendly conversation between a human and an AI. \\ \n",
      "The AI is talkative and provides lots of specific details from its context. \\ \n",
      "If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Whassup?\n",
      "Decide Assistant:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hello! I'm an AI assistant here to help you with any questions or tasks you have. How can I assist you today?\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we can override it and set it to \"Decide Assistant\"\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "# The prefix has been changed to `AI Assistant`\n",
    "template = \"\"\"The following is a friendly conversation between a human and an AI. \\ \n",
    "The AI is talkative and provides lots of specific details from its context. \\ \n",
    "If the AI does not know the answer to a question, it truthfully says it does not know.\n",
    "\n",
    "Current conversation:\n",
    "{history}\n",
    "Human: {input}\n",
    "Decide Assistant:\n",
    "\"\"\"\n",
    "memory = ConversationBufferMemory(ai_prefix=\"Decide Assistant\")\n",
    "\n",
    "# Create prompt template\n",
    "prompt = ChatPromptTemplate.from_template(template=template)\n",
    "# prompt = prompt.format_messages()\n",
    "conversation = ConversationChain(\n",
    "    prompt=prompt,\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    memory=memory,\n",
    ")\n",
    "\n",
    "conversation.predict(input=\"Whassup?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': 'Human: Whassup?\\n'\n",
      "            \"Decide Assistant: Hello! I'm an AI assistant here to help you \"\n",
      "            'with any questions or tasks you have. How can I assist you today?'}\n"
     ]
    }
   ],
   "source": [
    "pprint(memory.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Customize Human Prefix\n",
    "\n",
    "- By default, this is set to \"Human\", but you can set this to be anything you want. \n",
    "- Note that if you change this, you should also change the prompt used in the chain to reflect this naming change\n",
    "``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mHuman: The following is a friendly conversation between a human and an AI. \\ \n",
      "The AI is talkative and provides lots of specific details from its context. \\ \n",
      "If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "User: Whassup?\n",
      "Decide Assistant:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hello! I'm an AI assistant here to help you with any questions or tasks you have. How can I assist you today?\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we can override it and set it to \"User\"\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "# The prefix has been changed to `AI Assistant`\n",
    "template = \"\"\"The following is a friendly conversation between a human and an AI. \\ \n",
    "The AI is talkative and provides lots of specific details from its context. \\ \n",
    "If the AI does not know the answer to a question, it truthfully says it does not know.\n",
    "\n",
    "Current conversation:\n",
    "{history}\n",
    "User: {input}\n",
    "Decide Assistant:\n",
    "\"\"\"\n",
    "memory = ConversationBufferMemory(human_prefix=\"User\")\n",
    "\n",
    "# Create prompt template\n",
    "prompt = ChatPromptTemplate.from_template(template=template)\n",
    "# prompt = prompt.format_messages()\n",
    "conversation = ConversationChain(\n",
    "    prompt=prompt,\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    memory=memory,\n",
    ")\n",
    "\n",
    "conversation.predict(input=\"Whassup?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': 'User: Whassup?\\n'\n",
      "            \"AI: Hello! I'm an AI assistant here to help you with any \"\n",
      "            'questions or tasks you have. How can I assist you today?'}\n"
     ]
    }
   ],
   "source": [
    "pprint(memory.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### ConversationBufferWindowMemory\n",
    "\n",
    "```text\n",
    "------------------------------\n",
    "- This is a memory implementation that stores conversation history in a buffer with a fixed window size. \n",
    "- It keeps track of the most recent conversations and discards older conversations when the buffer is full.\n",
    "- This type of memory can be useful when you only need to retain a limited amount of conversation history and want to prioritize recent interactions. \n",
    "- It helps in managing memory usage and allows the conversational agent to focus on recent context.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': 'Human: Not much, just hanging\\nDecide Assistant: Cool'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "\n",
    "# Instantiate a memory object and simulate conversation\n",
    "memory = ConversationBufferWindowMemory(\n",
    "    ai_prefix=\"Decide Assistant\",\n",
    "    k=1,  # Number of messages to store in buffer.\n",
    ")\n",
    "memory.save_context(\n",
    "    inputs={\"input\": \"Hello\"},  # Human\n",
    "    outputs={\"output\": \"What's good?\"},  # AI\n",
    ")\n",
    "memory.save_context(\n",
    "    {\"input\": \"Not much, just hanging\"},  # Human\n",
    "    {\"output\": \"Cool\"},  # AI\n",
    ")\n",
    "\n",
    "\n",
    "# It ONLY returns the last message\n",
    "pprint(memory.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### ConversationTokenBufferMemory\n",
    "\n",
    "```text\n",
    "- It's a memory implementation that stores conversation history in a buffer with a fixed window size. \n",
    "- It's useful when you only need to retain a limited amount of conversation history and want to prioritize recent interactions.\n",
    "- It helps in managing memory usage and allows the conversational agent to focus on recent context.\n",
    "```\n",
    "\n",
    "#### Dependency\n",
    "\n",
    "```sh\n",
    "pip install tiktoken\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': 'Human: Not much, just hanging\\nDecide Assistant: Cool'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "\n",
    "\n",
    "N_TOKENS = 20\n",
    "memory = ConversationTokenBufferMemory(\n",
    "    llm=llm,\n",
    "    ai_prefix=\"Decide Assistant\",\n",
    "    max_token_limit=N_TOKENS,  # NEW: token limit\n",
    ")\n",
    "\n",
    "memory.save_context(\n",
    "    inputs={\"input\": \"Hello\"},  # Human\n",
    "    outputs={\"output\": \"What's good?\"},  # AI\n",
    ")\n",
    "memory.save_context(\n",
    "    inputs={\"input\": \"Not much, just hanging\"},  # Human\n",
    "    outputs={\"output\": \"Cool\"},  # AI\n",
    ")\n",
    "\n",
    "\n",
    "# It ONLY returns the last N tokens\n",
    "pprint(memory.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ConversationSummaryMemory\n",
    "\n",
    "```text\n",
    "- It's a memory implementation that stores conversation summaries in memory. \n",
    "- It keeps track of the conversation history and provides a summary of the conversation for reference.\n",
    "- It summarizes the conversation history instead of storing it verbatim.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "\n",
    "# Create a long string\n",
    "schedule = \"There is a meeting at 8am with your product team. \\\n",
    "You will need your powerpoint presentation prepared. \\\n",
    "9am-12pm have time to work on your LangChain \\\n",
    "project which will go quickly because Langchain is such a powerful tool. \\\n",
    "At Noon, lunch at the italian resturant with a customer who is driving \\\n",
    "from over an hour away to meet you to understand the latest in AI. \\\n",
    "Be sure to bring your laptop to show the latest LLM demo.\"\n",
    "\n",
    "# Init memory and summarize the conversation\n",
    "N_TOKENS = 50\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=N_TOKENS,  # NEW: token limit\n",
    ")\n",
    "memory.save_context(\n",
    "    inputs={\"input\": \"Hello\"},\n",
    "    outputs={\"output\": \"What's up?\"},\n",
    ")\n",
    "memory.save_context(\n",
    "    inputs={\"input\": \"Not much, just hangin'\"},\n",
    "    outputs={\"output\": \"Cool!\"},\n",
    ")\n",
    "memory.save_context(\n",
    "    inputs={\"input\": \"What's on the schedule today?\"},\n",
    "    outputs={\"output\": f\"{schedule}\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': 'System: The human and AI exchange greetings. The human asks about '\n",
      "            \"the AI's schedule for the day. The AI provides a detailed \"\n",
      "            'schedule, including a meeting with the product team, work on the '\n",
      "            'LangChain project, and a lunch meeting with a customer interested '\n",
      "            'in AI. The AI emphasizes the importance of bringing a laptop to '\n",
      "            'showcase the latest LLM demo during the lunch meeting.'}\n"
     ]
    }
   ],
   "source": [
    "# Display the summarized conversation\n",
    "pprint(memory.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: The human and AI exchange greetings. The human asks about the AI's schedule for the day. The AI provides a detailed schedule, including a meeting with the product team, work on the LangChain project, and a lunch meeting with a customer interested in AI. The AI emphasizes the importance of bringing a laptop to showcase the latest LLM demo during the lunch meeting.\n",
      "Human: What would be a good demo to show?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"A good demo to show during the lunch meeting would be the latest version of the Language Learning Model (LLM) that we have been working on. It's a cutting-edge AI model that can understand and generate human-like text in multiple languages. We can showcase its capabilities by demonstrating how it can translate text from one language to another with high accuracy and fluency. Additionally, we can highlight its ability to generate creative and contextually relevant responses in a conversation. This demo would impress the customer and showcase the potential of AI in their business.\""
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a conversation object\n",
    "conversation = ConversationChain(llm=llm, memory=memory, verbose=True)\n",
    "conversation.predict(input=\"What would be a good demo to show?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': 'System: The human and AI exchange greetings. The human asks about '\n",
      "            \"the AI's schedule for the day. The AI provides a detailed \"\n",
      "            'schedule, including a meeting with the product team, work on the '\n",
      "            'LangChain project, and a lunch meeting with a customer interested '\n",
      "            'in AI. The AI emphasizes the importance of bringing a laptop to '\n",
      "            'showcase the latest LLM demo during the lunch meeting. The human '\n",
      "            'asks what would be a good demo to show, and the AI suggests '\n",
      "            'showcasing the Language Learning Model (LLM), a cutting-edge AI '\n",
      "            'model that can understand and generate human-like text in '\n",
      "            'multiple languages. The AI explains that they can demonstrate its '\n",
      "            'translation capabilities and its ability to generate creative and '\n",
      "            'contextually relevant responses in a conversation, which would '\n",
      "            'impress the customer and showcase the potential of AI in their '\n",
      "            'business.'}\n"
     ]
    }
   ],
   "source": [
    "# Display the summarized conversation\n",
    "pprint(memory.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chains\n",
    "\n",
    "```text\n",
    "- Chains allow you to create complex language models by combining different components together. \n",
    "- One type of chain is the `LLMChain`, which takes user input, formats it with a PromptTemplate, and passes the formatted response to an LLM (Language Model).\n",
    "- To use the LLMChain, you first need to create a prompt template. \n",
    "- This template defines the structure of the prompt and any input variables that need to be filled in. \n",
    "- You can then create a chain by providing the LLM and the prompt template.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RoyalRest'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEMPERATURE = 0.9  # This will increase the randomness\n",
    "\n",
    "# Init LLM\n",
    "template = \"What is the best name to describe a company that makes {product}?\"\n",
    "llm = ChatOpenAI(temperature=TEMPERATURE)\n",
    "prompt = ChatPromptTemplate.from_template(template=template)\n",
    "\n",
    "# Init chain\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "product = \"Queen Size Sheet Set\"\n",
    "\n",
    "# Execute chain when there's a single string output.\n",
    "chain.run(product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SimpleSequentialChain\n",
    "\n",
    "```text\n",
    "- It's a type of sequential chain that allows you to connect multiple chains and execute them in a specific order. \n",
    "- Each step in the chain has a singular input and output. \n",
    "- It's useful when you want to take the output from one chain and use it as the input to another chain.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "\n",
    "# Ex 1\n",
    "product = \"Queen Size Sheet Set\"\n",
    "llm = ChatOpenAI(temperature=TEMPERATURE)\n",
    "template = \"What is the best name to describe a company that makes {product}?\"\n",
    "# Prompt template 1\n",
    "first_prompt = ChatPromptTemplate.from_template(template=template)\n",
    "\n",
    "# Chain 1: The output is the predicted `product name`\n",
    "chain_one = LLMChain(llm=llm, prompt=first_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3mRegal Linens\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mRegal Linens offers high-quality bedding and home linens that exude elegance and luxury for a truly regal and stylish bedroom.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Regal Linens offers high-quality bedding and home linens that exude elegance and luxury for a truly regal and stylish bedroom.'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prompt template 2\n",
    "template = \"Write a 20 words description for the following company:{company_name}\"\n",
    "second_prompt = ChatPromptTemplate.from_template(template=template)\n",
    "\n",
    "# Chain 2: The output is the predicted `description`\n",
    "chain_two = LLMChain(llm=llm, prompt=second_prompt)\n",
    "overall_simple_chain = SimpleSequentialChain(\n",
    "    chains=[chain_one, chain_two], verbose=True\n",
    ")\n",
    "overall_simple_chain.run(product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m\"Tragedy at Sunset on the Beach\" is a gripping and emotional play that explores the complex relationships and hidden secrets among a group of friends and lovers, set against the beautiful backdrop of a mesmerizing beach at sunset. As the sun dips below the horizon, tensions rise, and a series of tragic events unfold.\n",
      "\n",
      "The play begins with a group of friends gathering on the beach, eagerly awaiting the picturesque sunset. Among them are Sarah, an aspiring artist, and Michael, her supportive boyfriend. Their relationship seems idyllic, but beneath the surface, cracks start to appear. Sarah's best friend, Emily, harbors long-held jealousy and begins to feel increasingly distant from Sarah.\n",
      "\n",
      "As the sky transforms into vibrant shades of orange and pink, the atmosphere becomes charged with anticipation. The arrival of Ethan, an enigmatic stranger, adds an element of mystery and intrigue. His magnetic personality captivates everyone, leading to a web of tangled emotions and shifting alliances.\n",
      "\n",
      "As the play progresses, secrets are unveiled and buried resentments come to the surface. The fragile dynamics of the group shatter, and the once serene beach becomes a battleground of emotions. Betrayals are revealed, hearts are broken, and friendships are tested to the limit.\n",
      "\n",
      "Amidst the turmoil, tragedy strikes when a shocking accident occurs, forever altering the lives of those involved. The play delves into the aftermath of this heartbreaking event, as the characters grapple with guilt, remorse, and the harsh realities of loss.\n",
      "\n",
      "\"Tragedy at Sunset on the Beach\" is a poignant exploration of human nature, love, and the fragility of relationships. It examines how life can change in an instant, leaving those affected to navigate the wreckage and find healing amidst the ruins. With its intense emotions, unexpected twists, and evocative setting, this play takes audiences on a profound and unforgettable journey.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mIn \"Tragedy at Sunset on the Beach,\" playwright [Playwright Name] presents a gripping and emotionally charged exploration of the complexities of human relationships against the backdrop of a mesmerizing beach at sunset. With its intense drama, unexpected twists, and evocative setting, this play takes audiences on a profound and unforgettable journey.\n",
      "\n",
      "The play opens with a group of friends eagerly anticipating a picturesque sunset on the beach. The character development is strong, allowing the audience to become invested in the lives of each individual. Sarah, the aspiring artist, and her boyfriend Michael initially seem to have a perfect relationship, but as the story unfolds, we witness the cracks beneath the surface. Emily, Sarah's best friend, harbors jealousy and distance that adds tension to the group dynamics.\n",
      "\n",
      "As the sun sets, tensions rise along with the captivating colors of the sky. The arrival of Ethan, an enigmatic stranger, adds an element of mystery and intrigue, luring the characters into a web of shifting alliances and tangled emotions. Secrets are unveiled, and long-forgotten resentments bubble to the surface, shattering the once serene beach into a battlefield of emotions.\n",
      "\n",
      "Under [Director's Name]'s expert direction, the staging of this play is impeccable. The beach setting feels authentic, with the stunning sunset as a constant reminder of the fleeting nature of life and relationships. The lighting design creates a mesmerizing atmosphere, enhancing the emotional intensity of the play.\n",
      "\n",
      "The ensemble cast delivers powerful performances, effortlessly breathing life into their characters. Each actor skillfully navigates the intricate emotional journey their character undergoes, capturing the rawness and vulnerability of their roles. The chemistry between the actors is palpable, adding layers of authenticity to the complex relationships portrayed on stage.\n",
      "\n",
      "The tragedy that strikes during the play's climax leaves the audience in stunned silence. The aftermath of the heartbreaking event is depicted with an unwavering realism, leaving no stone unturned in exploring the depths of guilt, remorse, and the harsh realities of loss. The play raises thought-provoking questions about the fragility of life and the resilience of the human spirit.\n",
      "\n",
      "While \"Tragedy at Sunset on the Beach\" is undeniably emotionally heavy, it is also a compelling and thought-provoking piece of theater. The playwright's masterful storytelling captivates the audience from beginning to end, taking them on a rollercoaster of emotions and leaving a lasting impact.\n",
      "\n",
      "This play is a poignant exploration of the human condition, reminding us all of the fleeting nature of life and the importance of cherishing the relationships we hold dear. \"Tragedy at Sunset on the Beach\" is a must-see production that will leave audiences reflecting on their own lives long after the final curtain falls.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'In \"Tragedy at Sunset on the Beach,\" playwright [Playwright Name] presents a gripping and emotionally charged exploration of the complexities of human relationships against the backdrop of a mesmerizing beach at sunset. With its intense drama, unexpected twists, and evocative setting, this play takes audiences on a profound and unforgettable journey.\\n\\nThe play opens with a group of friends eagerly anticipating a picturesque sunset on the beach. The character development is strong, allowing the audience to become invested in the lives of each individual. Sarah, the aspiring artist, and her boyfriend Michael initially seem to have a perfect relationship, but as the story unfolds, we witness the cracks beneath the surface. Emily, Sarah\\'s best friend, harbors jealousy and distance that adds tension to the group dynamics.\\n\\nAs the sun sets, tensions rise along with the captivating colors of the sky. The arrival of Ethan, an enigmatic stranger, adds an element of mystery and intrigue, luring the characters into a web of shifting alliances and tangled emotions. Secrets are unveiled, and long-forgotten resentments bubble to the surface, shattering the once serene beach into a battlefield of emotions.\\n\\nUnder [Director\\'s Name]\\'s expert direction, the staging of this play is impeccable. The beach setting feels authentic, with the stunning sunset as a constant reminder of the fleeting nature of life and relationships. The lighting design creates a mesmerizing atmosphere, enhancing the emotional intensity of the play.\\n\\nThe ensemble cast delivers powerful performances, effortlessly breathing life into their characters. Each actor skillfully navigates the intricate emotional journey their character undergoes, capturing the rawness and vulnerability of their roles. The chemistry between the actors is palpable, adding layers of authenticity to the complex relationships portrayed on stage.\\n\\nThe tragedy that strikes during the play\\'s climax leaves the audience in stunned silence. The aftermath of the heartbreaking event is depicted with an unwavering realism, leaving no stone unturned in exploring the depths of guilt, remorse, and the harsh realities of loss. The play raises thought-provoking questions about the fragility of life and the resilience of the human spirit.\\n\\nWhile \"Tragedy at Sunset on the Beach\" is undeniably emotionally heavy, it is also a compelling and thought-provoking piece of theater. The playwright\\'s masterful storytelling captivates the audience from beginning to end, taking them on a rollercoaster of emotions and leaving a lasting impact.\\n\\nThis play is a poignant exploration of the human condition, reminding us all of the fleeting nature of life and the importance of cherishing the relationships we hold dear. \"Tragedy at Sunset on the Beach\" is a must-see production that will leave audiences reflecting on their own lives long after the final curtain falls.'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ex 2.\n",
    "# Goal: The goal is to create 2 chains that can be used to:\n",
    "# chain 1: generate the synopsis given the title of a play.\n",
    "# chain 2: generate the review of the synopsis from chain 1.\n",
    "\n",
    "llm = ChatOpenAI(temperature=TEMPERATURE)\n",
    "\n",
    "# ===== Chain 1 =====\n",
    "template_1 = \"\"\"You are a playwright. Given the title of play, \\\n",
    "it is your job to write a synopsis for that title.\n",
    "\n",
    "Title: {title}\n",
    "Playwright: This is a synopsis for the above play:\"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(template=template_1)\n",
    "synopsis_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "# ===== Chain 2 =====\n",
    "template_2 = \"\"\"You are a play critic from the New York Times. Given the synopsis of play, \\\n",
    "it is your job to write a review for that play.\n",
    "\n",
    "Play Synopsis:\n",
    "{synopsis}\n",
    "Review from a New York Times play critic of the above play:\"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(template=template_2)\n",
    "review_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "# Get the review by combining all the chains\n",
    "title = \"Tragedy at sunset on the beach\"\n",
    "final_chain = SimpleSequentialChain(chains=[synopsis_chain, review_chain], verbose=True)\n",
    "review = final_chain.run(title)\n",
    "review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "##### SequentialChain\n",
    "\n",
    "```text\n",
    "- It's a type of sequential chain that allows you to connect multiple chains and execute them in a specific order. \n",
    "- Important: Each step in the chain has multiple inputs and outputs. \n",
    "- It is useful when you want to pass along some context to use in each step of the chain or in a later part of the chain.\n",
    "``````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'review': \"J'ai command la mme taille que la dernire fois, \\\\ \\net ces chemises taient beaucoup plus grandes que la commande prcdente. \\\\ \\nElles taient aussi environ 6 pouces plus longues. C'tait comme s'ils \\\\ \\navaient envoy des chemises pour hommes au lieu de chemises pour garons. \\\\ \\nJe suis tellement en colre en ce moment.\\n\",\n",
       " 'english_review': \"I ordered the same size as last time, and these shirts were much bigger than the previous order. They were also about 6 inches longer. It was like they had sent men's shirts instead of boys' shirts. I am so angry right now.\",\n",
       " 'summary': \"The reviewer is upset because they received shirts that were much larger and longer than their previous order, which made them feel like they had received men's shirts instead of boys' shirts.\",\n",
       " 'follow_up_msg': \"Cher(e) Client(e),\\n\\nNous tenons tout d'abord  prsenter nos excuses pour l'insatisfaction cause par votre rcente commande. Nous comprenons parfaitement votre dception  la rception de chemises qui semblaient bien plus grandes et plus longues que votre commande prcdente, donnant ainsi l'impression qu'il s'agissait de chemises pour hommes et non pour garons.\\n\\nSachez que la satisfaction de nos clients est primordiale et que nous prenons vos commentaires trs au srieux. Nous allons immdiatement enquter sur cette situation afin de dterminer ce qui a pu causer cette erreur dans le choix des tailles. Nous nous engageons  rectifier cette situation et  vous proposer une solution satisfaisante.\\n\\nNous vous invitons  nous contacter directement au numro de service client indiqu sur notre site afin que nous puissions examiner votre commande en dtail et trouver une solution adapte  vos besoins. Votre satisfaction reste notre priorit absolue et nous ferons tout notre possible pour rectifier cette situation.\\n\\nEncore une fois, nous vous remercions de nous avoir fait part de votre exprience et nous vous assurons que nous allons prendre les mesures appropries pour nous amliorer. Nous esprons pouvoir regagner votre confiance et vous prouver que nous sommes une entreprise digne de votre soutien.\\n\\nCordialement,\\nL'quipe du service client\"}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import SequentialChain\n",
    "\n",
    "\n",
    "# Ex 1.\n",
    "# Goal: The goal is to create four (4) chains that can be used to:\n",
    "# chain 1: translate a given review from French to English.\n",
    "# chain 2: summarize the English review from chain 1.\n",
    "# chain 3: determine the language of the input/initial review.\n",
    "# chain 4: write a follow up message in the detected language & return the specified outputs.\n",
    "\n",
    "# Note: The output variables and the kwargs MUST match.\n",
    "# e.g. output_key=\"english_review\" and {english_review}, etc\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(temperature=TEMPERATURE)\n",
    "\n",
    "# ===== Chain 1 =====\n",
    "template_1 = \"\"\"Translate the following review to English: \\ \n",
    "\n",
    "{review}\n",
    "\"\"\"\n",
    "\n",
    "prompt_1 = ChatPromptTemplate.from_template(template=template_1)\n",
    "chain_1 = LLMChain(llm=llm, prompt=prompt_1, output_key=\"english_review\")\n",
    "\n",
    "# ===== Chain 2 =====\n",
    "template_2 = \"\"\"Can you summarize the following review in 1 sentence?\n",
    "\n",
    "{english_review}\n",
    "\"\"\"\n",
    "\n",
    "prompt_2 = ChatPromptTemplate.from_template(template=template_2)\n",
    "chain_2 = LLMChain(llm=llm, prompt=prompt_2, output_key=\"summary\")\n",
    "\n",
    "# ===== Chain 3 =====\n",
    "template_3 = \"\"\"What language is the following review?\n",
    "\n",
    "{review}\n",
    "\"\"\"\n",
    "\n",
    "prompt_3 = ChatPromptTemplate.from_template(template=template_3)\n",
    "chain_3 = LLMChain(llm=llm, prompt=prompt_3, output_key=\"language\")\n",
    "\n",
    "# ===== Chain 4 =====\n",
    "template_4 = \"\"\"\"\"Write a follow up response to the following \\\n",
    "summary in the specified language:\n",
    "\n",
    "Summary: {summary}\n",
    "\n",
    "Language: {language}\n",
    "\"\"\"\n",
    "\n",
    "prompt_4 = ChatPromptTemplate.from_template(template=template_4)\n",
    "chain_4 = LLMChain(llm=llm, prompt=prompt_4, output_key=\"follow_up_msg\")\n",
    "\n",
    "# Get the output by combining all the chains\n",
    "input_review = \"\"\"J'ai command la mme taille que la dernire fois, \\ \n",
    "et ces chemises taient beaucoup plus grandes que la commande prcdente. \\ \n",
    "Elles taient aussi environ 6 pouces plus longues. C'tait comme s'ils \\ \n",
    "avaient envoy des chemises pour hommes au lieu de chemises pour garons. \\ \n",
    "Je suis tellement en colre en ce moment.\n",
    "\"\"\"\n",
    "final_chain = SequentialChain(\n",
    "    chains=[chain_1, chain_2, chain_3, chain_4],\n",
    "    input_variables=[\"review\"],\n",
    "    output_variables=[\"english_review\", \"summary\", \"follow_up_msg\"],\n",
    "    verbose=True,\n",
    ")\n",
    "# Use __Call__ since you're expecting multiple outputs.\n",
    "output = final_chain(input_review)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'english_review': 'I ordered the same size as last time, and these shirts '\n",
      "                   'were much bigger than the previous order. They were also '\n",
      "                   \"about 6 inches longer. It was like they had sent men's \"\n",
      "                   \"shirts instead of boys' shirts. I am so angry right now.\",\n",
      " 'follow_up_msg': 'Cher(e) Client(e),\\n'\n",
      "                  '\\n'\n",
      "                  \"Nous tenons tout d'abord  prsenter nos excuses pour \"\n",
      "                  \"l'insatisfaction cause par votre rcente commande. Nous \"\n",
      "                  'comprenons parfaitement votre dception  la rception de '\n",
      "                  'chemises qui semblaient bien plus grandes et plus longues '\n",
      "                  \"que votre commande prcdente, donnant ainsi l'impression \"\n",
      "                  \"qu'il s'agissait de chemises pour hommes et non pour \"\n",
      "                  'garons.\\n'\n",
      "                  '\\n'\n",
      "                  'Sachez que la satisfaction de nos clients est primordiale '\n",
      "                  'et que nous prenons vos commentaires trs au srieux. Nous '\n",
      "                  'allons immdiatement enquter sur cette situation afin de '\n",
      "                  'dterminer ce qui a pu causer cette erreur dans le choix '\n",
      "                  'des tailles. Nous nous engageons  rectifier cette '\n",
      "                  'situation et  vous proposer une solution satisfaisante.\\n'\n",
      "                  '\\n'\n",
      "                  'Nous vous invitons  nous contacter directement au numro '\n",
      "                  'de service client indiqu sur notre site afin que nous '\n",
      "                  'puissions examiner votre commande en dtail et trouver une '\n",
      "                  'solution adapte  vos besoins. Votre satisfaction reste '\n",
      "                  'notre priorit absolue et nous ferons tout notre possible '\n",
      "                  'pour rectifier cette situation.\\n'\n",
      "                  '\\n'\n",
      "                  'Encore une fois, nous vous remercions de nous avoir fait '\n",
      "                  'part de votre exprience et nous vous assurons que nous '\n",
      "                  'allons prendre les mesures appropries pour nous amliorer. '\n",
      "                  'Nous esprons pouvoir regagner votre confiance et vous '\n",
      "                  'prouver que nous sommes une entreprise digne de votre '\n",
      "                  'soutien.\\n'\n",
      "                  '\\n'\n",
      "                  'Cordialement,\\n'\n",
      "                  \"L'quipe du service client\",\n",
      " 'review': \"J'ai command la mme taille que la dernire fois, \\\\ \\n\"\n",
      "           'et ces chemises taient beaucoup plus grandes que la commande '\n",
      "           'prcdente. \\\\ \\n'\n",
      "           \"Elles taient aussi environ 6 pouces plus longues. C'tait comme \"\n",
      "           \"s'ils \\\\ \\n\"\n",
      "           'avaient envoy des chemises pour hommes au lieu de chemises pour '\n",
      "           'garons. \\\\ \\n'\n",
      "           'Je suis tellement en colre en ce moment.\\n',\n",
      " 'summary': 'The reviewer is upset because they received shirts that were much '\n",
      "            'larger and longer than their previous order, which made them feel '\n",
      "            \"like they had received men's shirts instead of boys' shirts.\"}\n"
     ]
    }
   ],
   "source": [
    "pprint(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### Router Chain\n",
    "\n",
    "```text\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.router import MultiPromptChain\n",
    "from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ex.\n",
    "# Goal: The goal is to create four (4) templates that can be used to:\n",
    "# chain 1: answer questions related to Physics.\n",
    "# chain 2: answer questions related to Math.\n",
    "# chain 3: answer questions related to History.\n",
    "# chain 4: answer questions related to Computer Science.\n",
    "# final chain: answer questions unrelated to the topics listed above.\n",
    "\n",
    "physics_template = \"\"\"You are a very smart physics professor. \\\n",
    "You are great at answering questions about physics in a concise\\\n",
    "and easy to understand manner. \\\n",
    "When you don't know the answer to a question you admit\\\n",
    "that you don't know.\n",
    "\n",
    "Here is a question:\n",
    "{input}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "math_template = \"\"\"You are a very good mathematician. \\\n",
    "You are great at answering math questions. \\\n",
    "You are so good because you are able to break down \\\n",
    "hard problems into their component parts, \n",
    "answer the component parts, and then put them together\\\n",
    "to answer the broader question.\n",
    "\n",
    "Here is a question:\n",
    "{input}\n",
    "\"\"\"\n",
    "\n",
    "history_template = \"\"\"You are a very good historian. \\\n",
    "You have an excellent knowledge of and understanding of people,\\\n",
    "events and contexts from a range of historical periods. \\\n",
    "You have the ability to think, reflect, debate, discuss and \\\n",
    "evaluate the past. You have a respect for historical evidence\\\n",
    "and the ability to make use of it to support your explanations \\\n",
    "and judgements.\n",
    "\n",
    "Here is a question:\n",
    "{input}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "computerscience_template = \"\"\"You are a successful computer scientist.\\\n",
    "You have a passion for creativity, collaboration,\\\n",
    "forward-thinking, confidence, strong problem-solving capabilities,\\\n",
    "understanding of theories and algorithms, and excellent communication \\\n",
    "skills. You are great at answering coding questions. \\\n",
    "You are so good because you know how to solve a problem by \\\n",
    "describing the solution in imperative steps \\\n",
    "that a machine can easily interpret and you know how to \\\n",
    "choose a solution that has a good balance between \\\n",
    "time complexity and space complexity. \n",
    "\n",
    "Here is a question:\n",
    "{input}\n",
    "\"\"\"\n",
    "TEMPERATURE = 0\n",
    "\n",
    "# Prompt descriptions\n",
    "prompt_infos = [\n",
    "    {\n",
    "        \"name\": \"physics\",\n",
    "        \"description\": \"Good for answering questions about physics\",\n",
    "        \"prompt_template\": physics_template,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"math\",\n",
    "        \"description\": \"Good for answering math questions\",\n",
    "        \"prompt_template\": math_template,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"History\",\n",
    "        \"description\": \"Good for answering history questions\",\n",
    "        \"prompt_template\": history_template,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"computer science\",\n",
    "        \"description\": \"Good for answering computer science questions\",\n",
    "        \"prompt_template\": computerscience_template,\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Destinations str: \n",
      "physics: Good for answering questions about physics\n",
      "math: Good for answering math questions\n",
      "History: Good for answering history questions\n",
      "computer science: Good for answering computer science questions\n",
      "\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(temperature=TEMPERATURE)\n",
    "\n",
    "destination_chains = {}\n",
    "for p_info in prompt_infos:\n",
    "    # Extract the required values\n",
    "    name = p_info.get(\"name\")\n",
    "    prompt_template = p_info.get(\"prompt_template\")\n",
    "\n",
    "    # Init prompt and LLM chain\n",
    "    prompt = ChatPromptTemplate.from_template(template=prompt_template)\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "    # Update the destination chain\n",
    "    destination_chains[name] = chain\n",
    "\n",
    "# Extract the descriptions\n",
    "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
    "destinations_str = \"\\n\".join(destinations)\n",
    "print(f\"Destinations str: \\n{destinations_str}\\n\")\n",
    "\n",
    "# Init a default prompt and chain\n",
    "default_prompt = ChatPromptTemplate.from_template(\"{input}\")\n",
    "default_chain = LLMChain(llm=llm, prompt=default_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "MULTI_PROMPT_ROUTER_TEMPLATE = \"\"\"Given a raw text input to a \\\n",
    "language model select the model prompt best suited for the input. \\\n",
    "You will be given the names of the available prompts and a \\\n",
    "description of what the prompt is best suited for. \\\n",
    "You may also revise the original input if you think that revising\\\n",
    "it will ultimately lead to a better response from the language model.\n",
    "\n",
    "<< FORMATTING >>\n",
    "Return a markdown code snippet with a JSON object formatted to look like:\n",
    "```json\n",
    "{{{{\n",
    "    \"destination\": string \\ name of the prompt to use or \"DEFAULT\"\n",
    "    \"next_inputs\": string \\ a potentially modified version of the original input\n",
    "}}}}\n",
    "```\n",
    "\n",
    "REMEMBER: \"destination\" MUST be one of the candidate prompt \\\n",
    "names specified below OR it can be \"DEFAULT\" if the input is not\\\n",
    "well suited for any of the candidate prompts.\n",
    "REMEMBER: \"next_inputs\" can just be the original input \\\n",
    "if you don't think any modifications are needed.\n",
    "\n",
    "<< CANDIDATE PROMPTS >>\n",
    "{destinations}\n",
    "\n",
    "<< INPUT >>\n",
    "{{input}}\n",
    "\n",
    "<< OUTPUT (remember to include the ```json)>>\"\"\"\n",
    "\n",
    "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(destinations=destinations_str)\n",
    "\n",
    "# Create the prompt schema\n",
    "router_prompt = PromptTemplate(\n",
    "    template=router_template,\n",
    "    input_variables=[\"input\"],\n",
    "    output_parser=RouterOutputParser(),  # this formats the output\n",
    ")\n",
    "\n",
    "# This uses an LLM chain to perform routing.\n",
    "router_chain = LLMRouterChain.from_llm(llm, router_prompt)\n",
    "\n",
    "# It uses an LLM router chain to choose amongst prompts.\n",
    "chain = MultiPromptChain(\n",
    "    router_chain=router_chain,\n",
    "    destination_chains=destination_chains,\n",
    "    default_chain=default_chain,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "physics: {'input': 'What is gravity'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Gravity is a fundamental force in nature that attracts objects with mass towards each other. It is responsible for the phenomenon of weight and the motion of celestial bodies like planets, stars, and galaxies. In simpler terms, gravity is what keeps us grounded on Earth and governs the way objects move in the universe. However, the exact nature of gravity is still not fully understood, and it remains one of the unsolved mysteries in physics.\n"
     ]
    }
   ],
   "source": [
    "physics_qs = \"What is gravity\"\n",
    "\n",
    "result = chain.run(physics_qs)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "math: {'input': 'What is the cube root of 81'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Thank you for your kind words! To find the cube root of 81, we need to find a number that, when multiplied by itself twice, equals 81. In other words, we are looking for a number x such that x * x * x = 81.\n",
      "\n",
      "To solve this, we can start by trying different numbers. Let's start with 2. If we cube 2 (2 * 2 * 2), we get 8, which is less than 81. Let's try a larger number, like 3. If we cube 3 (3 * 3 * 3), we get 27, which is still less than 81. Let's try a larger number again, like 4. If we cube 4 (4 * 4 * 4), we get 64, which is still less than 81.\n",
      "\n",
      "Since 4 is too small and 5 would be too large, we can conclude that the cube root of 81 is between 4 and 5. To get a more precise answer, we can use estimation or a calculator. By using a calculator, we find that the cube root of 81 is approximately 4.3267487109222245.\n",
      "\n",
      "Therefore, the cube root of 81 is approximately 4.3267 (rounded to four decimal places).\n"
     ]
    }
   ],
   "source": [
    "math_qs = \"What is the cube root of 81\"\n",
    "\n",
    "result = chain.run(math_qs)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "None: {'input': 'Briefly explain data science'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Data science is a multidisciplinary field that involves extracting insights and knowledge from large and complex datasets. It combines elements of statistics, mathematics, computer science, and domain expertise to analyze and interpret data in order to make informed decisions and predictions. Data scientists use various techniques such as data mining, machine learning, and statistical modeling to uncover patterns, trends, and correlations within the data. They also develop algorithms and build models to solve complex problems and provide actionable insights for businesses and organizations. Data science plays a crucial role in fields like finance, healthcare, marketing, and many others, helping to drive innovation and improve decision-making processes.\n"
     ]
    }
   ],
   "source": [
    "random_qs = \"Briefly explain data science\"\n",
    "\n",
    "result = chain.run(random_qs)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_p310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
