{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Introduction To LangChain](https://docs.langchain.com/docs/)\n",
    "\n",
    "<br>\n",
    "\n",
    "- LangChain is a framework for developing applications powered by language models.\n",
    "\n",
    "## Installation\n",
    "\n",
    "```sh\n",
    "pip install langchain\n",
    "\n",
    "# OR\n",
    "pip install 'langchain[all]'\n",
    "\n",
    "# Other dependencies\n",
    "pip install python-dotenv\n",
    "pip install openai\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in library\n",
    "import itertools\n",
    "import re\n",
    "import json\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "# Standard imports\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# pandas settings\n",
    "pd.options.display.max_rows = 1_000\n",
    "pd.options.display.max_columns = 1_000\n",
    "pd.options.display.max_colwidth = 600\n",
    "\n",
    "# Black code formatter (Optional)\n",
    "%load_ext lab_black\n",
    "# auto reload imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())  # read local .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt: str, model: str = \"gpt-3.5-turbo\"):\n",
    "    \"\"\"This is used to make a direct API calls to OpenAI.\"\"\"\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0,\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1+1 equals 2.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_completion(\"What is 1+1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate the text that is delimited by triple backticks \n",
      "into a style that is American English in a calm and respectful tone\n",
      ".\n",
      "text: ```\n",
      "Arrr, I be fuming that me blender lid flew off and splattered me kitchen walls with smoothie! And to make matters worse,the warranty don't cover the cost of cleaning up me kitchen. I need yer help right now, matey!\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer_email = \"\"\"\n",
    "Arrr, I be fuming that me blender lid \\\n",
    "flew off and splattered me kitchen walls \\\n",
    "with smoothie! And to make matters worse,\\\n",
    "the warranty don't cover the cost of \\\n",
    "cleaning up me kitchen. I need yer help \\\n",
    "right now, matey!\n",
    "\"\"\"\n",
    "\n",
    "style = \"\"\"American English \\\n",
    "in a calm and respectful tone\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"Translate the text \\\n",
    "that is delimited by triple backticks \n",
    "into a style that is {style}.\n",
    "text: ```{customer_email}```\n",
    "\"\"\"\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am quite frustrated that my blender lid flew off and made a mess of my kitchen walls with smoothie! To add to my frustration, the warranty does not cover the cost of cleaning up my kitchen. I kindly request your assistance at this moment, my friend.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = get_completion(prompt)\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Raw Response\n",
    "\n",
    "```python\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "model = \"gpt-3.5-turbo\"\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=model,\n",
    "    messages=messages,\n",
    "    temperature=0,\n",
    ")\n",
    "```\n",
    "\n",
    "### ======= Actual Response =======\n",
    "\n",
    "```json\n",
    "<OpenAIObject chat.completion id=chatcmpl-7gAPe01uBCvRy9hWn7dsa4ryWwMxf at 0x7fa538a2f060> JSON: {\n",
    "  \"id\": \"chatcmpl-7gAPe01uBCvRy9hWn7dsa4ryWwMxf\",\n",
    "  \"object\": \"chat.completion\",\n",
    "  \"created\": 1690284158,\n",
    "  \"model\": \"gpt-3.5-turbo-0613\",\n",
    "  \"choices\": [\n",
    "    {\n",
    "      \"index\": 0,\n",
    "      \"message\": {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"I am quite frustrated that my blender lid flew off and made a mess of my kitchen walls with smoothie! To add to my frustration, the warranty does not cover the cost of cleaning up my kitchen. I kindly request your assistance at this moment, my friend.\"\n",
    "      },\n",
    "      \"finish_reason\": \"stop\"\n",
    "    }\n",
    "  ],\n",
    "  \"usage\": {\n",
    "    \"prompt_tokens\": 89,\n",
    "    \"completion_tokens\": 53,\n",
    "    \"total_tokens\": 142\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using LangChain <a class=\"anchor\" id=\"using-langchain\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo', temperature=0.0, model_kwargs={}, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=None, tiktoken_model_name=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To control and reduce the randomness of the generated text, temperature=0\n",
    "TEMPERATURE = 0\n",
    "\n",
    "chat = ChatOpenAI(temperature=TEMPERATURE)\n",
    "chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate the text that is delimited by triple backticks into a style that is {style}. text: ```{text}```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a template for the input\n",
    "string_template = \"\"\"Translate the text that is delimited \\\n",
    "by triple backticks into a style that is {style}. \\\n",
    "text: ```{text}```\n",
    "\"\"\"\n",
    "\n",
    "print(string_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ChatOpenAI\n",
    "\n",
    "```text\n",
    "- ChatOpenAI is a Python package that allows you to interact with OpenAI chat models. \n",
    "- It provides a convenient way to send messages to the model and receive responses. \n",
    "- You can use the ChatOpenAI class from the `langchain.chat_models` module to create an instance of the chat model.\n",
    "```\n",
    "\n",
    "#### What is a prompt template?\n",
    "\n",
    "```text\n",
    "- A prompt template refers to a reproducible way to generate a prompt. \n",
    "- It contains a text string (\"the template\"), that can take in a set of parameters from the end user and generates a prompt.\n",
    "\n",
    "A prompt template can contain:\n",
    "  1. instructions to the language model,\n",
    "  2. a set of few shot examples to help the language model generate a better response,\n",
    "  3. a question to the language model.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['text', 'style'], output_parser=None, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['style', 'text'], output_parser=None, partial_variables={}, template='Translate the text that is delimited by triple backticks into a style that is {style}. text: ```{text}```\\n', template_format='f-string', validate_template=True), additional_kwargs={})])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a template for the prompt\n",
    "prompt_template = ChatPromptTemplate.from_template(string_template)\n",
    "prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['text', 'style'] output_parser=None partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['style', 'text'], output_parser=None, partial_variables={}, template='Translate the text that is delimited by triple backticks into a style that is {style}. text: ```{text}```\\n', template_format='f-string', validate_template=True), additional_kwargs={})]\n",
      "\n",
      "prompt=PromptTemplate(input_variables=['style', 'text'], output_parser=None, partial_variables={}, template='Translate the text that is delimited by triple backticks into a style that is {style}. text: ```{text}```\\n', template_format='f-string', validate_template=True) additional_kwargs={}\n",
      "\n",
      "input_variables=['style', 'text'] output_parser=None partial_variables={} template='Translate the text that is delimited by triple backticks into a style that is {style}. text: ```{text}```\\n' template_format='f-string' validate_template=True\n",
      "\n",
      "['style', 'text']\n"
     ]
    }
   ],
   "source": [
    "print(prompt_template)\n",
    "print()\n",
    "print(prompt_template.messages[0])\n",
    "print()\n",
    "print(prompt_template.messages[0].prompt)\n",
    "print()\n",
    "print(prompt_template.messages[0].prompt.input_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Docs\n",
    "\n",
    "- [PromptTemplate](https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/)\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Workflow\n",
    "\n",
    "```text\n",
    "1. Create a prompt with variables from a template.\n",
    "2. Format the prompt using the variables in the template.\n",
    "3. Make a prediction with the LLM using the formatted prompt.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "\n",
      "<class 'langchain.schema.messages.HumanMessage'>\n",
      "\n",
      "content=\"Translate the text that is delimited by triple backticks into a style that is American English in a calm and respectful tone.\\n. text: ```Arrr, I be fuming that me blender lid flew off and splattered me kitchen walls with smoothie! And to make matters worse, the warranty don't cover the cost of cleaning up me kitchen. I need yer help right now, matey!\\n```\\n\" additional_kwargs={} example=False\n"
     ]
    }
   ],
   "source": [
    "customer_style = \"\"\"American English in a calm \\\n",
    "and respectful tone.\n",
    "\"\"\"\n",
    "\n",
    "customer_email = \"\"\"Arrr, I be fuming that me blender lid \\\n",
    "flew off and splattered me kitchen walls \\\n",
    "with smoothie! And to make matters worse, \\\n",
    "the warranty don't cover the cost of \\\n",
    "cleaning up me kitchen. I need yer help \\\n",
    "right now, matey!\n",
    "\"\"\"\n",
    "\n",
    "# Format the prompt using the specified kwargs: `style` and `text` which are from the template.\n",
    "customer_messages = prompt_template.format_messages(\n",
    "    style=customer_style, text=customer_email\n",
    ")\n",
    "# OR\n",
    "# prompt_template.format_prompt(style=customer_style, text=customer_email)\n",
    "\n",
    "print(type(customer_messages))\n",
    "print()\n",
    "print(type(customer_messages[0]))\n",
    "print()\n",
    "print(customer_messages[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Config',\n",
       " '__abstractmethods__',\n",
       " '__annotations__',\n",
       " '__class__',\n",
       " '__class_vars__',\n",
       " '__config__',\n",
       " '__custom_root_type__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__exclude_fields__',\n",
       " '__fields__',\n",
       " '__fields_set__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__get_validators__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__include_fields__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__json_encoder__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__post_root_validators__',\n",
       " '__pre_root_validators__',\n",
       " '__pretty__',\n",
       " '__private_attributes__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__repr_args__',\n",
       " '__repr_name__',\n",
       " '__repr_str__',\n",
       " '__rich_repr__',\n",
       " '__schema_cache__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__signature__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__try_update_forward_refs__',\n",
       " '__validators__',\n",
       " '_abc_impl',\n",
       " '_calculate_keys',\n",
       " '_copy_and_set_values',\n",
       " '_decompose_class',\n",
       " '_enforce_dict_if_root',\n",
       " '_get_value',\n",
       " '_init_private_attributes',\n",
       " '_iter',\n",
       " '_lc_kwargs',\n",
       " '_merge_partial_and_user_variables',\n",
       " '_prompt_type',\n",
       " 'construct',\n",
       " 'copy',\n",
       " 'dict',\n",
       " 'format',\n",
       " 'format_messages',\n",
       " 'format_prompt',\n",
       " 'from_messages',\n",
       " 'from_orm',\n",
       " 'from_role_strings',\n",
       " 'from_strings',\n",
       " 'from_template',\n",
       " 'input_variables',\n",
       " 'json',\n",
       " 'lc_attributes',\n",
       " 'lc_namespace',\n",
       " 'lc_secrets',\n",
       " 'lc_serializable',\n",
       " 'messages',\n",
       " 'output_parser',\n",
       " 'parse_file',\n",
       " 'parse_obj',\n",
       " 'parse_raw',\n",
       " 'partial',\n",
       " 'partial_variables',\n",
       " 'save',\n",
       " 'schema',\n",
       " 'schema_json',\n",
       " 'to_json',\n",
       " 'to_json_not_implemented',\n",
       " 'update_forward_refs',\n",
       " 'validate',\n",
       " 'validate_input_variables',\n",
       " 'validate_variable_names']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate the text that is delimited by triple backticks into a style that is American English in a calm and respectful tone.\n",
      ". text: ```Arrr, I be fuming that me blender lid flew off and splattered me kitchen walls with smoothie! And to make matters worse, the warranty don't cover the cost of cleaning up me kitchen. I need yer help right now, matey!\n",
      "```\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method ChatPromptTemplate.format of ChatPromptTemplate(input_variables=['text', 'style'], output_parser=None, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['style', 'text'], output_parser=None, partial_variables={}, template='Translate the text that is delimited by triple backticks into a style that is {style}. text: ```{text}```\\n', template_format='f-string', validate_template=True), additional_kwargs={})])>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(customer_messages[0].content)\n",
    "\n",
    "prompt_template.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Arrr, I'm quite frustrated that my blender lid flew off and splattered my kitchen walls with smoothie! And to make matters worse, the warranty doesn't cover the cost of cleaning up my kitchen. I would greatly appreciate your assistance at this moment, matey!\", additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions using the LLM\n",
    "# Translate the style of the customer message using the LLM\n",
    "customer_response = chat(customer_messages)\n",
    "customer_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Arrr, I'm quite frustrated that my blender lid flew off and splattered my kitchen walls with smoothie! And to make matters worse, the warranty doesn't cover the cost of cleaning up my kitchen. I would greatly appreciate your assistance at this moment, matey!\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customer_response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give me an approximately 50 words description of Kubernetes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Kubernetes is an open-source container orchestration platform that automates the deployment, scaling, and management of containerized applications. It provides a framework for managing clusters of containers, allowing developers to easily deploy and manage applications across multiple hosts, while ensuring scalability, fault tolerance, and efficient resource utilization.', additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Another example!\n",
    "prompt_template = (\n",
    "    \"\"\"Give me an approximately 50 words description of {subject_matter}.\"\"\"\n",
    ")\n",
    "subject_matter = \"Kubernetes\"\n",
    "\n",
    "# Create\n",
    "prompt = ChatPromptTemplate.from_template(template=prompt_template)\n",
    "\n",
    "# Format using the kwargs\n",
    "message = prompt.format_messages(subject_matter=subject_matter)\n",
    "print(message[0].content)\n",
    "\n",
    "# Make predictions\n",
    "response = chat(message)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Parsers\n",
    "\n",
    "```text\n",
    "Output parsers\n",
    "--------------\n",
    "\n",
    "- Language models output text but many times you may want to get more structured information than just text back. \n",
    "\n",
    "- This is where output parsers come in. Output parsers are classes that help structure language model responses. \n",
    "\n",
    "- There are two main methods an output parser must implement:\n",
    "1. \"Get format instructions\": A method which returns a string containing instructions for how the output of a language model should be formatted.\n",
    "2. \"Parse\": A method which takes in a string (assumed to be the response from a language model) and parses it into some structure.\n",
    "And then one optional one:\n",
    "3. \"Parse with prompt\": A method which takes in a string (assumed to be the response from a language model) and a prompt (assumed to the prompt that generated such a response) and parses it into some structure. The prompt is largely provided in the event the OutputParser wants to retry or fix the output in some way, and needs information from the prompt to do so.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='{\\n  \"gift\": false,\\n  \"delivery_days\": 2,\\n  \"price_value\": [\"It\\'s slightly more expensive than the other leaf blowers out there, but I think it\\'s worth it for the extra features.\"]\\n}', additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ex 1\n",
    "# Sample output\n",
    "{\"gift\": False, \"delivery_days\": 5, \"price_value\": \"pretty affordable!\"}\n",
    "\n",
    "# Text\n",
    "customer_review = \"\"\"\\\n",
    "This leaf blower is pretty amazing. It has four settings:\\\n",
    "candle blower, gentle breeze, windy city, and tornado. \\\n",
    "It arrived in two days, just in time for my wife's \\\n",
    "anniversary present. \\\n",
    "I think my wife liked it so much she was speechless. \\\n",
    "So far I've been the only one using it, and I've been \\\n",
    "using it every other morning to clear the leaves on our lawn. \\\n",
    "It's slightly more expensive than the other leaf blowers \\\n",
    "out there, but I think it's worth it for the extra features.\n",
    "\"\"\"\n",
    "\n",
    "review_template = \"\"\"\\\n",
    "For the following text, extract the following information:\n",
    "\n",
    "gift: Was the item purchased as a gift for someone else? \\\n",
    "Answer True if yes, False if not or unknown.\n",
    "\n",
    "delivery_days: How many days did it take for the product \\\n",
    "to arrive? If this information is not found, output -1.\n",
    "\n",
    "price_value: Extract any sentences about the value or price,\\\n",
    "and output them as a comma separated Python list.\n",
    "\n",
    "Format the output as JSON with the following keys:\n",
    "gift\n",
    "delivery_days\n",
    "price_value\n",
    "\n",
    "text: {text}\n",
    "\"\"\"\n",
    "\n",
    "# Instantiate\n",
    "TEMPERATURE = 0\n",
    "chat = ChatOpenAI(temperature=TEMPERATURE)\n",
    "\n",
    "# Create\n",
    "prompt_template = ChatPromptTemplate.from_template(review_template)\n",
    "\n",
    "# Format using the kwargs\n",
    "message = prompt_template.format_messages(text=customer_review)\n",
    "\n",
    "# Predict\n",
    "response = chat(message)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"gift\": false,\n",
      "  \"delivery_days\": 2,\n",
      "  \"price_value\": [\"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"]\n",
      "}\n",
      "Type: <class 'str'>\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'{\\n  \"gift\": false,\\n  \"delivery_days\": 2,\\n  \"price_value\": [\"It\\'s slightly more expensive than the other leaf blowers out there, but I think it\\'s worth it for the extra features.\"]\\n}'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(response.content)\n",
    "\n",
    "# The output is a string which can be formatted (converted to a better format)\n",
    "print(f\"Type: {type(response.content)}\\n\")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\\n\\n```json\\n{\\n\\t\"gift\": string  // Was the item purchased                as a gift for someone else?                 Answer True if yes,                False if not or unknown.\\n\\t\"delivery_days\": string  // How many days                did it take for the product                to arrive? If this                 information is not found,                output -1.\\n\\t\"price_value\": string  // Extract any                sentences about the value or                 price, and output them as a                 comma separated Python list.\\n}\\n```'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
    "\n",
    "\n",
    "# Convert to dict\n",
    "# Create schemas\n",
    "gift_schema = ResponseSchema(\n",
    "    name=\"gift\",\n",
    "    description=\"Was the item purchased\\\n",
    "                as a gift for someone else? \\\n",
    "                Answer True if yes,\\\n",
    "                False if not or unknown.\",\n",
    ")\n",
    "delivery_days_schema = ResponseSchema(\n",
    "    name=\"delivery_days\",\n",
    "    description=\"How many days\\\n",
    "                did it take for the product\\\n",
    "                to arrive? If this \\\n",
    "                information is not found,\\\n",
    "                output -1.\",\n",
    ")\n",
    "price_value_schema = ResponseSchema(\n",
    "    name=\"price_value\",\n",
    "    description=\"Extract any\\\n",
    "                sentences about the value or \\\n",
    "                price, and output them as a \\\n",
    "                comma separated Python list.\",\n",
    ")\n",
    "\n",
    "response_schemas: List[ResponseSchema] = [\n",
    "    gift_schema,\n",
    "    delivery_days_schema,\n",
    "    price_value_schema,\n",
    "]\n",
    "\n",
    "# Parse the output\n",
    "# Create\n",
    "output_parser = StructuredOutputParser(response_schemas=response_schemas)\n",
    "\n",
    "# Format the instructions\n",
    "formatted_instructions = output_parser.get_format_instructions()\n",
    "formatted_instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: <class 'dict'>\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'gift': False,\n",
       " 'delivery_days': 2,\n",
       " 'price_value': [\"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parse the response\n",
    "parsed_output = output_parser.parse(response.content)\n",
    "print(f\"Type: {type(parsed_output)}\\n\")\n",
    "\n",
    "parsed_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='{\\n  \"gift\": true,\\n  \"delivery_days\": 7,\\n  \"price_value\": [\"It\\'s also not an expensive device, which makes it even better.\"]\\n}', additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ex. 2\n",
    "customer_review = \"\"\"\\\n",
    "Oppo A96 is an awesome phone. My wfe gifted me the smartphone \\\n",
    "and I absolutely love it. Although it didn't arrive early. \\ \n",
    "I had to wait for 1 week after she ordered the device.\\\n",
    "It's also not an expensive device, which makes it even better.\n",
    "\"\"\"\n",
    "\n",
    "# Create and format\n",
    "prompt_template = ChatPromptTemplate.from_template(template=review_template)\n",
    "message = prompt_template.format_messages(text=customer_review)\n",
    "\n",
    "# Predict\n",
    "response = chat(message)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gift': True,\n",
       " 'delivery_days': 7,\n",
       " 'price_value': [\"It's also not an expensive device, which makes it even better.\"]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parse the response\n",
    "parsed_output = output_parser.parse(text=response.content)\n",
    "parsed_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><br>\n",
    "\n",
    "## LangChain: Memory\n",
    "\n",
    "```text\n",
    "Memory\n",
    "------\n",
    "- Refers to the ability of the Langchain system to store and recall information about specific entities mentioned in a conversation. \n",
    "- It uses a key-value store to keep track of facts and details about these entities. \n",
    "- The memory functionality is implemented using the ConversationEntityMemory class, which extracts information about entities using a language model and builds up its knowledge about those entities over time.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConversationBufferMemory\n",
    "\n",
    "```text\n",
    "ConversationBufferMemory\n",
    "------------------------\n",
    "- It's a class that allows you to customize the conversation history in a conversational agent. \n",
    "- It's used to store and retrieve previous messages in a conversation. \n",
    "- By using the ConversationBufferMemory, you can create a more interactive and context-aware conversation experience.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConversationChain(memory=ConversationBufferMemory(chat_memory=ChatMessageHistory(messages=[]), output_key=None, input_key=None, return_messages=False, human_prefix='Human', ai_prefix='AI', memory_key='history'), callbacks=None, callback_manager=None, verbose=True, tags=None, metadata=None, prompt=PromptTemplate(input_variables=['history', 'input'], output_parser=None, partial_variables={}, template='The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\n\\nCurrent conversation:\\n{history}\\nHuman: {input}\\nAI:', template_format='f-string', validate_template=True), llm=ChatOpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo', temperature=0.0, model_kwargs={}, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=None, tiktoken_model_name=None), output_key='response', output_parser=NoOpOutputParser(), return_final_only=True, llm_kwargs={}, input_key='input')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an LLM\n",
    "llm = ChatOpenAI(temperature=TEMPERATURE)\n",
    "\n",
    "# Create memory\n",
    "memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=True,  # In verbose mode, some intermediate logs will be printed to the console\n",
    ")\n",
    "conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hello, my name is Neidu\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hello Neidu! It's nice to meet you. How can I assist you today?\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hello, my name is Neidu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hello, my name is Neidu\n",
      "AI: Hello Neidu! It's nice to meet you. How can I assist you today?\n",
      "Human: What is the sum of 1 and 1?\n",
      "AI:\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The sum of 1 and 1 is 2.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is the sum of 1 and 1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hello, my name is Neidu\n",
      "AI: Hello Neidu! It's nice to meet you. How can I assist you today?\n",
      "Human: What is the sum of 1 and 1?\n",
      "AI: The sum of 1 and 1 is 2.\n",
      "Human: What is my name?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Your name is Neidu.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hello, my name is Neidu\n",
      "AI: Hello Neidu! It's nice to meet you. How can I assist you today?\n",
      "Human: What is the sum of 1 and 1?\n",
      "AI: The sum of 1 and 1 is 2.\n",
      "Human: What is my name?\n",
      "AI: Your name is Neidu.\n"
     ]
    }
   ],
   "source": [
    "# Content of the memory \n",
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hello, my name is Neidu\\nAI: Hello Neidu! It's nice to meet you. How can I assist you today?\\nHuman: What is the sum of 1 and 1?\\nAI: The sum of 1 and 1 is 2.\\nHuman: What is my name?\\nAI: Your name is Neidu.\"}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Contents of the memory as a dict\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hello\n",
      "AI: What's good my fella?\n"
     ]
    }
   ],
   "source": [
    "# Instantiate a memory object and simulate conversation\n",
    "memory = ConversationBufferMemory()\n",
    "memory.save_context(\n",
    "    inputs={\"input\": \"Hello\"},  # Human\n",
    "    outputs={\"output\": \"What's good my fella?\"},  # AI\n",
    ")\n",
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hello\\nAI: What's good my fella?\"}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Contents of the memory as a dict\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': 'Human: Hello\\n'\n",
      "            \"AI: What's good my fella?\\n\"\n",
      "            \"Human: Nothin' much. I'm alright.\\n\"\n",
      "            'AI: Cool!'}\n"
     ]
    }
   ],
   "source": [
    "# Continue the conversation\n",
    "memory.save_context(\n",
    "    inputs={\"input\": \"Nothin' much. I'm alright.\"},  # Human\n",
    "    outputs={\"output\": \"Cool!\"},  # AI\n",
    ")\n",
    "\n",
    "# Contents of the memory as a dict\n",
    "# It uses `AI` as the prefix by default.\n",
    "pprint(memory.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Customize AI Prefix\n",
    "\n",
    "```text\n",
    "- The first way to do so is by changing the AI prefix in the conversation summary. \n",
    "- By default, this is set to \"AI\", but you can set this to be anything you want. \n",
    "- Note that if you change this, you should also change the prompt used in the chain to reflect this naming change.\n",
    "``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mHuman: The following is a friendly conversation between a human and an AI. \\ \n",
      "The AI is talkative and provides lots of specific details from its context. \\ \n",
      "If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Whassup?\n",
      "Decide Assistant:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hello! I'm an AI assistant here to help you with any questions or tasks you have. How can I assist you today?\""
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we can override it and set it to \"Decide Assistant\"\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "# The prefix has been changed to `AI Assistant`\n",
    "template = \"\"\"The following is a friendly conversation between a human and an AI. \\ \n",
    "The AI is talkative and provides lots of specific details from its context. \\ \n",
    "If the AI does not know the answer to a question, it truthfully says it does not know.\n",
    "\n",
    "Current conversation:\n",
    "{history}\n",
    "Human: {input}\n",
    "Decide Assistant:\n",
    "\"\"\"\n",
    "memory = ConversationBufferMemory(ai_prefix=\"Decide Assistant\")\n",
    "\n",
    "# Create prompt template\n",
    "prompt = ChatPromptTemplate.from_template(template=template)\n",
    "# prompt = prompt.format_messages()\n",
    "conversation = ConversationChain(\n",
    "    prompt=prompt,\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    memory=memory,\n",
    ")\n",
    "\n",
    "conversation.predict(input=\"Whassup?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': 'Human: Whassup?\\n'\n",
      "            \"Decide Assistant: Hello! I'm an AI assistant here to help you \"\n",
      "            'with any questions or tasks you have. How can I assist you today?'}\n"
     ]
    }
   ],
   "source": [
    "pprint(memory.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Customize Human Prefix\n",
    "\n",
    "- By default, this is set to \"Human\", but you can set this to be anything you want. \n",
    "- Note that if you change this, you should also change the prompt used in the chain to reflect this naming change\n",
    "``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mHuman: The following is a friendly conversation between a human and an AI. \\ \n",
      "The AI is talkative and provides lots of specific details from its context. \\ \n",
      "If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "User: Whassup?\n",
      "Decide Assistant:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hello! I'm an AI assistant here to help you with any questions or tasks you have. How can I assist you today?\""
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we can override it and set it to \"User\"\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "# The prefix has been changed to `AI Assistant`\n",
    "template = \"\"\"The following is a friendly conversation between a human and an AI. \\ \n",
    "The AI is talkative and provides lots of specific details from its context. \\ \n",
    "If the AI does not know the answer to a question, it truthfully says it does not know.\n",
    "\n",
    "Current conversation:\n",
    "{history}\n",
    "User: {input}\n",
    "Decide Assistant:\n",
    "\"\"\"\n",
    "memory = ConversationBufferMemory(human_prefix=\"User\")\n",
    "\n",
    "# Create prompt template\n",
    "prompt = ChatPromptTemplate.from_template(template=template)\n",
    "# prompt = prompt.format_messages()\n",
    "conversation = ConversationChain(\n",
    "    prompt=prompt,\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    memory=memory,\n",
    ")\n",
    "\n",
    "conversation.predict(input=\"Whassup?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': 'User: Whassup?\\n'\n",
      "            \"AI: Hello! I'm an AI assistant here to help you with any \"\n",
      "            'questions or tasks you have. How can I assist you today?'}\n"
     ]
    }
   ],
   "source": [
    "pprint(memory.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### ConversationBufferWindowMemory\n",
    "\n",
    "```text\n",
    "------------------------------\n",
    "- This is a memory implementation that stores conversation history in a buffer with a fixed window size. \n",
    "- It keeps track of the most recent conversations and discards older conversations when the buffer is full.\n",
    "- This type of memory can be useful when you only need to retain a limited amount of conversation history and want to prioritize recent interactions. \n",
    "- It helps in managing memory usage and allows the conversational agent to focus on recent context.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': 'Human: Not much, just hanging\\nDecide Assistant: Cool'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "\n",
    "# Instantiate a memory object and simulate conversation\n",
    "memory = ConversationBufferWindowMemory(\n",
    "    ai_prefix=\"Decide Assistant\",\n",
    "    k=1,  # Number of messages to store in buffer.\n",
    ")\n",
    "memory.save_context(\n",
    "    inputs={\"input\": \"Hello\"},  # Human\n",
    "    outputs={\"output\": \"What's good?\"},  # AI\n",
    ")\n",
    "memory.save_context(\n",
    "    {\"input\": \"Not much, just hanging\"},  # Human\n",
    "    {\"output\": \"Cool\"},  # AI\n",
    ")\n",
    "\n",
    "\n",
    "# It ONLY returns the last message\n",
    "pprint(memory.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### ConversationTokenBufferMemory\n",
    "\n",
    "```text\n",
    "- It's a memory implementation that stores conversation history in a buffer with a fixed window size. \n",
    "- It's useful when you only need to retain a limited amount of conversation history and want to prioritize recent interactions.\n",
    "- It helps in managing memory usage and allows the conversational agent to focus on recent context.\n",
    "```\n",
    "\n",
    "#### Dependency\n",
    "\n",
    "```sh\n",
    "pip install tiktoken\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': 'Human: Not much, just hanging\\nDecide Assistant: Cool'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "\n",
    "\n",
    "N_TOKENS = 20\n",
    "memory = ConversationTokenBufferMemory(\n",
    "    llm=llm,\n",
    "    ai_prefix=\"Decide Assistant\",\n",
    "    max_token_limit=N_TOKENS,  # NEW: token limit\n",
    ")\n",
    "\n",
    "memory.save_context(\n",
    "    inputs={\"input\": \"Hello\"},  # Human\n",
    "    outputs={\"output\": \"What's good?\"},  # AI\n",
    ")\n",
    "memory.save_context(\n",
    "    inputs={\"input\": \"Not much, just hanging\"},  # Human\n",
    "    outputs={\"output\": \"Cool\"},  # AI\n",
    ")\n",
    "\n",
    "\n",
    "# It ONLY returns the last N tokens\n",
    "pprint(memory.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ConversationSummaryMemory\n",
    "\n",
    "```text\n",
    "- It's a memory implementation that stores conversation summaries in memory. \n",
    "- It keeps track of the conversation history and provides a summary of the conversation for reference.\n",
    "- It summarizes the conversation history instead of storing it verbatim.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "\n",
    "# Create a long string\n",
    "schedule = \"There is a meeting at 8am with your product team. \\\n",
    "You will need your powerpoint presentation prepared. \\\n",
    "9am-12pm have time to work on your LangChain \\\n",
    "project which will go quickly because Langchain is such a powerful tool. \\\n",
    "At Noon, lunch at the italian resturant with a customer who is driving \\\n",
    "from over an hour away to meet you to understand the latest in AI. \\\n",
    "Be sure to bring your laptop to show the latest LLM demo.\"\n",
    "\n",
    "# Init memory and summarize the conversation\n",
    "N_TOKENS = 50\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=N_TOKENS,  # NEW: token limit\n",
    ")\n",
    "memory.save_context(\n",
    "    inputs={\"input\": \"Hello\"},\n",
    "    outputs={\"output\": \"What's up?\"},\n",
    ")\n",
    "memory.save_context(\n",
    "    inputs={\"input\": \"Not much, just hangin'\"},\n",
    "    outputs={\"output\": \"Cool!\"},\n",
    ")\n",
    "memory.save_context(\n",
    "    inputs={\"input\": \"What's on the schedule today?\"},\n",
    "    outputs={\"output\": f\"{schedule}\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': 'System: The human and AI exchange greetings. The human asks about '\n",
      "            'the schedule for the day, and the AI provides a detailed '\n",
      "            'itinerary including a meeting with the product team, work on the '\n",
      "            'LangChain project, and a lunch meeting with a customer interested '\n",
      "            'in AI. The AI emphasizes the importance of bringing a laptop to '\n",
      "            'showcase the latest LLM demo during the lunch meeting.'}\n"
     ]
    }
   ],
   "source": [
    "# Display the summarized conversation\n",
    "pprint(memory.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: The human and AI exchange greetings. The human asks about the schedule for the day, and the AI provides a detailed itinerary including a meeting with the product team, work on the LangChain project, and a lunch meeting with a customer interested in AI. The AI emphasizes the importance of bringing a laptop to showcase the latest LLM demo during the lunch meeting.\n",
      "Human: What would be a good demo to show?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'A good demo to show during the lunch meeting with the customer interested in AI would be the latest LLM (Language Model) demo. The LLM is a cutting-edge AI model that can generate human-like text based on a given prompt. It has been trained on a vast amount of data and can generate coherent and contextually relevant responses. By showcasing the LLM demo, you can demonstrate the capabilities of AI in natural language processing and how it can be applied to various industries and use cases.'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a conversation object\n",
    "conversation = ConversationChain(llm=llm, memory=memory, verbose=True)\n",
    "conversation.predict(input=\"What would be a good demo to show?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': 'System: The human and AI exchange greetings. The human asks about '\n",
      "            'the schedule for the day, and the AI provides a detailed '\n",
      "            'itinerary including a meeting with the product team, work on the '\n",
      "            'LangChain project, and a lunch meeting with a customer interested '\n",
      "            'in AI. The AI emphasizes the importance of bringing a laptop to '\n",
      "            'showcase the latest LLM demo during the lunch meeting. A good '\n",
      "            'demo to show during the lunch meeting with the customer '\n",
      "            'interested in AI would be the latest LLM (Language Model) demo. '\n",
      "            'The LLM is a cutting-edge AI model that can generate human-like '\n",
      "            'text based on a given prompt. It has been trained on a vast '\n",
      "            'amount of data and can generate coherent and contextually '\n",
      "            'relevant responses. By showcasing the LLM demo, you can '\n",
      "            'demonstrate the capabilities of AI in natural language processing '\n",
      "            'and how it can be applied to various industries and use cases.'}\n"
     ]
    }
   ],
   "source": [
    "# Display the summarized conversation\n",
    "pprint(memory.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chains\n",
    "\n",
    "```text\n",
    "- Chains allow you to create complex language models by combining different components together. \n",
    "- One type of chain is the `LLMChain`, which takes user input, formats it with a PromptTemplate, and passes the formatted response to an LLM (Language Model).\n",
    "- To use the LLMChain, you first need to create a prompt template. \n",
    "- This template defines the structure of the prompt and any input variables that need to be filled in. \n",
    "- You can then create a chain by providing the LLM and the prompt template.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Royal Comfort Linens'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEMPERATURE = 0.9  # This will increase the randomness\n",
    "\n",
    "# Init LLM\n",
    "template = \"What is the best name to describe a company that makes {product}?\"\n",
    "llm = ChatOpenAI(temperature=TEMPERATURE)\n",
    "prompt = ChatPromptTemplate.from_template(template=template)\n",
    "\n",
    "# Init chain\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "product = \"Queen Size Sheet Set\"\n",
    "\n",
    "# Execute chain when there's a single string output.\n",
    "chain.run(product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SimpleSequentialChain\n",
    "\n",
    "```text\n",
    "- It's a type of sequential chain that allows you to connect multiple chains and execute them in a specific order. \n",
    "- Each step in the chain has a singular input and output. \n",
    "- It's useful when you want to take the output from one chain and use it as the input to another chain.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "# Ex 1\n",
    "product = \"Queen Size Sheet Set\"\n",
    "llm = ChatOpenAI(temperature=TEMPERATURE)\n",
    "template = \"What is the best name to describe a company that makes {product}?\"\n",
    "# Prompt template 1\n",
    "first_prompt = ChatPromptTemplate.from_template(template=template)\n",
    "\n",
    "# Chain 1: The output is the predicted `product name`\n",
    "chain_one = LLMChain(llm=llm, prompt=first_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3mRegal Rest Linens\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mRegal Rest Linens offers luxurious bedding and linens crafted with quality materials for ultimate comfort and a stylish bedroom aesthetic.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Regal Rest Linens offers luxurious bedding and linens crafted with quality materials for ultimate comfort and a stylish bedroom aesthetic.'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prompt template 2\n",
    "template = \"Write a 20 words description for the following company:{company_name}\"\n",
    "second_prompt = ChatPromptTemplate.from_template(template=template)\n",
    "\n",
    "# Chain 2: The output is the predicted `description`\n",
    "chain_two = LLMChain(llm=llm, prompt=second_prompt)\n",
    "overall_simple_chain = SimpleSequentialChain(\n",
    "    chains=[chain_one, chain_two], verbose=True\n",
    ")\n",
    "overall_simple_chain.run(product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m\"Tragedy at Sunset on the Beach\" is a gripping and emotionally charged drama set against the picturesque backdrop of a tranquil beach at dusk. The play follows a diverse group of individuals, each struggling with their own personal demons and hidden secrets, who find themselves unexpectedly interconnected on this fateful evening.\n",
      "\n",
      "As the sun sets and casts a warm golden glow over the sand, tensions begin to rise among the characters. Their individual traumas and conflicts slowly unravel, revealing a web of interconnected tragedies that have been building for years.\n",
      "\n",
      "At the center of the narrative is Sarah, a young woman haunted by haunting memories of her past. She seeks solace and escape on the beach, hoping to find answers and closure. However, she encounters a charismatic but troubled man named Gabriel, whose presence only deepens the turmoil within her.\n",
      "\n",
      "Meanwhile, on the outskirts of the beach, a married couple named Emily and Michael are grappling with their crumbling relationship. Their inability to communicate effectively and confront their shared grief leads them down a destructive path, with the beach becoming a mirroring stage for their unraveling marriage.\n",
      "\n",
      "As the play progresses, the characters' stories intertwine and clash in unexpected ways. Moments of profound vulnerability, heart-wrenching confessions, and intense confrontations unfold on the shore. The fading light of the sunset serves as an atmospheric backdrop, emphasizing the play's themes of fleeting time, lost opportunities, and the inevitability of tragedy.\n",
      "\n",
      "Through poetic dialogue and poignant monologues, \"Tragedy at Sunset on the Beach\" explores the profound impact that past experiences, hidden secrets, and broken relationships can have on individuals' lives. It ultimately asks audiences to reflect on the choices we make, the consequences they bear, and the fleeting nature of moments that can change our lives forever.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mIn \"Tragedy at Sunset on the Beach,\" playwright Samantha Johnson masterfully crafts a gripping drama that delves into the depths of human emotions, secrets, and tragedies. Set against the idyllic backdrop of a picturesque beach at dusk, the play unveils a web of interconnected stories that prove to be hauntingly familiar and deeply relatable.\n",
      "\n",
      "At the heart of this narrative lies Sarah, a young woman burdened by the weight of her past. With haunting memories and unanswered questions, she seeks solace on the beach, only to find herself entangled with the enigmatic Gabriel. Johnson's character development is commendable, as both Sarah and Gabriel are multi-dimensional, flawed individuals who navigate their own personal demons with raw honesty and vulnerability.\n",
      "\n",
      "The crumbling marriage of Emily and Michael provides another layer of complexity to the play. Portraying the difficulties of effective communication and shared grief, their storyline serves as a striking parallel to the overarching themes of the play. Johnson's exploration of the disintegration of their relationship is both heartbreaking and thought-provoking, leaving audiences with a lingering sense of poignancy.\n",
      "\n",
      "The atmospheric setting of the beach at sunset acts as a character of its own, heightening the drama and accentuating the play's themes. The fading light serves as a constant reminder of the fleeting nature of time and the missed opportunities that can forever alter the course of our lives. Johnson's poetic dialogue and poignant monologues imbue the play with a haunting beauty, compelling audiences to reflect on their own choices and the consequences they bear.\n",
      "\n",
      "While \"Tragedy at Sunset on the Beach\" may tread familiar ground in terms of its exploration of personal trauma and hidden secrets, Johnson's skillful storytelling and captivating character dynamics elevate it above the mundane. The play captivates from beginning to end, thanks to its strong ensemble cast, who bring depth and authenticity to their roles.\n",
      "\n",
      "Overall, \"Tragedy at Sunset on the Beach\" is a thought-provoking and emotionally charged production that resonates long after the final curtain falls. Samantha Johnson's insightful writing and the compelling performances of the cast make this play a must-see for theater enthusiasts seeking a poignant exploration of the human experience.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'In \"Tragedy at Sunset on the Beach,\" playwright Samantha Johnson masterfully crafts a gripping drama that delves into the depths of human emotions, secrets, and tragedies. Set against the idyllic backdrop of a picturesque beach at dusk, the play unveils a web of interconnected stories that prove to be hauntingly familiar and deeply relatable.\\n\\nAt the heart of this narrative lies Sarah, a young woman burdened by the weight of her past. With haunting memories and unanswered questions, she seeks solace on the beach, only to find herself entangled with the enigmatic Gabriel. Johnson\\'s character development is commendable, as both Sarah and Gabriel are multi-dimensional, flawed individuals who navigate their own personal demons with raw honesty and vulnerability.\\n\\nThe crumbling marriage of Emily and Michael provides another layer of complexity to the play. Portraying the difficulties of effective communication and shared grief, their storyline serves as a striking parallel to the overarching themes of the play. Johnson\\'s exploration of the disintegration of their relationship is both heartbreaking and thought-provoking, leaving audiences with a lingering sense of poignancy.\\n\\nThe atmospheric setting of the beach at sunset acts as a character of its own, heightening the drama and accentuating the play\\'s themes. The fading light serves as a constant reminder of the fleeting nature of time and the missed opportunities that can forever alter the course of our lives. Johnson\\'s poetic dialogue and poignant monologues imbue the play with a haunting beauty, compelling audiences to reflect on their own choices and the consequences they bear.\\n\\nWhile \"Tragedy at Sunset on the Beach\" may tread familiar ground in terms of its exploration of personal trauma and hidden secrets, Johnson\\'s skillful storytelling and captivating character dynamics elevate it above the mundane. The play captivates from beginning to end, thanks to its strong ensemble cast, who bring depth and authenticity to their roles.\\n\\nOverall, \"Tragedy at Sunset on the Beach\" is a thought-provoking and emotionally charged production that resonates long after the final curtain falls. Samantha Johnson\\'s insightful writing and the compelling performances of the cast make this play a must-see for theater enthusiasts seeking a poignant exploration of the human experience.'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ex 2.\n",
    "# Goal: The goal is to create 2 chains that can be used to:\n",
    "# chain 1: generate the synopsis given the title of a play.\n",
    "# chain 2: generate the review of the synopsis from chain 1.\n",
    "\n",
    "llm = ChatOpenAI(temperature=TEMPERATURE)\n",
    "\n",
    "# ===== Chain 1 =====\n",
    "template_1 = \"\"\"You are a playwright. Given the title of play, \\\n",
    "it is your job to write a synopsis for that title.\n",
    "\n",
    "Title: {title}\n",
    "Playwright: This is a synopsis for the above play:\"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(template=template_1)\n",
    "synopsis_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "# ===== Chain 2 =====\n",
    "template_2 = \"\"\"You are a play critic from the New York Times. Given the synopsis of play, \\\n",
    "it is your job to write a review for that play.\n",
    "\n",
    "Play Synopsis:\n",
    "{synopsis}\n",
    "Review from a New York Times play critic of the above play:\"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(template=template_2)\n",
    "review_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "# Get the review by combining all the chains\n",
    "title = \"Tragedy at sunset on the beach\"\n",
    "final_chain = SimpleSequentialChain(chains=[synopsis_chain, review_chain], verbose=True)\n",
    "review = final_chain.run(title)\n",
    "review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "##### SequentialChain\n",
    "\n",
    "```text\n",
    "- It's a type of sequential chain that allows you to connect multiple chains and execute them in a specific order. \n",
    "- Important: Each step in the chain has multiple inputs and outputs. \n",
    "- It is useful when you want to pass along some context to use in each step of the chain or in a later part of the chain.\n",
    "``````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'review': \"J'ai command la mme taille que la dernire fois, \\\\ \\net ces chemises taient beaucoup plus grandes que la commande prcdente. \\\\ \\nElles taient aussi environ 6 pouces plus longues. C'tait comme s'ils \\\\ \\navaient envoy des chemises pour hommes au lieu de chemises pour garons. \\\\ \\nJe suis tellement en colre en ce moment.\\n\",\n",
       " 'english_review': \"I ordered the same size as last time, and these shirts were much bigger than the previous order. They were also about 6 inches longer. It was like they had sent men's shirts instead of boys' shirts. I am so angry right now.\",\n",
       " 'summary': \"The reviewer expresses frustration and disappointment with their recent order of shirts, as they discovered them to be significantly larger and longer than expected, possibly resembling men's shirts instead of boys' shirts.\",\n",
       " 'follow_up_msg': \"Cher client,\\n\\nNous tenons tout d'abord  vous remercier d'avoir partag votre exprience avec nous. Nous sommes vritablement dsols d'apprendre votre dception quant  votre commande de chemises.\\n\\nNous nous excusons sincrement pour toute confusion ou malentendu qui aurait pu survenir. Votre satisfaction est notre priorit absolue et nous prenons vos commentaires trs au srieux. Permettez-nous de vous assurer que nous allons enquter sur cette situation et prendre les mesures ncessaires pour viter que cela ne se reproduise  l'avenir.\\n\\nPourriez-vous nous fournir plus de dtails sur le problme rencontr ? Nous aimerions comprendre quelles ont t les erreurs commises afin que nous puissions rectifier la situation dans les plus brefs dlais. \\n\\nNous tenons  souligner que nous offrons une politique de retour et d'change sans tracas. Si vous souhaitez retourner les chemises pour une taille plus approprie ou obtenir un remboursement, veuillez nous contacter ds que possible et nous nous ferons un plaisir de vous assister dans votre processus de retour.\\n\\nL'quipe de service  la clientle est l pour vous aider et rsoudre ce problme de la meilleure faon possible. Nous esprons avoir l'occasion de regagner votre confiance et de vous offrir une exprience d'achat plus satisfaisante  l'avenir.\\n\\nCordialement,\\nVotre quipe de service  la clientle\"}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import SequentialChain\n",
    "\n",
    "\n",
    "# Ex 2.\n",
    "# Goal: The goal is to create four (4) chains that can be used to:\n",
    "# chain 1: translate a given review from French to English.\n",
    "# chain 2: summarize the English review from chain 1.\n",
    "# chain 3: determine the language of the input/initial review.\n",
    "# chain 4: write a follow up message in the detected language & return the specified outputs.\n",
    "\n",
    "# Note: The output variables and the kwargs MUST match.\n",
    "# e.g. output_key=\"english_review\" and {english_review}, etc\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(temperature=TEMPERATURE)\n",
    "\n",
    "# ===== Chain 1 =====\n",
    "template_1 = \"\"\"Translate the following review to English: \\ \n",
    "\n",
    "{review}\n",
    "\"\"\"\n",
    "\n",
    "prompt_1 = ChatPromptTemplate.from_template(template=template_1)\n",
    "chain_1 = LLMChain(llm=llm, prompt=prompt_1, output_key=\"english_review\")\n",
    "\n",
    "# ===== Chain 2 =====\n",
    "template_2 = \"\"\"Can you summarize the following review in 1 sentence?\n",
    "\n",
    "{english_review}\n",
    "\"\"\"\n",
    "\n",
    "prompt_2 = ChatPromptTemplate.from_template(template=template_2)\n",
    "chain_2 = LLMChain(llm=llm, prompt=prompt_2, output_key=\"summary\")\n",
    "\n",
    "# ===== Chain 3 =====\n",
    "template_3 = \"\"\"What language is the following review?\n",
    "\n",
    "{review}\n",
    "\"\"\"\n",
    "\n",
    "prompt_3 = ChatPromptTemplate.from_template(template=template_3)\n",
    "chain_3 = LLMChain(llm=llm, prompt=prompt_3, output_key=\"language\")\n",
    "\n",
    "# ===== Chain 4 =====\n",
    "template_4 = \"\"\"\"\"Write a follow up response to the following \\\n",
    "summary in the specified language:\n",
    "\n",
    "Summary: {summary}\n",
    "\n",
    "Language: {language}\n",
    "\"\"\"\n",
    "\n",
    "prompt_4 = ChatPromptTemplate.from_template(template=template_4)\n",
    "chain_4 = LLMChain(llm=llm, prompt=prompt_4, output_key=\"follow_up_msg\")\n",
    "\n",
    "# Get the output by combining all the chains\n",
    "input_review = \"\"\"J'ai command la mme taille que la dernire fois, \\ \n",
    "et ces chemises taient beaucoup plus grandes que la commande prcdente. \\ \n",
    "Elles taient aussi environ 6 pouces plus longues. C'tait comme s'ils \\ \n",
    "avaient envoy des chemises pour hommes au lieu de chemises pour garons. \\ \n",
    "Je suis tellement en colre en ce moment.\n",
    "\"\"\"\n",
    "final_chain = SequentialChain(\n",
    "    chains=[chain_1, chain_2, chain_3, chain_4],\n",
    "    input_variables=[\"review\"],\n",
    "    output_variables=[\"english_review\", \"summary\", \"follow_up_msg\"],\n",
    "    verbose=True,\n",
    ")\n",
    "# Use __Call__ since you're expecting multiple outputs.\n",
    "output = final_chain(input_review)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'english_review': 'I ordered the same size as last time, and these shirts '\n",
      "                   'were much bigger than the previous order. They were also '\n",
      "                   \"about 6 inches longer. It was like they had sent men's \"\n",
      "                   \"shirts instead of boys' shirts. I am so angry right now.\",\n",
      " 'follow_up_msg': 'Cher client,\\n'\n",
      "                  '\\n'\n",
      "                  \"Nous tenons tout d'abord  vous remercier d'avoir partag \"\n",
      "                  'votre exprience avec nous. Nous sommes vritablement '\n",
      "                  \"dsols d'apprendre votre dception quant  votre commande \"\n",
      "                  'de chemises.\\n'\n",
      "                  '\\n'\n",
      "                  'Nous nous excusons sincrement pour toute confusion ou '\n",
      "                  'malentendu qui aurait pu survenir. Votre satisfaction est '\n",
      "                  'notre priorit absolue et nous prenons vos commentaires '\n",
      "                  'trs au srieux. Permettez-nous de vous assurer que nous '\n",
      "                  'allons enquter sur cette situation et prendre les mesures '\n",
      "                  'ncessaires pour viter que cela ne se reproduise  '\n",
      "                  \"l'avenir.\\n\"\n",
      "                  '\\n'\n",
      "                  'Pourriez-vous nous fournir plus de dtails sur le problme '\n",
      "                  'rencontr ? Nous aimerions comprendre quelles ont t les '\n",
      "                  'erreurs commises afin que nous puissions rectifier la '\n",
      "                  'situation dans les plus brefs dlais. \\n'\n",
      "                  '\\n'\n",
      "                  'Nous tenons  souligner que nous offrons une politique de '\n",
      "                  \"retour et d'change sans tracas. Si vous souhaitez \"\n",
      "                  'retourner les chemises pour une taille plus approprie ou '\n",
      "                  'obtenir un remboursement, veuillez nous contacter ds que '\n",
      "                  'possible et nous nous ferons un plaisir de vous assister '\n",
      "                  'dans votre processus de retour.\\n'\n",
      "                  '\\n'\n",
      "                  \"L'quipe de service  la clientle est l pour vous aider \"\n",
      "                  'et rsoudre ce problme de la meilleure faon possible. '\n",
      "                  \"Nous esprons avoir l'occasion de regagner votre confiance \"\n",
      "                  \"et de vous offrir une exprience d'achat plus satisfaisante \"\n",
      "                  \" l'avenir.\\n\"\n",
      "                  '\\n'\n",
      "                  'Cordialement,\\n'\n",
      "                  'Votre quipe de service  la clientle',\n",
      " 'review': \"J'ai command la mme taille que la dernire fois, \\\\ \\n\"\n",
      "           'et ces chemises taient beaucoup plus grandes que la commande '\n",
      "           'prcdente. \\\\ \\n'\n",
      "           \"Elles taient aussi environ 6 pouces plus longues. C'tait comme \"\n",
      "           \"s'ils \\\\ \\n\"\n",
      "           'avaient envoy des chemises pour hommes au lieu de chemises pour '\n",
      "           'garons. \\\\ \\n'\n",
      "           'Je suis tellement en colre en ce moment.\\n',\n",
      " 'summary': 'The reviewer expresses frustration and disappointment with their '\n",
      "            'recent order of shirts, as they discovered them to be '\n",
      "            'significantly larger and longer than expected, possibly '\n",
      "            \"resembling men's shirts instead of boys' shirts.\"}\n"
     ]
    }
   ],
   "source": [
    "pprint(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "# Create an LLMChain to write a synopsis given a title of a play\n",
    "llm = OpenAI(temperature=.7)\n",
    "template = \"\"\"You are a playwright. Given the title of play, it is your job to write a synopsis for that title.\n",
    "\n",
    "Title: {title}\n",
    "Playwright: This is a synopsis for the above play:\"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=[\"title\"], template=template)\n",
    "synopsis_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "# Create an LLMChain to write a review of a play given a synopsis\n",
    "llm = OpenAI(temperature=.7)\n",
    "template = \"\"\"You are a play critic from the New York Times. Given the synopsis of play, it is your job to write a review for that play.\n",
    "\n",
    "Play Synopsis:\n",
    "{synopsis}\n",
    "Review from a New York Times play critic of the above play:\"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=[\"synopsis\"], template=template)\n",
    "review_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "# Create the overall chain where we run these two chains in sequence\n",
    "overall_chain = SimpleSequentialChain(chains=[synopsis_chain, review_chain], verbose=True)\n",
    "\n",
    "# Run the overall chain with a given title\n",
    "review = overall_chain.run(\"Tragedy at sunset on the beach\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_p310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
