{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [LangChain](https://docs.langchain.com/docs/)\n",
    "\n",
    "<br>\n",
    "\n",
    "- LangChain is a framework for developing applications powered by language models.\n",
    "\n",
    "## Topics Covered\n",
    "\n",
    "1. Vectorstore Retrieval\n",
    "2. Chatbot\n",
    "\n",
    "## Installation\n",
    "\n",
    "```sh\n",
    "pip install langchain\n",
    "\n",
    "# OR\n",
    "pip install 'langchain[all]'\n",
    "\n",
    "# Other dependencies\n",
    "pip install python-dotenv\n",
    "pip install openai\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in library\n",
    "import itertools\n",
    "import re\n",
    "import json\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "# Standard imports\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# pandas settings\n",
    "pd.options.display.max_rows = 1_000\n",
    "pd.options.display.max_columns = 1_000\n",
    "pd.options.display.max_colwidth = 600\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Black code formatter (Optional)\n",
    "%load_ext lab_black\n",
    "# auto reload imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())  # read local .env file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Vectorstore Retrieval\n",
    "\n",
    "```text\n",
    "\n",
    "Retrieval\n",
    "---------\n",
    "- This is the process of finding documents that are relevant to a given query. \n",
    "- It's done by using a semantic search algorithm to calculate the similarity between the query and the documents in a corpus. \n",
    "- The documents that are most similar to the query are then retrieved.\n",
    "\n",
    "- It's an important part of NLP chatbot pipeline because it allows the system to access information that's not already stored in the language model. \n",
    "- It can be useful for a variety of tasks, such as answering questions, generating text, and translating languages.\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "```sh\n",
    "# Install dependency\n",
    "pip install lark\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "418\n"
     ]
    }
   ],
   "source": [
    "# Similarity Search\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "\n",
    "persist_directory = \"../../data/chroma/\"\n",
    "# Create embeddings\n",
    "embedding = OpenAIEmbeddings()\n",
    "vectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding)\n",
    "\n",
    "print(vectordb._collection.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Document Similarity\n",
    "\n",
    "- Semantic search algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='A mushroom with a large fruiting body is the Amanita phalloides. Some varieties are all-white.', metadata={}),\n",
       " Document(page_content='The Amanita phalloides has a large and imposing epigeous (aboveground) fruiting body (basidiocarp).', metadata={})]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = [\n",
    "    \"\"\"The Amanita phalloides has a large and imposing epigeous (aboveground) fruiting body (basidiocarp).\"\"\",\n",
    "    \"\"\"A mushroom with a large fruiting body is the Amanita phalloides. Some varieties are all-white.\"\"\",\n",
    "    \"\"\"A. phalloides, a.k.a Death Cap, is one of the most poisonous of all known mushrooms.\"\"\",\n",
    "]\n",
    "# Create DB\n",
    "smalldb = Chroma.from_texts(texts, embedding=embedding)\n",
    "question = \"Tell me about all-white mushrooms with large fruiting bodies\"\n",
    "\n",
    "# Find similar documents\n",
    "# This does NOT select docs that are diverse and contain important info.\n",
    "smalldb.similarity_search(question, k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Maximum Marginal Relevance\n",
    "\n",
    "```text\n",
    "- This returns docs selected using the maximal marginal relevance. \n",
    "- Maximal marginal relevance optimizes for `similarity to query` AND `diversity among selected documents`.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='A mushroom with a large fruiting body is the Amanita phalloides. Some varieties are all-white.', metadata={}),\n",
       " Document(page_content='A. phalloides, a.k.a Death Cap, is one of the most poisonous of all known mushrooms.', metadata={})]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smalldb.max_marginal_relevance_search(\n",
    "    question,\n",
    "    k=2,  # Num of Documents to return\n",
    "    fetch_k=3,  # Num of Documents to fetch to pass to MMR algorithm.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('those homeworks will be done in either MATLA B or in Octave, which is sort '\n",
      " 'of — I \\n'\n",
      " 'know some people ')\n",
      "('those homeworks will be done in either MATLA B or in Octave, which is sort '\n",
      " 'of — I \\n'\n",
      " 'know some people ')\n"
     ]
    }
   ],
   "source": [
    "question = \"what did they say about matlab?\"\n",
    "docs_ss = vectordb.similarity_search(question, k=3)\n",
    "\n",
    "# Notice that the results are exactly the same\n",
    "# i.e. they are duplicated\n",
    "pprint(docs_ss[0].page_content[:100])\n",
    "pprint(docs_ss[1].page_content[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('those homeworks will be done in either MATLA B or in Octave, which is sort '\n",
      " 'of — I \\n'\n",
      " 'know some people ')\n",
      "('into his office and he said, \"Oh, professo r, professor, thank you so much '\n",
      " 'for your \\n'\n",
      " 'machine learnin')\n"
     ]
    }
   ],
   "source": [
    "# Compare the difference in results with MMR.\n",
    "docs_mmr = vectordb.max_marginal_relevance_search(question, k=3)\n",
    "\n",
    "# Notice that the results are different!\n",
    "pprint(docs_mmr[0].page_content[:100])\n",
    "pprint(docs_mmr[1].page_content[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3a. Addressing Specificity: Working with metadata\n",
    "\n",
    "```text\n",
    "- There are times when the search results include results from other documents that are NOT relevant.\n",
    "- To address this, many vectorstores support operations on metadata.\n",
    "- Metadata provides context for each embedded chunk.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'page': 0, 'source': '../../data/cs229-data/MachineLearning-Lecture03.pdf'}\n",
      "{'page': 0, 'source': '../../data/cs229-data/MachineLearning-Lecture03.pdf'}\n",
      "{'page': 14, 'source': '../../data/cs229-data/MachineLearning-Lecture03.pdf'}\n"
     ]
    }
   ],
   "source": [
    "fp = \"../../data/cs229-data/MachineLearning-Lecture03.pdf\"\n",
    "question = \"what did they say about regression in the third lecture?\"\n",
    "\n",
    "# Search for similar docs using the metadata which provides more context.\n",
    "docs = vectordb.similarity_search(\n",
    "    question,\n",
    "    k=3,\n",
    "    filter={\"source\": fp},  # Ensures that this document is searched\n",
    ")\n",
    "for d in docs:\n",
    "    print(d.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b. Inferring The Metadata Using An LLM\n",
    "\n",
    "```text\n",
    "Addressing Specificity: working with metadata using self-query retriever.\n",
    "- There's an interesting challenge: we often want to infer the metadata from the query itself.\n",
    "- To address this, we can use `SelfQueryRetriever`, which uses an LLM to extract:\n",
    "- The query string to use for vector search\n",
    "  - A metadata filter to pass in as well.\n",
    "- Most vector databases support metadata filters, so this doesn't require any new databases or indexes.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AttributeInfo(name='source', description='The lecture the chunk is from, should be one of `../../data/cs229-data/MachineLearning-Lecture01.pdf`, `../../data/cs229-data/MachineLearning-Lecture02.pdf`, or `../../data/cs229-data/MachineLearning-Lecture03.pdf`', type='string'),\n",
       " AttributeInfo(name='page', description='The page from the lecture', type='integer')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "\n",
    "\n",
    "fp = \"../../data/cs229-data/MachineLearning-Lecture\"\n",
    "DESCRIPTION = f\"The lecture the chunk is from, should be one of `{fp}01.pdf`, `{fp}02.pdf`, or `{fp}03.pdf`\"\n",
    "\n",
    "METADATA_FIELD_INFO = [\n",
    "    AttributeInfo(\n",
    "        name=\"source\",\n",
    "        description=DESCRIPTION,\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"page\",\n",
    "        description=\"The page from the lecture\",\n",
    "        type=\"integer\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "METADATA_FIELD_INFO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query='regression' filter=Comparison(comparator=<Comparator.EQ: 'eq'>, attribute='source', value='../../data/cs229-data/MachineLearning-Lecture03.pdf') limit=None\n"
     ]
    }
   ],
   "source": [
    "DOCUMENT_CONTENT_DESCRIPTION = \"Lecture notes\"\n",
    "llm = OpenAI(temperature=0)\n",
    "retriever = SelfQueryRetriever.from_llm(\n",
    "    llm, vectordb, DOCUMENT_CONTENT_DESCRIPTION, METADATA_FIELD_INFO, verbose=True\n",
    ")\n",
    "question = \"what did they say about regression in the third lecture?\"\n",
    "\n",
    "docs = retriever.get_relevant_documents(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'page': 14, 'source': '../../data/cs229-data/MachineLearning-Lecture03.pdf'}\n",
      "{'page': 14, 'source': '../../data/cs229-data/MachineLearning-Lecture03.pdf'}\n",
      "{'page': 0, 'source': '../../data/cs229-data/MachineLearning-Lecture03.pdf'}\n",
      "{'page': 0, 'source': '../../data/cs229-data/MachineLearning-Lecture03.pdf'}\n"
     ]
    }
   ],
   "source": [
    "# You can see that all the query results are from lecture 3!\n",
    "for d in docs:\n",
    "    print(d.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student: It’s the lowest it –  \n",
      "Instructor (Andrew Ng) :No, exactly. Right. So zero to the same, this is not the same, \n",
      "right? And the reason is, in logi stic regression this is diffe rent from before\n",
      "\n",
      " ========================================================================================================================\n",
      "Student: It’s the lowest it –  \n",
      "Instructor (Andrew Ng) :No, exactly. Right. So zero to the same, this is not the same, \n",
      "right? And the reason is, in logi stic regression this is diffe rent from before\n",
      "\n",
      " ========================================================================================================================\n",
      "MachineLearning-Lecture03  \n",
      "Instructor (Andrew Ng) :Okay. Good morning and welcome b ack to the third lecture of \n",
      "this class. So here’s what I want to do t oday, and some of the topics I do today may \n",
      "\n",
      " ========================================================================================================================\n",
      "MachineLearning-Lecture03  \n",
      "Instructor (Andrew Ng) :Okay. Good morning and welcome b ack to the third lecture of \n",
      "this class. So here’s what I want to do t oday, and some of the topics I do today may \n",
      "\n",
      " ========================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# You can see that all the query results are from lecture 3!\n",
    "for d in docs:\n",
    "    print(d.page_content[:200])\n",
    "    print(\"\\n\", \"======\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smalldb.similarity_search(question, k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Compression\n",
    "\n",
    "```text\n",
    "- Compression is another approach for improving the quality of retrieved docs.\n",
    "- Information most relevant to a query may be buried in a document with a lot of irrelevant text.\n",
    "- Passing that full document through your application can lead to more expensive LLM calls and poorer responses.\n",
    "- Contextual compression is meant to fix this.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "\n",
    "\n",
    "def pretty_print_docs(docs):\n",
    "    print(\n",
    "        f\"\\n{'-' * 100}\\n\".join(\n",
    "            [f\"Document {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "\n",
      "\"MATLAB is I guess part of the programming language that makes it very easy to write codes using matrices, to write code for numerical routines, to move data around, to plot data. And it's sort of an extremely easy to learn tool to use for implementing a lot of learning algorithms.\"\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2:\n",
      "\n",
      "\"MATLAB is I guess part of the programming language that makes it very easy to write codes using matrices, to write code for numerical routines, to move data around, to plot data. And it's sort of an extremely easy to learn tool to use for implementing a lot of learning algorithms.\"\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 3:\n",
      "\n",
      "\"MATLAB is I guess part of the programming language that makes it very easy to write codes using matrices, to write code for numerical routines, to move data around, to plot data. And it's sort of an extremely easy to learn tool to use for implementing a lot of learning algorithms.\"\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 4:\n",
      "\n",
      "\"MATLAB is I guess part of the programming language that makes it very easy to write codes using matrices, to write code for numerical routines, to move data around, to plot data. And it's sort of an extremely easy to learn tool to use for implementing a lot of learning algorithms.\"\n"
     ]
    }
   ],
   "source": [
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "# DocumentCompressor that uses an LLM chain to extract the relevant parts of documents.\n",
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "\n",
    "# Retriever that compresses the result\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=vectordb.as_retriever(search_type=\"mmr\"),\n",
    ")\n",
    "\n",
    "question = \"what did they say about matlab?\"\n",
    "compressed_docs = compression_retriever.get_relevant_documents(question)\n",
    "pretty_print_docs(compressed_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Other types of retrieval\n",
    "\n",
    "```text\n",
    "- The LangChain retriever abstraction includes other ways to retrieve documents, such as TF-IDF or SVM.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MachineLearning-Lecture01  \\nInstructor (Andrew Ng):  Okay. Good morning. Welcome to CS229, the machi'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.retrievers import SVMRetriever\n",
    "from langchain.retrievers import TFIDFRetriever\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "# Load PDF\n",
    "fp = \"../../data/cs229-data/MachineLearning-Lecture01.pdf\"\n",
    "loader = PyPDFLoader(file_path=fp)\n",
    "pages = loader.load()\n",
    "\n",
    "# Extract all the text from the pages\n",
    "all_page_text = [p.page_content for p in pages]\n",
    "joined_page_text = \" \".join(all_page_text)\n",
    "joined_page_text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content=\"let me just check what questions you have righ t now. So if there are no questions, I'll just \\nclose with two reminders, which are after class today or as you start to talk with other \\npeople in this class, I just encourage you again to start to form project partners, to try to \\nfind project partners to do your project with. And also, this is a good time to start forming \\nstudy groups, so either talk to your friends  or post in the newsgroup, but we just \\nencourage you to try to star t to do both of those today, okay? Form study groups, and try \\nto find two other project partners.  \\nSo thank you. I'm looking forward to teaching this class, and I'll see you in a couple of \\ndays.   [End of Audio]  \\nDuration: 69 minutes\", metadata={})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the docs\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=150)\n",
    "splits = text_splitter.split_text(joined_page_text)\n",
    "\n",
    "# Retrieve the info using SVM\n",
    "svm_retriever = SVMRetriever.from_texts(splits, embedding)\n",
    "tfidf_retriever = TFIDFRetriever.from_texts(splits)\n",
    "\n",
    "question = \"What are major topics for this class?\"\n",
    "docs_svm = svm_retriever.get_relevant_documents(question)\n",
    "docs_svm[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content=\"Saxena and Min Sun here did, wh ich is given an image like this, right? This is actually a \\npicture taken of the Stanford campus. You can apply that sort of cl ustering algorithm and \\ngroup the picture into regions. Let me actually blow that up so that you can see it more \\nclearly. Okay. So in the middle, you see the lines sort of groupi ng the image together, \\ngrouping the image into [inaudible] regions.  \\nAnd what Ashutosh and Min did was they then  applied the learning algorithm to say can \\nwe take this clustering and us e it to build a 3D model of the world? And so using the \\nclustering, they then had a lear ning algorithm try to learn what the 3D structure of the \\nworld looks like so that they could come up with a 3D model that you can sort of fly \\nthrough, okay? Although many people used to th ink it's not possible to take a single \\nimage and build a 3D model, but using a lear ning algorithm and that sort of clustering \\nalgorithm is the first step. They were able to.  \\nI'll just show you one more example. I like this  because it's a picture of Stanford with our \\nbeautiful Stanford campus. So again, taking th e same sort of clustering algorithms, taking \\nthe same sort of unsupervised learning algor ithm, you can group the pixels into different \\nregions. And using that as a pre-processing step, they eventually built this sort of 3D model of Stanford campus in a single picture.  You can sort of walk  into the ceiling, look\", metadata={})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve the info using TFIDF\n",
    "question = \"what did they say about matlab?\"\n",
    "docs_tfidf = tfidf_retriever.get_relevant_documents(question)\n",
    "docs_tfidf[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><hr>\n",
    "\n",
    "## Chatbot\n",
    "\n",
    "- [Example notebook](https://github.com/pinecone-io/examples/blob/master/generation/langchain/handbook/05-langchain-retrieval-augmentation.ipynb)\n",
    "  \n",
    "<br>\n",
    "\n",
    "### Querying A Vectorstore\n",
    "\n",
    "<br>\n",
    "\n",
    "[![image.png](https://i.postimg.cc/7PWxf0dh/image.png)](https://postimg.cc/YjQcPGpB)\n",
    "\n",
    "<br>\n",
    "\n",
    "```text\n",
    "1. Using the embedding model create vector embeddings for the context to be indexed.\n",
    "2. Insert the veector embeddings into the vector DB with some reference to the original context the embeddings were created from.\n",
    "3. When a query is made using the application, embed the query using the same embedding model and query the vector DB for similar vector embeddings.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "\n",
    "persist_directory = \"../../data/chroma/\"\n",
    "embedding = OpenAIEmbeddings()\n",
    "\n",
    "# Load the embedded data\n",
    "vectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding)\n",
    "\n",
    "question = \"What are major topics for this class?\"\n",
    "docs = vectordb.similarity_search(question, k=3)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yes, probability is a class topic. Thanks for asking!'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "\n",
    "llm_name = \"gpt-3.5-turbo\"\n",
    "llm = ChatOpenAI(model_name=llm_name, temperature=0)\n",
    "\n",
    "\n",
    "# Build prompt\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end. \\\n",
    "If you don't know the answer, just say that you don't know, don't try to \\\n",
    "make up an answer. Use three sentences maximum. Keep the answer as concise as possible. \\\n",
    "Always say \"thanks for asking!\" at the end of the answer. \n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=template,\n",
    ")\n",
    "\n",
    "# Run chain\n",
    "question = \"Is probability a class topic?\"\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=vectordb.as_retriever(),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT},\n",
    ")\n",
    "\n",
    "result = qa_chain({\"query\": question})\n",
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yes, probability is a topic that will be covered in this class. The instructor assumes that students have familiarity with basic probability and statistics.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "\n",
    "# Create memory\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "# ConversationalRetrievalChain\n",
    "retriever = vectordb.as_retriever()\n",
    "qa = ConversationalRetrievalChain.from_llm(llm, retriever=retriever, memory=memory)\n",
    "question = \"Is probability a class topic?\"\n",
    "result = qa({\"question\": question})\n",
    "\n",
    "result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The instructor of the class was Andrew Ng.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"Who taught the class?\"\n",
    "result = qa({\"question\": question})\n",
    "\n",
    "result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MATLAB is described as a programming language that makes it easy to write codes using matrices, perform numerical routines, manipulate data, and plot data. It is also mentioned that MATLAB is an easy-to-learn tool for implementing learning algorithms. Additionally, it is mentioned that MATLAB can be used for the homework assignments in the class. It is also mentioned that Octave is a software package that is similar to MATLAB and can be downloaded for free off the internet. Octave has fewer features than MATLAB but can be used for most purposes in the class.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"what did they say about matlab?\"\n",
    "result = qa({\"question\": question})\n",
    "\n",
    "result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_p310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
