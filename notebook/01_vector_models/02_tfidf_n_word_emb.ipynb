{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF (from scratch) And Word Embeddings\n",
    "\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import spacy\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "\n",
    "# Custom imports\n",
    "\n",
    "# Built-in library\n",
    "import itertools\n",
    "import re\n",
    "import json\n",
    "from typing import Union, Optional, Any\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "# pandas settings\n",
    "pd.options.display.max_rows = 1_000\n",
    "pd.options.display.max_columns = 1_000\n",
    "pd.options.display.max_colwidth = 600\n",
    "\n",
    "# Black code formatter (Optional)\n",
    "%load_ext lab_black\n",
    "# auto reload imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure warnings and pther settings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sns.set()\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "def load_data(*, filename: str) -> pd.DataFrame:\n",
    "    \"\"\"This is used to load the data.\n",
    "\n",
    "    Params;\n",
    "        filename (str): The filepath.\n",
    "\n",
    "    Returns:\n",
    "        df (pd.DataFrame): The loaded dataframe.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(filename)\n",
    "    print(f\"Shape of df: {df.shape}\\n\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of df: (2225, 2)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ad sales boost Time Warner profit\\n\\nQuarterly profits at US media giant TimeWarner jumped 76% to $1.13bn (£600m) for the three months to December, from $639m year-earlier.\\n\\nThe firm, which is now one of the biggest investors in Google, benefited from sales of high-speed internet connections and higher advert sales. TimeWarner said fourth quarter sales rose 2% to $11.1bn from $10.9bn. Its profits were buoyed by one-off gains which offset a profit dip at Warner Bros, and less users for AOL.\\n\\nTime Warner said on Friday that it now owns 8% of search-engine Google. But its own internet bus...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dollar gains on Greenspan speech\\n\\nThe dollar has hit its highest level against the euro in almost three months after the Federal Reserve head said the US trade deficit is set to stabilise.\\n\\nAnd Alan Greenspan highlighted the US government's willingness to curb spending and rising household savings as factors which may help to reduce it. In late trading in New York, the dollar reached $1.2871 against the euro, from $1.2974 on Thursday. Market concerns about the deficit has hit the greenback in recent months. On Friday, Federal Reserve chairman Mr Greenspan's speech in London ahead of th...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      text  \\\n",
       "0  Ad sales boost Time Warner profit\\n\\nQuarterly profits at US media giant TimeWarner jumped 76% to $1.13bn (£600m) for the three months to December, from $639m year-earlier.\\n\\nThe firm, which is now one of the biggest investors in Google, benefited from sales of high-speed internet connections and higher advert sales. TimeWarner said fourth quarter sales rose 2% to $11.1bn from $10.9bn. Its profits were buoyed by one-off gains which offset a profit dip at Warner Bros, and less users for AOL.\\n\\nTime Warner said on Friday that it now owns 8% of search-engine Google. But its own internet bus...   \n",
       "1  Dollar gains on Greenspan speech\\n\\nThe dollar has hit its highest level against the euro in almost three months after the Federal Reserve head said the US trade deficit is set to stabilise.\\n\\nAnd Alan Greenspan highlighted the US government's willingness to curb spending and rising household savings as factors which may help to reduce it. In late trading in New York, the dollar reached $1.2871 against the euro, from $1.2974 on Thursday. Market concerns about the deficit has hit the greenback in recent months. On Friday, Federal Reserve chairman Mr Greenspan's speech in London ahead of th...   \n",
       "\n",
       "     labels  \n",
       "0  business  \n",
       "1  business  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = \"../../data/bbc_text_cls.csv\"\n",
    "data = load_data(filename=filename)\n",
    "\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    \"\"\"This is used to tokenize documents\"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.nlp = nlp\n",
    "\n",
    "    def __call__(self, doc: str, *args: Any, **kwargs: Any) -> list[str]:\n",
    "        # Tokenize\n",
    "        doc = nlp(doc)\n",
    "        tokenized_doc = [word.text.lower() for word in doc]\n",
    "        return tokenized_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thank you for being an awesome father</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I have an awesome God. I just wanna say thank you</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text labels\n",
       "0              Thank you for being an awesome father      a\n",
       "1  I have an awesome God. I just wanna say thank you      b"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {\n",
    "    \"text\": [\n",
    "        \"Thank you for being an awesome father\",\n",
    "        \"I have an awesome God. I just wanna say thank you\",\n",
    "    ],\n",
    "    \"labels\": [\"a\", \"b\"],\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(d)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BagOfWordsCalculator:\n",
    "    \"\"\"This tokenizes all the documents and calculates the bag of words.\n",
    "    i.e all the unique words in the document are stored and counted.\n",
    "\n",
    "    Returns:\n",
    "        tokenized_docs (list[int]): Tokenized documents i.e list of\n",
    "            tokenized documents where every row in the data is a document.\n",
    "        bag_of_words (dict[str, int]): A dict containing a unique word and the\n",
    "            unique numeric representation of the word.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.tokenizer = Tokenizer()\n",
    "\n",
    "    def __call__(\n",
    "        self, data: pd.DataFrame, *args: Any, **kwargs: Any\n",
    "    ) -> tuple[list, dict]:\n",
    "        \"\"\"This calculates the bag of words.\"\"\"\n",
    "        count = 0\n",
    "        bag_of_words = {}\n",
    "        tokenized_docs = []\n",
    "\n",
    "        for doc in data:\n",
    "            # Tokenize docs\n",
    "            tokenized_doc = self.tokenizer(doc=doc)\n",
    "            doc_as_num = []\n",
    "\n",
    "            for word in tokenized_doc:\n",
    "                # Store the unique words as numbers in the dict\n",
    "                if word not in bag_of_words:\n",
    "                    bag_of_words[word] = count\n",
    "                    count += 1\n",
    "                # Save the word as a number\n",
    "                doc_as_num.append(bag_of_words.get(word))\n",
    "            # Store the tokenized docs (converted to numbers) in a list\n",
    "            tokenized_docs.append(doc_as_num)\n",
    "        return (tokenized_docs, bag_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                Thank you for being an awesome father\n",
       "1    I have an awesome God. I just wanna say thank you\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[0, 1, 2, 3, 4, 5, 6], [7, 8, 4, 5, 9, 10, 7, 11, 12, 13, 0, 1]],\n",
       " {'thank': 0,\n",
       "  'you': 1,\n",
       "  'for': 2,\n",
       "  'being': 3,\n",
       "  'an': 4,\n",
       "  'awesome': 5,\n",
       "  'father': 6,\n",
       "  'i': 7,\n",
       "  'have': 8,\n",
       "  'god': 9,\n",
       "  '.': 10,\n",
       "  'just': 11,\n",
       "  'wanna': 12,\n",
       "  'say': 13})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_o_words_cal = BagOfWordsCalculator()\n",
    "t_docs, b_o_words = b_o_words_cal(data=df[\"text\"])\n",
    "\n",
    "# tokenized_docs, bag_of_words\n",
    "t_docs, b_o_words"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><hr>\n",
    "\n",
    "## Calculate The Term Frequency\n",
    "\n",
    "Term frequency, `tf(t,d)`, is the relative frequency of term ***`t`*** within document ***`d`***\n",
    "\n",
    "$$\n",
    "tf(t,d) = \\frac{count_{t/d}}{number_{terms/d}}\n",
    "$$\n",
    "\n",
    "where: \\\n",
    "$count_{t/d}$: Count of `t` in `d` \\\n",
    "$number_{terms/d}$: Number of `terms` in `d`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_o_words_cal = BagOfWordsCalculator()\n",
    "t_docs, b_o_words = b_o_words_cal(data=df[\"text\"])\n",
    "\n",
    "# Instantiate: number of docs and number of words\n",
    "N, V = df.shape[0], len(b_o_words)\n",
    "\n",
    "# Term Frequency\n",
    "tf = np.zeros((N, V))\n",
    "tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 2, 3, 4, 5, 6], [7, 8, 4, 5, 9, 10, 7, 11, 12, 13, 0, 1]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 1., 0., 0., 1., 1., 0., 2., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for each word in the doc and increment the count\n",
    "# of the word wherever it occurs.\n",
    "\n",
    "# Note::\n",
    "# document: a list of words/terms,\n",
    "# doc_idx: index of the current document,\n",
    "# doc_as_num: the current tokenized document,\n",
    "# words_idx: the words represented as numbers,\n",
    "\n",
    "for doc_idx, doc_as_num in enumerate(t_docs):\n",
    "    for words_idx in doc_as_num:\n",
    "        tf[doc_idx, words_idx] += 1\n",
    "\n",
    "tf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting It All Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCountVectorizer:\n",
    "    \"\"\"This is used to count the terms in a given document.\"\"\"\n",
    "\n",
    "    def __init__(self, data: pd.Series) -> None:\n",
    "        self.data = data\n",
    "        self.vocabulary = {}\n",
    "        self.tokenized_docs = []\n",
    "        self.tokenizer = Tokenizer()\n",
    "\n",
    "    def tokenize_docs(self) -> tuple[list, dict]:\n",
    "        \"\"\"This tokenizes the documents.\"\"\"\n",
    "        count = 0\n",
    "\n",
    "        for doc in self.data:\n",
    "            # Tokenize docs\n",
    "            tokenized_doc = self.tokenizer(doc=doc)\n",
    "            doc_as_num = []\n",
    "\n",
    "            for word in tokenized_doc:\n",
    "                # Store the unique words as numbers in the dict\n",
    "                if word not in self.vocabulary:\n",
    "                    self.vocabulary[word] = count\n",
    "                    count += 1\n",
    "                # Save the word as a number\n",
    "                doc_as_num.append(self.vocabulary.get(word))\n",
    "            # Store the tokenized docs (converted to numbers) in a list\n",
    "            self.tokenized_docs.append(doc_as_num)\n",
    "        return (self.tokenized_docs, self.vocabulary)\n",
    "\n",
    "    def calculate_term_frequency(self, *args: Any, **kwargs: Any) -> np.ndarray:\n",
    "        \"\"\"Calculate term frequency/bag of words.\"\"\"\n",
    "        self.tokenized_docs, self.vocabulary = self.tokenize_docs()\n",
    "        # Number of docs and number of words\n",
    "        N, W = self.data.shape[0], len(self.vocabulary)\n",
    "        tf = np.zeros((N, W))  # Instantiate tf\n",
    "\n",
    "        # Check each word in the doc and increment the count\n",
    "        # of the word wherever it occurs\n",
    "        for doc_idx, doc_as_num in enumerate(self.tokenized_docs):\n",
    "            for words_idx in doc_as_num:\n",
    "                tf[doc_idx, words_idx] += 1\n",
    "        return tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thank you for being an awesome father</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I have an awesome God. I just wanna say thank you</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text labels\n",
       "0              Thank you for being an awesome father      a\n",
       "1  I have an awesome God. I just wanna say thank you      b"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 1., 0., 0., 1., 1., 0., 2., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer = CustomCountVectorizer(data=df[\"text\"])\n",
    "tf = count_vectorizer.calculate_term_frequency()\n",
    "tf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><hr>\n",
    "\n",
    "## Calculate TF-IDF\n",
    "\n",
    "## Document Frequency\n",
    "\n",
    "`Document Frequency` is the number of documents, `d` in which the term, `t` is present.\n",
    "\n",
    "```code\n",
    "df = occurences of `t` in `d`\n",
    "```\n",
    "\n",
    "\n",
    "## Inverse Document Frequency (IDF)\n",
    "\n",
    "> `IDF` is a measure of whether a term is `common` or `rare` in a given document corpus. It is obtained by **dividing** the **total number of documents** by the **number of documents containing the term** in the corpus.\n",
    "\n",
    "Let $\\mathbf{d_{f}(t)}$ be the number of documents term `t` appears in. There are a few other issues with the `IDF`; for example, if the corpus, $N$,is large, say 100,000,000, the IDF value explodes; \n",
    "\n",
    "$$\n",
    "\\mathbf{idf}(t,d) = \\frac{100,000,000}{d_{f}(t)} \\approx{0}\n",
    "$$\n",
    "\n",
    "To avoid this effect, we take the `log` of `idf`.\n",
    "\n",
    "$$\n",
    "\\mathbf{idf}(t,d) = \\log(\\frac{N}{d_{f}(t)})\n",
    "$$\n",
    "\n",
    "\n",
    "When a word that is not in the vocab occurs during the query, the ***`document frequency`*** is 0. Because we can't divide by zero, we smooth the value by adding 1 to the denominator.\n",
    "\n",
    "$$\n",
    "\\mathbf{idf}(t,d) = \\log\\frac{N}{{d_{f}(t)}+ 1}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\mathbf{tfidf}(t,d) =tf(t,d) \\times idf(t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14,), 2)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute IDF\n",
    "# DF: Document Frequency is the num of documents the term occurs in.\n",
    "# IDF: Number of documents(N) divided by DF. The log is taken to\n",
    "# reduce the impact of extremely large documents.\n",
    "# Therefore, IDF = log(N / DF)\n",
    "N = df.shape[0]\n",
    "document_frequency = (tf > 0).sum(axis=0)  # shape (V,)\n",
    "document_frequency.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.69314718, 0.69314718, 0.        ,\n",
       "       0.        , 0.69314718, 0.69314718, 0.69314718, 0.69314718,\n",
       "       0.69314718, 0.69314718, 0.69314718, 0.69314718])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf = np.log(N / document_frequency)\n",
    "\n",
    "idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                Thank you for being an awesome father\n",
       "1    I have an awesome God. I just wanna say thank you\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.69314718, 0.69314718, 0.        ,\n",
       "        0.        , 0.69314718, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 1.38629436, 0.69314718, 0.69314718,\n",
       "        0.69314718, 0.69314718, 0.69314718, 0.69314718]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf = tf * idf\n",
    "tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting It Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTFIDF:\n",
    "    \"\"\"This is used to calculate the term frequency\n",
    "    inverse document frequency of a given corpus.\"\"\"\n",
    "\n",
    "    def __init__(self, data: pd.Series) -> None:\n",
    "        self.data = data\n",
    "        self.vocabulary = {}\n",
    "        self.tokenized_docs = []\n",
    "        self.tokenizer = Tokenizer()\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        \"This returns the string representation of the class.\"\n",
    "        return (\n",
    "            f\"{__class__.__name__}(num_vocab: {len(self.vocabulary)}, \"\n",
    "            f\"num_doc: {len(self.tokenized_docs)})\"\n",
    "        )\n",
    "\n",
    "    def tokenize_docs(self) -> tuple[list, dict]:\n",
    "        \"\"\"This is used to tokenize the documents.\"\"\"\n",
    "        count = 0\n",
    "\n",
    "        for doc in self.data:\n",
    "            # Tokenize docs\n",
    "            tokenized_doc = self.tokenizer(doc=doc)\n",
    "            doc_as_num = []\n",
    "\n",
    "            for word in tokenized_doc:\n",
    "                # Store the unique words as numbers in the dict\n",
    "                if word not in self.vocabulary:\n",
    "                    self.vocabulary[word] = count\n",
    "                    count += 1\n",
    "                # Save the word as a number\n",
    "                doc_as_num.append(self.vocabulary.get(word))\n",
    "            # Store the tokenized docs (converted to numbers) in a list\n",
    "            self.tokenized_docs.append(doc_as_num)\n",
    "        return (self.tokenized_docs, self.vocabulary)\n",
    "\n",
    "    def convert_numbers_to_words(self) -> dict:\n",
    "        \"\"\"This is used to map numbers to words.\"\"\"\n",
    "        _, self.vocabulary = self.tokenize_docs()\n",
    "        nums_to_words = {num: word for word, num in self.vocabulary.items()}\n",
    "        return nums_to_words\n",
    "\n",
    "    def calculate_term_frequency(self, *args: Any, **kwargs: Any) -> np.ndarray:\n",
    "        \"\"\"Calculate the term frequency or bag of words.\"\"\"\n",
    "        self.tokenized_docs, self.vocabulary = self.tokenize_docs()\n",
    "        # Number of docs and number of words\n",
    "        N, W = self.data.shape[0], len(self.vocabulary)\n",
    "        tf = np.zeros((N, W))  # Instantiate tf\n",
    "\n",
    "        # Check for each word in the doc and increment the count\n",
    "        # of the word wherever it occurs.\n",
    "\n",
    "        # Note::\n",
    "        # document: a list of words/terms,\n",
    "        # doc_idx: index of the current document,\n",
    "        # doc_as_num: the current tokenized document,\n",
    "        # words_idx: the words represented as numbers,\n",
    "        for doc_idx, doc_as_num in enumerate(self.tokenized_docs):\n",
    "            for words_idx in doc_as_num:\n",
    "                tf[doc_idx, words_idx] += 1\n",
    "        return tf\n",
    "\n",
    "    def calculate_term_freq_inv_doc_freq(self) -> np.ndarray:\n",
    "        \"\"\"This returns the term frequency inverse document frequency\n",
    "        of a given corpus.\"\"\"\n",
    "        # DF: Document Frequency is the num of documents the term occurs in.\n",
    "        # IDF: Number of documents(N) divided by DF. The log is taken to\n",
    "        # reduce the impact of extremely large documents.\n",
    "        # Therefore, IDF = log(N / DF)\n",
    "        N = self.data.shape[0]\n",
    "        tf = self.calculate_term_frequency()\n",
    "        document_frequency = (tf > 0).sum(axis=0)  # shape (W,)\n",
    "        inverse_doc_freq = np.log(N / document_frequency)\n",
    "        tf_idf = tf * inverse_doc_freq\n",
    "        return tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomTFIDF(num_vocab: 14, num_doc: 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.69314718, 0.69314718, 0.        ,\n",
       "        0.        , 0.69314718, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 1.38629436, 0.69314718, 0.69314718,\n",
       "        0.69314718, 0.69314718, 0.69314718, 0.69314718]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vec = CustomTFIDF(data=df[\"text\"])\n",
    "tfidf = tfidf_vec.calculate_term_freq_inv_doc_freq()\n",
    "print(tfidf_vec)\n",
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'thank',\n",
       " 1: 'you',\n",
       " 2: 'for',\n",
       " 3: 'being',\n",
       " 4: 'an',\n",
       " 5: 'awesome',\n",
       " 6: 'father',\n",
       " 7: 'i',\n",
       " 8: 'have',\n",
       " 9: 'god',\n",
       " 10: '.',\n",
       " 11: 'just',\n",
       " 12: 'wanna',\n",
       " 13: 'say'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_2_word = tfidf_vec.convert_numbers_to_words()\n",
    "idx_2_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thank you for being an awesome father</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I have an awesome God. I just wanna say thank you</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text labels\n",
       "0              Thank you for being an awesome father      a\n",
       "1  I have an awesome God. I just wanna say thank you      b"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: a\n",
      "Text: Thank you for being an awesome father\n",
      "Top 5 important terms:\n",
      "word :for, score: 0.693\n",
      "word :being, score: 0.693\n",
      "word :father, score: 0.693\n",
      "word :thank, score: 0.0\n",
      "word :you, score: 0.0\n"
     ]
    }
   ],
   "source": [
    "idx_2_word = tfidf_vec.convert_numbers_to_words()\n",
    "\n",
    "# pick a random document, show the top 5 terms (in terms of tf_idf score)\n",
    "i = np.random.choice(N)\n",
    "# i = 0\n",
    "row = df.iloc[i]\n",
    "print(\"Label:\", row[\"labels\"])\n",
    "print(\"Text:\", row[\"text\"])\n",
    "print(\"Top 5 important terms:\")\n",
    "\n",
    "# Select the tfidf (scores) of a given document\n",
    "# and sort the scores in descending order\n",
    "scores = tfidf[i]\n",
    "indices = (-scores).argsort()\n",
    "\n",
    "for idx in indices[:5]:\n",
    "    print(f\"word :{idx_2_word[idx]}, score: {round(scores[idx], 3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ad sales boost Time Warner profit\\n\\nQuarterly profits at US media giant TimeWarner jumped 76% to $1.13bn (£600m) for the three months to December, from $639m year-earlier.\\n\\nThe firm, which is now one of the biggest investors in Google, benefited from sales of high-speed internet connections and higher advert sales. TimeWarner said fourth quarter sales rose 2% to $11.1bn from $10.9bn. Its profits were buoyed by one-off gains which offset a profit dip at Warner Bros, and less users for AOL.\\n\\nTime Warner said on Friday that it now owns 8% of search-engine Google. But its own internet bus...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dollar gains on Greenspan speech\\n\\nThe dollar has hit its highest level against the euro in almost three months after the Federal Reserve head said the US trade deficit is set to stabilise.\\n\\nAnd Alan Greenspan highlighted the US government's willingness to curb spending and rising household savings as factors which may help to reduce it. In late trading in New York, the dollar reached $1.2871 against the euro, from $1.2974 on Thursday. Market concerns about the deficit has hit the greenback in recent months. On Friday, Federal Reserve chairman Mr Greenspan's speech in London ahead of th...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yukos unit buyer faces loan claim\\n\\nThe owners of embattled Russian oil giant Yukos are to ask the buyer of its former production unit to pay back a $900m (£479m) loan.\\n\\nState-owned Rosneft bought the Yugansk unit for $9.3bn in a sale forced by Russia to part settle a $27.5bn tax claim against Yukos. Yukos' owner Menatep Group says it will ask Rosneft to repay a loan that Yugansk had secured on its assets. Rosneft already faces a similar $540m repayment demand from foreign banks. Legal experts said Rosneft's purchase of Yugansk would include such obligations. \"The pledged assets are wit...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      text  \\\n",
       "0  Ad sales boost Time Warner profit\\n\\nQuarterly profits at US media giant TimeWarner jumped 76% to $1.13bn (£600m) for the three months to December, from $639m year-earlier.\\n\\nThe firm, which is now one of the biggest investors in Google, benefited from sales of high-speed internet connections and higher advert sales. TimeWarner said fourth quarter sales rose 2% to $11.1bn from $10.9bn. Its profits were buoyed by one-off gains which offset a profit dip at Warner Bros, and less users for AOL.\\n\\nTime Warner said on Friday that it now owns 8% of search-engine Google. But its own internet bus...   \n",
       "1  Dollar gains on Greenspan speech\\n\\nThe dollar has hit its highest level against the euro in almost three months after the Federal Reserve head said the US trade deficit is set to stabilise.\\n\\nAnd Alan Greenspan highlighted the US government's willingness to curb spending and rising household savings as factors which may help to reduce it. In late trading in New York, the dollar reached $1.2871 against the euro, from $1.2974 on Thursday. Market concerns about the deficit has hit the greenback in recent months. On Friday, Federal Reserve chairman Mr Greenspan's speech in London ahead of th...   \n",
       "2  Yukos unit buyer faces loan claim\\n\\nThe owners of embattled Russian oil giant Yukos are to ask the buyer of its former production unit to pay back a $900m (£479m) loan.\\n\\nState-owned Rosneft bought the Yugansk unit for $9.3bn in a sale forced by Russia to part settle a $27.5bn tax claim against Yukos. Yukos' owner Menatep Group says it will ask Rosneft to repay a loan that Yugansk had secured on its assets. Rosneft already faces a similar $540m repayment demand from foreign banks. Legal experts said Rosneft's purchase of Yugansk would include such obligations. \"The pledged assets are wit...   \n",
       "\n",
       "     labels  \n",
       "0  business  \n",
       "1  business  \n",
       "2  business  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vec = CustomTFIDF(data=data[\"text\"])\n",
    "tfidf = tfidf_vec.calculate_term_freq_inv_doc_freq()\n",
    "print(tfidf_vec)\n",
    "idx_2_word = tfidf_vec.convert_numbers_to_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a random document, show the top 5 terms (in terms of tf_idf score)\n",
    "N = data.shape[0]\n",
    "i = np.random.choice(N)\n",
    "row = data.iloc[i]\n",
    "\n",
    "print(f\"i: {i}\")\n",
    "print(\"Label:\", row[\"labels\"])\n",
    "print(\"Text:\", row[\"text\"].split(\"\\n\")[0])\n",
    "print(\"Top 5 terms:\")\n",
    "\n",
    "scores = tfidf[i]\n",
    "indices = (-scores).argsort()  # Sort in descending order\n",
    "\n",
    "for idx in indices[:5]:\n",
    "    print(idx_2_word[idx])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify Using SKLearn's TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_vec = TfidfVectorizer(\n",
    "    stop_words=\"english\", tokenizer=Tokenizer(), max_features=25_000\n",
    ")\n",
    "X = data[\"text\"]\n",
    "X_tr = tf_idf_vec.fit_transform(X)\n",
    "\n",
    "dict_ = tf_idf_vec.vocabulary_\n",
    "idx_2_word_dict = {num: word for word, num in dict_.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 594\n",
    "row = data.iloc[i]\n",
    "\n",
    "print(f\"i: {i}\")\n",
    "print(\"Label:\", row[\"labels\"])\n",
    "print(\"Text:\", row[\"text\"].split(\"\\n\")[0])\n",
    "print(\"Top 5 terms:\")\n",
    "\n",
    "scores = X_tr[i].toarray().flatten()\n",
    "indices = (-scores).argsort()  # Sort in descending order\n",
    "\n",
    "for idx in indices[:5]:\n",
    "    print(idx_2_word_dict[idx])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Text Summarization\n",
    "\n",
    "### Using TFIDF\n",
    "\n",
    "1. Split the document into sentences.\n",
    "2. Score each sentence (using the average TFIDF of the non zero scores)\n",
    "3. Rank each sentence by the scores.\n",
    "4. Summary is approximately the top N ranked sentences by score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentencizer:\n",
    "    \"\"\"This is used to convert a document into a list of sentences.\n",
    "    It returns sentences.\"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.nlp = nlp\n",
    "\n",
    "    def __call__(self, doc: str, *args: Any, **kwargs: Any) -> list[str]:\n",
    "        # Tokenize\n",
    "        doc = nlp(doc)\n",
    "        sentences = list(doc.sents)\n",
    "        tokenized_sentences = [str(sentence) for sentence in sentences]\n",
    "        return tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"labels\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select document\n",
    "sample_data = (\n",
    "    data.loc[data[\"labels\"] == \"entertainment\", \"text\"]\n",
    "    .sample(n=3, random_state=123)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "sample_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split ONCE using '\\n' and exclude the title\n",
    "doc = sample_data.iloc[0].split(\"\\n\", 1)[1]\n",
    "doc[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = Sentencizer()\n",
    "sentences = sents(doc=doc)\n",
    "print(len(sentences))\n",
    "\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spaCy stopwords\n",
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "spacy_stopwords = list(spacy_stopwords)\n",
    "spacy_stopwords[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the sentences\n",
    "tfidf = TfidfVectorizer(stop_words=spacy_stopwords, norm=\"l1\")\n",
    "X_tr = tfidf.fit_transform(sentences)\n",
    "X_tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sentence_score(tfidf_row):\n",
    "    \"\"\"This returns the average score of the non-zero tfidf value\n",
    "    for a given sentence.\"\"\"\n",
    "    x = tfidf_row[tfidf_row != 0]  # Select the non-zero values\n",
    "    return x.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the score\n",
    "scores = np.zeros(len(sentences))\n",
    "\n",
    "# Calculate the score for each sentence\n",
    "for idx in range(len(sentences)):\n",
    "    score = calculate_sentence_score(X_tr[idx, :])\n",
    "    scores[idx] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the scores in descending order\n",
    "sort_idx = np.argsort(-scores)\n",
    "sort_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Another method for calculating the scores\n",
    "# A = pd.DataFrame(X_tr.toarray())\n",
    "# # Calculate the average scores for each sentence\n",
    "# scores = A[A != 0].mean(axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Many options for how to choose which sentences to include:\n",
    "\n",
    "# 1) top N sentences\n",
    "# 2) top N words or characters.\n",
    "# 3) top X% sentences or top X% words\n",
    "# 4) sentences with scores > average score\n",
    "# 5) sentences with scores > factor * average score\n",
    "\n",
    "# You also don't have to sort. May make more sense in order.\n",
    "\n",
    "# Title\n",
    "title = sample_data.iloc[0].split(\"\\n\", 1)[0]\n",
    "\n",
    "print(f\"Title: {title}\\nGenerated summary:\")\n",
    "for i in sort_idx[:5]:\n",
    "    print(f\"{i}: {round(scores[i], 3)} {sentences[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Title\n",
    "sample_data.iloc[0].split(\"\\n\", 1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "def load_text_data(*, filepath) -> list[str]:\n",
    "    \"\"\"This returns the data as a list of sentences.\"\"\"\n",
    "\n",
    "    with open(filepath, \"r\") as f:\n",
    "        data = [line.strip() for line in f.readlines()]\n",
    "    print(f\"Number of lines: {len(data)}\\n\")\n",
    "    return data\n",
    "\n",
    "def preprocess_data(input_data:list[str]) -> tuple[str, list[str]]:\n",
    "    \"\"\"This is used to convert the data into sentences.It returns \n",
    "    the document as a string and as a list of sentences.\"\"\"\n",
    "     # Create the document\n",
    "    data_str = \"\".join(input_data)\n",
    "\n",
    "    # Extact and tokenize the sentences\n",
    "    sents = Sentencizer()\n",
    "    sentences = sents(doc=data_str)\n",
    "    return (data_str, sentences)\n",
    "\n",
    "def calculate_tfidf(input_data:str, stopwords: list[str]) ->scipy.sparse._csr.csr_matrix:\n",
    "    \"\"\"This calculates the TFIDF of the data.\n",
    "    \n",
    "    Params:\n",
    "        input_data (str): The input text data.\n",
    "        stopwords (list[str]): List of words that do not add value to the corpus.\n",
    "\n",
    "    Returns:\n",
    "        X_transformed (list[str]): The loades stopwords.\n",
    "    \"\"\"\n",
    "\n",
    "    tfidf = TfidfVectorizer(\n",
    "    stop_words=stopwords, norm=\"l1\"\n",
    "    )\n",
    "    # Calculate TFIDF data\n",
    "    X_transformed = tfidf.fit_transform(input_data)\n",
    "\n",
    "    return X_transformed\n",
    "\n",
    "def load_stop_words(add_words:list[str]) -> list[str]:\n",
    "    \"\"\"This loads spacy stopwords.\n",
    "    \n",
    "    Params:\n",
    "        add_words (tuple[str]): Additional stopwords to add.\n",
    "\n",
    "    Returns:\n",
    "        stopwords (list[str]): The loades stopwords.\n",
    "    \"\"\"\n",
    "    # Load spaCy stopwords\n",
    "    stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "    stopwords = list(stopwords)\n",
    "    if add_words:\n",
    "        stopwords.extend(add_words)\n",
    "    return stopwords\n",
    "\n",
    "def _calculate_sentence_score(tfidf_row:np.ndarray)-> float:\n",
    "    \"\"\"This returns the average score of the non-zero tfidf value\n",
    "    for a given sentence.\"\"\"\n",
    "    x = tfidf_row[tfidf_row != 0]  # Select the non-zero values\n",
    "    return x.mean()\n",
    "\n",
    "def rank_sentences(input_data:str, stopwords: list[str],sentences: list[str], num:int=5) -> None:\n",
    "    \"\"\"This ranks and prints out the top 'num' ranked sentences.\"\"\"\n",
    "    # Calculate TFIDF\n",
    "    X_transformed = calculate_tfidf(input_data, stopwords)\n",
    "    # Initialize the score\n",
    "    scores = np.zeros(len(sentences))\n",
    "\n",
    "    # Calculate the score for each sentence\n",
    "    for idx in range(len(sentences)):\n",
    "        score = _calculate_sentence_score(X_transformed[idx, :])\n",
    "        scores[idx] = score\n",
    "    # Sort the scores in descending order \n",
    "    # and return the sorted indices\n",
    "    sort_idx = np.argsort(-scores)\n",
    "\n",
    "    top_idx = sort_idx[:num]\n",
    "    sorted_idx = [idx for idx in top_idx]\n",
    "    top_sentences = [sentences[idx] for idx in top_idx]\n",
    "    result = tuple(itertools.zip_longest(sorted_idx, top_sentences))\n",
    "\n",
    "    result = sorted(result, key=lambda x: x[0])\n",
    "    print(f\"The summary of the document showing {num} sentences\")\n",
    "    for idx, sent in result:\n",
    "        print(f\"idx:{idx}; {sent}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_p310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5c160606400bd63443fe4361c23f8347e54b6f9986e7c6d27e878f1970943f47"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
