{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF (from scratch) And Word Embeddings\n",
    "\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import spacy\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "\n",
    "# Custom imports\n",
    "\n",
    "# Built-in library\n",
    "import itertools\n",
    "import re\n",
    "import json\n",
    "from typing import Union, Optional, Any\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "# pandas settings\n",
    "pd.options.display.max_rows = 1_000\n",
    "pd.options.display.max_columns = 1_000\n",
    "pd.options.display.max_colwidth = 600\n",
    "\n",
    "# Black code formatter (Optional)\n",
    "%load_ext lab_black\n",
    "# auto reload imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure warnings and pther settings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sns.set()\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "def load_data(*, filename: str) -> pd.DataFrame:\n",
    "    \"\"\"This is used to load the data.\n",
    "\n",
    "    Params;\n",
    "        filename (str): The filepath.\n",
    "\n",
    "    Returns:\n",
    "        df (pd.DataFrame): The loaded dataframe.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(filename)\n",
    "    print(f\"Shape of df: {df.shape}\\n\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of df: (2225, 2)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ad sales boost Time Warner profit\\n\\nQuarterly profits at US media giant TimeWarner jumped 76% to $1.13bn (£600m) for the three months to December, from $639m year-earlier.\\n\\nThe firm, which is now one of the biggest investors in Google, benefited from sales of high-speed internet connections and higher advert sales. TimeWarner said fourth quarter sales rose 2% to $11.1bn from $10.9bn. Its profits were buoyed by one-off gains which offset a profit dip at Warner Bros, and less users for AOL.\\n\\nTime Warner said on Friday that it now owns 8% of search-engine Google. But its own internet bus...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dollar gains on Greenspan speech\\n\\nThe dollar has hit its highest level against the euro in almost three months after the Federal Reserve head said the US trade deficit is set to stabilise.\\n\\nAnd Alan Greenspan highlighted the US government's willingness to curb spending and rising household savings as factors which may help to reduce it. In late trading in New York, the dollar reached $1.2871 against the euro, from $1.2974 on Thursday. Market concerns about the deficit has hit the greenback in recent months. On Friday, Federal Reserve chairman Mr Greenspan's speech in London ahead of th...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      text  \\\n",
       "0  Ad sales boost Time Warner profit\\n\\nQuarterly profits at US media giant TimeWarner jumped 76% to $1.13bn (£600m) for the three months to December, from $639m year-earlier.\\n\\nThe firm, which is now one of the biggest investors in Google, benefited from sales of high-speed internet connections and higher advert sales. TimeWarner said fourth quarter sales rose 2% to $11.1bn from $10.9bn. Its profits were buoyed by one-off gains which offset a profit dip at Warner Bros, and less users for AOL.\\n\\nTime Warner said on Friday that it now owns 8% of search-engine Google. But its own internet bus...   \n",
       "1  Dollar gains on Greenspan speech\\n\\nThe dollar has hit its highest level against the euro in almost three months after the Federal Reserve head said the US trade deficit is set to stabilise.\\n\\nAnd Alan Greenspan highlighted the US government's willingness to curb spending and rising household savings as factors which may help to reduce it. In late trading in New York, the dollar reached $1.2871 against the euro, from $1.2974 on Thursday. Market concerns about the deficit has hit the greenback in recent months. On Friday, Federal Reserve chairman Mr Greenspan's speech in London ahead of th...   \n",
       "\n",
       "     labels  \n",
       "0  business  \n",
       "1  business  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = \"../../data/bbc_text_cls.csv\"\n",
    "data = load_data(filename=filename)\n",
    "\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    \"\"\"This is used to tokenize documents\"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.nlp = nlp\n",
    "\n",
    "    def __call__(self, doc: str, *args: Any, **kwargs: Any) -> list[str]:\n",
    "        # Tokenize\n",
    "        doc = nlp(doc)\n",
    "        tokenized_doc = [word.text.lower() for word in doc]\n",
    "        return tokenized_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thank you for being an awesome father</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I have an awesome God. I just wanna say thank you</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text labels\n",
       "0              Thank you for being an awesome father      a\n",
       "1  I have an awesome God. I just wanna say thank you      b"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {\n",
    "    \"text\": [\n",
    "        \"Thank you for being an awesome father\",\n",
    "        \"I have an awesome God. I just wanna say thank you\",\n",
    "    ],\n",
    "    \"labels\": [\"a\", \"b\"],\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(d)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BagOfWordsCalculator:\n",
    "    \"\"\"This tokenizes all the documents and calculates the bag of words.\n",
    "    i.e all the unique words in the document are stored and counted.\n",
    "\n",
    "    Returns:\n",
    "        tokenized_docs (list[int]): Tokenized documents i.e list of\n",
    "            tokenized documents where every row in the data is a document.\n",
    "        bag_of_words (dict[str, int]): A dict containing a unique word and the\n",
    "            unique numeric representation of the word.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.tokenizer = Tokenizer()\n",
    "\n",
    "    def __call__(\n",
    "        self, data: pd.DataFrame, *args: Any, **kwargs: Any\n",
    "    ) -> tuple[list, dict]:\n",
    "        \"\"\"This calculates the bag of words.\"\"\"\n",
    "        count = 0\n",
    "        bag_of_words = {}\n",
    "        tokenized_docs = []\n",
    "\n",
    "        for doc in data:\n",
    "            # Tokenize docs\n",
    "            tokenized_doc = self.tokenizer(doc=doc)\n",
    "            doc_as_num = []\n",
    "\n",
    "            for word in tokenized_doc:\n",
    "                # Store the unique words as numbers in the dict\n",
    "                if word not in bag_of_words:\n",
    "                    bag_of_words[word] = count\n",
    "                    count += 1\n",
    "                # Save the word as a number\n",
    "                doc_as_num.append(bag_of_words.get(word))\n",
    "            # Store the tokenized docs (converted to numbers) in a list\n",
    "            tokenized_docs.append(doc_as_num)\n",
    "        return (tokenized_docs, bag_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                Thank you for being an awesome father\n",
       "1    I have an awesome God. I just wanna say thank you\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[0, 1, 2, 3, 4, 5, 6], [7, 8, 4, 5, 9, 10, 7, 11, 12, 13, 0, 1]],\n",
       " {'thank': 0,\n",
       "  'you': 1,\n",
       "  'for': 2,\n",
       "  'being': 3,\n",
       "  'an': 4,\n",
       "  'awesome': 5,\n",
       "  'father': 6,\n",
       "  'i': 7,\n",
       "  'have': 8,\n",
       "  'god': 9,\n",
       "  '.': 10,\n",
       "  'just': 11,\n",
       "  'wanna': 12,\n",
       "  'say': 13})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_o_words_cal = BagOfWordsCalculator()\n",
    "t_docs, b_o_words = b_o_words_cal(data=df[\"text\"])\n",
    "\n",
    "# tokenized_docs, bag_of_words\n",
    "t_docs, b_o_words"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><hr>\n",
    "\n",
    "## Calculate The Term Frequency\n",
    "\n",
    "Term frequency, `tf(t,d)`, is the relative frequency of term ***`t`*** within document ***`d`***\n",
    "\n",
    "$$\n",
    "tf(t,d) = \\frac{count_{t/d}}{number_{terms/d}}\n",
    "$$\n",
    "\n",
    "where: \\\n",
    "$count_{t/d}$: Count of `t` in `d` \\\n",
    "$number_{terms/d}$: Number of `terms` in `d`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_o_words_cal = BagOfWordsCalculator()\n",
    "t_docs, b_o_words = b_o_words_cal(data=df[\"text\"])\n",
    "\n",
    "# Instantiate: number of docs and number of words\n",
    "N, V = df.shape[0], len(b_o_words)\n",
    "\n",
    "# Term Frequency\n",
    "tf = np.zeros((N, V))\n",
    "tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 2, 3, 4, 5, 6], [7, 8, 4, 5, 9, 10, 7, 11, 12, 13, 0, 1]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 1., 0., 0., 1., 1., 0., 2., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for each word in the doc and increment the count\n",
    "# of the word wherever it occurs.\n",
    "\n",
    "# Note::\n",
    "# document: a list of words/terms,\n",
    "# doc_idx: index of the current document,\n",
    "# doc_as_num: the current tokenized document,\n",
    "# words_idx: the words represented as numbers,\n",
    "\n",
    "for doc_idx, doc_as_num in enumerate(t_docs):\n",
    "    for words_idx in doc_as_num:\n",
    "        tf[doc_idx, words_idx] += 1\n",
    "\n",
    "tf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting It All Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCountVectorizer:\n",
    "    \"\"\"This is used to count the terms in a given document.\"\"\"\n",
    "\n",
    "    def __init__(self, data: pd.Series) -> None:\n",
    "        self.data = data\n",
    "        self.vocabulary = {}\n",
    "        self.tokenized_docs = []\n",
    "        self.tokenizer = Tokenizer()\n",
    "\n",
    "    def tokenize_docs(self) -> tuple[list, dict]:\n",
    "        \"\"\"This tokenizes the documents.\"\"\"\n",
    "        count = 0\n",
    "\n",
    "        for doc in self.data:\n",
    "            # Tokenize docs\n",
    "            tokenized_doc = self.tokenizer(doc=doc)\n",
    "            doc_as_num = []\n",
    "\n",
    "            for word in tokenized_doc:\n",
    "                # Store the unique words as numbers in the dict\n",
    "                if word not in self.vocabulary:\n",
    "                    self.vocabulary[word] = count\n",
    "                    count += 1\n",
    "                # Save the word as a number\n",
    "                doc_as_num.append(self.vocabulary.get(word))\n",
    "            # Store the tokenized docs (converted to numbers) in a list\n",
    "            self.tokenized_docs.append(doc_as_num)\n",
    "        return (self.tokenized_docs, self.vocabulary)\n",
    "\n",
    "    def calculate_term_frequency(self, *args: Any, **kwargs: Any) -> np.ndarray:\n",
    "        \"\"\"Calculate term frequency/bag of words.\"\"\"\n",
    "        self.tokenized_docs, self.vocabulary = self.tokenize_docs()\n",
    "        # Number of docs and number of words\n",
    "        N, W = self.data.shape[0], len(self.vocabulary)\n",
    "        tf = np.zeros((N, W))  # Instantiate tf\n",
    "\n",
    "        # Check each word in the doc and increment the count\n",
    "        # of the word wherever it occurs\n",
    "        for doc_idx, doc_as_num in enumerate(self.tokenized_docs):\n",
    "            for words_idx in doc_as_num:\n",
    "                tf[doc_idx, words_idx] += 1\n",
    "        return tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thank you for being an awesome father</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I have an awesome God. I just wanna say thank you</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text labels\n",
       "0              Thank you for being an awesome father      a\n",
       "1  I have an awesome God. I just wanna say thank you      b"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 1., 0., 0., 1., 1., 0., 2., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer = CustomCountVectorizer(data=df[\"text\"])\n",
    "tf = count_vectorizer.calculate_term_frequency()\n",
    "tf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><hr>\n",
    "\n",
    "## Calculate TF-IDF\n",
    "\n",
    "## Document Frequency\n",
    "\n",
    "`Document Frequency` is the number of documents, `d` in which the term, `t` is present.\n",
    "\n",
    "```code\n",
    "df = occurences of `t` in `d`\n",
    "```\n",
    "\n",
    "\n",
    "## Inverse Document Frequency (IDF)\n",
    "\n",
    "> `IDF` is a measure of whether a term is `common` or `rare` in a given document corpus. It is obtained by **dividing** the **total number of documents** by the **number of documents containing the term** in the corpus.\n",
    "\n",
    "Let $\\mathbf{d_{f}(t)}$ be the number of documents term `t` appears in. There are a few other issues with the `IDF`; for example, if the corpus, $N$,is large, say 100,000,000, the IDF value explodes; \n",
    "\n",
    "$$\n",
    "\\mathbf{idf}(t,d) = \\frac{100,000,000}{d_{f}(t)} \\approx{0}\n",
    "$$\n",
    "\n",
    "To avoid this effect, we take the `log` of `idf`.\n",
    "\n",
    "$$\n",
    "\\mathbf{idf}(t,d) = \\log(\\frac{N}{d_{f}(t)})\n",
    "$$\n",
    "\n",
    "\n",
    "When a word that is not in the vocab occurs during the query, the ***`document frequency`*** is 0. Because we can't divide by zero, we smooth the value by adding 1 to the denominator.\n",
    "\n",
    "$$\n",
    "\\mathbf{idf}(t,d) = \\log\\frac{N}{{d_{f}(t)}+ 1}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\mathbf{tfidf}(t,d) =tf(t,d) \\times idf(t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute IDF\n",
    "# DF: Document Frequency is the num of documents the term occurs in.\n",
    "# IDF: Number of documents(N) divided by DF. The log is taken to\n",
    "# reduce the impact of extremely large documents.\n",
    "# Therefore, IDF = log(N / DF)\n",
    "N = df.shape[0]\n",
    "document_frequency = (tf > 0).sum(axis=0)  # shape (V,)\n",
    "document_frequency.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.69314718, 0.69314718, 0.        ,\n",
       "       0.        , 0.69314718, 0.69314718, 0.69314718, 0.69314718,\n",
       "       0.69314718, 0.69314718, 0.69314718, 0.69314718])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf = np.log(N / document_frequency)\n",
    "\n",
    "idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                Thank you for being an awesome father\n",
       "1    I have an awesome God. I just wanna say thank you\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.69314718, 0.69314718, 0.        ,\n",
       "        0.        , 0.69314718, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 1.38629436, 0.69314718, 0.69314718,\n",
       "        0.69314718, 0.69314718, 0.69314718, 0.69314718]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf = tf * idf\n",
    "tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting It Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTFIDF:\n",
    "    \"\"\"This is used to calculate the term frequency\n",
    "    inverse document frequency of a given corpus.\"\"\"\n",
    "\n",
    "    def __init__(self, data: pd.Series) -> None:\n",
    "        self.data = data\n",
    "        self.vocabulary = {}\n",
    "        self.tokenized_docs = []\n",
    "        self.tokenizer = Tokenizer()\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        \"This returns the string representation of the class.\"\n",
    "        return (\n",
    "            f\"{__class__.__name__}(num_vocab: {len(self.vocabulary)}, \"\n",
    "            f\"num_doc: {len(self.tokenized_docs)})\"\n",
    "        )\n",
    "\n",
    "    def tokenize_docs(self) -> tuple[list, dict]:\n",
    "        \"\"\"This is used to tokenize the documents.\"\"\"\n",
    "        count = 0\n",
    "\n",
    "        for doc in self.data:\n",
    "            # Tokenize docs\n",
    "            tokenized_doc = self.tokenizer(doc=doc)\n",
    "            doc_as_num = []\n",
    "\n",
    "            for word in tokenized_doc:\n",
    "                # Store the unique words as numbers in the dict\n",
    "                if word not in self.vocabulary:\n",
    "                    self.vocabulary[word] = count\n",
    "                    count += 1\n",
    "                # Save the word as a number\n",
    "                doc_as_num.append(self.vocabulary.get(word))\n",
    "            # Store the tokenized docs (converted to numbers) in a list\n",
    "            self.tokenized_docs.append(doc_as_num)\n",
    "        return (self.tokenized_docs, self.vocabulary)\n",
    "\n",
    "    def convert_numbers_to_words(self) -> dict:\n",
    "        \"\"\"This is used to map numbers to words.\"\"\"\n",
    "        _, self.vocabulary = self.tokenize_docs()\n",
    "        nums_to_words = {num: word for word, num in self.vocabulary.items()}\n",
    "        return nums_to_words\n",
    "\n",
    "    def calculate_term_frequency(self, *args: Any, **kwargs: Any) -> np.ndarray:\n",
    "        \"\"\"Calculate the term frequency or bag of words.\"\"\"\n",
    "        self.tokenized_docs, self.vocabulary = self.tokenize_docs()\n",
    "        # Number of docs and number of words\n",
    "        N, W = self.data.shape[0], len(self.vocabulary)\n",
    "        tf = np.zeros((N, W))  # Instantiate tf\n",
    "\n",
    "        # Check for each word in the doc and increment the count\n",
    "        # of the word wherever it occurs.\n",
    "\n",
    "        # Note::\n",
    "        # document: a list of words/terms,\n",
    "        # doc_idx: index of the current document,\n",
    "        # doc_as_num: the current tokenized document,\n",
    "        # words_idx: the words represented as numbers,\n",
    "        for doc_idx, doc_as_num in enumerate(self.tokenized_docs):\n",
    "            for words_idx in doc_as_num:\n",
    "                tf[doc_idx, words_idx] += 1\n",
    "        return tf\n",
    "\n",
    "    def calculate_term_freq_inv_doc_freq(self) -> np.ndarray:\n",
    "        \"\"\"This returns the term frequency inverse document frequency\n",
    "        of a given corpus.\"\"\"\n",
    "        # DF: Document Frequency is the num of documents the term occurs in.\n",
    "        # IDF: Number of documents(N) divided by DF. The log is taken to\n",
    "        # reduce the impact of extremely large documents.\n",
    "        # Therefore, IDF = log(N / DF)\n",
    "        N = self.data.shape[0]\n",
    "        tf = self.calculate_term_frequency()\n",
    "        document_frequency = (tf > 0).sum(axis=0)  # shape (W,)\n",
    "        inverse_doc_freq = np.log(N / document_frequency)\n",
    "        tf_idf = tf * inverse_doc_freq\n",
    "        return tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomTFIDF(num_vocab: 14, num_doc: 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.69314718, 0.69314718, 0.        ,\n",
       "        0.        , 0.69314718, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 1.38629436, 0.69314718, 0.69314718,\n",
       "        0.69314718, 0.69314718, 0.69314718, 0.69314718]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vec = CustomTFIDF(data=df[\"text\"])\n",
    "tfidf = tfidf_vec.calculate_term_freq_inv_doc_freq()\n",
    "print(tfidf_vec)\n",
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'thank',\n",
       " 1: 'you',\n",
       " 2: 'for',\n",
       " 3: 'being',\n",
       " 4: 'an',\n",
       " 5: 'awesome',\n",
       " 6: 'father',\n",
       " 7: 'i',\n",
       " 8: 'have',\n",
       " 9: 'god',\n",
       " 10: '.',\n",
       " 11: 'just',\n",
       " 12: 'wanna',\n",
       " 13: 'say'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_2_word = tfidf_vec.convert_numbers_to_words()\n",
    "idx_2_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thank you for being an awesome father</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I have an awesome God. I just wanna say thank you</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text labels\n",
       "0              Thank you for being an awesome father      a\n",
       "1  I have an awesome God. I just wanna say thank you      b"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: b\n",
      "Text: I have an awesome God. I just wanna say thank you\n",
      "Top 5 important terms:\n",
      "word :i, score: 1.386\n",
      "word :have, score: 0.693\n",
      "word :god, score: 0.693\n",
      "word :., score: 0.693\n",
      "word :just, score: 0.693\n"
     ]
    }
   ],
   "source": [
    "idx_2_word = tfidf_vec.convert_numbers_to_words()\n",
    "\n",
    "# pick a random document, show the top 5 terms (in terms of tf_idf score)\n",
    "i = np.random.choice(N)\n",
    "# i = 0\n",
    "row = df.iloc[i]\n",
    "print(\"Label:\", row[\"labels\"])\n",
    "print(\"Text:\", row[\"text\"])\n",
    "print(\"Top 5 important terms:\")\n",
    "\n",
    "# Select the tfidf (scores) of a given document\n",
    "# and sort the scores in descending order\n",
    "scores = tfidf[i]\n",
    "indices = (-scores).argsort()\n",
    "\n",
    "for idx in indices[:5]:\n",
    "    print(f\"word :{idx_2_word[idx]}, score: {round(scores[idx], 3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ad sales boost Time Warner profit\\n\\nQuarterly profits at US media giant TimeWarner jumped 76% to $1.13bn (£600m) for the three months to December, from $639m year-earlier.\\n\\nThe firm, which is now one of the biggest investors in Google, benefited from sales of high-speed internet connections and higher advert sales. TimeWarner said fourth quarter sales rose 2% to $11.1bn from $10.9bn. Its profits were buoyed by one-off gains which offset a profit dip at Warner Bros, and less users for AOL.\\n\\nTime Warner said on Friday that it now owns 8% of search-engine Google. But its own internet bus...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dollar gains on Greenspan speech\\n\\nThe dollar has hit its highest level against the euro in almost three months after the Federal Reserve head said the US trade deficit is set to stabilise.\\n\\nAnd Alan Greenspan highlighted the US government's willingness to curb spending and rising household savings as factors which may help to reduce it. In late trading in New York, the dollar reached $1.2871 against the euro, from $1.2974 on Thursday. Market concerns about the deficit has hit the greenback in recent months. On Friday, Federal Reserve chairman Mr Greenspan's speech in London ahead of th...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yukos unit buyer faces loan claim\\n\\nThe owners of embattled Russian oil giant Yukos are to ask the buyer of its former production unit to pay back a $900m (£479m) loan.\\n\\nState-owned Rosneft bought the Yugansk unit for $9.3bn in a sale forced by Russia to part settle a $27.5bn tax claim against Yukos. Yukos' owner Menatep Group says it will ask Rosneft to repay a loan that Yugansk had secured on its assets. Rosneft already faces a similar $540m repayment demand from foreign banks. Legal experts said Rosneft's purchase of Yugansk would include such obligations. \"The pledged assets are wit...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      text  \\\n",
       "0  Ad sales boost Time Warner profit\\n\\nQuarterly profits at US media giant TimeWarner jumped 76% to $1.13bn (£600m) for the three months to December, from $639m year-earlier.\\n\\nThe firm, which is now one of the biggest investors in Google, benefited from sales of high-speed internet connections and higher advert sales. TimeWarner said fourth quarter sales rose 2% to $11.1bn from $10.9bn. Its profits were buoyed by one-off gains which offset a profit dip at Warner Bros, and less users for AOL.\\n\\nTime Warner said on Friday that it now owns 8% of search-engine Google. But its own internet bus...   \n",
       "1  Dollar gains on Greenspan speech\\n\\nThe dollar has hit its highest level against the euro in almost three months after the Federal Reserve head said the US trade deficit is set to stabilise.\\n\\nAnd Alan Greenspan highlighted the US government's willingness to curb spending and rising household savings as factors which may help to reduce it. In late trading in New York, the dollar reached $1.2871 against the euro, from $1.2974 on Thursday. Market concerns about the deficit has hit the greenback in recent months. On Friday, Federal Reserve chairman Mr Greenspan's speech in London ahead of th...   \n",
       "2  Yukos unit buyer faces loan claim\\n\\nThe owners of embattled Russian oil giant Yukos are to ask the buyer of its former production unit to pay back a $900m (£479m) loan.\\n\\nState-owned Rosneft bought the Yugansk unit for $9.3bn in a sale forced by Russia to part settle a $27.5bn tax claim against Yukos. Yukos' owner Menatep Group says it will ask Rosneft to repay a loan that Yugansk had secured on its assets. Rosneft already faces a similar $540m repayment demand from foreign banks. Legal experts said Rosneft's purchase of Yugansk would include such obligations. \"The pledged assets are wit...   \n",
       "\n",
       "     labels  \n",
       "0  business  \n",
       "1  business  \n",
       "2  business  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomTFIDF(num_vocab: 30582, num_doc: 2225)\n"
     ]
    }
   ],
   "source": [
    "tfidf_vec = CustomTFIDF(data=data[\"text\"])\n",
    "tfidf = tfidf_vec.calculate_term_freq_inv_doc_freq()\n",
    "print(tfidf_vec)\n",
    "idx_2_word = tfidf_vec.convert_numbers_to_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 1348\n",
      "Label: sport\n",
      "Text: Jones doping probe begins\n",
      "Top 5 terms:\n",
      "word: conte, score: 23.046\n",
      "word: rogge, score: 15.415\n",
      "word: jones, score: 13.302\n",
      "word: doping, score: 13.235\n",
      "word: ioc, score: 12.196\n",
      "word: medals, score: 10.81\n",
      "word: olympic, score: 9.939\n",
      "word: victor, score: 9.749\n",
      "word: pound, score: 9.424\n",
      "word: truth, score: 8.68\n"
     ]
    }
   ],
   "source": [
    "# Pick a random document, show the top 5 terms (in terms of tf_idf score)\n",
    "N = data.shape[0]\n",
    "i = np.random.choice(N)\n",
    "row = data.iloc[i]\n",
    "\n",
    "print(f\"i: {i}\")\n",
    "print(\"Label:\", row[\"labels\"])\n",
    "print(\"Text:\", row[\"text\"].split(\"\\n\")[0])\n",
    "print(\"Top 5 terms:\")\n",
    "\n",
    "scores = tfidf[i]\n",
    "indices = (-scores).argsort()  # Sort in descending order\n",
    "\n",
    "for idx in indices[:10]:\n",
    "    print(f\"word: {idx_2_word[idx]}, score: {round(scores[idx], 3)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify Using Sci-kit Learn's TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_vec = TfidfVectorizer(\n",
    "    stop_words=\"english\", tokenizer=Tokenizer(), max_features=25_000\n",
    ")\n",
    "X = data[\"text\"]\n",
    "X_tr = tf_idf_vec.fit_transform(X)\n",
    "\n",
    "dict_ = tf_idf_vec.vocabulary_\n",
    "idx_2_word_dict = {num: word for word, num in dict_.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 1348\n",
      "Label: sport\n",
      "Text: Jones doping probe begins\n",
      "Top 5 terms:\n",
      "word: conte, score: 0.38\n",
      "word: jones, score: 0.247\n",
      "word: doping, score: 0.231\n",
      "word: rogge, score: 0.23\n",
      "word: ioc, score: 0.198\n",
      "word: olympic, score: 0.185\n",
      "word: medals, score: 0.181\n",
      "word: victor, score: 0.167\n",
      "word: pound, score: 0.162\n",
      "word: truth, score: 0.152\n"
     ]
    }
   ],
   "source": [
    "# i = 594\n",
    "row = data.iloc[i]\n",
    "\n",
    "print(f\"i: {i}\")\n",
    "print(\"Label:\", row[\"labels\"])\n",
    "print(\"Text:\", row[\"text\"].split(\"\\n\")[0])\n",
    "print(\"Top 5 terms:\")\n",
    "\n",
    "scores = X_tr[i].toarray().flatten()\n",
    "indices = (-scores).argsort()  # Sort in descending order\n",
    "\n",
    "for idx in indices[:10]:\n",
    "    print(f\"word: {idx_2_word_dict[idx]}, score: {round(scores[idx], 3)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Text Summarization\n",
    "\n",
    "### Using TFIDF\n",
    "\n",
    "1. Split the document into sentences.\n",
    "2. Score each sentence (using the average TFIDF of the non zero scores)\n",
    "3. Rank each sentence by the scores.\n",
    "4. Summary is approximately the top N ranked sentences by score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ad sales boost Time Warner profit\\n\\nQuarterly profits at US media giant TimeWarner jumped 76% to $1.13bn (£600m) for the three months to December, from $639m year-earlier.\\n\\nThe firm, which is now one of the biggest investors in Google, benefited from sales of high-speed internet connections and higher advert sales. TimeWarner said fourth quarter sales rose 2% to $11.1bn from $10.9bn. Its profits were buoyed by one-off gains which offset a profit dip at Warner Bros, and less users for AOL.\\n\\nTime Warner said on Friday that it now owns 8% of search-engine Google. But its own internet bus...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      text  \\\n",
       "0  Ad sales boost Time Warner profit\\n\\nQuarterly profits at US media giant TimeWarner jumped 76% to $1.13bn (£600m) for the three months to December, from $639m year-earlier.\\n\\nThe firm, which is now one of the biggest investors in Google, benefited from sales of high-speed internet connections and higher advert sales. TimeWarner said fourth quarter sales rose 2% to $11.1bn from $10.9bn. Its profits were buoyed by one-off gains which offset a profit dip at Warner Bros, and less users for AOL.\\n\\nTime Warner said on Friday that it now owns 8% of search-engine Google. But its own internet bus...   \n",
       "\n",
       "     labels  \n",
       "0  business  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentencizer:\n",
    "    \"\"\"This is used to convert a document into a list of sentences.\n",
    "    It returns sentences.\"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.nlp = nlp\n",
    "\n",
    "    def __call__(self, doc: str, *args: Any, **kwargs: Any) -> list[str]:\n",
    "        # Tokenize the documents and extract the sentences.\n",
    "        doc = nlp(doc)\n",
    "        sentences = list(doc.sents)\n",
    "        tokenized_sentences = [str(sentence) for sentence in sentences]\n",
    "        return tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['business', 'entertainment', 'politics', 'sport', 'tech'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"labels\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Goodrem wins top female MTV prize\\n\\nPop singer Delta Goodrem has scooped one of the top individual prizes at the first Australian MTV Music Awards.\\n\\nThe 21-year-old singer won the award for best female artist, with Australian Idol runner-up Shannon Noll taking the title of best male at the ceremony. Goodrem, known in both Britain and Australia for her role as Nina Tucker in TV soap Neighbours, also performed a duet with boyfriend Brian McFadden. Other winners included Green Day, voted best group, and the Black Eyed Peas. Goodrem, Green Day and the Black Eyed Peas took home two awards ea...\n",
       "1    Tough schedule delays Elliot show\\n\\nPreview performances of the £3m musical Billy Elliot have been delayed to give the child actors a less arduous rehearsal schedule.\\n\\nDirector Stephen Daldry made the decision to re-schedule the previews to protect the young stars. Three boys will rotate the demanding role of ballet dancer Billy, which requires them to sing, dance and act. The show's opening night on 12 May at the Victoria Palace Theatre in London remains unaffected by the changes. Preview performances will now be held on 14, 20 and 27 April. \"This is one of the most ambitious projects ...\n",
       "2    U2's desire to be number one\\n\\nU2, who have won three prestigious Grammy Awards for their hit Vertigo, are stubbornly clinging to their status as one of the biggest bands in the world.\\n\\nThe most popular groups in the history of rock all have several things in common. The music must be inspired and appeal across generations and be distinctive, if not always groundbreaking. But such success is down to more than music. They have to be compelling performers, charismatic and intelligent enough to make good decisions and keep their feet on the ground. They also have to want it. They have to w...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select document\n",
    "sample_data = (\n",
    "    data.loc[data[\"labels\"] == \"entertainment\", \"text\"]\n",
    "    .sample(n=3, random_state=123)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "sample_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPop singer Delta Goodrem has scooped one of the top individual prizes at the first Australian MTV Music Awards.\\n\\nThe 21-year-old singer won the award for best female artist, with Australian Idol runn'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split ONCE using '\\n' and exclude the title\n",
    "doc = sample_data.iloc[0].split(\"\\n\", 1)[1]\n",
    "doc[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['\\nPop singer Delta Goodrem has scooped one of the top individual prizes at the first Australian MTV Music Awards.\\n\\n',\n",
       " 'The 21-year-old singer won the award for best female artist, with Australian Idol runner-up Shannon Noll taking the title of best male at the ceremony.',\n",
       " 'Goodrem, known in both Britain and Australia for her role as Nina Tucker in TV soap Neighbours, also performed a duet with boyfriend Brian McFadden.',\n",
       " 'Other winners included Green Day, voted best group, and the Black Eyed Peas.',\n",
       " 'Goodrem, Green Day and the Black Eyed Peas took home two awards each.',\n",
       " 'As well as best female, Goodrem also took home the Pepsi Viewers Choice Award, whilst Green Day bagged the prize for best rock video for American Idiot.',\n",
       " \"The Black Eyed Peas won awards for best R 'n' B video and sexiest video, both for Hey Mama.\",\n",
       " 'Local singer and songwriter Missy Higgins took the title of breakthrough artist of the year, with Australian Idol winner Guy Sebastian taking the honours for best pop video.',\n",
       " 'The VH1 First Music Award went to Cher honouring her achievements within the music industry.',\n",
       " 'The ceremony was held at the Luna Park fairground in Sydney Harbour and was hosted by the Osbourne family.',\n",
       " 'Artists including Carmen Electra, Missy Higgins, Kelly Osbourne, Green Day, Ja Rule and Natalie Imbruglia gave live performances at the event.']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents = Sentencizer()\n",
    "sentences = sents(doc=doc)\n",
    "print(len(sentences))\n",
    "\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anyhow', 'they', 'each', 'could', 'also']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load spaCy stopwords\n",
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "spacy_stopwords = list(spacy_stopwords)\n",
    "spacy_stopwords[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 102)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vectorize the sentences\n",
    "tfidf = TfidfVectorizer(stop_words=spacy_stopwords, norm=\"l1\")\n",
    "X_tr = tfidf.fit_transform(sentences)\n",
    "X_tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sentence_score(tfidf_row):\n",
    "    \"\"\"This returns the average score of the non-zero tfidf value\n",
    "    for a given sentence.\"\"\"\n",
    "    # Select the non-zero values\n",
    "    x = tfidf_row[tfidf_row != 0]\n",
    "    return x.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the score\n",
    "scores = np.zeros(len(sentences))\n",
    "\n",
    "# Calculate the score for each sentence\n",
    "for idx in range(len(sentences)):\n",
    "    score = calculate_sentence_score(X_tr[idx, :])\n",
    "    scores[idx] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8,  4,  9,  3,  6,  0,  2,  1,  5, 10,  7])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort the scores in descending order\n",
    "sort_idx = np.argsort(-scores)\n",
    "sort_idx"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Another method for calculating the scores\n",
    "A = pd.DataFrame(X_tr.toarray())\n",
    "# Calculate the average scores for each sentence\n",
    "scores = A[A != 0].mean(axis=1).values\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>66</th>\n",
       "      <th>67</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078008</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.078008</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.103772</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.069713</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.103772</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.103772</td>\n",
       "      <td>0.088701</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.088701</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.103772</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.103772</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078008</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.061326</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.052419</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.046100</td>\n",
       "      <td>0.0461</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.074386</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.052419</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.052419</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.052419</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.061326</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.061326</td>\n",
       "      <td>0.061326</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.061326</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.061326</td>\n",
       "      <td>0.046100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.052419</td>\n",
       "      <td>0.052419</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.052419</td>\n",
       "      <td>0.052419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.068158</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.068158</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.068158</td>\n",
       "      <td>0.068158</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.068158</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.045788</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.068158</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.068158</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.068158</td>\n",
       "      <td>0.068158</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.068158</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.068158</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.068158</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.068158</td>\n",
       "      <td>0.068158</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.073914</td>\n",
       "      <td>0.091615</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.081873</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.091615</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081873</td>\n",
       "      <td>0.121874</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.121874</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.091615</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.121874</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.121874</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.113403</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.113403</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.101345</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.113403</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.101345</td>\n",
       "      <td>0.101345</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.128949</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.113403</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.113403</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0    1    2         3    4         5         6       7         8    \\\n",
       "0  0.000000  0.0  0.0  0.000000  0.0  0.000000  0.078008  0.0000  0.078008   \n",
       "1  0.061326  0.0  0.0  0.052419  0.0  0.000000  0.046100  0.0461  0.000000   \n",
       "2  0.000000  0.0  0.0  0.000000  0.0  0.068158  0.000000  0.0000  0.000000   \n",
       "3  0.000000  0.0  0.0  0.000000  0.0  0.000000  0.000000  0.0000  0.000000   \n",
       "4  0.000000  0.0  0.0  0.000000  0.0  0.000000  0.000000  0.0000  0.113403   \n",
       "\n",
       "   9         10        11        12   13        14        15   16        17   \\\n",
       "0  0.0  0.000000  0.000000  0.000000  0.0  0.000000  0.000000  0.0  0.000000   \n",
       "1  0.0  0.074386  0.000000  0.000000  0.0  0.000000  0.000000  0.0  0.052419   \n",
       "2  0.0  0.000000  0.000000  0.068158  0.0  0.068158  0.068158  0.0  0.000000   \n",
       "3  0.0  0.073914  0.091615  0.000000  0.0  0.000000  0.000000  0.0  0.000000   \n",
       "4  0.0  0.000000  0.113403  0.000000  0.0  0.000000  0.000000  0.0  0.000000   \n",
       "\n",
       "   18   19        20        21        22   23   24        25   26   27   \\\n",
       "0  0.0  0.0  0.000000  0.103772  0.000000  0.0  0.0  0.000000  0.0  0.0   \n",
       "1  0.0  0.0  0.000000  0.000000  0.000000  0.0  0.0  0.000000  0.0  0.0   \n",
       "2  0.0  0.0  0.000000  0.000000  0.068158  0.0  0.0  0.000000  0.0  0.0   \n",
       "3  0.0  0.0  0.081873  0.000000  0.000000  0.0  0.0  0.091615  0.0  0.0   \n",
       "4  0.0  0.0  0.101345  0.000000  0.000000  0.0  0.0  0.113403  0.0  0.0   \n",
       "\n",
       "        28   29        30        31        32   33   34   35   36   37   \\\n",
       "0  0.000000  0.0  0.069713  0.000000  0.000000  0.0  0.0  0.0  0.0  0.0   \n",
       "1  0.052419  0.0  0.000000  0.000000  0.000000  0.0  0.0  0.0  0.0  0.0   \n",
       "2  0.000000  0.0  0.045788  0.000000  0.000000  0.0  0.0  0.0  0.0  0.0   \n",
       "3  0.000000  0.0  0.000000  0.081873  0.121874  0.0  0.0  0.0  0.0  0.0   \n",
       "4  0.000000  0.0  0.101345  0.101345  0.000000  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "        38   39   40   41   42        43   44        45   46        47   48   \\\n",
       "0  0.000000  0.0  0.0  0.0  0.0  0.000000  0.0  0.000000  0.0  0.103772  0.0   \n",
       "1  0.000000  0.0  0.0  0.0  0.0  0.052419  0.0  0.000000  0.0  0.000000  0.0   \n",
       "2  0.000000  0.0  0.0  0.0  0.0  0.000000  0.0  0.000000  0.0  0.000000  0.0   \n",
       "3  0.000000  0.0  0.0  0.0  0.0  0.000000  0.0  0.121874  0.0  0.000000  0.0   \n",
       "4  0.128949  0.0  0.0  0.0  0.0  0.000000  0.0  0.000000  0.0  0.000000  0.0   \n",
       "\n",
       "   49   50        51   52   53   54        55   56        57   58        59   \\\n",
       "0  0.0  0.0  0.000000  0.0  0.0  0.0  0.000000  0.0  0.000000  0.0  0.103772   \n",
       "1  0.0  0.0  0.000000  0.0  0.0  0.0  0.061326  0.0  0.000000  0.0  0.000000   \n",
       "2  0.0  0.0  0.068158  0.0  0.0  0.0  0.000000  0.0  0.068158  0.0  0.000000   \n",
       "3  0.0  0.0  0.000000  0.0  0.0  0.0  0.000000  0.0  0.000000  0.0  0.000000   \n",
       "4  0.0  0.0  0.000000  0.0  0.0  0.0  0.000000  0.0  0.000000  0.0  0.000000   \n",
       "\n",
       "        60   61        62        63        64        65   66   67        68   \\\n",
       "0  0.088701  0.0  0.000000  0.000000  0.000000  0.000000  0.0  0.0  0.000000   \n",
       "1  0.000000  0.0  0.000000  0.000000  0.061326  0.061326  0.0  0.0  0.000000   \n",
       "2  0.000000  0.0  0.068158  0.068158  0.000000  0.000000  0.0  0.0  0.000000   \n",
       "3  0.000000  0.0  0.000000  0.000000  0.000000  0.000000  0.0  0.0  0.091615   \n",
       "4  0.000000  0.0  0.000000  0.000000  0.000000  0.000000  0.0  0.0  0.113403   \n",
       "\n",
       "   69   70        71        72   73        74   75        76   77        78   \\\n",
       "0  0.0  0.0  0.000000  0.088701  0.0  0.103772  0.0  0.000000  0.0  0.000000   \n",
       "1  0.0  0.0  0.000000  0.000000  0.0  0.000000  0.0  0.000000  0.0  0.061326   \n",
       "2  0.0  0.0  0.068158  0.000000  0.0  0.000000  0.0  0.068158  0.0  0.000000   \n",
       "3  0.0  0.0  0.000000  0.000000  0.0  0.000000  0.0  0.000000  0.0  0.000000   \n",
       "4  0.0  0.0  0.000000  0.000000  0.0  0.000000  0.0  0.000000  0.0  0.000000   \n",
       "\n",
       "        79   80   81        82        83        84   85   86        87   \\\n",
       "0  0.103772  0.0  0.0  0.000000  0.078008  0.000000  0.0  0.0  0.000000   \n",
       "1  0.000000  0.0  0.0  0.061326  0.046100  0.000000  0.0  0.0  0.052419   \n",
       "2  0.000000  0.0  0.0  0.000000  0.000000  0.068158  0.0  0.0  0.000000   \n",
       "3  0.000000  0.0  0.0  0.000000  0.000000  0.000000  0.0  0.0  0.000000   \n",
       "4  0.000000  0.0  0.0  0.000000  0.000000  0.000000  0.0  0.0  0.000000   \n",
       "\n",
       "        88        89        90        91   92   93   94        95   96   97   \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.0  0.0  0.0  0.000000  0.0  0.0   \n",
       "1  0.052419  0.000000  0.000000  0.000000  0.0  0.0  0.0  0.000000  0.0  0.0   \n",
       "2  0.000000  0.000000  0.068158  0.068158  0.0  0.0  0.0  0.000000  0.0  0.0   \n",
       "3  0.000000  0.000000  0.000000  0.000000  0.0  0.0  0.0  0.121874  0.0  0.0   \n",
       "4  0.000000  0.113403  0.000000  0.000000  0.0  0.0  0.0  0.000000  0.0  0.0   \n",
       "\n",
       "   98        99        100       101  \n",
       "0  0.0  0.000000  0.000000  0.000000  \n",
       "1  0.0  0.000000  0.052419  0.052419  \n",
       "2  0.0  0.000000  0.000000  0.000000  \n",
       "3  0.0  0.121874  0.000000  0.000000  \n",
       "4  0.0  0.000000  0.000000  0.000000  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Another method for calculating the scores\n",
    "tfidf_arr = pd.DataFrame(X_tr.toarray())\n",
    "tfidf_arr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['21',\n",
       " 'achievements',\n",
       " 'american',\n",
       " 'artist',\n",
       " 'artists',\n",
       " 'australia',\n",
       " 'australian',\n",
       " 'award',\n",
       " 'awards',\n",
       " 'bagged']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort the vocab using the index in descending order.\n",
    "vocab = tfidf.vocabulary_\n",
    "vocab = sorted(vocab.items(), key=lambda x: x[1], reverse=False)\n",
    "vocab = [key for key, value in vocab]\n",
    "vocab[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>21</th>\n",
       "      <th>achievements</th>\n",
       "      <th>american</th>\n",
       "      <th>artist</th>\n",
       "      <th>artists</th>\n",
       "      <th>australia</th>\n",
       "      <th>australian</th>\n",
       "      <th>award</th>\n",
       "      <th>awards</th>\n",
       "      <th>bagged</th>\n",
       "      <th>best</th>\n",
       "      <th>black</th>\n",
       "      <th>boyfriend</th>\n",
       "      <th>breakthrough</th>\n",
       "      <th>brian</th>\n",
       "      <th>britain</th>\n",
       "      <th>carmen</th>\n",
       "      <th>ceremony</th>\n",
       "      <th>cher</th>\n",
       "      <th>choice</th>\n",
       "      <th>day</th>\n",
       "      <th>delta</th>\n",
       "      <th>duet</th>\n",
       "      <th>electra</th>\n",
       "      <th>event</th>\n",
       "      <th>eyed</th>\n",
       "      <th>fairground</th>\n",
       "      <th>family</th>\n",
       "      <th>female</th>\n",
       "      <th>gave</th>\n",
       "      <th>goodrem</th>\n",
       "      <th>green</th>\n",
       "      <th>group</th>\n",
       "      <th>guy</th>\n",
       "      <th>harbour</th>\n",
       "      <th>held</th>\n",
       "      <th>hey</th>\n",
       "      <th>higgins</th>\n",
       "      <th>home</th>\n",
       "      <th>honouring</th>\n",
       "      <th>honours</th>\n",
       "      <th>hosted</th>\n",
       "      <th>idiot</th>\n",
       "      <th>idol</th>\n",
       "      <th>imbruglia</th>\n",
       "      <th>included</th>\n",
       "      <th>including</th>\n",
       "      <th>individual</th>\n",
       "      <th>industry</th>\n",
       "      <th>ja</th>\n",
       "      <th>kelly</th>\n",
       "      <th>known</th>\n",
       "      <th>live</th>\n",
       "      <th>local</th>\n",
       "      <th>luna</th>\n",
       "      <th>male</th>\n",
       "      <th>mama</th>\n",
       "      <th>mcfadden</th>\n",
       "      <th>missy</th>\n",
       "      <th>mtv</th>\n",
       "      <th>music</th>\n",
       "      <th>natalie</th>\n",
       "      <th>neighbours</th>\n",
       "      <th>nina</th>\n",
       "      <th>noll</th>\n",
       "      <th>old</th>\n",
       "      <th>osbourne</th>\n",
       "      <th>park</th>\n",
       "      <th>peas</th>\n",
       "      <th>pepsi</th>\n",
       "      <th>performances</th>\n",
       "      <th>performed</th>\n",
       "      <th>pop</th>\n",
       "      <th>prize</th>\n",
       "      <th>prizes</th>\n",
       "      <th>rock</th>\n",
       "      <th>role</th>\n",
       "      <th>rule</th>\n",
       "      <th>runner</th>\n",
       "      <th>scooped</th>\n",
       "      <th>sebastian</th>\n",
       "      <th>sexiest</th>\n",
       "      <th>shannon</th>\n",
       "      <th>singer</th>\n",
       "      <th>soap</th>\n",
       "      <th>songwriter</th>\n",
       "      <th>sydney</th>\n",
       "      <th>taking</th>\n",
       "      <th>title</th>\n",
       "      <th>took</th>\n",
       "      <th>tucker</th>\n",
       "      <th>tv</th>\n",
       "      <th>vh1</th>\n",
       "      <th>video</th>\n",
       "      <th>viewers</th>\n",
       "      <th>voted</th>\n",
       "      <th>went</th>\n",
       "      <th>whilst</th>\n",
       "      <th>winner</th>\n",
       "      <th>winners</th>\n",
       "      <th>won</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078008</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.078008</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.103772</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.069713</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.103772</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.103772</td>\n",
       "      <td>0.088701</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.088701</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.103772</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.103772</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078008</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.061326</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.052419</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.046100</td>\n",
       "      <td>0.0461</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.074386</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.052419</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.052419</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.052419</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.061326</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.061326</td>\n",
       "      <td>0.061326</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.061326</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.061326</td>\n",
       "      <td>0.046100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.052419</td>\n",
       "      <td>0.052419</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.052419</td>\n",
       "      <td>0.052419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.068158</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.068158</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.068158</td>\n",
       "      <td>0.068158</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.068158</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.045788</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.068158</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.068158</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.068158</td>\n",
       "      <td>0.068158</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.068158</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.068158</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.068158</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.068158</td>\n",
       "      <td>0.068158</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.073914</td>\n",
       "      <td>0.091615</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.081873</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.091615</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081873</td>\n",
       "      <td>0.121874</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.121874</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.091615</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.121874</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.121874</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.113403</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.113403</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.101345</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.113403</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.101345</td>\n",
       "      <td>0.101345</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.128949</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.113403</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.113403</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         21  achievements  american    artist  artists  australia  australian  \\\n",
       "0  0.000000           0.0       0.0  0.000000      0.0   0.000000    0.078008   \n",
       "1  0.061326           0.0       0.0  0.052419      0.0   0.000000    0.046100   \n",
       "2  0.000000           0.0       0.0  0.000000      0.0   0.068158    0.000000   \n",
       "3  0.000000           0.0       0.0  0.000000      0.0   0.000000    0.000000   \n",
       "4  0.000000           0.0       0.0  0.000000      0.0   0.000000    0.000000   \n",
       "\n",
       "    award    awards  bagged      best     black  boyfriend  breakthrough  \\\n",
       "0  0.0000  0.078008     0.0  0.000000  0.000000   0.000000           0.0   \n",
       "1  0.0461  0.000000     0.0  0.074386  0.000000   0.000000           0.0   \n",
       "2  0.0000  0.000000     0.0  0.000000  0.000000   0.068158           0.0   \n",
       "3  0.0000  0.000000     0.0  0.073914  0.091615   0.000000           0.0   \n",
       "4  0.0000  0.113403     0.0  0.000000  0.113403   0.000000           0.0   \n",
       "\n",
       "      brian   britain  carmen  ceremony  cher  choice       day     delta  \\\n",
       "0  0.000000  0.000000     0.0  0.000000   0.0     0.0  0.000000  0.103772   \n",
       "1  0.000000  0.000000     0.0  0.052419   0.0     0.0  0.000000  0.000000   \n",
       "2  0.068158  0.068158     0.0  0.000000   0.0     0.0  0.000000  0.000000   \n",
       "3  0.000000  0.000000     0.0  0.000000   0.0     0.0  0.081873  0.000000   \n",
       "4  0.000000  0.000000     0.0  0.000000   0.0     0.0  0.101345  0.000000   \n",
       "\n",
       "       duet  electra  event      eyed  fairground  family    female  gave  \\\n",
       "0  0.000000      0.0    0.0  0.000000         0.0     0.0  0.000000   0.0   \n",
       "1  0.000000      0.0    0.0  0.000000         0.0     0.0  0.052419   0.0   \n",
       "2  0.068158      0.0    0.0  0.000000         0.0     0.0  0.000000   0.0   \n",
       "3  0.000000      0.0    0.0  0.091615         0.0     0.0  0.000000   0.0   \n",
       "4  0.000000      0.0    0.0  0.113403         0.0     0.0  0.000000   0.0   \n",
       "\n",
       "    goodrem     green     group  guy  harbour  held  hey  higgins      home  \\\n",
       "0  0.069713  0.000000  0.000000  0.0      0.0   0.0  0.0      0.0  0.000000   \n",
       "1  0.000000  0.000000  0.000000  0.0      0.0   0.0  0.0      0.0  0.000000   \n",
       "2  0.045788  0.000000  0.000000  0.0      0.0   0.0  0.0      0.0  0.000000   \n",
       "3  0.000000  0.081873  0.121874  0.0      0.0   0.0  0.0      0.0  0.000000   \n",
       "4  0.101345  0.101345  0.000000  0.0      0.0   0.0  0.0      0.0  0.128949   \n",
       "\n",
       "   honouring  honours  hosted  idiot      idol  imbruglia  included  \\\n",
       "0        0.0      0.0     0.0    0.0  0.000000        0.0  0.000000   \n",
       "1        0.0      0.0     0.0    0.0  0.052419        0.0  0.000000   \n",
       "2        0.0      0.0     0.0    0.0  0.000000        0.0  0.000000   \n",
       "3        0.0      0.0     0.0    0.0  0.000000        0.0  0.121874   \n",
       "4        0.0      0.0     0.0    0.0  0.000000        0.0  0.000000   \n",
       "\n",
       "   including  individual  industry   ja  kelly     known  live  local  luna  \\\n",
       "0        0.0    0.103772       0.0  0.0    0.0  0.000000   0.0    0.0   0.0   \n",
       "1        0.0    0.000000       0.0  0.0    0.0  0.000000   0.0    0.0   0.0   \n",
       "2        0.0    0.000000       0.0  0.0    0.0  0.068158   0.0    0.0   0.0   \n",
       "3        0.0    0.000000       0.0  0.0    0.0  0.000000   0.0    0.0   0.0   \n",
       "4        0.0    0.000000       0.0  0.0    0.0  0.000000   0.0    0.0   0.0   \n",
       "\n",
       "       male  mama  mcfadden  missy       mtv     music  natalie  neighbours  \\\n",
       "0  0.000000   0.0  0.000000    0.0  0.103772  0.088701      0.0    0.000000   \n",
       "1  0.061326   0.0  0.000000    0.0  0.000000  0.000000      0.0    0.000000   \n",
       "2  0.000000   0.0  0.068158    0.0  0.000000  0.000000      0.0    0.068158   \n",
       "3  0.000000   0.0  0.000000    0.0  0.000000  0.000000      0.0    0.000000   \n",
       "4  0.000000   0.0  0.000000    0.0  0.000000  0.000000      0.0    0.000000   \n",
       "\n",
       "       nina      noll       old  osbourne  park      peas  pepsi  \\\n",
       "0  0.000000  0.000000  0.000000       0.0   0.0  0.000000    0.0   \n",
       "1  0.000000  0.061326  0.061326       0.0   0.0  0.000000    0.0   \n",
       "2  0.068158  0.000000  0.000000       0.0   0.0  0.000000    0.0   \n",
       "3  0.000000  0.000000  0.000000       0.0   0.0  0.091615    0.0   \n",
       "4  0.000000  0.000000  0.000000       0.0   0.0  0.113403    0.0   \n",
       "\n",
       "   performances  performed       pop  prize    prizes  rock      role  rule  \\\n",
       "0           0.0   0.000000  0.088701    0.0  0.103772   0.0  0.000000   0.0   \n",
       "1           0.0   0.000000  0.000000    0.0  0.000000   0.0  0.000000   0.0   \n",
       "2           0.0   0.068158  0.000000    0.0  0.000000   0.0  0.068158   0.0   \n",
       "3           0.0   0.000000  0.000000    0.0  0.000000   0.0  0.000000   0.0   \n",
       "4           0.0   0.000000  0.000000    0.0  0.000000   0.0  0.000000   0.0   \n",
       "\n",
       "     runner   scooped  sebastian  sexiest   shannon    singer      soap  \\\n",
       "0  0.000000  0.103772        0.0      0.0  0.000000  0.078008  0.000000   \n",
       "1  0.061326  0.000000        0.0      0.0  0.061326  0.046100  0.000000   \n",
       "2  0.000000  0.000000        0.0      0.0  0.000000  0.000000  0.068158   \n",
       "3  0.000000  0.000000        0.0      0.0  0.000000  0.000000  0.000000   \n",
       "4  0.000000  0.000000        0.0      0.0  0.000000  0.000000  0.000000   \n",
       "\n",
       "   songwriter  sydney    taking     title      took    tucker        tv  vh1  \\\n",
       "0         0.0     0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.0   \n",
       "1         0.0     0.0  0.052419  0.052419  0.000000  0.000000  0.000000  0.0   \n",
       "2         0.0     0.0  0.000000  0.000000  0.000000  0.068158  0.068158  0.0   \n",
       "3         0.0     0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.0   \n",
       "4         0.0     0.0  0.000000  0.000000  0.113403  0.000000  0.000000  0.0   \n",
       "\n",
       "   video  viewers     voted  went  whilst  winner   winners       won  \\\n",
       "0    0.0      0.0  0.000000   0.0     0.0     0.0  0.000000  0.000000   \n",
       "1    0.0      0.0  0.000000   0.0     0.0     0.0  0.000000  0.052419   \n",
       "2    0.0      0.0  0.000000   0.0     0.0     0.0  0.000000  0.000000   \n",
       "3    0.0      0.0  0.121874   0.0     0.0     0.0  0.121874  0.000000   \n",
       "4    0.0      0.0  0.000000   0.0     0.0     0.0  0.000000  0.000000   \n",
       "\n",
       "       year  \n",
       "0  0.000000  \n",
       "1  0.052419  \n",
       "2  0.000000  \n",
       "3  0.000000  \n",
       "4  0.000000  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rename the columns\n",
    "tfidf_arr.columns = vocab\n",
    "tfidf_arr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.09090909, 0.05555556, 0.06666667, 0.1       , 0.11111111,\n",
       "       0.05555556, 0.1       , 0.05      , 0.125     , 0.1       ,\n",
       "       0.05555556])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the average scores for each sentence\n",
    "scores = tfidf_arr[tfidf_arr != 0].mean(axis=1).values\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8,  4,  9,  6,  3,  0,  2,  1,  5, 10,  7])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort the scores in descending order\n",
    "sort_idx = np.argsort(-scores)\n",
    "sort_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Goodrem wins top female MTV prize\n",
      "Generated summary:\n",
      "8: 0.125 The VH1 First Music Award went to Cher honouring her achievements within the music industry.\n",
      "4: 0.111 Goodrem, Green Day and the Black Eyed Peas took home two awards each.\n",
      "9: 0.1 The ceremony was held at the Luna Park fairground in Sydney Harbour and was hosted by the Osbourne family.\n",
      "6: 0.1 The Black Eyed Peas won awards for best R 'n' B video and sexiest video, both for Hey Mama.\n",
      "3: 0.1 Other winners included Green Day, voted best group, and the Black Eyed Peas.\n"
     ]
    }
   ],
   "source": [
    "# Many options for how to choose which sentences to include:\n",
    "\n",
    "# 1) top N sentences\n",
    "# 2) top N words or characters.\n",
    "# 3) top X% sentences or top X% words\n",
    "# 4) sentences with scores > average score\n",
    "# 5) sentences with scores > factor * average score\n",
    "\n",
    "# You also don't have to sort. May make more sense in order.\n",
    "\n",
    "# Title\n",
    "title = sample_data.iloc[0].split(\"\\n\", 1)[0]\n",
    "\n",
    "print(f\"Title: {title}\\nGenerated summary:\")\n",
    "for i in sort_idx[:5]:\n",
    "    print(f\"{i}: {round(scores[i], 3)} {sentences[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Goodrem wins top female MTV prize'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Title\n",
    "sample_data.iloc[0].split(\"\\n\", 1)[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting It All Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "\n",
    "def load_text_data(*, filepath) -> list[str]:\n",
    "    \"\"\"This returns the data as a list of sentences.\"\"\"\n",
    "\n",
    "    with open(filepath, \"r\") as f:\n",
    "        data = [line.strip() for line in f.readlines()]\n",
    "    print(f\"Number of lines: {len(data)}\\n\")\n",
    "    return data\n",
    "\n",
    "\n",
    "def preprocess_data(input_data: list[str]) -> tuple[str, list[str]]:\n",
    "    \"\"\"This is used to convert the data into sentences.It returns\n",
    "    the document as a string and as a list of sentences.\"\"\"\n",
    "    # Create the document\n",
    "    data_str = \"\".join(input_data)\n",
    "\n",
    "    # Extact and tokenize the sentences\n",
    "    sents = Sentencizer()\n",
    "    sentences = sents(doc=data_str)\n",
    "    return (data_str, sentences)\n",
    "\n",
    "\n",
    "def calculate_tfidf(\n",
    "    input_data: str, stopwords: list[str]\n",
    ") -> scipy.sparse._csr.csr_matrix:\n",
    "    \"\"\"This calculates the TFIDF of the data.\n",
    "\n",
    "    Params:\n",
    "        input_data (str): The input text data.\n",
    "        stopwords (list[str]): List of words that do not add value to the corpus.\n",
    "\n",
    "    Returns:\n",
    "        X_transformed (list[str]): The loades stopwords.\n",
    "    \"\"\"\n",
    "\n",
    "    tfidf = TfidfVectorizer(stop_words=stopwords, norm=\"l1\")\n",
    "    # Calculate TFIDF data\n",
    "    X_transformed = tfidf.fit_transform(input_data)\n",
    "\n",
    "    return X_transformed\n",
    "\n",
    "\n",
    "def load_stop_words(add_words: list[str]) -> list[str]:\n",
    "    \"\"\"This loads spacy stopwords.\n",
    "\n",
    "    Params:\n",
    "        add_words (tuple[str]): Additional stopwords to add.\n",
    "\n",
    "    Returns:\n",
    "        stopwords (list[str]): The loades stopwords.\n",
    "    \"\"\"\n",
    "    # Load spaCy stopwords\n",
    "    stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "    stopwords = list(stopwords)\n",
    "    if add_words:\n",
    "        stopwords.extend(add_words)\n",
    "    return stopwords\n",
    "\n",
    "\n",
    "def _calculate_sentence_score(tfidf_row: np.ndarray) -> float:\n",
    "    \"\"\"This returns the average score of the non-zero tfidf value\n",
    "    for a given sentence.\"\"\"\n",
    "    x = tfidf_row[tfidf_row != 0]  # Select the non-zero values\n",
    "    return x.mean()\n",
    "\n",
    "\n",
    "def rank_sentences(\n",
    "    input_data: str, stopwords: list[str], sentences: list[str], num: int = 5\n",
    ") -> None:\n",
    "    \"\"\"This ranks and prints out the top 'num' ranked sentences.\"\"\"\n",
    "    # Calculate TFIDF\n",
    "    X_transformed = calculate_tfidf(input_data, stopwords)\n",
    "    # Initialize the score\n",
    "    scores = np.zeros(len(sentences))\n",
    "\n",
    "    # Calculate the score for each sentence\n",
    "    for idx in range(len(sentences)):\n",
    "        score = _calculate_sentence_score(X_transformed[idx, :])\n",
    "        scores[idx] = score\n",
    "    # Sort the scores in descending order\n",
    "    # and return the sorted indices\n",
    "    sort_idx = np.argsort(-scores)\n",
    "\n",
    "    top_idx = sort_idx[:num]\n",
    "    sorted_idx = [idx for idx in top_idx]\n",
    "    top_sentences = [sentences[idx] for idx in top_idx]\n",
    "    result = tuple(itertools.zip_longest(sorted_idx, top_sentences))\n",
    "\n",
    "    result = sorted(result, key=lambda x: x[0])\n",
    "    print(f\"The summary of the document showing {num} sentences\")\n",
    "    for idx, sent in result:\n",
    "        print(f\"idx:{idx}; {sent}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_p310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5c160606400bd63443fe4361c23f8347e54b6f9986e7c6d27e878f1970943f47"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
