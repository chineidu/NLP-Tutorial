{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF (from scratch) And Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Feature engine\n",
    "from feature_engine.encoding import (\n",
    "    RareLabelEncoder,\n",
    "    OrdinalEncoder,\n",
    ")\n",
    "\n",
    "import spacy\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "\n",
    "# Custom imports\n",
    "\n",
    "# Built-in library\n",
    "import itertools\n",
    "import re\n",
    "import json\n",
    "from typing import Union, Optional, Any\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "# pandas settings\n",
    "pd.options.display.max_rows = 1_000\n",
    "pd.options.display.max_columns = 1_000\n",
    "pd.options.display.max_colwidth = 600\n",
    "\n",
    "# Black code formatter (Optional)\n",
    "%load_ext lab_black\n",
    "# auto reload imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure warnings and pther settings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sns.set()\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "def load_data(*, filename: str) -> pd.DataFrame:\n",
    "    \"\"\"This is used to load the data.\n",
    "\n",
    "    Params;\n",
    "        filename (str): The filepath.\n",
    "\n",
    "    Returns:\n",
    "        df (pd.DataFrame): The loaded dataframe.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(filename)\n",
    "    print(f\"Shape of df: {df.shape}\\n\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of df: (2225, 2)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ad sales boost Time Warner profit\\n\\nQuarterly profits at US media giant TimeWarner jumped 76% to $1.13bn (£600m) for the three months to December, from $639m year-earlier.\\n\\nThe firm, which is now one of the biggest investors in Google, benefited from sales of high-speed internet connections and higher advert sales. TimeWarner said fourth quarter sales rose 2% to $11.1bn from $10.9bn. Its profits were buoyed by one-off gains which offset a profit dip at Warner Bros, and less users for AOL.\\n\\nTime Warner said on Friday that it now owns 8% of search-engine Google. But its own internet bus...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dollar gains on Greenspan speech\\n\\nThe dollar has hit its highest level against the euro in almost three months after the Federal Reserve head said the US trade deficit is set to stabilise.\\n\\nAnd Alan Greenspan highlighted the US government's willingness to curb spending and rising household savings as factors which may help to reduce it. In late trading in New York, the dollar reached $1.2871 against the euro, from $1.2974 on Thursday. Market concerns about the deficit has hit the greenback in recent months. On Friday, Federal Reserve chairman Mr Greenspan's speech in London ahead of th...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      text  \\\n",
       "0  Ad sales boost Time Warner profit\\n\\nQuarterly profits at US media giant TimeWarner jumped 76% to $1.13bn (£600m) for the three months to December, from $639m year-earlier.\\n\\nThe firm, which is now one of the biggest investors in Google, benefited from sales of high-speed internet connections and higher advert sales. TimeWarner said fourth quarter sales rose 2% to $11.1bn from $10.9bn. Its profits were buoyed by one-off gains which offset a profit dip at Warner Bros, and less users for AOL.\\n\\nTime Warner said on Friday that it now owns 8% of search-engine Google. But its own internet bus...   \n",
       "1  Dollar gains on Greenspan speech\\n\\nThe dollar has hit its highest level against the euro in almost three months after the Federal Reserve head said the US trade deficit is set to stabilise.\\n\\nAnd Alan Greenspan highlighted the US government's willingness to curb spending and rising household savings as factors which may help to reduce it. In late trading in New York, the dollar reached $1.2871 against the euro, from $1.2974 on Thursday. Market concerns about the deficit has hit the greenback in recent months. On Friday, Federal Reserve chairman Mr Greenspan's speech in London ahead of th...   \n",
       "\n",
       "     labels  \n",
       "0  business  \n",
       "1  business  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = \"../../data/bbc_text_cls.csv\"\n",
    "data = load_data(filename=filename)\n",
    "\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenize:\n",
    "    def __init__(self) -> None:\n",
    "        self.nlp = nlp\n",
    "\n",
    "    def __call__(self, doc: str, *args: Any, **kwargs: Any) -> list[str]:\n",
    "        # Tokenize\n",
    "        doc = nlp(doc)\n",
    "        tokenized_doc = [word.text.lower() for word in doc]\n",
    "        return tokenized_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thank you for being an awesome father</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I have an awesome God. I just wanna say thank you</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text label\n",
       "0              Thank you for being an awesome father     a\n",
       "1  I have an awesome God. I just wanna say thank you     b"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {\n",
    "    \"text\": [\n",
    "        \"Thank you for being an awesome father\",\n",
    "        \"I have an awesome God. I just wanna say thank you\",\n",
    "    ],\n",
    "    \"label\": [\"a\", \"b\"],\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(d)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BagOfWordsCalculator:\n",
    "    \"\"\"This tokenizes all the documents and calculates the bag of words.\"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.tokenizer = Tokenize()\n",
    "\n",
    "    def __call__(\n",
    "        self, data: pd.DataFrame, *args: Any, **kwargs: Any\n",
    "    ) -> tuple[list, dict]:\n",
    "        \"\"\"This calculates the bag of words.\"\"\"\n",
    "        count = 0\n",
    "        bag_of_words = {}\n",
    "        tokenized_docs = []\n",
    "\n",
    "        for doc in data[\"text\"]:\n",
    "            # Tokenize docs\n",
    "            tokenized_doc = self.tokenizer(doc=doc)\n",
    "            doc_as_num = []\n",
    "\n",
    "            for word in tokenized_doc:\n",
    "                # Store the unique words as numbers in the dict\n",
    "                if word not in bag_of_words:\n",
    "                    bag_of_words[word] = count\n",
    "                    count += 1\n",
    "                # Save the word as a number\n",
    "                doc_as_num.append(bag_of_words.get(word))\n",
    "            # Store the tokenized docs (converted to numbers) in a list\n",
    "            tokenized_docs.append(doc_as_num)\n",
    "        return (tokenized_docs, bag_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bow_calculator = BagOfWordsCalculator()\n",
    "# tokenized_docs, bag_of_words = bow_calculator(data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_o_words_cal = BagOfWordsCalculator()\n",
    "t_docs, b_o_words = b_o_words_cal(data=df)\n",
    "\n",
    "# number of docs and number of words\n",
    "N, V = df.shape[0], len(b_o_words)\n",
    "tf = np.zeros((N, V))\n",
    "tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 1., 0., 0., 1., 1., 0., 2., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for each word in the doc and increment the count\n",
    "# of the word wherever it occurs\n",
    "for doc_idx, doc_as_num in enumerate(t_docs):\n",
    "    for words_idx in doc_as_num:\n",
    "        tf[doc_idx, words_idx] += 1\n",
    "\n",
    "tf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting It All Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCountVectorizer:\n",
    "    \"\"\"This is used to count the terms in a given document.\"\"\"\n",
    "\n",
    "    def __init__(self, data: pd.Series) -> None:\n",
    "        self.data = data\n",
    "        self.bag_of_words = {}\n",
    "        self.tokenized_docs = []\n",
    "        self.tokenizer = Tokenize()\n",
    "\n",
    "    def tokenize_docs_n_cal_bag_of_words(self) -> tuple[list, dict]:\n",
    "        \"\"\"This tokenizes the documents abd calculates the bag of words.\"\"\"\n",
    "        count = 0\n",
    "\n",
    "        for doc in self.data:\n",
    "            # Tokenize docs\n",
    "            tokenized_doc = self.tokenizer(doc=doc)\n",
    "            doc_as_num = []\n",
    "\n",
    "            for word in tokenized_doc:\n",
    "                # Store the unique words as numbers in the dict\n",
    "                if word not in self.bag_of_words:\n",
    "                    self.bag_of_words[word] = count\n",
    "                    count += 1\n",
    "                # Save the word as a number\n",
    "                doc_as_num.append(self.bag_of_words.get(word))\n",
    "            # Store the tokenized docs (converted to numbers) in a list\n",
    "            self.tokenized_docs.append(doc_as_num)\n",
    "        return (self.tokenized_docs, self.bag_of_words)\n",
    "\n",
    "    def calculate_term_frequency(self, *args: Any, **kwargs: Any) -> np.ndarray:\n",
    "        \"\"\"Count the terms in the document. i.e term frequency\"\"\"\n",
    "        self.tokenized_docs, self.bag_of_words = self.tokenize_docs_n_cal_bag_of_words()\n",
    "        N, V = self.data.shape[0], len(self.bag_of_words)\n",
    "        tf = np.zeros((N, V))\n",
    "\n",
    "        # Check for each word in the doc and increment the count\n",
    "        # of the word wherever it occurs\n",
    "        for doc_idx, doc_as_num in enumerate(self.tokenized_docs):\n",
    "            for words_idx in doc_as_num:\n",
    "                tf[doc_idx, words_idx] += 1\n",
    "        return tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thank you for being an awesome father</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I have an awesome God. I just wanna say thank you</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text label\n",
       "0              Thank you for being an awesome father     a\n",
       "1  I have an awesome God. I just wanna say thank you     b"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 1., 0., 0., 1., 1., 0., 2., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer = CustomCountVectorizer(data=df[\"text\"])\n",
    "tf = count_vectorizer.calculate_term_frequency()\n",
    "tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_p310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5c160606400bd63443fe4361c23f8347e54b6f9986e7c6d27e878f1970943f47"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
