{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python implementation: CPython\n",
      "Python version       : 3.11.8\n",
      "IPython version      : 8.22.2\n",
      "\n",
      "numpy    : 1.26.4\n",
      "pandas   : 2.2.1\n",
      "polars   : 0.20.18\n",
      "omegaconf: 2.3.0\n",
      "\n",
      "conda environment: torch_p11\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -v -p numpy,pandas,polars,omegaconf --conda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in library\n",
    "from pathlib import Path\n",
    "import re\n",
    "import json\n",
    "from typing import Any, Optional, Union\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "# Standard imports\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from rich.console import Console\n",
    "from rich.theme import Theme\n",
    "\n",
    "custom_theme = Theme(\n",
    "    {\n",
    "        \"info\": \"#76FF7B\",\n",
    "        \"warning\": \"#FBDDFE\",\n",
    "        \"error\": \"#FF0000\",\n",
    "    }\n",
    ")\n",
    "console = Console(theme=custom_theme)\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Pandas settings\n",
    "pd.options.display.max_rows = 1_000\n",
    "pd.options.display.max_columns = 1_000\n",
    "pd.options.display.max_colwidth = 600\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# Black code formatter (Optional)\n",
    "%load_ext lab_black\n",
    "\n",
    "# auto reload imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed: int = 123\n",
    "\n",
    "GPT_CONFIG_124M: dict[str, Any] = {\n",
    "    \"vocab_size\": 50_257,\n",
    "    \"context_length\": 1_024,\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,  # Number of attention heads\n",
    "    \"n_layers\": 12,\n",
    "    \"drop_rate\": 0.1,  # Dropout rate\n",
    "    \"qkv_bias\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, in_feats: int, out_feats: int, qkv_bias: bool = False) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Size: (seq_len, emb_dim)\n",
    "        self.query_weights = nn.Linear(in_feats, out_feats, bias=qkv_bias)\n",
    "        self.key_weights = nn.Linear(in_feats, out_feats, bias=qkv_bias)\n",
    "        self.value_weights = nn.Linear(in_feats, out_feats, bias=qkv_bias)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        # b_size, seq_len, emb_dim = x.shape\n",
    "        # (b_size, emb_dim, seq_len) @ (seq_len, emb_dim) -> (b_size, emb_dim, emb_dim)\n",
    "        query = self.query_weights(x)\n",
    "        key = self.key_weights(x)\n",
    "        value = self.value_weights(x)\n",
    "\n",
    "        # Attention scores\n",
    "        # (b_size, emb_dim, seq_len) @ (seq_len, emb_dim) -> (b_size, emb_dim, emb_dim)\n",
    "        attn_scores: Tensor = torch.matmul(query, key.transpose(-1, -2))\n",
    "        attn_weights: Tensor = F.softmax(attn_scores / key.shape[1] ** 0.5, dim=-1)\n",
    "        # (seq_len, emb_dim) @ (b_size, emb_dim, emb_dim) -> (b_size, seq_len, emb_dim)\n",
    "        context_vector: Tensor = torch.matmul(attn_weights, value)\n",
    "        return context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4171,  0.3059,  0.0703,  0.5011, -0.3046, -0.0724,  0.0533,\n",
       "           0.2701,  0.3423, -0.1482,  0.1561, -0.1429, -0.1945, -0.3114,\n",
       "           0.1051, -0.4797],\n",
       "         [-0.4133,  0.3059,  0.0693,  0.4982, -0.3075, -0.0707,  0.0502,\n",
       "           0.2698,  0.3414, -0.1518,  0.1533, -0.1413, -0.1940, -0.3147,\n",
       "           0.1037, -0.4768],\n",
       "         [-0.4148,  0.3054,  0.0728,  0.5028, -0.3058, -0.0762,  0.0506,\n",
       "           0.2726,  0.3429, -0.1534,  0.1544, -0.1431, -0.1975, -0.3136,\n",
       "           0.1112, -0.4802],\n",
       "         [-0.4140,  0.3058,  0.0721,  0.5005, -0.3061, -0.0759,  0.0493,\n",
       "           0.2709,  0.3421, -0.1531,  0.1538, -0.1436, -0.1974, -0.3152,\n",
       "           0.1090, -0.4789],\n",
       "         [-0.4137,  0.3057,  0.0704,  0.4968, -0.3051, -0.0744,  0.0506,\n",
       "           0.2679,  0.3405, -0.1492,  0.1553, -0.1450, -0.1951, -0.3144,\n",
       "           0.1035, -0.4777],\n",
       "         [-0.4123,  0.3047,  0.0694,  0.4985, -0.3070, -0.0703,  0.0537,\n",
       "           0.2725,  0.3402, -0.1508,  0.1551, -0.1417, -0.1916, -0.3101,\n",
       "           0.1034, -0.4775],\n",
       "         [-0.4131,  0.3031,  0.0703,  0.5028, -0.3108, -0.0694,  0.0531,\n",
       "           0.2794,  0.3384, -0.1574,  0.1519, -0.1390, -0.1960, -0.3103,\n",
       "           0.1074, -0.4768],\n",
       "         [-0.4122,  0.3043,  0.0697,  0.4994, -0.3076, -0.0692,  0.0532,\n",
       "           0.2742,  0.3404, -0.1531,  0.1544, -0.1404, -0.1923, -0.3100,\n",
       "           0.1053, -0.4780]],\n",
       "\n",
       "        [[-0.2827,  0.2711,  0.0436,  0.3877, -0.2152, -0.2374,  0.0118,\n",
       "           0.3257,  0.3321, -0.1471,  0.0629, -0.2310, -0.2475, -0.2195,\n",
       "           0.1321, -0.3405],\n",
       "         [-0.2833,  0.2693,  0.0502,  0.3878, -0.2168, -0.2342,  0.0107,\n",
       "           0.3233,  0.3344, -0.1493,  0.0635, -0.2311, -0.2478, -0.2252,\n",
       "           0.1343, -0.3447],\n",
       "         [-0.2840,  0.2729,  0.0507,  0.3831, -0.2125, -0.2429,  0.0072,\n",
       "           0.3220,  0.3324, -0.1457,  0.0637, -0.2392, -0.2493, -0.2261,\n",
       "           0.1328, -0.3428],\n",
       "         [-0.2828,  0.2719,  0.0462,  0.3860, -0.2167, -0.2381,  0.0101,\n",
       "           0.3256,  0.3334, -0.1499,  0.0616, -0.2305, -0.2495, -0.2242,\n",
       "           0.1349, -0.3439],\n",
       "         [-0.2790,  0.2674,  0.0485,  0.3835, -0.2140, -0.2371,  0.0129,\n",
       "           0.3210,  0.3329, -0.1458,  0.0691, -0.2305, -0.2433, -0.2179,\n",
       "           0.1315, -0.3451],\n",
       "         [-0.2852,  0.2719,  0.0487,  0.3876, -0.2149, -0.2383,  0.0087,\n",
       "           0.3240,  0.3330, -0.1477,  0.0615, -0.2363, -0.2498, -0.2260,\n",
       "           0.1335, -0.3412],\n",
       "         [-0.2822,  0.2739,  0.0404,  0.3849, -0.2131, -0.2430,  0.0113,\n",
       "           0.3262,  0.3302, -0.1445,  0.0635, -0.2324, -0.2475, -0.2175,\n",
       "           0.1304, -0.3393],\n",
       "         [-0.2807,  0.2698,  0.0510,  0.3826, -0.2167, -0.2376,  0.0097,\n",
       "           0.3224,  0.3343, -0.1503,  0.0648, -0.2306, -0.2469, -0.2252,\n",
       "           0.1351, -0.3474]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size: int = 27\n",
    "embedding_dim: int = 16\n",
    "context_size: int = 8\n",
    "batch_size: int = 2\n",
    "\n",
    "input_seq: Tensor = torch.rand(\n",
    "    size=(batch_size, context_size, embedding_dim), dtype=torch.float32\n",
    ")\n",
    "self_attn: SelfAttention = SelfAttention(embedding_dim, embedding_dim)\n",
    "context_vector: Tensor = self_attn(input_seq)\n",
    "context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        context_size: int,\n",
    "        dropout_pct: float = 0.0,\n",
    "        qkv_bias: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Size: (seq_len, emb_dim)\n",
    "        self.query_weights = nn.Linear(d_model, d_model, bias=qkv_bias)\n",
    "        self.key_weights = nn.Linear(d_model, d_model, bias=qkv_bias)\n",
    "        self.value_weights = nn.Linear(d_model, d_model, bias=qkv_bias)\n",
    "\n",
    "        self.register_buffer(\n",
    "            \"mask\", torch.triu(torch.ones(context_size, context_size), diagonal=1)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(p=dropout_pct)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        b_size, seq_len, emb_dim = x.shape\n",
    "        # (b_size, emb_dim, seq_len) @ (seq_len, emb_dim) -> (b_size, emb_dim, emb_dim)\n",
    "        query = self.query_weights(x)\n",
    "        key = self.key_weights(x)\n",
    "        value = self.value_weights(x)\n",
    "\n",
    "        # Attention scores\n",
    "        # (b_size, emb_dim, seq_len) @ (seq_len, emb_dim) -> (b_size, emb_dim, emb_dim)\n",
    "        attn_scores: Tensor = torch.matmul(query, key.transpose(-1, -2))\n",
    "        # Apply mask (inplace). The slicing ensures that the seq_len is consistent across the batch.\n",
    "        attn_scores.masked_fill_(self.mask.bool()[:seq_len, :seq_len], -torch.inf)\n",
    "\n",
    "        attn_weights: Tensor = F.softmax(attn_scores / key.shape[1] ** 0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        # (seq_len, emb_dim) @ (b_size, emb_dim, emb_dim) -> (b_size, seq_len, emb_dim)\n",
    "        context_vector: Tensor = torch.matmul(attn_weights, value)\n",
    "        return context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8, 16])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(seed)\n",
    "\n",
    "input_seq: Tensor = torch.rand(\n",
    "    size=(batch_size, context_size, embedding_dim), dtype=torch.float32\n",
    ")\n",
    "causal_self_attn: CausalSelfAttention = CausalSelfAttention(\n",
    "    d_model=embedding_dim, context_size=context_size, dropout_pct=0.1\n",
    ")\n",
    "context_vector: Tensor = causal_self_attn(input_seq)\n",
    "context_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialMultiHeadAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        context_size: int,\n",
    "        num_heads: int,\n",
    "        dropout: float = 0.0,\n",
    "        qkv_bias: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.heads = nn.ModuleList(\n",
    "            [\n",
    "                CausalSelfAttention(d_model, context_size, dropout, qkv_bias)\n",
    "                for _ in range(num_heads)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        # Concat along the feature (emb) dimension\n",
    "        return torch.cat([head(x) for head in self.heads], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_seq.shape = torch.Size([2, 8, 16])\n",
      "multi_head_attn = SequentialMultiHeadAttention(\n",
      "  (heads): ModuleList(\n",
      "    (0-2): 3 x CausalSelfAttention(\n",
      "      (query_weights): Linear(in_features=16, out_features=16, bias=False)\n",
      "      (key_weights): Linear(in_features=16, out_features=16, bias=False)\n",
      "      (value_weights): Linear(in_features=16, out_features=16, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "output.shape = torch.Size([2, 8, 48])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(seed)\n",
    "\n",
    "multi_head_attn: SequentialMultiHeadAttention = SequentialMultiHeadAttention(\n",
    "    d_model=embedding_dim,\n",
    "    context_size=context_size,\n",
    "    num_heads=3,\n",
    "    dropout=0.1,\n",
    ")\n",
    "print(f\"{input_seq.shape = }\")\n",
    "print(f\"{multi_head_attn = }\")\n",
    "output: Tensor = multi_head_attn(input_seq)\n",
    "print(f\"{output.shape = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><br><br>\n",
    "\n",
    "### Multi-head Attention\n",
    "\n",
    "- Instead of relying on a `single attention mechanism`, `multi-head attention` uses multiple \"`heads`\" that work in parallel.\n",
    "- Each `head` analyzes the input sequence from a `slightly different perspective`.\n",
    "- These individual analyses are then `combined` (concatenated) to create a `richer understanding` of the relationships between elements in the sequence.\n",
    "\n",
    "#### Here's a breakdown of the key points with clarification:\n",
    "\n",
    "- **`Causal self-attention`**: This refers to a type of attention where an element in the sequence only attends to the elements that come before it in the sequence.\n",
    "\n",
    "- **`Multiple heads in parallel`**: The core concept of `Multi-head Attention`. Instead of one attention mechanism, multiple \"heads\" analyze the data simultaneously.\n",
    "\n",
    "- **`Input sequence split and processed`**: Each head gets a portion of the original input data (`d_model`) based on the number of heads (`num_heads`). This creates a lower dimension for each head (`head_dim`) for processing.\n",
    "\n",
    "- **`Concatenation`**: After each head analyzes its portion of the data, the results are combined (concatenated) to create a richer representation that captures insights from all the heads.\n",
    "  - E.g. \n",
    "    - With a `d_model` of 64 (original input has 64 features) and 4 heads, each head gets 16 dimensions (features) to process (64 / 4). \n",
    "    - These 4 heads analyze the data in `parallel`, and then their outputs are `combined` to create a `final representation` with potentially deeper understanding than a single head could achieve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    A Multi-Head Attention layer for use in neural network architectures.\n",
    "\n",
    "    Args:\n",
    "        d_model (int): The dimension of the input and output features.\n",
    "        context_size (int): The size of the context window (neighborhood considered for attention).\n",
    "        num_heads (int): The number of heads used in the Multi-Head Attention.\n",
    "        dropout_pct (float, optional): The dropout probability for the attention weights. Defaults to 0.1.\n",
    "        qkv_bias (bool, optional): Whether to add bias terms to the linear transformations for queries, keys,\n",
    "        and values. Defaults to False.\n",
    "\n",
    "    Raises:\n",
    "        AssertionError: If `d_model` is not divisible by `num_heads`.\n",
    "\n",
    "    Shapes:\n",
    "        - Input: (batch_size, seq_len, d_model)\n",
    "        - Output: (batch_size, seq_len, d_model)\n",
    "\n",
    "    Note:\n",
    "        B, T, C: (batch, seq_len, d_model)\n",
    "\n",
    "    Example:\n",
    "        >>> import torch\n",
    "        >>> model = MultiHeadAttention(d_model=512, context_size=32, num_heads=8)\n",
    "        >>> input_tensor = torch.randn(16, 100, 512)\n",
    "        >>> output_tensor = model(input_tensor)\n",
    "        >>> print(output_tensor.shape)\n",
    "        torch.Size([16, 100, 512])\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        context_size: int,\n",
    "        num_heads: int,\n",
    "        dropout_pct: float = 0.1,\n",
    "        qkv_bias: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads  # Dim of each head\n",
    "        self.context_size = context_size\n",
    "        self.dropout = nn.Dropout(dropout_pct)\n",
    "\n",
    "        self.query_W = nn.Linear(d_model, d_model, bias=qkv_bias)\n",
    "        self.key_W = nn.Linear(d_model, d_model, bias=qkv_bias)\n",
    "        self.value_W = nn.Linear(d_model, d_model, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def _split_heads(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"Split the features at each head by reshaping and transposing them.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Tensor of shape (batch_size, num_heads, seq_length, head_dim).\n",
    "        \"\"\"\n",
    "        # B, T, C\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "\n",
    "        # After transposing: (B, n_heads, T, h_dim)\n",
    "        x_split: Tensor = x.view(\n",
    "            batch_size, seq_len, self.num_heads, self.head_dim\n",
    "        ).transpose(1, 2)\n",
    "        return x_split\n",
    "\n",
    "    def _concat_heads(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Concatenates the heads of the input tensor along the last dimension.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (B, n_heads, T, h_dim).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Concatenated tensor of shape (B, T, n_heads * h_dim).\n",
    "        \"\"\"\n",
    "        B, n_heads, T, h_dim = x.size()\n",
    "        # After transposing: (B, T, n_heads * h_dim)\n",
    "        # self.d_model = n_heads * h_dim\n",
    "        x_concat: Tensor = x.transpose(1, 2).contiguous().view(B, T, (n_heads * h_dim))\n",
    "        return x_concat\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        # Compute the query, key and value features\n",
    "        # (B, T, C) @ (C, C) -> (B, T, C)\n",
    "        queries: Tensor = self.query_W(x)  # (B, T, C)\n",
    "        keys: Tensor = self.key_W(x)  # (B, T, C)\n",
    "        values: Tensor = self.value_W(x)  # (B, T, C)\n",
    "\n",
    "        # Split the features\n",
    "        # C = n_heads * h_dim\n",
    "        # (B, T, C) -> (B, n_heads, T, h_dim)\n",
    "        queries = self._split_heads(queries)\n",
    "        keys = self._split_heads(keys)\n",
    "        values = self._split_heads(values)\n",
    "\n",
    "        # Calculate the attention\n",
    "        # (B, n_heads, T, h_dim) @ (B, n_heads, h_dim, T) -> (B, n_heads, T, T)\n",
    "        attn_scores: Tensor = queries @ keys.transpose(-1, -2)  # (B, n_heads, T, T)\n",
    "        attn_weights: Tensor = F.softmax(attn_scores / keys.shape[-1], dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)  # (B, n_heads, T, T)\n",
    "\n",
    "        # (B, n_heads, T, T) @ (B, n_heads, T, h_dim) -> (B, n_heads, T, h_dim)\n",
    "        context_vectors: Tensor = attn_weights @ values  # (B, n_heads, T, h_dim)\n",
    "        # Concatenate the attention and the features\n",
    "        context_vectors = self._concat_heads(context_vectors)  # (B, T, n_heads * h_dim)\n",
    "        # (B, T, C) @ (C, C) -> (B, T, C)\n",
    "        context_vectors = self.out_proj(context_vectors)  # (B, T, C)\n",
    "        return context_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_seq.shape = torch.Size([2, 8, 16])\n",
      "multi_head_attn = MultiHeadAttention(\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (query_W): Linear(in_features=16, out_features=16, bias=False)\n",
      "  (key_W): Linear(in_features=16, out_features=16, bias=False)\n",
      "  (value_W): Linear(in_features=16, out_features=16, bias=False)\n",
      "  (out_proj): Linear(in_features=16, out_features=16, bias=True)\n",
      ")\n",
      "output.shape = torch.Size([2, 8, 16])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3011, -0.4950, -0.2240, -0.1188,  0.0195,  0.1083,  0.2722,\n",
       "           0.2290,  0.1635, -0.0806, -0.0211, -0.2224, -0.0863,  0.0350,\n",
       "          -0.3421, -0.4037],\n",
       "         [-0.2906, -0.5310, -0.2491, -0.1227,  0.0060,  0.1080,  0.2924,\n",
       "           0.2448,  0.1576, -0.0843, -0.0560, -0.2239, -0.1180,  0.0230,\n",
       "          -0.3593, -0.4231],\n",
       "         [-0.2966, -0.4273, -0.1875, -0.1295,  0.0417,  0.1205,  0.2007,\n",
       "           0.2339,  0.1815, -0.0691,  0.0205, -0.1844, -0.0510,  0.0703,\n",
       "          -0.3123, -0.3564],\n",
       "         [-0.2888, -0.5329, -0.2493, -0.1240,  0.0080,  0.1064,  0.2928,\n",
       "           0.2439,  0.1564, -0.0811, -0.0586, -0.2241, -0.1182,  0.0230,\n",
       "          -0.3588, -0.4239],\n",
       "         [-0.3018, -0.4952, -0.2254, -0.1186,  0.0177,  0.1094,  0.2727,\n",
       "           0.2299,  0.1639, -0.0833, -0.0211, -0.2217, -0.0873,  0.0346,\n",
       "          -0.3431, -0.4041],\n",
       "         [-0.2925, -0.4761, -0.1932, -0.1313,  0.0508,  0.1081,  0.2533,\n",
       "           0.1996,  0.1579, -0.0480, -0.0030, -0.2259, -0.0573,  0.0572,\n",
       "          -0.3117, -0.3877],\n",
       "         [-0.2787, -0.3970, -0.1111, -0.1366,  0.1385,  0.0798,  0.1871,\n",
       "           0.1913,  0.1445,  0.0231,  0.0779, -0.2319,  0.0231,  0.1164,\n",
       "          -0.2457, -0.2974],\n",
       "         [-0.2775, -0.5044, -0.2250, -0.1316,  0.0248,  0.1012,  0.2444,\n",
       "           0.2338,  0.1523, -0.0395, -0.0432, -0.1997, -0.0939,  0.0108,\n",
       "          -0.3133, -0.3939]],\n",
       "\n",
       "        [[-0.2950, -0.5879, -0.2216, -0.0709,  0.0350, -0.0149,  0.4072,\n",
       "           0.2721,  0.1438, -0.0711, -0.0411, -0.2365, -0.0914, -0.0863,\n",
       "          -0.3566, -0.4619],\n",
       "         [-0.2820, -0.5580, -0.1804, -0.0771,  0.0824, -0.0176,  0.3778,\n",
       "           0.2344,  0.1431, -0.0353, -0.0180, -0.2401, -0.0643, -0.0666,\n",
       "          -0.3297, -0.4414],\n",
       "         [-0.2939, -0.5862, -0.2212, -0.0709,  0.0360, -0.0148,  0.4054,\n",
       "           0.2733,  0.1450, -0.0707, -0.0419, -0.2347, -0.0912, -0.0860,\n",
       "          -0.3568, -0.4609],\n",
       "         [-0.2830, -0.5805, -0.2273, -0.0730,  0.0316, -0.0112,  0.3824,\n",
       "           0.2796,  0.1535, -0.0692, -0.0421, -0.2117, -0.0853, -0.0881,\n",
       "          -0.3466, -0.4475],\n",
       "         [-0.2968, -0.5419, -0.1575, -0.0831,  0.0633, -0.0050,  0.3431,\n",
       "           0.2416,  0.1412, -0.0241,  0.0186, -0.2421, -0.0440, -0.0469,\n",
       "          -0.3002, -0.4204],\n",
       "         [-0.3041, -0.5601, -0.1841, -0.0795,  0.0559, -0.0070,  0.3721,\n",
       "           0.2584,  0.1416, -0.0440, -0.0102, -0.2439, -0.0676, -0.0519,\n",
       "          -0.3310, -0.4441],\n",
       "         [-0.2663, -0.5009, -0.1346, -0.0954,  0.1009,  0.0051,  0.3039,\n",
       "           0.2231,  0.1636, -0.0009,  0.0048, -0.2146, -0.0361, -0.0265,\n",
       "          -0.2894, -0.4070],\n",
       "         [-0.2860, -0.5825, -0.2224, -0.0701,  0.0376, -0.0126,  0.3846,\n",
       "           0.2758,  0.1466, -0.0577, -0.0452, -0.2220, -0.0901, -0.1040,\n",
       "          -0.3426, -0.4500]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (B, T, D) @ (D, D) -> (B, T, D)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "multi_head_attn: MultiHeadAttention = MultiHeadAttention(\n",
    "    d_model=embedding_dim,\n",
    "    context_size=context_size,\n",
    "    num_heads=2,\n",
    "    dropout_pct=0.1,\n",
    ")\n",
    "print(f\"{input_seq.shape = }\")\n",
    "print(f\"{multi_head_attn = }\")\n",
    "output: Tensor = multi_head_attn(input_seq)\n",
    "print(f\"{output.shape = }\")\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch p_311",
   "language": "python",
   "name": "torch_p11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
