{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python implementation: CPython\n",
      "Python version       : 3.11.8\n",
      "IPython version      : 8.22.2\n",
      "\n",
      "numpy    : 1.26.4\n",
      "pandas   : 2.2.1\n",
      "polars   : 0.20.18\n",
      "omegaconf: 2.3.0\n",
      "\n",
      "conda environment: torch_p11\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -v -p numpy,pandas,polars,omegaconf --conda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in library\n",
    "from pathlib import Path\n",
    "import re\n",
    "import json\n",
    "from typing import Any, Optional, Union\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "# Standard imports\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from rich.console import Console\n",
    "from rich.theme import Theme\n",
    "\n",
    "custom_theme = Theme(\n",
    "    {\n",
    "        \"info\": \"#76FF7B\",\n",
    "        \"warning\": \"#FBDDFE\",\n",
    "        \"error\": \"#FF0000\",\n",
    "    }\n",
    ")\n",
    "console = Console(theme=custom_theme)\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Pandas settings\n",
    "pd.options.display.max_rows = 1_000\n",
    "pd.options.display.max_columns = 1_000\n",
    "pd.options.display.max_colwidth = 600\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# Black code formatter (Optional)\n",
    "%load_ext lab_black\n",
    "\n",
    "# auto reload imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed: int = 123\n",
    "\n",
    "GPT_CONFIG_124M: dict[str, Any] = {\n",
    "    \"vocab_size\": 50_257,\n",
    "    \"context_length\": 1_024,\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,  # Number of attention heads\n",
    "    \"n_layers\": 12,\n",
    "    \"drop_rate\": 0.1,  # Dropout rate\n",
    "    \"qkv_bias\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, in_feats: int, out_feats: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Size: (seq_len, emb_dim)\n",
    "        self.query_weights = nn.Linear(in_feats, out_feats, bias=False)\n",
    "        self.key_weights = nn.Linear(in_feats, out_feats, bias=False)\n",
    "        self.value_weights = nn.Linear(in_feats, out_feats, bias=False)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        # b_size, seq_len, emb_dim = x.shape\n",
    "        # (b_size, emb_dim, seq_len) @ (seq_len, emb_dim) -> (b_size, emb_dim, emb_dim)\n",
    "        query = self.query_weights(x)\n",
    "        key = self.key_weights(x)\n",
    "        value = self.value_weights(x)\n",
    "\n",
    "        # Attention scores\n",
    "        # (b_size, emb_dim, seq_len) @ (seq_len, emb_dim) -> (b_size, emb_dim, emb_dim)\n",
    "        attn_scores: Tensor = torch.matmul(query, key.transpose(-1, -2))\n",
    "        attn_weights: Tensor = F.softmax(attn_scores / key.shape[1] ** 0.5, dim=-1)\n",
    "        # (seq_len, emb_dim) @ (b_size, emb_dim, emb_dim) -> (b_size, seq_len, emb_dim)\n",
    "        context_vector: Tensor = torch.matmul(attn_weights, value)\n",
    "        return context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1268,  0.2215,  0.2211,  0.0352,  0.1281],\n",
       "         [-0.1261,  0.2206,  0.2204,  0.0339,  0.1301],\n",
       "         [-0.1264,  0.2221,  0.2206,  0.0354,  0.1285],\n",
       "         [-0.1298,  0.2196,  0.2243,  0.0342,  0.1274],\n",
       "         [-0.1276,  0.2200,  0.2218,  0.0346,  0.1279],\n",
       "         [-0.1278,  0.2188,  0.2224,  0.0333,  0.1288],\n",
       "         [-0.1292,  0.2210,  0.2237,  0.0351,  0.1274],\n",
       "         [-0.1265,  0.2204,  0.2207,  0.0339,  0.1295]],\n",
       "\n",
       "        [[-0.2475,  0.2025,  0.3551, -0.0171,  0.1869],\n",
       "         [-0.2461,  0.2054,  0.3540, -0.0152,  0.1852],\n",
       "         [-0.2463,  0.2076,  0.3551, -0.0145,  0.1836],\n",
       "         [-0.2456,  0.2084,  0.3543, -0.0140,  0.1834],\n",
       "         [-0.2457,  0.2098,  0.3550, -0.0135,  0.1823],\n",
       "         [-0.2456,  0.2077,  0.3541, -0.0138,  0.1833],\n",
       "         [-0.2452,  0.2077,  0.3535, -0.0138,  0.1838],\n",
       "         [-0.2475,  0.2017,  0.3548, -0.0174,  0.1875]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size: int = 27\n",
    "embedding_dim: int = 5\n",
    "context_size: int = 8\n",
    "batch_size: int = 2\n",
    "\n",
    "input_seq: Tensor = torch.rand(\n",
    "    size=(batch_size, context_size, embedding_dim), dtype=torch.float32\n",
    ")\n",
    "self_attn: SelfAttention = SelfAttention(embedding_dim, embedding_dim)\n",
    "context_vector: Tensor = self_attn(input_seq)\n",
    "context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, in_feats: int, out_feats: int, context_size: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Size: (seq_len, emb_dim)\n",
    "        self.query_weights = nn.Linear(in_feats, out_feats, bias=False)\n",
    "        self.key_weights = nn.Linear(in_feats, out_feats, bias=False)\n",
    "        self.value_weights = nn.Linear(in_feats, out_feats, bias=False)\n",
    "\n",
    "        self.register_buffer(\n",
    "            \"mask\", torch.triu(torch.ones(context_size, context_size), diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        b_size, seq_len, emb_dim = x.shape\n",
    "        # (b_size, emb_dim, seq_len) @ (seq_len, emb_dim) -> (b_size, emb_dim, emb_dim)\n",
    "        query = self.query_weights(x)\n",
    "        key = self.key_weights(x)\n",
    "        value = self.value_weights(x)\n",
    "\n",
    "        # Attention scores\n",
    "        # (b_size, emb_dim, seq_len) @ (seq_len, emb_dim) -> (b_size, emb_dim, emb_dim)\n",
    "        attn_scores: Tensor = torch.matmul(query, key.transpose(-1, -2))\n",
    "        # Apply mask (inplace)\n",
    "        attn_scores.masked_fill_(self.mask.bool()[:seq_len, :seq_len], -torch.inf)\n",
    "\n",
    "        attn_weights: Tensor = F.softmax(attn_scores / key.shape[1] ** 0.5, dim=-1)\n",
    "        # (seq_len, emb_dim) @ (b_size, emb_dim, emb_dim) -> (b_size, seq_len, emb_dim)\n",
    "        context_vector: Tensor = torch.matmul(attn_weights, value)\n",
    "        return context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2318,  0.0440,  0.2652, -0.4445,  0.0685],\n",
       "         [-0.2561, -0.1346,  0.1961, -0.4771, -0.0082],\n",
       "         [-0.1968, -0.0901,  0.1365, -0.4585, -0.0332],\n",
       "         [-0.2134, -0.1038,  0.1803, -0.4520,  0.0084],\n",
       "         [-0.2601, -0.1058,  0.2350, -0.5068,  0.0147],\n",
       "         [-0.2830, -0.1005,  0.2751, -0.5042,  0.0258],\n",
       "         [-0.3018, -0.1143,  0.2859, -0.5648,  0.0235],\n",
       "         [-0.2846, -0.1180,  0.2660, -0.5321,  0.0213]],\n",
       "\n",
       "        [[-0.4226, -0.0772,  0.4811, -0.6062,  0.2411],\n",
       "         [-0.4354, -0.0966,  0.4816, -0.6960,  0.2383],\n",
       "         [-0.4003, -0.0931,  0.4299, -0.6829,  0.1654],\n",
       "         [-0.3426, -0.0509,  0.4048, -0.5703,  0.1671],\n",
       "         [-0.2918, -0.0457,  0.3191, -0.5597,  0.0942],\n",
       "         [-0.2636, -0.0650,  0.2737, -0.5394,  0.0660],\n",
       "         [-0.2520, -0.0853,  0.2341, -0.5581,  0.0223],\n",
       "         [-0.2528, -0.0887,  0.2248, -0.5806,  0.0233]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(seed)\n",
    "\n",
    "input_seq: Tensor = torch.rand(\n",
    "    size=(batch_size, context_size, embedding_dim), dtype=torch.float32\n",
    ")\n",
    "causal_self_attn: CausalSelfAttention = CausalSelfAttention(\n",
    "    embedding_dim, embedding_dim, context_size\n",
    ")\n",
    "context_vector: Tensor = causal_self_attn(input_seq)\n",
    "context_vector  # .shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (5) must match the size of tensor b (8) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcontext_vector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmasked_fill\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtriu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiagonal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minf\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (5) must match the size of tensor b (8) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "context_vector.masked_fill(\n",
    "    mask=torch.triu(torch.ones(5, 5), diagonal=1), value=-torch.inf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 1., 1., 1.],\n",
       "        [0., 0., 1., 1., 1.],\n",
       "        [0., 0., 0., 1., 1.],\n",
       "        [0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.triu(torch.ones(5, 5), diagonal=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(inf, inf)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.inf, np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch p_311",
   "language": "python",
   "name": "torch_p11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
