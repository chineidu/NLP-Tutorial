{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing A GPT-model From Scratch\n",
    "\n",
    "- Coding a GPT-like LLM that can be trained to generate uman-like text.\n",
    "- Normalizing layer activations to stabilize NN training.\n",
    "- Adding `shortcut connections` to train deep NNs effectively.\n",
    "- Implementing transformer blocks to create GPT models of various sizes.\n",
    "- Computing the number of parameters and storage requiremnts of GPT models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark -v -p numpy,pandas,polars,torch,lightning --conda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in library\n",
    "from pathlib import Path\n",
    "import re\n",
    "import json\n",
    "from typing import Any, Optional, Union\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "# Standard imports\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from rich.console import Console\n",
    "from rich.theme import Theme\n",
    "\n",
    "custom_theme = Theme(\n",
    "    {\n",
    "        \"info\": \"#76FF7B\",\n",
    "        \"warning\": \"#FBDDFE\",\n",
    "        \"error\": \"#FF0000\",\n",
    "    }\n",
    ")\n",
    "console = Console(theme=custom_theme)\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Pandas settings\n",
    "pd.options.display.max_rows = 1_000\n",
    "pd.options.display.max_columns = 1_000\n",
    "pd.options.display.max_colwidth = 600\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# Black code formatter (Optional)\n",
    "%load_ext lab_black\n",
    "\n",
    "# auto reload imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M: dict[str, Any] = {\n",
    "    \"vocab_size\": 50_257,\n",
    "    \"context_length\": 1_024,\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,  # Number of attention heads\n",
    "    \"n_layers\": 12,\n",
    "    \"drop_rate\": 0.1,  # Dropout rate\n",
    "    \"qkv_bias\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Placeholder GPT Backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self, cfg: dict[str, Any]) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.tok_emb = nn.Embedding(cfg.get(\"vocab_size\"), cfg.get(\"emb_dim\"))\n",
    "        self.pos_emb = nn.Embedding(cfg.get(\"context_length\"), cfg.get(\"emb_dim\"))\n",
    "        self.drop_emb = nn.Dropout(cfg.get(\"drop_rate\"))\n",
    "        self.transformer_blocks = nn.Sequential(\n",
    "            *[DummyTransformerBlock(cfg) for _ in range(cfg.get(\"n_layers\"))]\n",
    "        )\n",
    "\n",
    "        self.final_norm = DummyLayerNorm(cfg.get(\"emb_dim\"))\n",
    "        self.out_head = nn.Linear(cfg.get(\"emb_dim\"), cfg.get(\"vocab_size\"), bias=False)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        batch_size, seq_len = x.shape\n",
    "        tok_embeds: Tensor = self.tok_emb(x)\n",
    "        pos_embeds: Tensor = self.pos_emb(torch.arange(seq_len, device=x.device))\n",
    "        x = tok_embeds + pos_embeds  # Add token and positional embeddings to get input embeddings\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.transformer_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits: Tensor = self.out_head(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class DummyTransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg: dict[str, Any]) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return x\n",
    "\n",
    "\n",
    "class DummyLayerNorm(nn.Module):\n",
    "    def __init__(self, cfg: dict[str, Any]) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "<img src=\"../08-Makemore/images/ch04_mental_.gpt_picpng.png\" alt=\"gpt\" width=\"600\">\n",
    "\n",
    "[image source](https://livebook.manning.com/book/build-a-large-language-model-from-scratch/chapter-4/v-7/36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize a batch consisting of sample texts.\n",
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "text1: str = \"Every effort moves you\"\n",
    "text2: str = \"Everyday holds a\"\n",
    "batch: list[int] = []\n",
    "\n",
    "batch.append(torch.tensor(tokenizer.encode(text1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(text2)))\n",
    "batch_tensor: Tensor = torch.stack(batch, dim=0)\n",
    "\n",
    "print(f\"Shape of batch_tensor: {tuple(batch_tensor.shape)}\\n\")\n",
    "print(f\"Batch_tensor: \\n{batch_tensor}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the DummyGPTModel\n",
    "seed: int = 5\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "model = DummyGPTModel(GPT_CONFIG_124M)\n",
    "logits: Tensor = model(batch_tensor)\n",
    "\n",
    "print(f\"Shape of logits: {tuple(logits.shape)}\\n\")\n",
    "print(f\"logits: \\n{logits}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment\n",
    "\n",
    "- The output (2, 4, 50_257) has `2` rows corresponding to the number of input data.\n",
    "- It has `4` rows which is the context length of the input and `50_257` is the number of unique words (vocab_size) in the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing Activations With Layer Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_p11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
