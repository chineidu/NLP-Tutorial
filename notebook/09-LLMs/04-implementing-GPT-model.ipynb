{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing A GPT-model From Scratch\n",
    "\n",
    "- Coding a GPT-like LLM that can be trained to generate uman-like text.\n",
    "- Normalizing layer activations to stabilize NN training.\n",
    "- Adding `shortcut connections` to train deep NNs effectively.\n",
    "- Implementing transformer blocks to create GPT models of various sizes.\n",
    "- Computing the number of parameters and storage requiremnts of GPT models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark -v -p numpy,pandas,polars,torch,lightning --conda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in library\n",
    "from pathlib import Path\n",
    "import re\n",
    "import json\n",
    "from typing import Any, Optional, Union\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "# Standard imports\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from rich.console import Console\n",
    "from rich.theme import Theme\n",
    "\n",
    "custom_theme = Theme(\n",
    "    {\n",
    "        \"info\": \"#76FF7B\",\n",
    "        \"warning\": \"#FBDDFE\",\n",
    "        \"error\": \"#FF0000\",\n",
    "    }\n",
    ")\n",
    "console = Console(theme=custom_theme)\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Pandas settings\n",
    "pd.options.display.max_rows = 1_000\n",
    "pd.options.display.max_columns = 1_000\n",
    "pd.options.display.max_colwidth = 600\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# Black code formatter (Optional)\n",
    "%load_ext lab_black\n",
    "\n",
    "# auto reload imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M: dict[str, Any] = {\n",
    "    \"vocab_size\": 50_257,\n",
    "    \"context_length\": 1_024,\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,  # Number of attention heads\n",
    "    \"n_layers\": 12,\n",
    "    \"drop_rate\": 0.1,  # Dropout rate\n",
    "    \"qkv_bias\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Placeholder GPT Backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self, cfg: dict[str, Any]) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.tok_emb = nn.Embedding(cfg.get(\"vocab_size\"), cfg.get(\"emb_dim\"))\n",
    "        self.pos_emb = nn.Embedding(cfg.get(\"context_length\"), cfg.get(\"emb_dim\"))\n",
    "        self.drop_emb = nn.Dropout(cfg.get(\"drop_rate\"))\n",
    "        self.transformer_blocks = nn.Sequential(\n",
    "            *[DummyTransformerBlock(cfg) for _ in range(cfg.get(\"n_layers\"))]\n",
    "        )\n",
    "\n",
    "        self.final_norm = DummyLayerNorm(cfg.get(\"emb_dim\"))\n",
    "        self.out_head = nn.Linear(cfg.get(\"emb_dim\"), cfg.get(\"vocab_size\"), bias=False)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        batch_size, seq_len = x.shape\n",
    "        tok_embeds: Tensor = self.tok_emb(x)\n",
    "        pos_embeds: Tensor = self.pos_emb(torch.arange(seq_len, device=x.device))\n",
    "        x = (\n",
    "            tok_embeds + pos_embeds\n",
    "        )  # Add token and positional embeddings to get input embeddings\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.transformer_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits: Tensor = self.out_head(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class DummyTransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg: dict[str, Any]) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return x\n",
    "\n",
    "\n",
    "class DummyLayerNorm(nn.Module):\n",
    "    def __init__(self, cfg: dict[str, Any]) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "<img src=\"../08-Makemore/images/ch04_mental_.gpt_pic.png\" alt=\"gpt\" width=\"600\">\n",
    "\n",
    "[image source](https://livebook.manning.com/book/build-a-large-language-model-from-scratch/chapter-4/v-7/36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize a batch consisting of sample texts.\n",
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "text1: str = \"Every effort moves you\"\n",
    "text2: str = \"Everyday holds a\"\n",
    "batch: list[int] = []\n",
    "\n",
    "batch.append(torch.tensor(tokenizer.encode(text1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(text2)))\n",
    "batch_tensor: Tensor = torch.stack(batch, dim=0)\n",
    "\n",
    "print(f\"Shape of batch_tensor: {tuple(batch_tensor.shape)}\\n\")\n",
    "print(f\"Batch_tensor: \\n{batch_tensor}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the DummyGPTModel\n",
    "seed: int = 5\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "model = DummyGPTModel(GPT_CONFIG_124M)\n",
    "logits: Tensor = model(batch_tensor)\n",
    "\n",
    "print(f\"Shape of logits: {tuple(logits.shape)}\\n\")\n",
    "print(f\"logits: \\n{logits}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment\n",
    "\n",
    "- The output (2, 4, 50_257) has `2` rows corresponding to the number of input data.\n",
    "- It has `4` rows which is the context length of the input and `50_257` is the number of unique words (vocab_size) in the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing Activations With Layer Normalization\n",
    "\n",
    "- Layer  normalization is used to improve the stability and efficiency of neural networks.\n",
    "- It aims to to adjust the activations at the output of a NN layer to have a mean of 0 and a standard deviation of 1.\n",
    "- These adjustments speed up the convergence to effective weights and ensures consistent, reliable training.\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"../08-Makemore/images/ch04_norm_layer.png\" width=\"600\">\n",
    "\n",
    "[image source](https://livebook.manning.com/book/build-a-large-language-model-from-scratch/chapter-4/v-7/52)\n",
    "\n",
    "<br><hr>\n",
    "\n",
    "#### Layer Norm vs Batch Norm\n",
    "\n",
    "- Batch norm normalizes across the batch dimension, whereas layer norm normalizes across the feature/channel dimension.\n",
    "- Layer norm is preferred because it normalizes each input independently of the batch size and it's more efficient and stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "\n",
    "batch_tensor: Tensor = torch.randn(2, 5)\n",
    "layer = nn.Sequential(nn.Linear(5, 6), nn.ReLU())\n",
    "out: Tensor = layer(batch_tensor)\n",
    "print(f\"out: \\n{out}\\n\")\n",
    "\n",
    "mean: Tensor = out.mean(dim=-1, keepdim=True)\n",
    "variance: Tensor = out.var(dim=-1, unbiased=False, keepdim=True)\n",
    "print(\"===== Before Layer Normalization ===== \\n\")\n",
    "print(f\"mean: \\n{mean}\\n\\nvariance: \\n{variance}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the output\n",
    "out_norm: Tensor = (out - mean) / torch.sqrt(variance)\n",
    "mean_1: Tensor = out_norm.mean(dim=-1, keepdim=True)\n",
    "variance_1: Tensor = out_norm.var(dim=-1, unbiased=False, keepdim=True)\n",
    "print(f\"out_norm: \\n{out_norm}\\n\")\n",
    "print(\"===== After Layer Normalization ===== \\n\")\n",
    "print(f\"mean: \\n{mean_1}\\n\\nvariance: \\n{variance_1}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improve readability\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "print(\"===== After Layer Normalization ===== \\n\")\n",
    "print(f\"mean: \\n{mean_1}\\n\\nvariance: \\n{variance_1}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, embed_dim: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.eps: float = 1e-5  # Prevents zero division\n",
    "\n",
    "        # Trainable params that's automatically adjusted by the LLM during\n",
    "        # training to improve model performance\n",
    "        self.scale = nn.Parameter(torch.ones(embed_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(embed_dim))\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        std = x.std(dim=-1, unbiased=False, keepdim=True)\n",
    "        norm_x: Tensor = (x - mean) / (std + self.eps)\n",
    "\n",
    "        return (self.scale * norm_x) + self.shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_norm = LayerNorm(embed_dim=5)\n",
    "out_layer_norm: Tensor = layer_norm(batch_tensor)\n",
    "\n",
    "mean: Tensor = out_layer_norm.mean(dim=-1, keepdim=True)\n",
    "variance: Tensor = out_layer_norm.var(dim=-1, unbiased=False, keepdim=True)\n",
    "print(f\"out_layer_norm: \\n{out_layer_norm}\\n\")\n",
    "print(\"===== After Layer Normalization ===== \\n\")\n",
    "print(f\"mean: \\n{mean}\\n\\nvariance: \\n{variance}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing GELU (Gaussian Error Linear Unit)\n",
    "\n",
    "- **Smoothness**: Unlike ReLU's sharp cutoff at zero, `GELU` has a smoother gradient transition. This smoothness helps with training, as optimization algorithms can navigate the function's landscape more effectively.\n",
    "\n",
    "- **Non-zero gradients**: ReLU outputs zero for negative inputs, killing the gradient and hindering learning in those regions. `GELU` maintains non-zero gradients even for negative inputs, allowing the network to learn from a wider range of data. i.e. neurons with -ve gradients can still contribute to the learning process.\n",
    "\n",
    "#### Note\n",
    "\n",
    "- **Computational cost**: The original `GELU` formulation can be computationally expensive. Luckily, there are efficient approximations that retain the benefits without the high cost.\n",
    "\n",
    "<!-- [![image.png](https://i.postimg.cc/9XSGKNZ9/image.png)](https://postimg.cc/14GnNBWR) -->\n",
    "[![image.png](https://i.postimg.cc/pTrF1Ddk/image.png)](https://postimg.cc/wyKM1RLs)\n",
    "\n",
    "$$GELU(x) = 0.5x * ( 1 + tanh( \\sqrt{\\frac{2}{\\pi}} * (x + 0.044715x^{3}) ) ) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        gelu: Tensor = (\n",
    "            0.5\n",
    "            * x\n",
    "            * (\n",
    "                1\n",
    "                + torch.tanh(\n",
    "                    torch.sqrt(torch.tensor(2.0 / torch.pi))\n",
    "                    * (x + 0.044715 * torch.pow(x, 3))\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        return gelu\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"Applies a feed-forward neural network to the input tensor `x`. The feed-forward network\n",
    "    consists of two linear layers with a GELU activation in between. The first linear layer\n",
    "    expands the input dimension by a factor of 4, and the second linear layer projects the result\n",
    "    back to the original input dimension.\n",
    "\n",
    "    Args:\n",
    "        x (torch.Tensor): The input tensor to be passed through the feed-forward network.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The output tensor after passing through the feed-forward network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cfg: dict[str, Any]) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg.get(\"emb_dim\"), 4 * cfg.get(\"emb_dim\")),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg.get(\"emb_dim\"), cfg.get(\"emb_dim\")),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gelu, relu = GELU(), nn.ReLU()\n",
    "\n",
    "x: Tensor = torch.linspace(-3, 3, 100)\n",
    "y_gelu, y_relu = gelu(x), relu(x)\n",
    "plt.figure(figsize=(8, 3))\n",
    "\n",
    "for i, (y, label) in enumerate(zip([y_gelu, y_relu], [\"GELU\", \"ReLU\"]), 1):\n",
    "    plt.subplot(1, 2, i)\n",
    "    plt.plot(x, y)\n",
    "    plt.title(f\"{label} activation function\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(f\"{label}(x)\")\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "\n",
    "ff_network = FeedForward(cfg=GPT_CONFIG_124M)\n",
    "x: Tensor = torch.randn(2, 3, 768)\n",
    "out: Tensor = ff_network(x)\n",
    "\n",
    "print(f\"out: {out.shape}\")\n",
    "print(f\"out: \\n{out}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feed Forward Network With GELU\n",
    "\n",
    "<img src=\"../08-Makemore/images/ch04__ffn.png\" width=\"600\">\n",
    "\n",
    "[image source](https://livebook.manning.com/book/build-a-large-language-model-from-scratch/chapter-4/v-7/112)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shortcut Connections\n",
    "\n",
    "- AKA Skip/Residual connections.\n",
    "- Shortcut Connections involve adding the output of one layer to the input of the next layer.\n",
    "- It mitigates the vanishing gradient problem.\n",
    "\n",
    "<img src=\"../08-Makemore/images/ch04__shortcut.png\" width=\"600\">\n",
    "\n",
    "[image source](https://livebook.manning.com/book/build-a-large-language-model-from-scratch/chapter-4/v-7/119)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleDeepNeuralNetwork_v1(nn.Module):\n",
    "    def __init__(self, layer_sizes: list[int], use_shortcut: bool = True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.use_shortcut = use_shortcut\n",
    "        assert len(layer_sizes) == 6, \"layer_sizes must be of length 6\"\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                # 5 layers\n",
    "                nn.Sequential(nn.Linear(layer_sizes[0], layer_sizes[1]), GELU()),\n",
    "                nn.Sequential(nn.Linear(layer_sizes[1], layer_sizes[2]), GELU()),\n",
    "                nn.Sequential(nn.Linear(layer_sizes[2], layer_sizes[3]), GELU()),\n",
    "                nn.Sequential(nn.Linear(layer_sizes[3], layer_sizes[4]), GELU()),\n",
    "                nn.Sequential(nn.Linear(layer_sizes[4], layer_sizes[5]), GELU()),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        for layer in self.layers:\n",
    "            # Compute the output of the current layer\n",
    "            layer_output: Tensor = layer(x)\n",
    "            # Check if shortcut connections can be applied\n",
    "            if self.use_shortcut and x.shape == layer_output.shape:\n",
    "                # Apply the shortcut connection\n",
    "                x = x + layer_output\n",
    "            else:\n",
    "                # Otherwise, just use the layer output\n",
    "                x = layer_output\n",
    "        return x\n",
    "\n",
    "\n",
    "def print_gradients(model: nn.Module, x: Tensor) -> None:\n",
    "    \"\"\"This is used to print the gradients of the model.\"\"\"\n",
    "    # Forward pass\n",
    "    output: Tensor = model(x)\n",
    "    target: Tensor = torch.tensor([[0.0]])\n",
    "\n",
    "    # Calculate the loss\n",
    "    criterion = nn.MSELoss()\n",
    "    loss: Tensor = criterion(output, target)\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"weight\" in name:\n",
    "            print(f\"{name} has gradient mean of {param.grad.abs().mean().item():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[f\"Linear({idx}, {idx + 1})\" for idx in range(6 - 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "\n",
    "layer_sizes: list[int] = [3, 3, 3, 3, 3, 1]\n",
    "sample_input: Tensor = torch.tensor([[1.0, 0.0, -1.0]])\n",
    "\n",
    "model_wfout_shortcut = ExampleDeepNeuralNetwork_v1(\n",
    "    layer_sizes=layer_sizes, use_shortcut=False\n",
    ")\n",
    "print(f\"input: {sample_input.shape}\")\n",
    "\n",
    "model_wfout_shortcut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_gradients(model=model_wfout_shortcut, x=sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleDeepNeuralNetwork_v2(nn.Module):\n",
    "    def __init__(self, layer_sizes: list[int], use_shortcut: bool = True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.use_shortcut = use_shortcut\n",
    "        assert len(layer_sizes) == 6, \"layer_sizes must be of length 6\"\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                # 5 layers\n",
    "                nn.Sequential(nn.Linear(layer_sizes[idx], layer_sizes[idx + 1]), GELU())\n",
    "                for idx in range(len(layer_sizes) - 1)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        for layer in self.layers:\n",
    "            # Compute the output of the current layer\n",
    "            layer_output: Tensor = layer(x)\n",
    "            # Check if shortcut connections can be applied\n",
    "            if self.use_shortcut and x.shape == layer_output.shape:\n",
    "                # Apply the shortcut connection\n",
    "                x = x + layer_output\n",
    "            else:\n",
    "                # Otherwise, just use the layer output\n",
    "                x = layer_output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "\n",
    "layer_sizes: list[int] = [3, 3, 3, 3, 3, 1]\n",
    "sample_input: Tensor = torch.tensor([[1.0, 0.0, -1.0]])\n",
    "\n",
    "model_wfout_shortcut = ExampleDeepNeuralNetwork_v2(\n",
    "    layer_sizes=layer_sizes, use_shortcut=False\n",
    ")\n",
    "print(f\"input: {sample_input.shape}\")\n",
    "print(model_wfout_shortcut)\n",
    "print()\n",
    "\n",
    "# The weights shrink to a vanishingly small value\n",
    "print_gradients(model=model_wfout_shortcut, x=sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "\n",
    "\n",
    "model_wf_shortcut = ExampleDeepNeuralNetwork_v2(\n",
    "    layer_sizes=layer_sizes,\n",
    "    use_shortcut=True,  # NEW!\n",
    ")\n",
    "print(f\"input: {sample_input.shape}\")\n",
    "print(model_wf_shortcut)\n",
    "print()\n",
    "\n",
    "# The weights are slightly larger than the ones without the shortcut.\n",
    "print_gradients(model=model_wf_shortcut, x=sample_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "### Connecting Attention And Linear Layers In A Transformer Block\n",
    "\n",
    "- Implement the transformer block.\n",
    "- The transformer block in the 124 million parameter GPT-2 architecture was repeated 12 times and it combines several concepts like:\n",
    "  - multi-head attention\n",
    "  - layer norm, dropout\n",
    "  - feedforward layers\n",
    "  - and GELU activations.\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"../08-Makemore/images/ch04__trnsf_block.png\" width=\"600\">\n",
    "\n",
    "[image source](https://livebook.manning.com/book/build-a-large-language-model-from-scratch/chapter-4/v-7/145)\n",
    "\n",
    "<br>\n",
    "\n",
    "- When a transformer block processes an input sequence, each element in the sequence is represented by a fixed-size vector.\n",
    "- The operations w/in the transformer block are designed to transform the input vectors in a way that preserves the dimensions.\n",
    "- The multi-head attention block figures out how different parts of the sequence relate to each other, while the feedforward network modifies the data individually at each position.\n",
    "- This combination leads to a more nuanced understanding and processing of the input and improves the model's capacity to handle complex data patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_p11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
