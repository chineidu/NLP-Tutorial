{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing A GPT-model From Scratch\n",
    "\n",
    "- Coding a GPT-like LLM that can be trained to generate uman-like text.\n",
    "- Normalizing layer activations to stabilize NN training.\n",
    "- Adding `shortcut connections` to train deep NNs effectively.\n",
    "- Implementing transformer blocks to create GPT models of various sizes.\n",
    "- Computing the number of parameters and storage requiremnts of GPT models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark -v -p numpy,pandas,polars,torch,lightning --conda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in library\n",
    "from pathlib import Path\n",
    "import re\n",
    "import json\n",
    "from typing import Any, Optional, Union\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "# Standard imports\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from rich.console import Console\n",
    "from rich.theme import Theme\n",
    "\n",
    "custom_theme = Theme(\n",
    "    {\n",
    "        \"info\": \"#76FF7B\",\n",
    "        \"warning\": \"#FBDDFE\",\n",
    "        \"error\": \"#FF0000\",\n",
    "    }\n",
    ")\n",
    "console = Console(theme=custom_theme)\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Pandas settings\n",
    "pd.options.display.max_rows = 1_000\n",
    "pd.options.display.max_columns = 1_000\n",
    "pd.options.display.max_colwidth = 600\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# Black code formatter (Optional)\n",
    "%load_ext lab_black\n",
    "\n",
    "# auto reload imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M: dict[str, Any] = {\n",
    "    \"vocab_size\": 50_257,\n",
    "    \"context_length\": 1_024,\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,  # Number of attention heads\n",
    "    \"n_layers\": 12,\n",
    "    \"drop_rate\": 0.1,  # Dropout rate\n",
    "    \"qkv_bias\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Placeholder GPT Backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self, cfg: dict[str, Any]) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.tok_emb = nn.Embedding(cfg.get(\"vocab_size\"), cfg.get(\"emb_dim\"))\n",
    "        self.pos_emb = nn.Embedding(cfg.get(\"context_length\"), cfg.get(\"emb_dim\"))\n",
    "        self.drop_emb = nn.Dropout(cfg.get(\"drop_rate\"))\n",
    "        self.transformer_blocks = nn.Sequential(\n",
    "            *[DummyTransformerBlock(cfg) for _ in range(cfg.get(\"n_layers\"))]\n",
    "        )\n",
    "\n",
    "        self.final_norm = DummyLayerNorm(cfg.get(\"emb_dim\"))\n",
    "        self.out_head = nn.Linear(cfg.get(\"emb_dim\"), cfg.get(\"vocab_size\"), bias=False)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        batch_size, seq_len = x.shape\n",
    "        tok_embeds: Tensor = self.tok_emb(x)\n",
    "        pos_embeds: Tensor = self.pos_emb(torch.arange(seq_len, device=x.device))\n",
    "        x = (\n",
    "            tok_embeds + pos_embeds\n",
    "        )  # Add token and positional embeddings to get input embeddings\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.transformer_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits: Tensor = self.out_head(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class DummyTransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg: dict[str, Any]) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return x\n",
    "\n",
    "\n",
    "class DummyLayerNorm(nn.Module):\n",
    "    def __init__(self, cfg: dict[str, Any]) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "<img src=\"../08-Makemore/images/ch04_mental_.gpt_picpng.png\" alt=\"gpt\" width=\"600\">\n",
    "\n",
    "[image source](https://livebook.manning.com/book/build-a-large-language-model-from-scratch/chapter-4/v-7/36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize a batch consisting of sample texts.\n",
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "text1: str = \"Every effort moves you\"\n",
    "text2: str = \"Everyday holds a\"\n",
    "batch: list[int] = []\n",
    "\n",
    "batch.append(torch.tensor(tokenizer.encode(text1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(text2)))\n",
    "batch_tensor: Tensor = torch.stack(batch, dim=0)\n",
    "\n",
    "print(f\"Shape of batch_tensor: {tuple(batch_tensor.shape)}\\n\")\n",
    "print(f\"Batch_tensor: \\n{batch_tensor}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the DummyGPTModel\n",
    "seed: int = 5\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "model = DummyGPTModel(GPT_CONFIG_124M)\n",
    "logits: Tensor = model(batch_tensor)\n",
    "\n",
    "print(f\"Shape of logits: {tuple(logits.shape)}\\n\")\n",
    "print(f\"logits: \\n{logits}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment\n",
    "\n",
    "- The output (2, 4, 50_257) has `2` rows corresponding to the number of input data.\n",
    "- It has `4` rows which is the context length of the input and `50_257` is the number of unique words (vocab_size) in the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing Activations With Layer Normalization\n",
    "\n",
    "- Layer  normalization is used to improve the stability and efficiency of neural networks.\n",
    "- It aims to to adjust the activations at the output of a NN layer to have a mean of 0 and a standard deviation of 1.\n",
    "- These adjustments speed up the convergence to effective weights and ensures consistent, reliable training.\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"../08-Makemore/images/ch04_norm_layer.png\" width=\"600\">\n",
    "\n",
    "[image source](https://livebook.manning.com/book/build-a-large-language-model-from-scratch/chapter-4/v-7/52)\n",
    "\n",
    "<br><hr>\n",
    "\n",
    "#### Layer Norm vs Batch Norm\n",
    "\n",
    "- Batch norm normalizes across the batch dimension, whereas layer norm normalizes across the feature/channel dimension.\n",
    "- Layer norm is preferred because it normalizes each input independently of the batch size and it's more efficient and stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "\n",
    "batch_tensor: Tensor = torch.randn(2, 5)\n",
    "layer = nn.Sequential(nn.Linear(5, 6), nn.ReLU())\n",
    "out: Tensor = layer(batch_tensor)\n",
    "print(f\"out: \\n{out}\\n\")\n",
    "\n",
    "mean: Tensor = out.mean(dim=-1, keepdim=True)\n",
    "variance: Tensor = out.var(dim=-1, unbiased=False, keepdim=True)\n",
    "print(\"===== Before Layer Normalization ===== \\n\")\n",
    "print(f\"mean: \\n{mean}\\n\\nvariance: \\n{variance}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the output\n",
    "out_norm: Tensor = (out - mean) / torch.sqrt(variance)\n",
    "mean_1: Tensor = out_norm.mean(dim=-1, keepdim=True)\n",
    "variance_1: Tensor = out_norm.var(dim=-1, unbiased=False, keepdim=True)\n",
    "print(f\"out_norm: \\n{out_norm}\\n\")\n",
    "print(\"===== After Layer Normalization ===== \\n\")\n",
    "print(f\"mean: \\n{mean_1}\\n\\nvariance: \\n{variance_1}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improve readability\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "print(\"===== After Layer Normalization ===== \\n\")\n",
    "print(f\"mean: \\n{mean_1}\\n\\nvariance: \\n{variance_1}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, embed_dim: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.eps: float = 1e-5  # Prevents zero division\n",
    "\n",
    "        # Trainable params that's automatically adjusted by the LLM during\n",
    "        # training to improve model performance\n",
    "        self.scale = nn.Parameter(torch.ones(embed_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(embed_dim))\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        std = x.std(dim=-1, unbiased=False, keepdim=True)\n",
    "        norm_x: Tensor = (x - mean) / (std + self.eps)\n",
    "\n",
    "        return (self.scale * norm_x) + self.shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_norm = LayerNorm(embed_dim=5)\n",
    "out_layer_norm: Tensor = layer_norm(batch_tensor)\n",
    "\n",
    "mean: Tensor = out_layer_norm.mean(dim=-1, keepdim=True)\n",
    "variance: Tensor = out_layer_norm.var(dim=-1, unbiased=False, keepdim=True)\n",
    "print(f\"out_layer_norm: \\n{out_layer_norm}\\n\")\n",
    "print(\"===== After Layer Normalization ===== \\n\")\n",
    "print(f\"mean: \\n{mean}\\n\\nvariance: \\n{variance}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing GELU (Gaussian Error Linear Unit)\n",
    "\n",
    "- **Smoothness**: Unlike ReLU's sharp cutoff at zero, `GELU` has a smoother gradient transition. This smoothness helps with training, as optimization algorithms can navigate the function's landscape more effectively.\n",
    "\n",
    "- **Non-zero gradients**: ReLU outputs zero for negative inputs, killing the gradient and hindering learning in those regions. `GELU` maintains non-zero gradients even for negative inputs, allowing the network to learn from a wider range of data. i.e. neurons with -ve gradients can still contribute to the learning process.\n",
    "\n",
    "#### Note\n",
    "\n",
    "- **Computational cost**: The original `GELU` formulation can be computationally expensive. Luckily, there are efficient approximations that retain the benefits without the high cost.\n",
    "\n",
    "<!-- [![image.png](https://i.postimg.cc/9XSGKNZ9/image.png)](https://postimg.cc/14GnNBWR) -->\n",
    "[![image.png](https://i.postimg.cc/pTrF1Ddk/image.png)](https://postimg.cc/wyKM1RLs)\n",
    "\n",
    "$$GELU(x) = 0.5x * ( 1 + tanh( \\sqrt{\\frac{2}{\\pi}} * (x + 0.044715x^{3}) ) ) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        gelu: Tensor = (\n",
    "            0.5\n",
    "            * x\n",
    "            * (\n",
    "                1\n",
    "                + torch.tanh(\n",
    "                    torch.sqrt(torch.tensor(2.0 / torch.pi))\n",
    "                    * (x + 0.044715 * torch.pow(x, 3))\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        return gelu\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"Applies a feed-forward neural network to the input tensor `x`. The feed-forward network\n",
    "    consists of two linear layers with a GELU activation in between. The first linear layer\n",
    "    expands the input dimension by a factor of 4, and the second linear layer projects the result\n",
    "    back to the original input dimension.\n",
    "\n",
    "    Args:\n",
    "        x (torch.Tensor): The input tensor to be passed through the feed-forward network.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The output tensor after passing through the feed-forward network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cfg: dict[str, Any]) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg.get(\"emb_dim\"), 4 * cfg.get(\"emb_dim\")),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg.get(\"emb_dim\"), cfg.get(\"emb_dim\")),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gelu, relu = GELU(), nn.ReLU()\n",
    "\n",
    "x: Tensor = torch.linspace(-3, 3, 100)\n",
    "y_gelu, y_relu = gelu(x), relu(x)\n",
    "plt.figure(figsize=(8, 3))\n",
    "\n",
    "for i, (y, label) in enumerate(zip([y_gelu, y_relu], [\"GELU\", \"ReLU\"]), 1):\n",
    "    plt.subplot(1, 2, i)\n",
    "    plt.plot(x, y)\n",
    "    plt.title(f\"{label} activation function\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(f\"{label}(x)\")\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "\n",
    "ff_network = FeedForward(cfg=GPT_CONFIG_124M)\n",
    "x: Tensor = torch.randn(2, 3, 768)\n",
    "out: Tensor = ff_network(x)\n",
    "\n",
    "print(f\"out: {out.shape}\")\n",
    "print(f\"out: \\n{out}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feed Forward Network With GELU\n",
    "\n",
    "<img src=\"../08-Makemore/images/ch04__ffn.png\" width=\"600\">\n",
    "\n",
    "[image source](https://livebook.manning.com/book/build-a-large-language-model-from-scratch/chapter-4/v-7/112)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_p11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
