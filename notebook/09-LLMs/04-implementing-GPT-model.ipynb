{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing A GPT-model From Scratch\n",
    "\n",
    "- Coding a GPT-like LLM that can be trained to generate uman-like text.\n",
    "- Normalizing layer activations to stabilize NN training.\n",
    "- Adding `shortcut connections` to train deep NNs effectively.\n",
    "- Implementing transformer blocks to create GPT models of various sizes.\n",
    "- Computing the number of parameters and storage requiremnts of GPT models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark -v -p numpy,pandas,polars,torch,lightning --conda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in library\n",
    "from pathlib import Path\n",
    "import re\n",
    "import json\n",
    "from typing import Any, Optional, Union\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "# Standard imports\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from rich.console import Console\n",
    "from rich.theme import Theme\n",
    "\n",
    "custom_theme = Theme(\n",
    "    {\n",
    "        \"info\": \"#76FF7B\",\n",
    "        \"warning\": \"#FBDDFE\",\n",
    "        \"error\": \"#FF0000\",\n",
    "    }\n",
    ")\n",
    "console = Console(theme=custom_theme)\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Pandas settings\n",
    "pd.options.display.max_rows = 1_000\n",
    "pd.options.display.max_columns = 1_000\n",
    "pd.options.display.max_colwidth = 600\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# Black code formatter (Optional)\n",
    "%load_ext lab_black\n",
    "\n",
    "# auto reload imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M: dict[str, Any] = {\n",
    "    \"vocab_size\": 50_257,\n",
    "    \"context_length\": 1_024,\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,  # Number of attention heads\n",
    "    \"n_layers\": 12,\n",
    "    \"drop_rate\": 0.1,  # Dropout rate\n",
    "    \"qkv_bias\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Placeholder GPT Backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self, cfg: dict[str, Any]) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.tok_emb = nn.Embedding(cfg.get(\"vocab_size\"), cfg.get(\"emb_dim\"))\n",
    "        self.pos_emb = nn.Embedding(cfg.get(\"context_length\"), cfg.get(\"emb_dim\"))\n",
    "        self.drop_emb = nn.Dropout(cfg.get(\"drop_rate\"))\n",
    "        self.transformer_blocks = nn.Sequential(\n",
    "            *[DummyTransformerBlock(cfg) for _ in range(cfg.get(\"n_layers\"))]\n",
    "        )\n",
    "\n",
    "        self.final_norm = DummyLayerNorm(cfg.get(\"emb_dim\"))\n",
    "        self.out_head = nn.Linear(cfg.get(\"emb_dim\"), cfg.get(\"vocab_size\"), bias=False)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        batch_size, seq_len = x.shape\n",
    "        tok_embeds: Tensor = self.tok_emb(x)\n",
    "        pos_embeds: Tensor = self.pos_emb(torch.arange(seq_len, device=x.device))\n",
    "        x = (\n",
    "            tok_embeds + pos_embeds\n",
    "        )  # Add token and positional embeddings to get input embeddings\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.transformer_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits: Tensor = self.out_head(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class DummyTransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg: dict[str, Any]) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return x\n",
    "\n",
    "\n",
    "class DummyLayerNorm(nn.Module):\n",
    "    def __init__(self, cfg: dict[str, Any]) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "<img src=\"./images/ch04_mental_.gpt_pic.png\" alt=\"gpt\" width=\"600\">\n",
    "\n",
    "[image source](https://livebook.manning.com/book/build-a-large-language-model-from-scratch/chapter-4/v-7/36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize a batch consisting of sample texts.\n",
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "text1: str = \"Every effort moves you\"\n",
    "text2: str = \"Everyday holds a\"\n",
    "batch: list[int] = []\n",
    "\n",
    "batch.append(torch.tensor(tokenizer.encode(text1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(text2)))\n",
    "batch_tensor: Tensor = torch.stack(batch, dim=0)\n",
    "\n",
    "print(f\"Shape of batch_tensor: {tuple(batch_tensor.shape)}\\n\")\n",
    "print(f\"Batch_tensor: \\n{batch_tensor}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the DummyGPTModel\n",
    "seed: int = 5\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "model = DummyGPTModel(GPT_CONFIG_124M)\n",
    "logits: Tensor = model(batch_tensor)\n",
    "\n",
    "print(f\"Shape of logits: {tuple(logits.shape)}\\n\")\n",
    "print(f\"logits: \\n{logits}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment\n",
    "\n",
    "- The output (2, 4, 50_257) has `2` rows corresponding to the number of input data.\n",
    "- It has `4` rows which is the context length of the input and `50_257` is the number of unique words (vocab_size) in the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing Activations With Layer Normalization\n",
    "\n",
    "- Layer  normalization is used to improve the stability and efficiency of neural networks.\n",
    "- It aims to to adjust the activations at the output of a NN layer to have a mean of 0 and a standard deviation of 1.\n",
    "- These adjustments speed up the convergence to effective weights and ensures consistent, reliable training.\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"./images/ch04_norm_layer.png\" width=\"600\">\n",
    "\n",
    "[image source](https://livebook.manning.com/book/build-a-large-language-model-from-scratch/chapter-4/v-7/52)\n",
    "\n",
    "<br><hr>\n",
    "\n",
    "#### Layer Norm vs Batch Norm\n",
    "\n",
    "- Batch norm normalizes across the batch dimension, whereas layer norm normalizes across the feature/channel dimension.\n",
    "- Layer norm is preferred because it normalizes each input independently of the batch size and it's more efficient and stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "\n",
    "batch_tensor: Tensor = torch.randn(2, 5)\n",
    "layer = nn.Sequential(nn.Linear(5, 6), nn.ReLU())\n",
    "out: Tensor = layer(batch_tensor)\n",
    "print(f\"out: \\n{out}\\n\")\n",
    "\n",
    "mean: Tensor = out.mean(dim=-1, keepdim=True)\n",
    "variance: Tensor = out.var(dim=-1, unbiased=False, keepdim=True)\n",
    "print(\"===== Before Layer Normalization ===== \\n\")\n",
    "print(f\"mean: \\n{mean}\\n\\nvariance: \\n{variance}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the output\n",
    "out_norm: Tensor = (out - mean) / torch.sqrt(variance)\n",
    "mean_1: Tensor = out_norm.mean(dim=-1, keepdim=True)\n",
    "variance_1: Tensor = out_norm.var(dim=-1, unbiased=False, keepdim=True)\n",
    "print(f\"out_norm: \\n{out_norm}\\n\")\n",
    "print(\"===== After Layer Normalization ===== \\n\")\n",
    "print(f\"mean: \\n{mean_1}\\n\\nvariance: \\n{variance_1}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improve readability\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "print(\"===== After Layer Normalization ===== \\n\")\n",
    "print(f\"mean: \\n{mean_1}\\n\\nvariance: \\n{variance_1}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, embed_dim: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.eps: float = 1e-5  # Prevents zero division\n",
    "\n",
    "        # Trainable params that's automatically adjusted by the LLM during\n",
    "        # training to improve model performance\n",
    "        self.scale = nn.Parameter(torch.ones(embed_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(embed_dim))\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        std = x.std(dim=-1, unbiased=False, keepdim=True)\n",
    "        norm_x: Tensor = (x - mean) / (std + self.eps)\n",
    "\n",
    "        return (self.scale * norm_x) + self.shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_norm = LayerNorm(embed_dim=5)\n",
    "out_layer_norm: Tensor = layer_norm(batch_tensor)\n",
    "\n",
    "mean: Tensor = out_layer_norm.mean(dim=-1, keepdim=True)\n",
    "variance: Tensor = out_layer_norm.var(dim=-1, unbiased=False, keepdim=True)\n",
    "print(f\"out_layer_norm: \\n{out_layer_norm}\\n\")\n",
    "print(\"===== After Layer Normalization ===== \\n\")\n",
    "print(f\"mean: \\n{mean}\\n\\nvariance: \\n{variance}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing GELU (Gaussian Error Linear Unit)\n",
    "\n",
    "- **Smoothness**: Unlike ReLU's sharp cutoff at zero, `GELU` has a smoother gradient transition. This smoothness helps with training, as optimization algorithms can navigate the function's landscape more effectively.\n",
    "\n",
    "- **Non-zero gradients**: ReLU outputs zero for negative inputs, killing the gradient and hindering learning in those regions. `GELU` maintains non-zero gradients even for negative inputs, allowing the network to learn from a wider range of data. i.e. neurons with -ve gradients can still contribute to the learning process.\n",
    "\n",
    "#### Note\n",
    "\n",
    "- **Computational cost**: The original `GELU` formulation can be computationally expensive. Luckily, there are efficient approximations that retain the benefits without the high cost.\n",
    "\n",
    "<!-- [![image.png](https://i.postimg.cc/9XSGKNZ9/image.png)](https://postimg.cc/14GnNBWR) -->\n",
    "[![image.png](https://i.postimg.cc/pTrF1Ddk/image.png)](https://postimg.cc/wyKM1RLs)\n",
    "\n",
    "$$GELU(x) = 0.5x * ( 1 + tanh( \\sqrt{\\frac{2}{\\pi}} * (x + 0.044715x^{3}) ) ) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        gelu: Tensor = (\n",
    "            0.5\n",
    "            * x\n",
    "            * (\n",
    "                1\n",
    "                + torch.tanh(\n",
    "                    torch.sqrt(torch.tensor(2.0 / torch.pi))\n",
    "                    * (x + 0.044715 * torch.pow(x, 3))\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        return gelu\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"Applies a feed-forward neural network to the input tensor `x`. The feed-forward network\n",
    "    consists of two linear layers with a GELU activation in between. The first linear layer\n",
    "    expands the input dimension by a factor of 4, and the second linear layer projects the result\n",
    "    back to the original input dimension.\n",
    "\n",
    "    Args:\n",
    "        x (torch.Tensor): The input tensor to be passed through the feed-forward network.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The output tensor after passing through the feed-forward network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cfg: dict[str, Any]) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg.get(\"emb_dim\"), 4 * cfg.get(\"emb_dim\")),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg.get(\"emb_dim\"), cfg.get(\"emb_dim\")),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gelu, relu = GELU(), nn.ReLU()\n",
    "\n",
    "x: Tensor = torch.linspace(-3, 3, 100)\n",
    "y_gelu, y_relu = gelu(x), relu(x)\n",
    "plt.figure(figsize=(8, 3))\n",
    "\n",
    "for i, (y, label) in enumerate(zip([y_gelu, y_relu], [\"GELU\", \"ReLU\"]), 1):\n",
    "    plt.subplot(1, 2, i)\n",
    "    plt.plot(x, y)\n",
    "    plt.title(f\"{label} activation function\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(f\"{label}(x)\")\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "\n",
    "ff_network = FeedForward(cfg=GPT_CONFIG_124M)\n",
    "x: Tensor = torch.randn(2, 3, 768)\n",
    "out: Tensor = ff_network(x)\n",
    "\n",
    "print(f\"out: {out.shape}\")\n",
    "print(f\"out: \\n{out}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feed Forward Network With GELU\n",
    "\n",
    "<img src=\"./images/ch04__ffn.png\" width=\"600\">\n",
    "\n",
    "[image source](https://livebook.manning.com/book/build-a-large-language-model-from-scratch/chapter-4/v-7/112)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shortcut Connections\n",
    "\n",
    "- AKA Skip/Residual connections.\n",
    "- Shortcut Connections involve adding the output of one layer to the input of the next layer.\n",
    "- It mitigates the vanishing gradient problem.\n",
    "\n",
    "<img src=\"./images/ch04__shortcut.png\" width=\"600\">\n",
    "\n",
    "[image source](https://livebook.manning.com/book/build-a-large-language-model-from-scratch/chapter-4/v-7/119)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleDeepNeuralNetwork_v1(nn.Module):\n",
    "    def __init__(self, layer_sizes: list[int], use_shortcut: bool = True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.use_shortcut = use_shortcut\n",
    "        assert len(layer_sizes) == 6, \"layer_sizes must be of length 6\"\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                # 5 layers\n",
    "                nn.Sequential(nn.Linear(layer_sizes[0], layer_sizes[1]), GELU()),\n",
    "                nn.Sequential(nn.Linear(layer_sizes[1], layer_sizes[2]), GELU()),\n",
    "                nn.Sequential(nn.Linear(layer_sizes[2], layer_sizes[3]), GELU()),\n",
    "                nn.Sequential(nn.Linear(layer_sizes[3], layer_sizes[4]), GELU()),\n",
    "                nn.Sequential(nn.Linear(layer_sizes[4], layer_sizes[5]), GELU()),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        for layer in self.layers:\n",
    "            # Compute the output of the current layer\n",
    "            layer_output: Tensor = layer(x)\n",
    "            # Check if shortcut connections can be applied\n",
    "            if self.use_shortcut and x.shape == layer_output.shape:\n",
    "                # Apply the shortcut connection\n",
    "                x = x + layer_output\n",
    "            else:\n",
    "                # Otherwise, just use the layer output\n",
    "                x = layer_output\n",
    "        return x\n",
    "\n",
    "\n",
    "def print_gradients(model: nn.Module, x: Tensor) -> None:\n",
    "    \"\"\"This is used to print the gradients of the model.\"\"\"\n",
    "    # Forward pass\n",
    "    output: Tensor = model(x)\n",
    "    target: Tensor = torch.tensor([[0.0]])\n",
    "\n",
    "    # Calculate the loss\n",
    "    criterion = nn.MSELoss()\n",
    "    loss: Tensor = criterion(output, target)\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"weight\" in name:\n",
    "            print(f\"{name} has gradient mean of {param.grad.abs().mean().item():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[f\"Linear({idx}, {idx + 1})\" for idx in range(6 - 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "\n",
    "layer_sizes: list[int] = [3, 3, 3, 3, 3, 1]\n",
    "sample_input: Tensor = torch.tensor([[1.0, 0.0, -1.0]])\n",
    "\n",
    "model_wfout_shortcut = ExampleDeepNeuralNetwork_v1(\n",
    "    layer_sizes=layer_sizes, use_shortcut=False\n",
    ")\n",
    "print(f\"input: {sample_input.shape}\")\n",
    "\n",
    "model_wfout_shortcut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_gradients(model=model_wfout_shortcut, x=sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleDeepNeuralNetwork_v2(nn.Module):\n",
    "    def __init__(self, layer_sizes: list[int], use_shortcut: bool = True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.use_shortcut = use_shortcut\n",
    "        assert len(layer_sizes) == 6, \"layer_sizes must be of length 6\"\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                # 5 layers\n",
    "                nn.Sequential(nn.Linear(layer_sizes[idx], layer_sizes[idx + 1]), GELU())\n",
    "                for idx in range(len(layer_sizes) - 1)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        for layer in self.layers:\n",
    "            # Compute the output of the current layer\n",
    "            layer_output: Tensor = layer(x)\n",
    "            # Check if shortcut connections can be applied\n",
    "            if self.use_shortcut and x.shape == layer_output.shape:\n",
    "                # Apply the shortcut connection\n",
    "                x = x + layer_output\n",
    "            else:\n",
    "                # Otherwise, just use the layer output\n",
    "                x = layer_output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "\n",
    "layer_sizes: list[int] = [3, 3, 3, 3, 3, 1]\n",
    "sample_input: Tensor = torch.tensor([[1.0, 0.0, -1.0]])\n",
    "\n",
    "model_wfout_shortcut = ExampleDeepNeuralNetwork_v2(\n",
    "    layer_sizes=layer_sizes, use_shortcut=False\n",
    ")\n",
    "print(f\"input: {sample_input.shape}\")\n",
    "print(model_wfout_shortcut)\n",
    "print()\n",
    "\n",
    "# The weights shrink to a vanishingly small value\n",
    "print_gradients(model=model_wfout_shortcut, x=sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "\n",
    "\n",
    "model_wf_shortcut = ExampleDeepNeuralNetwork_v2(\n",
    "    layer_sizes=layer_sizes,\n",
    "    use_shortcut=True,  # NEW!\n",
    ")\n",
    "print(f\"input: {sample_input.shape}\")\n",
    "print(model_wf_shortcut)\n",
    "print()\n",
    "\n",
    "# The weights are slightly larger than the ones without the shortcut.\n",
    "print_gradients(model=model_wf_shortcut, x=sample_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "### Connecting Attention And Linear Layers In A Transformer Block\n",
    "\n",
    "- Implement the transformer block.\n",
    "- The transformer block in the 124 million parameter GPT-2 architecture was repeated 12 times and it combines several concepts like:\n",
    "  - multi-head attention\n",
    "  - layer norm, dropout\n",
    "  - feedforward layers\n",
    "  - and GELU activations.\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"./images/ch04__trnsf_block.png\" width=\"600\">\n",
    "\n",
    "[image source](https://livebook.manning.com/book/build-a-large-language-model-from-scratch/chapter-4/v-7/145)\n",
    "\n",
    "<br>\n",
    "\n",
    "- When a transformer block processes an input sequence, each element in the sequence is represented by a fixed-size vector.\n",
    "- The operations w/in the transformer block are designed to transform the input vectors in a way that preserves the dimensions.\n",
    "- The multi-head attention block figures out how different parts of the sequence relate to each other, while the feedforward network modifies the data individually at each position.\n",
    "- This combination leads to a more nuanced understanding and processing of the input and improves the model's capacity to handle complex data patterns.\n",
    "\n",
    "#### Note\n",
    "\n",
    "- If the LayerNorm is applied before Multi-head attention/FeedForward network, it's called `Pre-LayerNorm`.\n",
    "- Older architectures such as the original transformer model applied the LayerNorm after the multi-head attention/feedforward network which is called `Post-LayerNorrm`.\n",
    "- `Post-LayerNorm` leads to worse training performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import MultiHeadAttention\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg: dict[str, Any]) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.attn = MultiHeadAttention(\n",
    "            d_in=cfg.get(\"emb_dim\"),\n",
    "            d_out=cfg.get(\"emb_dim\"),\n",
    "            context_length=cfg.get(\"context_length\"),\n",
    "            num_heads=cfg.get(\"n_heads\"),\n",
    "            dropout=cfg.get(\"drop_rate\"),\n",
    "            qkv_bias=cfg.get(\"qkv_bias\"),\n",
    "        )\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg.get(\"emb_dim\"))\n",
    "        self.norm2 = LayerNorm(cfg.get(\"emb_dim\"))\n",
    "        self.dropout = nn.Dropout(cfg.get(\"drop_rate\"))\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "\n",
    "        shortcut: Tensor = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.attn(x)\n",
    "        x = self.dropout(x)\n",
    "        # Add the original input to the output.\n",
    "        x = x + shortcut\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.dropout(x)\n",
    "        # Apply shortcut connection.\n",
    "        x = x + shortcut\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "\n",
    "x: Tensor = torch.randn(2, 4, 768)\n",
    "transf_block = TransformerBlock(cfg=GPT_CONFIG_124M)\n",
    "output: Tensor = transf_block(x)\n",
    "\n",
    "# The input and output shapes are the same!\n",
    "print(f\"Input shape: {tuple(x.shape)}\\n\")\n",
    "print(f\"Output shape: {tuple(output.shape)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding The GPT Model\n",
    "\n",
    "- Update the `DummyGPTModel` class.\n",
    "\n",
    "<img src=\"./images/ch04__gpt-model.png\" width=\"600\">\n",
    "\n",
    "[image source](https://livebook.manning.com/book/build-a-large-language-model-from-scratch/chapter-4/v-7/167)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg: dict[str, Any]) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.tok_emb = nn.Embedding(cfg.get(\"vocab_size\"), cfg.get(\"emb_dim\"))\n",
    "        self.pos_emb = nn.Embedding(cfg.get(\"context_length\"), cfg.get(\"emb_dim\"))\n",
    "        self.drop_emb = nn.Dropout(cfg.get(\"drop_rate\"))\n",
    "        self.transformer_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg.get(\"n_layers\"))]\n",
    "        )\n",
    "\n",
    "        self.final_norm = LayerNorm(cfg.get(\"emb_dim\"))\n",
    "        self.out_head = nn.Linear(cfg.get(\"emb_dim\"), cfg.get(\"vocab_size\"), bias=False)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "\n",
    "        batch_size, seq_len = x.shape\n",
    "        tok_embeds: Tensor = self.tok_emb(x)\n",
    "        pos_embeds: Tensor = self.pos_emb(torch.arange(seq_len, device=x.device))\n",
    "        # Add token and positional embeddings to get input embeddings\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.transformer_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits: Tensor = self.out_head(x)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "text1: str = \"Every effort moves you\"\n",
    "text2: str = \"Everyday holds a\"\n",
    "batch: list[int] = []\n",
    "\n",
    "batch.append(torch.tensor(tokenizer.encode(text1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(text2)))\n",
    "batch_tensor: Tensor = torch.stack(batch, dim=0)\n",
    "\n",
    "print(f\"Shape of batch_tensor: {tuple(batch_tensor.shape)}\\n\")\n",
    "print(f\"Batch_tensor: \\n{batch_tensor}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "logits: Tensor = model(batch_tensor)\n",
    "\n",
    "print(f\"batch_tensor: \\n{batch_tensor}\\n\")\n",
    "\n",
    "print(f\"Input shape: {tuple(batch_tensor.shape)}\\n\")\n",
    "print(f\"Output shape: {tuple(logits.shape)}\\n\")\n",
    "print(f\"logits: \\n{logits}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of trainable parameters\n",
    "total_params: int = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Number of model weights: {total_params:,} total parameters.\")\n",
    "\n",
    "# To apply weight tying, reuse the same weights at the token ebedding and output layers.\n",
    "# i.e. Subtract the output layer weights from the total model parameters.\n",
    "output_params: int = sum(\n",
    "    p.numel() for p in model.out_head.parameters() if p.requires_grad\n",
    ")\n",
    "\n",
    "print(f\"With weight tying: {total_params - output_params:,} trainable parameters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Comment\n",
    "\n",
    "- Input data shape: (2, 4) --> (batch_size, max_seq_len)\n",
    "- Output data shape: (2, 4, 50257) --> (batch_size, max_seq_len, vocab_size)\n",
    "- GPT-2 small model has about 124M parameters but our version has 163M parameters. This is because we initialized two identical layers (token embedding and output embedding) with different weights.\n",
    "- The original GPT-2 small architecture uses the same weights for the token embedding and the output embedding which is called `weight tying`.\n",
    "- `Weight tying` reduces the overall memory footprint and computational complexity of the model but it also reduces the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trf_model = TransformerBlock(cfg=GPT_CONFIG_124M)\n",
    "trf_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4.1 Number of parameters in feed forward and attention modules\n",
    "# Calculate and compare the number of parameters that are contained in the feed forward module\n",
    "# and those that are contained in the multi-head attention module.\n",
    "\n",
    "ff_params: int = sum(\n",
    "    p.numel() for p in model.transformer_blocks[1].ff.parameters() if p.requires_grad\n",
    ")\n",
    "multi_head_attn_params: int = sum(\n",
    "    p.numel() for p in model.transformer_blocks[1].attn.parameters() if p.requires_grad\n",
    ")\n",
    "print(f\"Number of parameters in the feed forward module: {ff_params:,}\")\n",
    "print(\n",
    "    f\"Number of parameters in the multi-head attention module: {multi_head_attn_params:,}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Memory Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_size_bytes: int = total_params * 4\n",
    "# Convert to megabytes: 1 MB = 1,048,576 bytes\n",
    "total_size_mb: float = total_size_bytes / (1024**2)\n",
    "print(f\"Total size of the model (MB): {total_size_mb:,.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_num_params(model: nn.Module, weight_tying: bool = False) -> int:\n",
    "    \"\"\"Calculate the number of trainable parameters.\"\"\"\n",
    "    total_params: int = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "    if not weight_tying:\n",
    "        result: int = total_params\n",
    "        print(f\"Without weight tying: \\n{result:,} trainable parameters.\")\n",
    "        return result\n",
    "\n",
    "    # To apply weight tying, reuse the same weights at the token ebedding and output layers.\n",
    "    # i.e. Subtract the output layer weights from the total model parameters.\n",
    "    output_params: int = sum(\n",
    "        p.numel() for p in model.out_head.parameters() if p.requires_grad\n",
    "    )\n",
    "    result = total_params - output_params\n",
    "    print(f\"With weight tying: \\n{result:,} trainable parameters.\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_num_params(model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4.2 Initializing larger GPT models\n",
    "# In this chapter, we initialized a 124 million parameter GPT model, which is known as “GPT-2 small.”\n",
    "# Without making any code modifications besides updating the configuration file, use the GPTModel\n",
    "# class to implement GPT-2 medium (using 1,024-dimensional embeddings, 24 transformer blocks,\n",
    "# 16 multi-head attention heads), GPT-2 large (1,280-dimensional embeddings, 36 transformer blocks,\n",
    "# 20 multi-head attention heads), and GPT-2 XL (1,600-dimensional embeddings, 48 transformer blocks,\n",
    "# 25 multi-head attention heads). As a bonus, calculate the total number of parameters in each GPT model.\n",
    "\n",
    "GPT_CONFIG_MEDIUM: dict[str, Any] = GPT_CONFIG_124M.copy()\n",
    "GPT_CONFIG_MEDIUM[\"emb_dim\"] = 1_024\n",
    "GPT_CONFIG_MEDIUM[\"n_heads\"] = 16\n",
    "GPT_CONFIG_MEDIUM[\"n_layers\"] = 24\n",
    "\n",
    "GPT_CONFIG_LARGE: dict[str, Any] = GPT_CONFIG_124M.copy()\n",
    "GPT_CONFIG_LARGE[\"emb_dim\"] = 1_280\n",
    "GPT_CONFIG_LARGE[\"n_heads\"] = 20\n",
    "GPT_CONFIG_LARGE[\"n_layers\"] = 36\n",
    "\n",
    "GPT_CONFIG_XL: dict[str, Any] = GPT_CONFIG_124M.copy()\n",
    "GPT_CONFIG_XL[\"emb_dim\"] = 1_600\n",
    "GPT_CONFIG_XL[\"n_heads\"] = 25\n",
    "GPT_CONFIG_XL[\"n_layers\"] = 48\n",
    "\n",
    "model_medium = GPTModel(GPT_CONFIG_MEDIUM)\n",
    "model_large = GPTModel(GPT_CONFIG_LARGE)\n",
    "model_xl = GPTModel(GPT_CONFIG_XL)\n",
    "\n",
    "# Ihis cell took ~ 30s to run on my machine.\n",
    "calculate_num_params(model=model_medium)\n",
    "print()\n",
    "calculate_num_params(model=model_large)\n",
    "print()\n",
    "calculate_num_params(model=model_xl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Text\n",
    "\n",
    "<img src=\"./images/ch04_generate_txt.png\" width=\"600\">\n",
    "\n",
    "[image source](https://livebook.manning.com/book/build-a-large-language-model-from-scratch/chapter-4/v-7/209)\n",
    "\n",
    "<br><br>\n",
    "\n",
    "### Breakdown\n",
    "\n",
    "- The next-token generation process involves generating the next token given the input.\n",
    "\n",
    "- The model outputs a matrix with vectors representing potential next tokens in each step.\n",
    "\n",
    "- The vector corresponding to the next token is extracted and converted into a probability distribution using the softmax function.\n",
    "\n",
    "- The index of the highest value in the vector of probability scores is located, identifying the token ID.\n",
    "\n",
    "- The token ID is decoded back into text to produce the next token in the sequence.\n",
    "\n",
    "- This token is appended to the previous inputs, forming a new input sequence for the next iteration.\n",
    "\n",
    "- This process is repeated iteratively, building coherent phrases and sentences from the initial input context.\n",
    "\n",
    "- The process continues for a user-specified number of iterations to generate the desired number of tokens.\n",
    "\n",
    "<br>\n",
    "\n",
    "[![image.png](https://i.postimg.cc/QxFsVJ8g/image.png)](https://postimg.cc/RNx2Dfnq)\n",
    "\n",
    "[image source](https://livebook.manning.com/book/build-a-large-language-model-from-scratch/chapter-4/v-7/209)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(\n",
    "    model: nn.Module, idx: Tensor, max_new_tokens: int, context_length: int\n",
    ") -> Tensor:\n",
    "    \"\"\"Generate text using a language model by iteratively predicting the next token.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The language model used for generating text.\n",
    "        idx (Tensor): The input tensor containing the initial context tokens.\n",
    "        max_new_tokens (int): The maximum number of new tokens to generate.\n",
    "        context_length (int): The maximum length of the context that the model can handle.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: The tensor containing the generated sequence of tokens.\n",
    "    \"\"\"\n",
    "    for _ in range(max_new_tokens):\n",
    "        # Crop the context if it exceeds the maximum length supported by the LLM.\n",
    "        # i.e. if the LLM supports only 5 tokens and the context is 10 tokens long, reduce it to 5 tokens.\n",
    "        idx_cond: Tensor = idx[:, -context_length:]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits: Tensor = model(idx_cond)\n",
    "\n",
    "        # Get the last token from the sequence\n",
    "        logits = logits[:, -1, :]\n",
    "        probas: Tensor = torch.softmax(logits, dim=-1)\n",
    "        idx_next: Tensor = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "        # Append the last token to the context\n",
    "        idx = torch.cat([idx, idx_next], dim=1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_context: str = \"Hello, I am\"\n",
    "encoded: list[int] = tokenizer.encode(start_context)\n",
    "print(f\"{encoded = }\")\n",
    "# Convert to tensor and add batch dimension (index 0)\n",
    "encoded_tensor: Tensor = torch.tensor(encoded).unsqueeze(dim=0)\n",
    "print(f\"encoded_tensor shape: {encoded_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate texts\n",
    "# Switch the model to eval mode which disables random components like dropout which\n",
    "# are only used during training.\n",
    "model.eval()\n",
    "output: Tensor = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=encoded_tensor,\n",
    "    max_new_tokens=6,\n",
    "    context_length=GPT_CONFIG_124M.get(\"context_length\"),\n",
    ")\n",
    "\n",
    "print(f\"Input length: {len(encoded_tensor[0])}\")\n",
    "print(f\"Output: {output}\")\n",
    "print(f\"Output length: {len(output[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten and convert to list\n",
    "output_list: list[int] = output.squeeze(dim=0).tolist()\n",
    "print(f\"{output_list = }\\n\")\n",
    "\n",
    "decoded_text: str = tokenizer.decode(output_list)\n",
    "print(f\"{decoded_text = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_p11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
