{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Mechanism\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark -v -p numpy,pandas,polars,torch,lightning --conda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in library\n",
    "from pathlib import Path\n",
    "import re\n",
    "import json\n",
    "from typing import Any, Optional, Union\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "# Standard imports\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from rich.console import Console\n",
    "from rich.theme import Theme\n",
    "\n",
    "custom_theme = Theme(\n",
    "    {\n",
    "        \"info\": \"#76FF7B\",\n",
    "        \"warning\": \"#FBDDFE\",\n",
    "        \"error\": \"#FF0000\",\n",
    "    }\n",
    ")\n",
    "console = Console(theme=custom_theme)\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Pandas settings\n",
    "pd.options.display.max_rows = 1_000\n",
    "pd.options.display.max_columns = 1_000\n",
    "pd.options.display.max_colwidth = 600\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# Black code formatter (Optional)\n",
    "%load_ext lab_black\n",
    "\n",
    "# auto reload imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context Vector\n",
    "\n",
    "- Context vector is the weighted sum of the input vectors that captures the relevent information from the entire sequence for a given position. i.e. it can be thought of as an enriched embedding vector of the inout\n",
    "\n",
    "#### Calculate Context Vector\n",
    "\n",
    "- Attention Score: \n",
    "  - it's calculated by finding the dot product of the token's query vector and the key vector of the other tokens in the sequence.\n",
    "  - The scores are normalized using softmax to produce the attention weights.\n",
    "- Multiply the embedded input tokens with their corresponding attention weights and sum the resulting vectors to get the context vector.\n",
    "- This is done for each position in the sequence to get the context vector for the entire sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without Trainable Parameters (Simplified Version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed: int = 5\n",
    "\n",
    "# Assume that we have an input with a 3-D embeddings shown below:\n",
    "inputs: Tensor = torch.tensor(\n",
    "    [\n",
    "        [0.43, 0.15, 0.89],  # Your (x^1)\n",
    "        [0.55, 0.87, 0.66],  # journey (x^2)\n",
    "        [0.57, 0.85, 0.64],  # starts (x^3)\n",
    "        [0.22, 0.58, 0.33],  # with (x^4)\n",
    "        [0.77, 0.25, 0.10],  # one (x^5)\n",
    "        [0.05, 0.80, 0.55],  # step (x^6)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Calculate the context vector for the 2nd token (x^2)\n",
    "# 1: Cal the attention scores\n",
    "query: Tensor = inputs[1]\n",
    "attn_scores_index_1: Tensor = torch.empty(inputs.shape[0])\n",
    "\n",
    "for idx, x_1 in enumerate(inputs):\n",
    "    # Cal the dot product of the query vector and each key vector in the input\n",
    "    attn_scores_index_1[idx] = torch.dot(x_1, query)\n",
    "\n",
    "print(f\"{attn_scores_index_1 = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2: Normalize the attention scores to obtain the attention weights\n",
    "attn_scores_weights_1: Tensor = torch.softmax(attn_scores_index_1, dim=-1)\n",
    "print(f\"{attn_scores_weights_1 = }\")\n",
    "attn_scores_weights_1.sum(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_shape: tuple = tuple(inputs.shape)\n",
    "attn_scores_weights_1_shape: tuple = tuple(attn_scores_weights_1.shape)\n",
    "print(f\"{attn_scores_weights_1_shape = } AND {inputs_shape = }\")\n",
    "\n",
    "# 3: Calculate the context vector as the weighted average of the values\n",
    "# Transpose the inputs so that we can perform matrix multiplication\n",
    "context_vector_1: Tensor = attn_scores_weights_1 @ inputs\n",
    "context_vector_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate The Attention Weights Of The Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Calculate the attention scores\n",
    "print(f\"{inputs.shape = } AND {inputs.T.shape = }\")\n",
    "attn_scores: Tensor = inputs @ inputs.T\n",
    "print(f\"\\n{attn_scores.shape = }\")\n",
    "\n",
    "attn_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Calculate the attention weights. i.e. normalize the attention scores using softmax\n",
    "attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "print(f\"\\n{attn_weights.shape = }\")\n",
    "attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Calculate the context vector\n",
    "print(f\"{attn_weights.shape = } AND {inputs.shape = }\")\n",
    "\n",
    "context_vector: Tensor = attn_weights @ inputs\n",
    "context_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><hr>\n",
    "\n",
    "## Implement Self-Attention With Trainable Parameters\n",
    "\n",
    "- AKA **Scaled Dot-Product Attention**\n",
    "- Add weight matrices that are updated during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the trainable attention weights for a given token in the input\n",
    "x_1: Tensor = inputs[1]\n",
    "print(f\"{x_1.shape = }\")\n",
    "# Embedding dimension\n",
    "d_in: int = x_1.shape[-1]\n",
    "# Output embedding size\n",
    "d_out: int = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "\n",
    "# Trainable parameters: requires_grad=False (to reduce the clutter and keep things simple)\n",
    "W_query: Tensor = nn.Parameter(torch.randn(d_in, d_out), requires_grad=False)\n",
    "W_key: Tensor = nn.Parameter(torch.randn(d_in, d_out), requires_grad=False)\n",
    "W_value: Tensor = nn.Parameter(torch.randn(d_in, d_out), requires_grad=False)\n",
    "\n",
    "# Compute the query, key, and value tensors for the given index\n",
    "query_1: Tensor = torch.matmul(x_1, W_query)\n",
    "key_1: Tensor = torch.matmul(x_1, W_key)\n",
    "value_1: Tensor = torch.matmul(x_1, W_value)\n",
    "\n",
    "# Compute the key and value tensors for ALL the input\n",
    "query: Tensor = torch.matmul(inputs, W_query)\n",
    "key: Tensor = torch.matmul(inputs, W_key)\n",
    "value: Tensor = torch.matmul(inputs, W_value)\n",
    "\n",
    "print(f\"{query_1.shape = }\")\n",
    "query_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{query_1.shape = } | {key_1.shape = } | {value_1.shape = }\")\n",
    "print(f\"{query.shape = } | {key.shape = } | {value.shape = }\")\n",
    "print()\n",
    "\n",
    "# Calculate the attention scores\n",
    "# For a single token in the query\n",
    "attn_score_1: Tensor = torch.matmul(query_1, key.T)  # query_1 @ key_1\n",
    "\n",
    "# For all the tokens in the query\n",
    "attn_scores: Tensor = torch.matmul(query, key.T)\n",
    "\n",
    "print(f\"{attn_score_1.shape =} | {attn_score_1 = }\")\n",
    "print()\n",
    "print(f\"{attn_scores.shape = } | {attn_scores = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the scaled attention weights. It's scaled by the square root of\n",
    "# the dimension size to improve the training performance and avoid small gradients.\n",
    "attn_weights_1: Tensor = torch.softmax(attn_score_1 / (d_out**0.5), dim=-1)\n",
    "attn_weights: Tensor = torch.softmax(attn_scores / (d_out**0.5), dim=-1)\n",
    "\n",
    "\n",
    "print(f\"{attn_weights_1.shape =} | {attn_weights_1 = }\")\n",
    "print()\n",
    "print(f\"{attn_weights.shape = } | {attn_weights = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{attn_weights_1.shape = } | {value.shape = }\")\n",
    "print(f\"{attn_weights.shape = } | {value.shape = }\")\n",
    "\n",
    "context_vector_1: Tensor = attn_weights_1 @ value\n",
    "context_vector: Tensor = attn_weights @ value\n",
    "print()\n",
    "print(f\"{context_vector_1 = }\\n\\n\")\n",
    "\n",
    "print(f\"{context_vector = }\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query, Key and Value\n",
    "\n",
    "- **Query** : The query is analogous to a `search` in a `database`. It represents the current item/token the model focuses on.\n",
    "- **Key** : The key is analogous to the `index` in a `database`. It represents the item/token that the model compares the query to.\n",
    "- **Value** : The value is analogous to the `value` in a `key-value` pair. It represents the actual content or representation of the item/token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention_v1(nn.Module):\n",
    "    def __init__(self, d_in: int, d_out: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_out = d_out\n",
    "\n",
    "        self.W_query = nn.Parameter(torch.randn(d_in, d_out))\n",
    "        self.W_key = nn.Parameter(torch.randn(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.randn(d_in, d_out))\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        queries: Tensor = torch.matmul(x, self.W_query)\n",
    "        keys: Tensor = torch.matmul(x, self.W_key)\n",
    "        values: Tensor = torch.matmul(x, self.W_value)\n",
    "        attn_scores: Tensor = queries @ keys.T\n",
    "        attn_weights: Tensor = torch.softmax(attn_scores / self.d_out**0.5, dim=-1)\n",
    "        context_vector: Tensor = torch.matmul(attn_weights, values)\n",
    "        return context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "self_attn_v1 = SelfAttention_v1(d_in=d_in, d_out=d_out)\n",
    "print(f\"{self_attn_v1 = }\")\n",
    "print(self_attn_v1(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update\n",
    "\n",
    "- Improve the `SelfAttention_v1` implementation using PyTorch's `nn.Linear` layers instead of `nn.Parameter` layers.\n",
    "\n",
    "- This is because:\n",
    "  - `nn.Linear` performs effective matrix multiplication when the bias units are disabled.\n",
    "  - `nn.Linear` has a an optimized weight initialization scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention_v2(nn.Module):\n",
    "    def __init__(self, d_in: int, d_out: int, qkv_bias: bool = False) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_out = d_out\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        queries: Tensor = self.W_query(x)\n",
    "        keys: Tensor = self.W_key(x)\n",
    "        values: Tensor = self.W_value(x)\n",
    "        attn_scores: Tensor = queries @ keys.T\n",
    "        attn_weights: Tensor = torch.softmax(attn_scores / self.d_out**0.5, dim=-1)\n",
    "        context_vector: Tensor = torch.matmul(attn_weights, values)\n",
    "\n",
    "        return context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "self_attn_v2 = SelfAttention_v2(d_in=d_in, d_out=d_out, qkv_bias=False)\n",
    "print(f\"{self_attn_v2 = }\")\n",
    "print(self_attn_v2(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{self_attn_v1.W_query.shape = } | {self_attn_v2.W_query.weight.shape = }\")\n",
    "self_attn_v1.W_query, self_attn_v2.W_query.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Parameter(self_attn_v2.W_query.weight.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the weights of v2, generate the context vector using v1\n",
    "self_attn_v1.W_query = nn.Parameter(self_attn_v2.W_query.weight.T)\n",
    "self_attn_v1.W_key = nn.Parameter(self_attn_v2.W_key.weight.T)\n",
    "self_attn_v1.W_value = nn.Parameter(self_attn_v2.W_value.weight.T)\n",
    "\n",
    "self_attn_v1(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hidding Future Words With Causal Attention\n",
    "\n",
    "- Causal attention AKA Masked Attention.\n",
    "- It restricts the model to only attend to past and current tokens in the input sequence.\n",
    "\n",
    "<img src=\"../08-Makemore/images/causal-attention.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying A Causal Attention Mask\n",
    "\n",
    "- One way of obtaining the masked attention weight causally is to apply a softmax function to the attention scores, zeroing out the elements above the diagonal and normalizing the resulting matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "\n",
    "# Step 1: Calculate the weight matrices\n",
    "self_attn_v2 = SelfAttention_v2(d_in=d_in, d_out=d_out)\n",
    "\n",
    "# Step 2: Queries, Keys, and Values\n",
    "queries: Tensor = self_attn_v2.W_query(inputs)\n",
    "keys: Tensor = self_attn_v2.W_key(inputs)\n",
    "values: Tensor = self_attn_v2.W_value(inputs)\n",
    "\n",
    "print(f\"{queries.shape=} | {keys.shape=} | {values.shape=}\")\n",
    "\n",
    "# Step 3: Calculate the attention weights\n",
    "attn_scores: Tensor = torch.matmul(queries, keys.T)\n",
    "attn_weights: Tensor = F.softmax(attn_scores / self_attn_v2.d_out**0.5, dim=-1)\n",
    "attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Create a mask\n",
    "context_length: int = attn_scores.shape[1]\n",
    "mask_simple: Tensor = torch.tril(torch.ones(context_length, context_length))\n",
    "print(f\"mask_simple: \\n{mask_simple }\\n\")\n",
    "\n",
    "# Step 5: Multiply the mask with the attention weights\n",
    "masked_simple: Tensor = attn_weights * mask_simple\n",
    "print(f\"masked_simple: \\n{masked_simple }\\n\")\n",
    "\n",
    "# Step 6: Re-normaalize the attention weights\n",
    "row_sums: Tensor = masked_simple.sum(dim=1, keepdim=True)\n",
    "# Step 5: Multiply the mask with the attention weights\n",
    "masked_simple_norm: Tensor = masked_simple / row_sums\n",
    "print(f\"masked_simple_norm: \\n{masked_simple_norm }\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A More Efficient Implementation of Causal Self-Attention\n",
    "\n",
    "- Calculate the attention scores.\n",
    "- Mask with `-Inf` values above the diagonals.\n",
    "- Mask the attention scores.\n",
    "- Calculate the attention weights by applying softmax.\n",
    "- Calculate the context vector by multiplying the attention weights with the values vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a triangular matrix mask with ones above the diagonal\n",
    "# and zeros on and below the diagonal\n",
    "\n",
    "mask: Tensor = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "# Fill the elements with True with -inf and False with the actual values\n",
    "\n",
    "masked: Tensor = attn_scores.masked_fill(mask.bool(), float(\"-inf\"))\n",
    "\n",
    "# Normalize the attention scores\n",
    "attn_weights: Tensor = F.softmax(masked / keys.shape[-1] ** 0.5, dim=1)\n",
    "\n",
    "print(f\"mask: \\n{mask}\\n\")\n",
    "print(f\"mask.bool(): \\n{mask.bool()}\\n\")\n",
    "print(f\"masked: \\n{masked}\\n\")\n",
    "print(f\"attn_weights: \\n{attn_weights}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masking Additional Attention Weights With Dropout\n",
    "\n",
    "- `Dropout` in deep learning is a technique to prevent overfitting. It works by randomly turning off some neurons in a layer during training.\n",
    "- This forces the network to learn features that are independent of any specific neuron, making it more robust and adaptable to unseen data.\n",
    "- Dropout is `ONLY` used during training and turned of during evaluation or inference.\n",
    "- Dropout can be applied in transformer architectures in the following phases:\n",
    "  - after calculating the attention scores.\n",
    "  - after applying the softmax to normalize the attention scores (after computing the `attn_weights`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "dropout_pct: float = 0.5\n",
    "dropout = nn.Dropout(dropout_pct)\n",
    "example: Tensor = torch.ones(6, 6)\n",
    "print(f\"example: \\n{example}\\n\")\n",
    "print(f\"dropout: \\n{dropout(example)}\\n\")\n",
    "print(f\"dropout_attn_weight: \\n{dropout(attn_weights)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate batch inputs\n",
    "# 2 inputs with 6 tokens each and a dimension of 3: (2, 6, 3)\n",
    "batch_input: Tensor = torch.stack([inputs, inputs], dim=0)\n",
    "print(f\"{batch_input.shape = }\\n\")\n",
    "print(f\"batch_input: \\n{batch_input}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new: Tensor = torch.randn((2, 6, 3), dtype=torch.float32)\n",
    "print(f\"{new.shape = }\\n\")\n",
    "print(f\"new: {new}\\n\")\n",
    "print(f\"new.T: \\n{new.T.shape}\\n\")\n",
    "print(f\"new.transpose(-1, -2): \\n{new.transpose(-1, -2).shape}\\n\")\n",
    "print(\n",
    "    f\"Shape `new.transpose(-1, -2)`: {tuple(new.transpose(-1, -2).shape)}\\n{new.transpose(-1, -2)}\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_in: int,\n",
    "        d_out: int,\n",
    "        context_length: int,\n",
    "        dropout: float = 0.0,\n",
    "        qkv_bias: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "        # self.mask: Create a mask to prevent attention to the future tokens\n",
    "        self.register_buffer(\n",
    "            \"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        # Batch size, sequence length, input dimension\n",
    "        B, num_tokens, d_in = x.shape\n",
    "\n",
    "        queries: Tensor = self.W_query(x)\n",
    "        keys: Tensor = self.W_key(x)\n",
    "        values: Tensor = self.W_value(x)\n",
    "        # Switch the last 2 dimensions\n",
    "        attn_scores: Tensor = queries @ keys.transpose(-1, -2)\n",
    "        # Inplace operation\n",
    "        attn_scores.masked_fill_(\n",
    "            self.mask.bool()[:num_tokens, :num_tokens], float(\"-inf\")\n",
    "        )\n",
    "        attn_weights: Tensor = torch.softmax(attn_scores / self.d_out**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        context_vector: Tensor = attn_weights @ values\n",
    "\n",
    "        return context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "\n",
    "d_in: int = 3\n",
    "d_out: int = 2\n",
    "context_length: int = batch_input.shape[1]\n",
    "dropout: float = 0.0\n",
    "causal_attn = CausalAttention(\n",
    "    d_in=d_in,\n",
    "    d_out=d_out,\n",
    "    context_length=context_length,\n",
    "    dropout=dropout,\n",
    "    qkv_bias=False,\n",
    ")\n",
    "batch_input: Tensor = torch.stack([inputs, inputs], dim=0)\n",
    "print(f\"causal_attn: \\n{causal_attn}\\n\")\n",
    "\n",
    "context_vectors: Tensor = causal_attn(batch_input)\n",
    "print(f\"context_vectors: \\n{context_vectors.shape}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extending Single-head Attention To Multi-head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_p11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
