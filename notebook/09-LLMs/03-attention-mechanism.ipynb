{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Mechanism\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark -v -p numpy,pandas,polars,torch,lightning --conda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in library\n",
    "from pathlib import Path\n",
    "import re\n",
    "import json\n",
    "from typing import Any, Optional, Union\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "# Standard imports\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from rich.console import Console\n",
    "from rich.theme import Theme\n",
    "\n",
    "custom_theme = Theme(\n",
    "    {\n",
    "        \"info\": \"#76FF7B\",\n",
    "        \"warning\": \"#FBDDFE\",\n",
    "        \"error\": \"#FF0000\",\n",
    "    }\n",
    ")\n",
    "console = Console(theme=custom_theme)\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Pandas settings\n",
    "pd.options.display.max_rows = 1_000\n",
    "pd.options.display.max_columns = 1_000\n",
    "pd.options.display.max_colwidth = 600\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# Black code formatter (Optional)\n",
    "%load_ext lab_black\n",
    "\n",
    "# auto reload imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context Vector\n",
    "\n",
    "- Context vector is the weighted sum of the input vectors that captures the relevent information from the entire sequence for a given position. i.e. it can be thought of as an enriched embedding vector of the inout\n",
    "\n",
    "#### Calculate Context Vector\n",
    "\n",
    "- Attention Score: \n",
    "  - it's calculated by finding the dot product of the token's query vector and the key vector of the other tokens in the sequence.\n",
    "  - The scores are normalized using softmax to produce the attention weights.\n",
    "- Multiply the embedded input tokens with their corresponding attention weights and sum the resulting vectors to get the context vector.\n",
    "- This is done for each position in the sequence to get the context vector for the entire sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without Trainable Parameters (Simplified Version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed: int = 5\n",
    "\n",
    "# Assume that we have an input with a 3-D embeddings shown below:\n",
    "inputs: Tensor = torch.tensor(\n",
    "    [\n",
    "        [0.43, 0.15, 0.89],  # Your (x^1)\n",
    "        [0.55, 0.87, 0.66],  # journey (x^2)\n",
    "        [0.57, 0.85, 0.64],  # starts (x^3)\n",
    "        [0.22, 0.58, 0.33],  # with (x^4)\n",
    "        [0.77, 0.25, 0.10],  # one (x^5)\n",
    "        [0.05, 0.80, 0.55],  # step (x^6)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Calculate the context vector for the 2nd token (x^2)\n",
    "# 1: Cal the attention scores\n",
    "query: Tensor = inputs[1]\n",
    "attn_scores_index_1: Tensor = torch.empty(inputs.shape[0])\n",
    "\n",
    "for idx, x_1 in enumerate(inputs):\n",
    "    # Cal the dot product of the query vector and each key vector in the input\n",
    "    attn_scores_index_1[idx] = torch.dot(x_1, query)\n",
    "\n",
    "print(f\"{attn_scores_index_1 = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2: Normalize the attention scores to obtain the attention weights\n",
    "attn_scores_weights_1: Tensor = torch.softmax(attn_scores_index_1, dim=-1)\n",
    "print(f\"{attn_scores_weights_1 = }\")\n",
    "attn_scores_weights_1.sum(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_shape: tuple = tuple(inputs.shape)\n",
    "attn_scores_weights_1_shape: tuple = tuple(attn_scores_weights_1.shape)\n",
    "print(f\"{attn_scores_weights_1_shape = } AND {inputs_shape = }\")\n",
    "\n",
    "# 3: Calculate the context vector as the weighted average of the values\n",
    "# Transpose the inputs so that we can perform matrix multiplication\n",
    "context_vector_1: Tensor = attn_scores_weights_1 @ inputs\n",
    "context_vector_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate The Attention Weights Of The Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Calculate the attention scores\n",
    "print(f\"{inputs.shape = } AND {inputs.T.shape = }\")\n",
    "attn_scores: Tensor = inputs @ inputs.T\n",
    "print(f\"\\n{attn_scores.shape = }\")\n",
    "\n",
    "attn_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Calculate the attention weights. i.e. normalize the attention scores using softmax\n",
    "attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "print(f\"\\n{attn_weights.shape = }\")\n",
    "attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Calculate the context vector\n",
    "print(f\"{attn_weights.shape = } AND {inputs.shape = }\")\n",
    "\n",
    "context_vector: Tensor = attn_weights @ inputs\n",
    "context_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><hr>\n",
    "\n",
    "## Implement Self-Attention With Trainable Parameters\n",
    "\n",
    "- AKA **Scaled Dot-Product Attention**\n",
    "- Add weight matrices that are updated during training.\n",
    "- It's scaled by the square root of the dimension size to improve the training performance and avoid small gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the trainable attention weights for a given token in the input\n",
    "x_1: Tensor = inputs[1]\n",
    "print(f\"{x_1.shape = }\")\n",
    "# Embedding dimension\n",
    "d_in: int = x_1.shape[-1]\n",
    "# Output embedding size\n",
    "d_out: int = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "\n",
    "# Trainable parameters: requires_grad=False (to reduce the clutter and keep things simple)\n",
    "W_query: Tensor = nn.Parameter(torch.randn(d_in, d_out), requires_grad=False)\n",
    "W_key: Tensor = nn.Parameter(torch.randn(d_in, d_out), requires_grad=False)\n",
    "W_value: Tensor = nn.Parameter(torch.randn(d_in, d_out), requires_grad=False)\n",
    "\n",
    "# Compute the query, key, and value tensors for the given index\n",
    "query_1: Tensor = torch.matmul(x_1, W_query)\n",
    "key_1: Tensor = torch.matmul(x_1, W_key)\n",
    "value_1: Tensor = torch.matmul(x_1, W_value)\n",
    "\n",
    "# Compute the key and value tensors for ALL the input\n",
    "query: Tensor = torch.matmul(inputs, W_query)\n",
    "key: Tensor = torch.matmul(inputs, W_key)\n",
    "value: Tensor = torch.matmul(inputs, W_value)\n",
    "\n",
    "print(f\"{query_1.shape = }\")\n",
    "query_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{query_1.shape = } | {key_1.shape = } | {value_1.shape = }\")\n",
    "print(f\"{query.shape = } | {key.shape = } | {value.shape = }\")\n",
    "print()\n",
    "\n",
    "# Calculate the attention scores\n",
    "# For a single token in the query\n",
    "attn_score_1: Tensor = torch.matmul(query_1, key.T)  # query_1 @ key_1\n",
    "\n",
    "# For all the tokens in the query\n",
    "attn_scores: Tensor = torch.matmul(query, key.T)\n",
    "\n",
    "print(f\"{attn_score_1.shape = } | {attn_score_1 = }\")\n",
    "print()\n",
    "print(f\"{attn_scores.shape = } | {attn_scores = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the scaled attention weights. It's scaled by the square root of\n",
    "# the dimension size to improve the training performance and avoid small gradients.\n",
    "attn_weights_1: Tensor = torch.softmax(attn_score_1 / (d_out**0.5), dim=-1)\n",
    "attn_weights: Tensor = torch.softmax(attn_scores / (d_out**0.5), dim=-1)\n",
    "\n",
    "\n",
    "print(f\"{attn_weights_1.shape =} | {attn_weights_1 = }\")\n",
    "print()\n",
    "print(f\"{attn_weights.shape = } | {attn_weights = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{attn_weights_1.shape = } | {value.shape = }\")\n",
    "print(f\"{attn_weights.shape = } | {value.shape = }\")\n",
    "\n",
    "context_vector_1: Tensor = attn_weights_1 @ value\n",
    "context_vector: Tensor = attn_weights @ value\n",
    "print()\n",
    "print(f\"{context_vector_1 = }\\n\\n\")\n",
    "\n",
    "print(f\"{context_vector = }\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query, Key and Value\n",
    "\n",
    "- **Query** : The query is analogous to a `search` in a `database`. It represents the current item/token the model focuses on.\n",
    "- **Key** : The key is analogous to the `index` in a `database`. It represents the item/token that the model compares the query to.\n",
    "- **Value** : The value is analogous to the `value` in a `key-value` pair. It represents the actual content or representation of the item/token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention_v1(nn.Module):\n",
    "    def __init__(self, d_in: int, d_out: int, qkv_bias: bool = False) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_out = d_out\n",
    "\n",
    "        self.W_query = self._init_weights(d_in, d_out, qkv_bias)\n",
    "        self.W_key = self._init_weights(d_in, d_out, qkv_bias)\n",
    "        self.W_value = self._init_weights(d_in, d_out, qkv_bias)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        queries: Tensor = torch.matmul(x, self.W_query)\n",
    "        keys: Tensor = torch.matmul(x, self.W_key)\n",
    "        values: Tensor = torch.matmul(x, self.W_value)\n",
    "        attn_scores: Tensor = queries @ keys.T\n",
    "        attn_weights: Tensor = torch.softmax(attn_scores / self.d_out**0.5, dim=-1)\n",
    "        context_vector: Tensor = torch.matmul(attn_weights, values)\n",
    "        return context_vector\n",
    "\n",
    "    @staticmethod\n",
    "    def _init_weights(d_in: int, d_out: int, qkv_bias: bool = False) -> nn.Parameter:\n",
    "        \"\"\"This is used to initialize the weights.\"\"\"\n",
    "        if qkv_bias:\n",
    "            weight: Tensor = torch.randn(d_in, d_out) + torch.randn(d_out)\n",
    "        else:\n",
    "            weight = torch.randn(d_in, d_out)\n",
    "\n",
    "        return nn.Parameter(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "\n",
    "self_attn_v1 = SelfAttention_v1(d_in=d_in, d_out=d_out)\n",
    "print(f\"{self_attn_v1 = }\")\n",
    "print(self_attn_v1(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update\n",
    "\n",
    "- Improve the `SelfAttention_v1` implementation using PyTorch's `nn.Linear` layers instead of `nn.Parameter` layers.\n",
    "\n",
    "- This is because:\n",
    "  - `nn.Linear` performs effective matrix multiplication when the bias units are disabled.\n",
    "  - `nn.Linear` has a an optimized weight initialization scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention_v2(nn.Module):\n",
    "    def __init__(self, d_in: int, d_out: int, qkv_bias: bool = False) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_out = d_out\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        queries: Tensor = self.W_query(x)\n",
    "        keys: Tensor = self.W_key(x)\n",
    "        values: Tensor = self.W_value(x)\n",
    "        attn_scores: Tensor = queries @ keys.T\n",
    "        attn_weights: Tensor = torch.softmax(attn_scores / self.d_out**0.5, dim=-1)\n",
    "        context_vector: Tensor = torch.matmul(attn_weights, values)\n",
    "\n",
    "        return context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "self_attn_v2 = SelfAttention_v2(d_in=d_in, d_out=d_out, qkv_bias=False)\n",
    "print(f\"{self_attn_v2 = }\")\n",
    "print(self_attn_v2(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{self_attn_v1.W_query.shape = } | {self_attn_v2.W_query.weight.shape = }\")\n",
    "self_attn_v1.W_query, self_attn_v2.W_query.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Parameter(self_attn_v2.W_query.weight.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the weights of v2, generate the context vector using v1\n",
    "self_attn_v1.W_query = nn.Parameter(self_attn_v2.W_query.weight.T)\n",
    "self_attn_v1.W_key = nn.Parameter(self_attn_v2.W_key.weight.T)\n",
    "self_attn_v1.W_value = nn.Parameter(self_attn_v2.W_value.weight.T)\n",
    "\n",
    "self_attn_v1(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hidding Future Words With Causal Attention\n",
    "\n",
    "- Causal attention AKA Masked Attention.\n",
    "- It restricts the model to only attend to past and current tokens in the input sequence.\n",
    "\n",
    "<img src=\"../08-Makemore/images/causal-attention.png\" width=\"600\">\n",
    "\n",
    "[image source](https://livebook.manning.com/book/build-a-large-language-model-from-scratch/chapter-3/v-7/198)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying A Causal Attention Mask\n",
    "\n",
    "- One way of obtaining the masked attention weight causally is to apply a softmax function to the attention scores, zeroing out the elements above the diagonal and normalizing the resulting matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "\n",
    "# Step 1: Calculate the weight matrices\n",
    "self_attn_v2 = SelfAttention_v2(d_in=d_in, d_out=d_out)\n",
    "\n",
    "# Step 2: Queries, Keys, and Values\n",
    "queries: Tensor = self_attn_v2.W_query(inputs)\n",
    "keys: Tensor = self_attn_v2.W_key(inputs)\n",
    "values: Tensor = self_attn_v2.W_value(inputs)\n",
    "\n",
    "print(f\"{queries.shape=} | {keys.shape=} | {values.shape=}\")\n",
    "\n",
    "# Step 3: Calculate the attention weights\n",
    "attn_scores: Tensor = torch.matmul(queries, keys.T)\n",
    "attn_weights: Tensor = F.softmax(attn_scores / self_attn_v2.d_out**0.5, dim=-1)\n",
    "attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Create a mask\n",
    "context_length: int = attn_scores.shape[1]\n",
    "mask_simple: Tensor = torch.tril(torch.ones(context_length, context_length))\n",
    "print(f\"mask_simple: \\n{mask_simple }\\n\")\n",
    "\n",
    "# Step 5: Multiply the mask with the attention weights\n",
    "masked_simple: Tensor = attn_weights * mask_simple\n",
    "print(f\"masked_simple: \\n{masked_simple }\\n\")\n",
    "\n",
    "# Step 6: Re-normaalize the attention weights\n",
    "row_sums: Tensor = masked_simple.sum(dim=1, keepdim=True)\n",
    "# Step 5: Multiply the mask with the attention weights\n",
    "masked_simple_norm: Tensor = masked_simple / row_sums\n",
    "print(f\"masked_simple_norm: \\n{masked_simple_norm }\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A More Efficient Implementation of Causal Self-Attention\n",
    "\n",
    "- Calculate the attention scores.\n",
    "- Mask with `-Inf` values above the diagonals.\n",
    "- Mask the attention scores.\n",
    "- Calculate the attention weights by applying softmax.\n",
    "- Calculate the context vector by multiplying the attention weights with the values vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a triangular matrix mask with ones above the diagonal\n",
    "# and zeros on and below the diagonal\n",
    "\n",
    "mask: Tensor = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "# Fill the elements with True with -inf and False with the actual values\n",
    "\n",
    "masked: Tensor = attn_scores.masked_fill(mask.bool(), float(\"-inf\"))\n",
    "\n",
    "# Normalize the attention scores\n",
    "attn_weights: Tensor = F.softmax(masked / keys.shape[-1] ** 0.5, dim=1)\n",
    "\n",
    "print(f\"mask: \\n{mask}\\n\")\n",
    "print(f\"mask.bool(): \\n{mask.bool()}\\n\")\n",
    "print(f\"masked: \\n{masked}\\n\")\n",
    "print(f\"attn_weights: \\n{attn_weights}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masking Additional Attention Weights With Dropout\n",
    "\n",
    "- `Dropout` in deep learning is a technique to prevent overfitting. It works by randomly turning off some neurons in a layer during training.\n",
    "- This forces the network to learn features that are independent of any specific neuron, making it more robust and adaptable to unseen data.\n",
    "- Dropout is `ONLY` used during training and turned of during evaluation or inference.\n",
    "- Dropout can be applied in transformer architectures in the following phases:\n",
    "  - after calculating the attention scores.\n",
    "  - after applying the softmax to normalize the attention scores (after computing the `attn_weights`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "dropout_pct: float = 0.5\n",
    "dropout = nn.Dropout(dropout_pct)\n",
    "example: Tensor = torch.ones(6, 6)\n",
    "print(f\"example: \\n{example}\\n\")\n",
    "print(f\"dropout: \\n{dropout(example)}\\n\")\n",
    "print(f\"dropout_attn_weight: \\n{dropout(attn_weights)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate batch inputs\n",
    "# 2 inputs with 6 tokens each and a dimension of 3: (2, 6, 3)\n",
    "batch_input: Tensor = torch.stack([inputs, inputs], dim=0)\n",
    "print(f\"{batch_input.shape = }\\n\")\n",
    "print(f\"batch_input: \\n{batch_input}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new: Tensor = torch.randn((2, 6, 3), dtype=torch.float32)\n",
    "print(f\"{new.shape = }\\n\")\n",
    "print(f\"new: {new}\\n\")\n",
    "print(f\"new.T: \\n{new.T.shape}\\n\")\n",
    "print(f\"new.transpose(-1, -2): \\n{new.transpose(-1, -2).shape}\\n\")\n",
    "print(\n",
    "    f\"Shape `new.transpose(-1, -2)`: {tuple(new.transpose(-1, -2).shape)}\\n{new.transpose(-1, -2)}\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_in: int,\n",
    "        d_out: int,\n",
    "        context_length: int,\n",
    "        dropout: float = 0.0,\n",
    "        qkv_bias: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "        # self.mask: Create a mask to prevent attention to the future tokens\n",
    "        self.register_buffer(\n",
    "            \"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        # Batch size, sequence length, input dimension\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        queries: Tensor = self.W_query(x)\n",
    "        keys: Tensor = self.W_key(x)\n",
    "        values: Tensor = self.W_value(x)\n",
    "        # Switch the last 2 dimensions\n",
    "        attn_scores: Tensor = queries @ keys.transpose(-1, -2)\n",
    "        # Inplace operation\n",
    "        attn_scores.masked_fill_(\n",
    "            self.mask.bool()[:num_tokens, :num_tokens], float(\"-inf\")\n",
    "        )\n",
    "        attn_weights: Tensor = torch.softmax(attn_scores / self.d_out**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        context_vector: Tensor = attn_weights @ values\n",
    "\n",
    "        return context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "\n",
    "d_in: int = 3\n",
    "d_out: int = 2\n",
    "batch_input: Tensor = torch.stack([inputs, inputs], dim=0)\n",
    "context_length: int = batch_input.shape[1]\n",
    "dropout: float = 0.0\n",
    "\n",
    "causal_attn = CausalAttention(\n",
    "    d_in=d_in,\n",
    "    d_out=d_out,\n",
    "    context_length=context_length,\n",
    "    dropout=dropout,\n",
    "    qkv_bias=False,\n",
    ")\n",
    "print(f\"causal_attn: \\n{causal_attn}\\n\")\n",
    "\n",
    "context_vectors: Tensor = causal_attn(batch_input)\n",
    "print(f\"context_vectors: \\n{context_vectors.shape}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extending Single-head Attention To Multi-head Attention\n",
    "\n",
    "<img src=\"../08-Makemore/images/multi-head.png\" width=\"600\">\n",
    "\n",
    "[image source](https://livebook.manning.com/book/build-a-large-language-model-from-scratch/chapter-3/v-7/282)\n",
    "\n",
    "<br>\n",
    "\n",
    "```text\n",
    "Source: Gemini\n",
    "\n",
    "Multi-head attention is a core component in Transformer models that allows them to focus on specific parts of an input sequence. Here's a breakdown:\n",
    "\n",
    "- Attention mechanism: It attends to relevant parts of the sequence, like focusing on specific words in a sentence.\n",
    "\n",
    "- Multiple heads: Instead of just one attention mechanism, it has multiple \"heads\" working in parallel.\n",
    "Different perspectives: Each head learns to attend to the sequence from a slightly different angle, capturing various relationships between words.\n",
    "\n",
    "- Combined power: The outputs from all heads are combined, giving the model a richer understanding of the sequence.\n",
    "\n",
    "Think of it like having multiple analysts examining the same text. Each analyst focuses on slightly different aspects, and by combining their insights, you get a more comprehensive understanding of the content. This allows Transformers to deal with complex relationships within sequential data.\n",
    "```\n",
    "\n",
    "- The goal of multi-head attention is to run the attention mechanism in parallel with different, learned linear projections (i.e. the result of multiplying the query, key and value matrix by a learned matrix).\n",
    "\n",
    "- In code, it can be achieved by implementing a simple `MultiHeadAttentionWrapper` class that stacks multiple instances of the previously implemented `CausalAttention` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_in: int,\n",
    "        d_out: int,\n",
    "        context_length: int,\n",
    "        num_heads: int,\n",
    "        dropout: float = 0.1,\n",
    "        qkv_bias: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.heads = nn.ModuleList(\n",
    "            [\n",
    "                CausalAttention(d_in, d_out, context_length, dropout, qkv_bias)\n",
    "                for _ in range(num_heads)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"Process the data `sequentially`.\"\"\"\n",
    "        return torch.cat([head(x) for head in self.heads], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we use the `MultiHeadAttentionWrapper` class with 2 attention heads (i.e. num_heads=2) and\n",
    "# `CausalAttention` output dimension (d_out=2), this results in a 4 dimensional context vector.\n",
    "# i.e. ( d_out * num_heads = 4 )\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "d_in: int = 3\n",
    "d_out: int = 2\n",
    "batch_input: Tensor = torch.stack([inputs, inputs], dim=0)\n",
    "context_length: int = batch_input.shape[1]\n",
    "dropout: float = 0.0\n",
    "\n",
    "multi_head_attn = MultiHeadAttentionWrapper(\n",
    "    d_in=d_in,\n",
    "    d_out=d_out,\n",
    "    context_length=context_length,\n",
    "    num_heads=2,\n",
    "    dropout=dropout,\n",
    "    qkv_bias=False,\n",
    ")\n",
    "print(f\"multi_head_attn: \\n{multi_head_attn}\\n\")\n",
    "\n",
    "context_vectors: Tensor = multi_head_attn(batch_input)\n",
    "print(f\"context_vectors: \\n{context_vectors.shape}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape: (2, 6, 4). The 1st dimension of the result is 2 because we have 2 input texts which\n",
    "# have been duplicated. The 2nd dimension is 6 because we have 6 tokens in each input. The\n",
    "# 3rd dimension is 4 because we have 4-dimensional embedding of each token.\n",
    "context_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment\n",
    "\n",
    "- Shape: (2, 6, 4). The 1st dimension of the result is 2 because we have 2 input texts which have been duplicated.\n",
    "- The 2nd dimension is 6 because we have 6 tokens in each input. \n",
    "- The 3rd dimension is 4 because we have 4-dimensional embedding of each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ex 3.2 (Return 2-dimensional embedding vectors)\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "d_in: int = 3\n",
    "d_out: int = 1\n",
    "batch_input: Tensor = torch.stack([inputs, inputs], dim=0)\n",
    "context_length: int = batch_input.shape[1]\n",
    "dropout: float = 0.0\n",
    "\n",
    "multi_head_attn = MultiHeadAttentionWrapper(\n",
    "    d_in=d_in,\n",
    "    d_out=d_out,\n",
    "    context_length=context_length,\n",
    "    num_heads=2,\n",
    "    dropout=dropout,\n",
    "    qkv_bias=False,\n",
    ")\n",
    "print(f\"Shape of batch_input: \\n{batch_input.shape}\\n\")\n",
    "print(f\"multi_head_attn: \\n{multi_head_attn}\\n\")\n",
    "\n",
    "context_vectors: Tensor = multi_head_attn(batch_input)\n",
    "print(f\"Shape of context_vectors: \\n{context_vectors.shape}\\n\")\n",
    "print(f\"context_vectors: \\n{context_vectors}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### implementing Multi-head Attention With Weight Splits\n",
    "\n",
    "- Efficient implementation of `MultiHeadAttentionWrapper` class.\n",
    "- The inputs are split into multiple heads by reshaping the projected query, key and value tensors and then combines the results from these heads after computing attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_in: int,\n",
    "        d_out: int,\n",
    "        context_length: int,\n",
    "        dropout: float,\n",
    "        num_heads: int,\n",
    "        qkv_bias: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"`d_out` should be divisible by `num_heads`\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        self.register_buffer(\n",
    "            \"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        # Batch size, sequence length, input dimension\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        queries: Tensor = self.W_query(x)\n",
    "        keys: Tensor = self.W_key(x)\n",
    "        values: Tensor = self.W_value(x)\n",
    "\n",
    "        # Reshape and transpose the data\n",
    "        # Split the matrix by adding `num_heads` dimension and unroll the last dimension\n",
    "        # i.e. [b, num_tokens, d_out] -> [b, num_tokens, num_heads, head_dim]\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim).transpose(\n",
    "            1, 2\n",
    "        )\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim).transpose(\n",
    "            1, 2\n",
    "        )\n",
    "\n",
    "        attn_scores: Tensor = torch.matmul(queries, keys.transpose(-2, -1))\n",
    "        # Mask truncated to the number of tokens and and use the mask to fill the attention scores\n",
    "        mask_bool: Tensor = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        # Compute the attention weights\n",
    "        attn_weights: Tensor = F.softmax(attn_scores / keys.shape[-1] ** 0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        context_vector: Tensor = torch.matmul(attn_weights, values).transpose(1, 2)\n",
    "\n",
    "        # Combine the heads\n",
    "        context_vector = context_vector.contiguous().view(b, num_tokens, self.d_out)\n",
    "        # Apply optional linear output projection\n",
    "        context_vector = self.out_proj(context_vector)\n",
    "        return context_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment\n",
    "\n",
    "\n",
    "<img src=\"../08-Makemore/images/ch03_multihead_attn.png\" width=600>\n",
    "\n",
    "[image source](https://livebook.manning.com/book/build-a-large-language-model-from-scratch/chapter-3/v-7/304)\n",
    "\n",
    "- In the `MultiheadAttention` class, we initialize one larger weight matrix `Wq` , only perform one matrix multiplication with the inputs to obtain a query matrix `Q`, and then split the query matrix into `Q1` and `Q2`, as shown at the top of this figure.\n",
    "\n",
    "- The same is done for the keys and values, which are not shown to reduce visual clutter.\n",
    "\n",
    "- The splitting of the query, key and value tensors as shown in the diagram above is achieved through tensor reshaping and transposing operations using `view` and `transpose` methods.\n",
    "\n",
    "- The key operation is to split the `d_out` dimension into `num_heads` and `head_dim` where head_dim = d_out / num_heads.\n",
    "\n",
    "- The splitting is then achieved using the `.view` method. i.e. (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim).\n",
    "\n",
    "- The tensors are transposed such that `num_heads` dimension is moved to the front. i.e. (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim).\n",
    "\n",
    "- This is important for correctly aligning the queries, keys, and values across the different heads and performing batched matrix multiplications efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ex. to illustrate batched matrix multiplication\n",
    "A: Tensor = torch.tensor(\n",
    "    [\n",
    "        [\n",
    "            [\n",
    "                [0.2745, 0.6584, 0.2775, 0.8573],\n",
    "                [0.8993, 0.0390, 0.9268, 0.7388],\n",
    "                [0.7179, 0.7058, 0.9156, 0.4340],\n",
    "            ],\n",
    "            [\n",
    "                [0.0772, 0.3565, 0.1479, 0.5331],\n",
    "                [0.4066, 0.2318, 0.4545, 0.9737],\n",
    "                [0.4606, 0.5159, 0.4220, 0.5786],\n",
    "            ],\n",
    "        ]\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"A.shape: {tuple(A.shape)}\")\n",
    "print(f\"A.transpose(2, 3).shape: {tuple(A.transpose(2, 3).shape)}\")\n",
    "\n",
    "# (1, 2, 3, 4): (batch_size, num_heads, sequence_length, dim_size)\n",
    "# (.., .., 3, 4) @ (.., .., 4, 3) i.e. transpose the last two dimensions\n",
    "# A.transpose(2, 3) --> (.., .., 4, 3) i.e. transpose the last two dimensions (indexes 2 and 3)\n",
    "# Matrix multiplication: (1, 2, 3, 4) @ (1, 2, 4, 3)\n",
    "A @ A.transpose(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_head: Tensor = A[0, 0, :, :]\n",
    "first_res: Tensor = first_head @ first_head.T  # first_head @ first_head.transpose(0, 1)\n",
    "print(f\"first_head.shape: {tuple(first_head.shape)}\")\n",
    "print(f\"first_res: \\n{first_res}\\n\")\n",
    "\n",
    "second_head: Tensor = A[0, 1, :, :]\n",
    "second_res: Tensor = second_head @ second_head.T\n",
    "print(f\"second_head.shape: {tuple(second_head.shape)}\")\n",
    "print(f\"second_res: \\n{second_res}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment\n",
    "\n",
    "- After the computation of the attention weights and the context vectors, the context vectors from all the heads are transposed back to the shape (b, num_tokens, num_heads, head_dim).\n",
    "- They are then concatenated (flattened) into the shape (b, num_tokens, d_out) effectively combining the outputs from all the heads.\n",
    "- An optional output projection layer (`self.out_proj`) is added after combinng the heads.\n",
    "- The `MultiHeadAttention` class is more complicated than the `MultiHeadAttentionWrapper` class due to the additional reshaping and transposition of the tensors, it's more efficient.\n",
    "- It's more efficient because it needs ONLY one matrix multiplication to compute the queries, keys, and values.\n",
    "- In `MultiHeadAttentionWrapper`, we need to repeat this multiplication `num_heads` times which is computationally expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "\n",
    "\n",
    "batch_input: Tensor = torch.stack([inputs, inputs], dim=0)\n",
    "batch_size, context_length, d_in = batch_input.shape\n",
    "d_out: int = 2\n",
    "dropout: float = 0.0\n",
    "\n",
    "multi_head_attn = MultiHeadAttention(\n",
    "    d_in=d_in,\n",
    "    d_out=d_out,\n",
    "    context_length=context_length,\n",
    "    num_heads=2,\n",
    "    dropout=dropout,\n",
    "    qkv_bias=False,\n",
    ")\n",
    "\n",
    "print(f\"Shape of batch_input: \\n{batch_input.shape}\\n\")\n",
    "print(f\"multi_head_attn: \\n{multi_head_attn}\\n\")\n",
    "\n",
    "context_vectors: Tensor = multi_head_attn(batch_input)\n",
    "print(f\"Shape of context_vectors: \\n{context_vectors.shape}\\n\")\n",
    "print(f\"context_vectors: \\n{context_vectors}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_p11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
