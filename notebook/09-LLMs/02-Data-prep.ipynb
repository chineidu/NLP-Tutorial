{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation For LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python implementation: CPython\n",
      "Python version       : 3.11.8\n",
      "IPython version      : 8.22.2\n",
      "\n",
      "numpy    : 1.26.4\n",
      "pandas   : 2.2.1\n",
      "polars   : 0.20.18\n",
      "torch    : 2.2.2\n",
      "lightning: 2.2.1\n",
      "\n",
      "conda environment: torch_p11\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -v -p numpy,pandas,polars,torch,lightning --conda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in library\n",
    "from pathlib import Path\n",
    "import re\n",
    "import json\n",
    "from typing import Any, Optional, Union\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "# Standard imports\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from rich.console import Console\n",
    "from rich.theme import Theme\n",
    "\n",
    "custom_theme = Theme(\n",
    "    {\n",
    "        \"info\": \"#76FF7B\",\n",
    "        \"warning\": \"#FBDDFE\",\n",
    "        \"error\": \"#FF0000\",\n",
    "    }\n",
    ")\n",
    "console = Console(theme=custom_theme)\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Pandas settings\n",
    "pd.options.display.max_rows = 1_000\n",
    "pd.options.display.max_columns = 1_000\n",
    "pd.options.display.max_colwidth = 600\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# Black code formatter (Optional)\n",
    "%load_ext lab_black\n",
    "\n",
    "# auto reload imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characers: 20,479\n",
      "\n",
      "\n",
      "The first 100 characters: ========================================\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no g\n"
     ]
    }
   ],
   "source": [
    "## Load the data\n",
    "fp: str = \"../../data/the-verdict.txt\"\n",
    "\n",
    "with open(fp, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = f.read()\n",
    "\n",
    "print(f\"Total number of characers: {len(data):,}\\n\\n\")\n",
    "print(f\"The first 100 characters: {'====' * 10}\\n{data[:100]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " ' ',\n",
       " 'HAD',\n",
       " ' ',\n",
       " 'always',\n",
       " ' ',\n",
       " 'thought',\n",
       " ' ',\n",
       " 'Jack',\n",
       " ' ',\n",
       " 'Gisburn',\n",
       " ' ',\n",
       " 'rather',\n",
       " ' ',\n",
       " 'a',\n",
       " ' ',\n",
       " 'cheap',\n",
       " ' ',\n",
       " 'genius',\n",
       " '--',\n",
       " 'though',\n",
       " ' ',\n",
       " 'a',\n",
       " ' ',\n",
       " 'good',\n",
       " ' ',\n",
       " 'fellow',\n",
       " ' ',\n",
       " 'enough',\n",
       " '--',\n",
       " 'so',\n",
       " ' ',\n",
       " 'it',\n",
       " ' ',\n",
       " 'was',\n",
       " ' ',\n",
       " 'no',\n",
       " ' ',\n",
       " 'g']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the text on white spaces and punctuation. The words are intentionally NOT normalized.\n",
    "# This is because it enables the LLM to differentiate between proper and regular nouns, etc.\n",
    "text: str = data[:100]\n",
    "pattern: str = r'([,.?_!\"()\\']|--|\\s)'\n",
    "re.split(pattern=pattern, string=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'HAD',\n",
       " 'always',\n",
       " 'thought',\n",
       " 'Jack',\n",
       " 'Gisburn',\n",
       " 'rather',\n",
       " 'a',\n",
       " 'cheap',\n",
       " 'genius',\n",
       " '--',\n",
       " 'though',\n",
       " 'a',\n",
       " 'good',\n",
       " 'fellow',\n",
       " 'enough',\n",
       " '--',\n",
       " 'so',\n",
       " 'it',\n",
       " 'was',\n",
       " 'no',\n",
       " 'g']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove whitespaces\n",
    "preprocessed: list[str] = re.split(pattern=pattern, string=text)\n",
    "preprocessed = [ch for ch in preprocessed if ch.strip()]\n",
    "preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4649, 20479)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The entire data\n",
    "# Remove whitespaces\n",
    "preprocessed: list[str] = re.split(pattern=pattern, string=data)\n",
    "preprocessed = [ch for ch in preprocessed if ch.strip()]\n",
    "len(preprocessed), len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vocabulary. i.e. a dict containing all the distinct words mapped to unique ineger values.\n",
    "unk_token: str = \"<|unk|>\"\n",
    "vocab: dict[str, any] = {\n",
    "    ch: idx for idx, ch in enumerate(sorted(set(preprocessed)), start=1)\n",
    "}\n",
    "vocab[unk_token] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0, 739, 1014, 0, 739, 0, 0, 0, 6, 1020, 0, 120, 0, 0, 739, 0, 0, 4, 3, 0, 6, 0, 5'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert tokens to IDs (encode)\n",
    "text: str = (\n",
    "    \"Because of the scale of many ML systems, they consume a massive amount of data - ('Neidu, 2024)\"\n",
    ")\n",
    "tok_text: list[str] = re.split(pattern=pattern, string=text)\n",
    "tok_text = [ch for ch in tok_text if ch.strip()]\n",
    "tok_IDs: list[int] = [vocab.get(ch, 0) for ch in tok_text]\n",
    "\n",
    "\", \".join([str(ch) for ch in tok_IDs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<|unk|> of the <|unk|> of <|unk|> <|unk|> <|unk|>, they <|unk|> a <|unk|> <|unk|> of <|unk|> <|unk|>(' <|unk|>, <|unk|>)\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert token IDs back to tokens\n",
    "idx_to_text: dict[int, str] = {idx: ch for ch, idx in vocab.items()}\n",
    "\n",
    "res: list[str] = [idx_to_text.get(idx) for idx in tok_IDs]\n",
    "\n",
    "# Remove the whitespaces after punctuation\n",
    "pattern_1: str = r'\\s+([,.?!\"()\\'])'\n",
    "res: str = \" \".join(res)\n",
    "res = re.sub(pattern=pattern_1, repl=r\"\\1\", string=res)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    \"\"\"\n",
    "    A simple tokenizer that splits text into tokens based on a predefined vocabulary.\n",
    "\n",
    "    The `SimpleTokenizerV1` class provides methods to encode text into a list of token IDs and decode a list\n",
    "    of token IDs back into text. It uses a predefined vocabulary to map between tokens and their corresponding IDs.\n",
    "\n",
    "    Args:\n",
    "        vocab (dict[str, int]): A dictionary mapping tokens to their corresponding IDs.\n",
    "\n",
    "    Methods:\n",
    "        encode(text: str) -> list[int]:\n",
    "            Tokenize a string into a list of token IDs.\n",
    "        decode(tok_IDs: list[int]) -> str:\n",
    "            Convert a list of token IDs back into a string.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab: dict[str, int]):\n",
    "        self.vocab = vocab\n",
    "        self.pattern_1: str = r'([,.?_!\"()\\']|--|\\s)'\n",
    "        self.pattern_2: str = r'\\s+([,.?!\"()\\'])'\n",
    "        self.idx_to_text: dict[int, str] = {idx: ch for ch, idx in self.vocab.items()}\n",
    "\n",
    "    def encode(self, text: str) -> list[int]:\n",
    "        \"\"\"Tokenize a string into a list of tokens.\"\"\"\n",
    "        tok_text: list[str] = re.split(pattern=self.pattern_1, string=text)\n",
    "        tok_text = [ch for ch in tok_text if ch.strip()]\n",
    "        tok_IDs: list[int] = [vocab.get(ch, 0) for ch in tok_text]\n",
    "        return tok_IDs\n",
    "\n",
    "    def decode(self, tok_IDs: list[int]) -> str:\n",
    "        \"\"\"Convert a list of tokens into a string.\"\"\"\n",
    "        text: str = \" \".join([self.idx_to_text.get(idx) for idx in tok_IDs])\n",
    "        text = re.sub(pattern=self.pattern_2, repl=r\"\\1\", string=text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 596, 1014, 518, 0, 580, 1014, 0, 11]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text: str = \"Who is the greatest striker in the world?\"\n",
    "tokenizer: SimpleTokenizerV1 = SimpleTokenizerV1(vocab=vocab)\n",
    "tok_IDs: list[int] = tokenizer.encode(text)\n",
    "tok_IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|unk|> is the greatest <|unk|> in the <|unk|>?'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tok_IDs=tok_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Byte Pair Encoding\n",
    "\n",
    "```sh\n",
    "pip install tiktoken\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8241, 318, 262, 6000, 19099, 287, 262, 995, 30]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "tok_IDs: list[int] = tokenizer.encode(text)\n",
    "tok_IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Who is the greatest striker in the world?'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tok_IDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_p11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
