{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation For LLMs\n",
    "\n",
    "- Preparing the input text for an LLM involves:\n",
    "  - tokenizing the text\n",
    "  - converting the tokens into integers (IDs)\n",
    "  - converting the integers into vector embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark -v -p numpy,pandas,polars,torch,lightning --conda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in library\n",
    "from pathlib import Path\n",
    "import re\n",
    "import json\n",
    "from typing import Any, Optional, Union\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "# Standard imports\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from rich.console import Console\n",
    "from rich.theme import Theme\n",
    "\n",
    "custom_theme = Theme(\n",
    "    {\n",
    "        \"info\": \"#76FF7B\",\n",
    "        \"warning\": \"#FBDDFE\",\n",
    "        \"error\": \"#FF0000\",\n",
    "    }\n",
    ")\n",
    "console = Console(theme=custom_theme)\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Pandas settings\n",
    "pd.options.display.max_rows = 1_000\n",
    "pd.options.display.max_columns = 1_000\n",
    "pd.options.display.max_colwidth = 600\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# Black code formatter (Optional)\n",
    "%load_ext lab_black\n",
    "\n",
    "# auto reload imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the data\n",
    "fp: str = \"../../data/the-verdict.txt\"\n",
    "\n",
    "with open(fp, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = f.read()\n",
    "\n",
    "print(f\"Total number of characers: {len(data):,}\\n\\n\")\n",
    "print(f\"The first 100 characters: {'====' * 10}\\n{data[:100]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the text on white spaces and punctuation. The words are intentionally NOT normalized.\n",
    "# This is because it enables the LLM to differentiate between proper and regular nouns, etc.\n",
    "text: str = data[:100]\n",
    "pattern: str = r'([,.?_!\"()\\']|--|\\s)'\n",
    "re.split(pattern=pattern, string=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove whitespaces\n",
    "preprocessed: list[str] = re.split(pattern=pattern, string=text)\n",
    "preprocessed = [ch for ch in preprocessed if ch.strip()]\n",
    "preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The entire data\n",
    "# Remove whitespaces\n",
    "preprocessed: list[str] = re.split(pattern=pattern, string=data)\n",
    "preprocessed = [ch for ch in preprocessed if ch.strip()]\n",
    "len(preprocessed), len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vocabulary. i.e. a dict containing all the distinct words mapped to unique ineger values. (bag of words)\n",
    "unk_token: str = \"<|unk|>\"\n",
    "end_of_text: str = \"<|endoftext|>\"\n",
    "vocab: dict[str, any] = {\n",
    "    ch: idx for idx, ch in enumerate(sorted(set(preprocessed)), start=0)\n",
    "}\n",
    "vocab[unk_token] = len(vocab) + 1\n",
    "vocab[end_of_text] = len(vocab) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tokens to IDs (encode)\n",
    "text: str = (\n",
    "    \"Because of the scale of many ML systems, they consume a massive amount of data - ('Neidu, 2024)\"\n",
    ")\n",
    "tok_text: list[str] = re.split(pattern=pattern, string=text)\n",
    "tok_text = [ch for ch in tok_text if ch.strip()]\n",
    "tok_IDs: list[int] = [\n",
    "    vocab.get(ch) if ch in vocab else vocab.get(unk_token) for ch in tok_text\n",
    "]\n",
    "\n",
    "\", \".join([str(ch) for ch in tok_IDs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert token IDs back to tokens\n",
    "idx_to_text: dict[int, str] = {idx: ch for ch, idx in vocab.items()}\n",
    "\n",
    "res: list[str] = [idx_to_text.get(idx) for idx in tok_IDs]\n",
    "\n",
    "# Remove the whitespaces after punctuation\n",
    "pattern_1: str = r'\\s+([,.?!\"()\\'])'\n",
    "res: str = \" \".join(res)\n",
    "res = re.sub(pattern=pattern_1, repl=r\"\\1\", string=res)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    \"\"\"\n",
    "    A simple tokenizer that splits text into tokens based on a predefined vocabulary.\n",
    "\n",
    "    The `SimpleTokenizerV1` class provides methods to encode text into a list of token IDs and decode a list\n",
    "    of token IDs back into text. It uses a predefined vocabulary to map between tokens and their corresponding IDs.\n",
    "\n",
    "    Args:\n",
    "        vocab (dict[str, int]): A dictionary mapping tokens to their corresponding IDs.\n",
    "\n",
    "    Methods:\n",
    "        encode(text: str) -> list[int]:\n",
    "            Tokenize a string into a list of token IDs.\n",
    "        decode(tok_IDs: list[int]) -> str:\n",
    "            Convert a list of token IDs back into a string.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab: dict[str, int]):\n",
    "        self.vocab = vocab\n",
    "        self.pattern_1: str = r'([,.?_!\"()\\']|--|\\s)'\n",
    "        self.pattern_2: str = r'\\s+([,.?!\"()\\'])'\n",
    "        self.idx_to_text: dict[int, str] = {idx: ch for ch, idx in self.vocab.items()}\n",
    "\n",
    "    def encode(self, text: str) -> list[int]:\n",
    "        \"\"\"Tokenize a string into a list of tokens.\"\"\"\n",
    "        unk_token: str = \"<|unk|>\"\n",
    "        tok_text: list[str] = re.split(pattern=self.pattern_1, string=text)\n",
    "        tok_text = [ch for ch in tok_text if ch.strip()]\n",
    "        tok_IDs: list[int] = [\n",
    "            vocab.get(ch) if ch in vocab else vocab.get(unk_token) for ch in tok_text\n",
    "        ]\n",
    "        return tok_IDs\n",
    "\n",
    "    def decode(self, tok_IDs: list[int]) -> str:\n",
    "        \"\"\"Convert a list of tokens into a string.\"\"\"\n",
    "        text: str = \" \".join([self.idx_to_text.get(idx) for idx in tok_IDs])\n",
    "        # Clean up the spaces around punctuation\n",
    "        text = re.sub(pattern=self.pattern_2, repl=r\"\\1\", string=text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text: str = \"Who is the greatest striker in the world?\"\n",
    "tokenizer: SimpleTokenizerV1 = SimpleTokenizerV1(vocab=vocab)\n",
    "tok_IDs: list[int] = tokenizer.encode(text)\n",
    "tok_IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(tok_IDs=tok_IDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1: str = \"Hello, do you like tea?\"\n",
    "text2: str = \"In the sunlit terraces of the palace.\"\n",
    "text: str = \" <|endoftext|> \".join((text1, text2))\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "print(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(tokenizer.encode(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Byte Pair Encoding\n",
    "\n",
    "```sh\n",
    "pip install tiktoken\n",
    "```\n",
    "\n",
    "- It encodes unknown words properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "\n",
    "text: str = \"Who is the greatest striker in the world?\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "tok_IDs: list[int] = tokenizer.encode(text)\n",
    "tok_IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(tok_IDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_of_text: str = \"<|endoftext|>\"\n",
    "\n",
    "text: str = f\"Who is the greatest striker in the world? {end_of_text} AI is booming!\"\n",
    "tok_IDs: list[int] = tokenizer.encode(text, allowed_special={end_of_text})\n",
    "tok_IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(tok_IDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How does BPE handle unknown workds/tokens??\n",
    "text: str = \"ChineiduTheGreat\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "tok_IDs: list[int] = tokenizer.encode(text, allowed_special={end_of_text})\n",
    "tok_IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BPE breaks down unkowned tokens into subwords and individual characters. This prevents BPE from replacing\n",
    "# unknown tokens with a special token sunch as <|unk|>\n",
    "(\n",
    "    tokenizer.decode([1925]),  # Ch\n",
    "    tokenizer.decode([500]),  # ine\n",
    "    tokenizer.decode([312]),  # id\n",
    "    tokenizer.decode([84]),  # u\n",
    "    tokenizer.decode([464]),  # The\n",
    "    tokenizer.decode([13681]),  # Great\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Sampling With A Sliding Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the entire data using BPE\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "tok_data: list[int] = tokenizer.encode(data)\n",
    "\n",
    "tok_data[:5], len(tok_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create input-target pairs for the next-word prediction\n",
    "enc_sample: list[int] = tok_data[:50]\n",
    "context_size: int = 4\n",
    "x: list[int] = enc_sample[:context_size]\n",
    "y: list[int] = enc_sample[1 : context_size + 1]\n",
    "\n",
    "print(f\"{x = }\")\n",
    "print(f\"{y = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(1, context_size + 1):\n",
    "    print(f\"{enc_sample[:idx]} ---> {enc_sample[idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(1, context_size + 1):\n",
    "    print(\n",
    "        f\"{tokenizer.decode(enc_sample[:idx])} ---> {tokenizer.decode([enc_sample[idx]])}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Loader Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class GPTDataset(Dataset):\n",
    "    def __init__(self, text: str, tokenizer: Any, max_length: int, stride: int):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "        token_ids = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "        print(f\"{len(token_ids) = :,}\")\n",
    "\n",
    "        for idx in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk: list[int] = token_ids[idx : (idx + max_length)]\n",
    "            target_chunk: list[int] = token_ids[idx + 1 : (idx + max_length + 1)]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> tuple[torch.Tensor, ...]:\n",
    "        x = self.input_ids[idx]\n",
    "        y = self.target_ids[idx]\n",
    "        return (x, y)\n",
    "\n",
    "\n",
    "def create_dataloader(\n",
    "    text: str,\n",
    "    batch_size: int = 4,\n",
    "    max_length: int = 256,\n",
    "    stride: int = 128,\n",
    "    shuffle: bool = True,\n",
    "    drop_last: bool = True,\n",
    ") -> DataLoader:\n",
    "    \"\"\"Create a dataloader for the given text data.\"\"\"\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset: Dataset = GPTDataset(\n",
    "        text=text, tokenizer=tokenizer, max_length=max_length, stride=stride\n",
    "    )\n",
    "    dataloader = DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last\n",
    "    )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "dataset_ = GPTDataset(text=data[:30], tokenizer=tokenizer, max_length=4, stride=1)\n",
    "print(f\"{dataset_.input_ids=}, \\n{dataset_.target_ids=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ = GPTDataset(text=data[:30], tokenizer=tokenizer, max_length=4, stride=2)\n",
    "print(f\"{dataset_.input_ids=}, \\n{dataset_.target_ids=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = create_dataloader(\n",
    "    text=data, batch_size=4, max_length=4, stride=1, shuffle=False, drop_last=False\n",
    ")\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "\n",
    "first_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_batch = next(data_iter)\n",
    "second_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using stride=2\n",
    "stride: int = 2\n",
    "\n",
    "dataloader = create_dataloader(\n",
    "    text=data, batch_size=8, max_length=4, stride=stride, shuffle=False, drop_last=False\n",
    ")\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "\n",
    "console.print(f\"{inputs=}, \\n{targets=}\")\n",
    "\n",
    "# e.g. slide by 2 index positions\n",
    "# [   40,   367,  2885,  1464],\n",
    "# [ 2885,  1464,  1807,  3619]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token Embeddings\n",
    "\n",
    "- Preparing the input text for an LLM involves:\n",
    "  - tokenizing the text\n",
    "  - converting the tokens into integers (IDs)\n",
    "  - converting the integers into vector embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the embedding weights with random values which will be optimized during training.\n",
    "input_ids: Tensor = torch.tensor([2, 3, 5, 1])\n",
    "\n",
    "torch.manual_seed(42)\n",
    "vocab_size: int = 6\n",
    "output_dim: int = 3\n",
    "embedding_layer = nn.Embedding(vocab_size, output_dim)  # lookup table\n",
    "print(f\"{embedding_layer.weight.shape=}\\n\")\n",
    "print(embedding_layer.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create it manually!\n",
    "torch.manual_seed(42)\n",
    "torch.randn(vocab_size, output_dim, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed the input\n",
    "print(f\"{embedding_layer(torch.tensor([3]))}\")\n",
    "\n",
    "# OR (using matrix multiplication)\n",
    "res: Tensor = (\n",
    "    F.one_hot(torch.tensor([3]), num_classes=6).float() @ embedding_layer.weight\n",
    ")\n",
    "print(f\"OR\\n{res}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed the entire input\n",
    "embedding_layer(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Encoding Word Positions\n",
    "\n",
    "- The transformer architecture, unlike recurrent neural networks (RNNs), processes all words in a sentence simultaneously.\n",
    "- This parallel processing is efficient but lacks a built-in mechanism to understand the order of words, which is crucial for language understanding.\n",
    "- In summary, `positional encodings` are essential in the transformer architecture to provide information about the `order of words` in a sequence, enabling the model to `understand` and process natural language effectively.\n",
    "\n",
    "#### Absolute Positional Encoding\n",
    "\n",
    "- Absolute positional encoding is a technique used in transformer architectures to `encode the positions of tokens in a sequence`.\n",
    "- It provides each position in the sequence with a unique representation, which is added to the corresponding word embeddings to inform the model about the position of each word.\n",
    "- This is necessary because transformers process the entire input sequence simultaneously and, without positional encoding, would lack the ability to understand the order of words.\n",
    "\n",
    "#### Relative Positional Encoding\n",
    "\n",
    "- Relative positional encodings are an alternative to absolute positional encodings used in transformer models to `incorporate information about the relative positions of tokens in a sequence`, rather than their absolute positions.\n",
    "- This approach can be more flexible and efficient, particularly for tasks where the relationships between tokens are more important than their fixed positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "dir(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "vocab_size: int = tokenizer.n_vocab\n",
    "output_dim: int = 256\n",
    "token_embedding_layer = nn.Embedding(vocab_size, output_dim)  # lookup table\n",
    "print(f\"{token_embedding_layer.weight.shape=}\\n\")  # (vocab_size, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size: int = 8\n",
    "max_length: int = 4\n",
    "stride: int = 4  # max_length and stride are equal to prevent overlapping.\n",
    "\n",
    "dataloader = create_dataloader(\n",
    "    text=data,\n",
    "    batch_size=batch_size,\n",
    "    max_length=max_length,\n",
    "    stride=stride,\n",
    "    shuffle=False,\n",
    ")\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(f\"{inputs.shape = }\")  # (batch_size, max_length)\n",
    "print(f\"{targets.shape = }\")  # (batch_size, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each token ID is embedded as an `output_dim` dimensional tensor output\n",
    "token_embeddings = token_embedding_layer(inputs)  # (batch_size, max_length, output_dim)\n",
    "print(f\"{token_embeddings.shape = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the positional embedding\n",
    "context_length: int = max_length\n",
    "pos_embedding_layer = nn.Embedding(context_length, output_dim)\n",
    "pos_embeddings = pos_embedding_layer(torch.arange(context_length)) \n",
    "print(f\"{pos_embeddings.shape = }\")  # (context_length, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embeddings[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embeddings.shape, pos_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the input embeddings. i.e. add positional embeddings to the token embeddings to enable the LLM to learn word positions.\n",
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(f\"{input_embeddings.shape = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_p11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
