{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Token Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in library\n",
    "import re\n",
    "import json\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rich import print\n",
    "import torch\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Pandas settings\n",
    "pd.options.display.max_rows = 1_000\n",
    "pd.options.display.max_columns = 1_000\n",
    "pd.options.display.max_colwidth = 600\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Black code formatter (Optional)\n",
    "%load_ext lab_black\n",
    "\n",
    "# auto reload imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "\n",
    "- The data can be found [here](https://huggingface.co/datasets/conll2003)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 14041\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3250\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3453\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets.dataset_dict import Dataset, DatasetDict\n",
    "\n",
    "\n",
    "PATH: str = \"conll2003\"\n",
    "raw_datasets: DatasetDict = load_dataset(path=PATH)\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '0',\n",
       " 'tokens': ['EU',\n",
       "  'rejects',\n",
       "  'German',\n",
       "  'call',\n",
       "  'to',\n",
       "  'boycott',\n",
       "  'British',\n",
       "  'lamb',\n",
       "  '.'],\n",
       " 'pos_tags': [22, 42, 16, 21, 35, 37, 16, 21, 7],\n",
       " 'chunk_tags': [11, 21, 11, 12, 21, 22, 11, 12, 0],\n",
       " 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets.get(\"train\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': Value(dtype='string', id=None),\n",
       " 'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
       " 'pos_tags': Sequence(feature=ClassLabel(names=['\"', \"''\", '#', '$', '(', ')', ',', '.', ':', '``', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'NN|SYM', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB'], id=None), length=-1, id=None),\n",
       " 'chunk_tags': Sequence(feature=ClassLabel(names=['O', 'B-ADJP', 'I-ADJP', 'B-ADVP', 'I-ADVP', 'B-CONJP', 'I-CONJP', 'B-INTJ', 'I-INTJ', 'B-LST', 'I-LST', 'B-NP', 'I-NP', 'B-PP', 'I-PP', 'B-PRT', 'I-PRT', 'B-SBAR', 'I-SBAR', 'B-UCP', 'I-UCP', 'B-VP', 'I-VP'], id=None), length=-1, id=None),\n",
       " 'ner_tags': Sequence(feature=ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'], id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets.get(\"train\").features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_names: list[str] = (\n",
    "    raw_datasets.get(\"train\").features.get(\"ner_tags\").feature.names\n",
    ")\n",
    "label_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labels\n",
    "\n",
    "```text\n",
    "- O means the word doesnâ€™t correspond to any entity.\n",
    "- B-PER/I-PER means the word corresponds to the beginning of/is inside a person entity.\n",
    "- B-ORG/I-ORG means the word corresponds to the beginning of/is inside an organization entity.\n",
    "- B-LOC/I-LOC means the word corresponds to the beginning of/is inside a location entity.\n",
    "- B-MISC/I-MISC means the word corresponds to the beginning of/is inside a miscellaneous entity.\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "```python\n",
    "{\n",
    "    \"O\" :      0,\n",
    "    \"B-PER\" :  1,\n",
    "    \"I-PER\" :  2,\n",
    "    \"B-ORG\" :  3,\n",
    "    \"I-ORG\" :  4,\n",
    "    \"B-LOC\" :  5,\n",
    "    \"I-LOC\" :  6,\n",
    "    \"B-MISC\" : 7,\n",
    "    \"I-MISC\" : 8,\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">words: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'EU'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'rejects'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'German'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'call'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'to'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'boycott'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'British'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'lamb'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'.'</span><span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "words: \u001b[1m[\u001b[0m\u001b[32m'EU'\u001b[0m, \u001b[32m'rejects'\u001b[0m, \u001b[32m'German'\u001b[0m, \u001b[32m'call'\u001b[0m, \u001b[32m'to'\u001b[0m, \u001b[32m'boycott'\u001b[0m, \u001b[32m'British'\u001b[0m, \u001b[32m'lamb'\u001b[0m, \u001b[32m'.'\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">labels: <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "labels: \u001b[1m[\u001b[0m\u001b[1;36m3\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m7\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m7\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "words: list[str] = raw_datasets[\"train\"][0][\"tokens\"]\n",
    "labels: list[str] = raw_datasets[\"train\"][0][\"ner_tags\"]\n",
    "\n",
    "print(f\"words: {words}\")\n",
    "print(f\"labels: {labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">EU    rejects German call to boycott British lamb . \n",
       "</pre>\n"
      ],
      "text/plain": [
       "EU    rejects German call to boycott British lamb . \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">B-ORG O       B-MISC O    O  O       B-MISC  O    O \n",
       "</pre>\n"
      ],
      "text/plain": [
       "B-ORG O       B-MISC O    O  O       B-MISC  O    O \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "words: list[str] = raw_datasets[\"train\"][0][\"tokens\"]\n",
    "labels: list[str] = raw_datasets[\"train\"][0][\"ner_tags\"]\n",
    "line1: str = \"\"\n",
    "line2: str = \"\"\n",
    "\n",
    "for word, label in zip(words, labels):\n",
    "    full_label = label_names[label]\n",
    "    max_length = max(len(word), len(full_label))\n",
    "    line1 += word + \" \" * (max_length - len(word) + 1)\n",
    "    line2 += full_label + \" \" * (max_length - len(full_label) + 1)\n",
    "\n",
    "print(line1)\n",
    "print(line2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_tokens_nerTags(*, idx: int) -> None:\n",
    "    \"\"\"This prints the tokens and their corresponding NER tags.\"\"\"\n",
    "    words: list[str] = raw_datasets[\"train\"][idx][\"tokens\"]\n",
    "    labels: list[str] = raw_datasets[\"train\"][idx][\"ner_tags\"]\n",
    "    line1: str = \"\"\n",
    "    line2: str = \"\"\n",
    "\n",
    "    for word, label in zip(words, labels):\n",
    "        full_label = label_names[label]\n",
    "        max_length = max(len(word), len(full_label))\n",
    "        line1 += word + \" \" * (max_length - len(word) + 1)\n",
    "        line2 += full_label + \" \" * (max_length - len(full_label) + 1)\n",
    "\n",
    "    print(line1)\n",
    "    print(line2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Peter Blackburn \n",
       "</pre>\n"
      ],
      "text/plain": [
       "Peter Blackburn \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">B-PER I-PER     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "B-PER I-PER     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">\" We do n't support any such recommendation because we do n't see any grounds for it , \"</span> the Commission 's chief \n",
       "spokesman Nikolaus van   der   Pas   told a news briefing . \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m\" We do n't support any such recommendation because we do n't see any grounds for it , \"\u001b[0m the Commission 's chief \n",
       "spokesman Nikolaus van   der   Pas   told a news briefing . \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">O O  O  O   O       O   O    O              O       O  O  O   O   O   O       O   O  O O O   B-ORG      O  O     O \n",
       "B-PER    I-PER I-PER I-PER O    O O    O        O \n",
       "</pre>\n"
      ],
      "text/plain": [
       "O O  O  O   O       O   O    O              O       O  O  O   O   O   O       O   O  O O O   B-ORG      O  O     O \n",
       "B-PER    I-PER I-PER I-PER O    O O    O        O \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_tokens_nerTags(idx=1)\n",
    "\n",
    "display_tokens_nerTags(idx=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create A Tokenizer Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "model_checkpoint: str = \"bert-base-cased\"\n",
    "tokenizer: AutoTokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "# Check that the tokenizer object is backed by ðŸ¤— Tokenizers:\n",
    "assert tokenizer.is_fast is True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">texts: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'EU'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'rejects'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'German'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'call'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'to'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'boycott'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'British'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'lamb'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'.'</span><span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "texts: \u001b[1m[\u001b[0m\u001b[32m'EU'\u001b[0m, \u001b[32m'rejects'\u001b[0m, \u001b[32m'German'\u001b[0m, \u001b[32m'call'\u001b[0m, \u001b[32m'to'\u001b[0m, \u001b[32m'boycott'\u001b[0m, \u001b[32m'British'\u001b[0m, \u001b[32m'lamb'\u001b[0m, \u001b[32m'.'\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">tokens: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'[CLS]'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'EU'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'rejects'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'German'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'call'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'to'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'boycott'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'British'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'la'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'##mb'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'.'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'[SEP]'</span><span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "tokens: \u001b[1m[\u001b[0m\u001b[32m'\u001b[0m\u001b[32m[\u001b[0m\u001b[32mCLS\u001b[0m\u001b[32m]\u001b[0m\u001b[32m'\u001b[0m, \u001b[32m'EU'\u001b[0m, \u001b[32m'rejects'\u001b[0m, \u001b[32m'German'\u001b[0m, \u001b[32m'call'\u001b[0m, \u001b[32m'to'\u001b[0m, \u001b[32m'boycott'\u001b[0m, \u001b[32m'British'\u001b[0m, \u001b[32m'la'\u001b[0m, \u001b[32m'##mb'\u001b[0m, \u001b[32m'.'\u001b[0m, \u001b[32m'\u001b[0m\u001b[32m[\u001b[0m\u001b[32mSEP\u001b[0m\u001b[32m]\u001b[0m\u001b[32m'\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenize a pre-tokenized input using is_split_into_words=True:\n",
    "texts: list[str] = raw_datasets[\"train\"][0][\"tokens\"]\n",
    "inputs: dict[str, Any] = tokenizer(texts, is_split_into_words=True)\n",
    "\n",
    "print(f\"texts: {texts}\")\n",
    "print(f\"tokens: {inputs.tokens()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, 0, 1, 2, 3, 4, 5, 6, 7, 7, 8, None]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtain the IDs (the converted tokens in integer format)\n",
    "inputs.word_ids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note:\n",
    "\n",
    "```text\n",
    "- The size of the tokenized ID is different from the size of the labels.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">([</span><span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>, <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span><span style=\"font-weight: bold\">]</span>, <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[3;35mNone\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m2\u001b[0m, \u001b[1;36m3\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m5\u001b[0m, \u001b[1;36m6\u001b[0m, \u001b[1;36m7\u001b[0m, \u001b[1;36m7\u001b[0m, \u001b[1;36m8\u001b[0m, \u001b[3;35mNone\u001b[0m\u001b[1m]\u001b[0m, \u001b[1m[\u001b[0m\u001b[1;36m3\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m7\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m7\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Size: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> != <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Size: \u001b[1;36m12\u001b[0m != \u001b[1;36m9\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The size of the tokenized ID is different from the size of the labels\n",
    "output: list[Optional[int]] = inputs.word_ids()\n",
    "original: list[int] = raw_datasets[\"train\"][0][\"ner_tags\"]\n",
    "\n",
    "print((output, original))\n",
    "print(f\"Size: {len(output)} != {len(original)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_labels_with_tokens(labels: list[int], word_ids: list[Optional[int]]):\n",
    "    new_labels: list[int] = []\n",
    "    current_word: Optional[int] = None\n",
    "\n",
    "    for word_id in word_ids:\n",
    "        # if the current_word is not None\n",
    "        if word_id != current_word:\n",
    "            # Start of a new word!\n",
    "            # Update the current_word\n",
    "            current_word = word_id\n",
    "            label: int = -100 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "\n",
    "        elif word_id is None:\n",
    "            # Special token\n",
    "            new_labels.append(-100)\n",
    "\n",
    "        else:\n",
    "            # Same word as previous token\n",
    "            label: int = labels[word_id]\n",
    "            # If the label is B-XXX we change it to I-XXX\n",
    "            if label % 2 == 1:\n",
    "                label += 1\n",
    "            new_labels.append(label)\n",
    "\n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">tokens: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'EU'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'rejects'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'German'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'call'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'to'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'boycott'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'British'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'lamb'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'.'</span><span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "tokens: \u001b[1m[\u001b[0m\u001b[32m'EU'\u001b[0m, \u001b[32m'rejects'\u001b[0m, \u001b[32m'German'\u001b[0m, \u001b[32m'call'\u001b[0m, \u001b[32m'to'\u001b[0m, \u001b[32m'boycott'\u001b[0m, \u001b[32m'British'\u001b[0m, \u001b[32m'lamb'\u001b[0m, \u001b[32m'.'\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">labels: <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "labels: \u001b[1m[\u001b[0m\u001b[1;36m3\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m7\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m7\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">word_ids: <span style=\"font-weight: bold\">[</span><span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>, <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span><span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "word_ids: \u001b[1m[\u001b[0m\u001b[3;35mNone\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m2\u001b[0m, \u001b[1;36m3\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m5\u001b[0m, \u001b[1;36m6\u001b[0m, \u001b[1;36m7\u001b[0m, \u001b[1;36m7\u001b[0m, \u001b[1;36m8\u001b[0m, \u001b[3;35mNone\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels = raw_datasets[\"train\"][0][\"ner_tags\"]\n",
    "# Convert the pre-tokenized words to IDs\n",
    "word_ids = inputs.word_ids()\n",
    "\n",
    "print(f'tokens: {raw_datasets[\"train\"][0][\"tokens\"]}')\n",
    "print(f\"labels: {labels}\")\n",
    "print(f\"word_ids: {word_ids}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "```text\n",
    "- The function added the -100 for the two special tokens at the beginning and the end, and a new 0 for our word that was split into two tokens.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">labels: <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "labels: \u001b[1m[\u001b[0m\u001b[1;36m3\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m7\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m7\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">new_labels: <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-100</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-100</span><span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "new_labels: \u001b[1m[\u001b[0m\u001b[1;36m-100\u001b[0m, \u001b[1;36m3\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m7\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m7\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m-100\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "new_labels: list[int] = align_labels_with_tokens(labels, word_ids)\n",
    "\n",
    "print(f\"labels: {labels}\")\n",
    "print(f\"new_labels: {new_labels}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "```text\n",
    "Ex 2:\n",
    "- Some researchers prefer to attribute only one label per word, and assign -100 to the other subtokens in a given word. This is to avoid long words that split into lots of subtokens contributing heavily to the loss. Change the previous function to align labels with input IDs by following this rule.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_labels_with_tokens_ex_2(labels: list[int], word_ids: list[Optional[int]]):\n",
    "    \"\"\"Implementation of Ex 2.\"\"\"\n",
    "\n",
    "    new_labels: list[int] = []\n",
    "    current_word: Optional[int] = None\n",
    "\n",
    "    for word_id in word_ids:\n",
    "        # if the current_word is not None\n",
    "        if word_id != current_word:\n",
    "            # Start of a new word!\n",
    "            # Update the current_word\n",
    "            current_word = word_id\n",
    "            label: int = -100 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "\n",
    "        elif word_id is None:\n",
    "            # Special token\n",
    "            new_labels.append(-100)\n",
    "\n",
    "        else:\n",
    "            # Same word as previous token\n",
    "            label: int = labels[word_id]\n",
    "            # If the label is B-XXX we change it to I-XXX\n",
    "            if label % 2 == 1:\n",
    "                label += 1\n",
    "            new_labels.append(-100)\n",
    "\n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">labels: <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "labels: \u001b[1m[\u001b[0m\u001b[1;36m3\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m7\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m7\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">new_labels: <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-100</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-100</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-100</span><span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "new_labels: \u001b[1m[\u001b[0m\u001b[1;36m-100\u001b[0m, \u001b[1;36m3\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m7\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m7\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m-100\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m-100\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "new_labels: list[int] = align_labels_with_tokens_ex_2(labels, word_ids)\n",
    "\n",
    "print(f\"labels: {labels}\")\n",
    "print(f\"new_labels: {new_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```text\n",
    "- To preprocess the entire dataset, we tokenize all inputs and align the labels with the corresponding tokens using align_labels_with_tokens(). To improve processing speed, we create a function that handles a list of examples and use Dataset.map() with batched=True. \n",
    "- Additionally, for inputs in the form of lists of texts (or lists of lists of words), we modify the word_ids() function to include the index of the desired example.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples: dict[str, Any]):\n",
    "    tokenized_inputs: dict[str, Any] = tokenizer(\n",
    "        examples[\"tokens\"], truncation=True, is_split_into_words=True\n",
    "    )\n",
    "    all_labels: list[str] = examples[\"ner_tags\"]\n",
    "    new_labels: list[str] = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0812eb888a9045dab264333face89555",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply the tokenization on the entire dataset\n",
    "tokenized_datasets: Dataset = raw_datasets.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"train\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'input_ids'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">101</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7270</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">22961</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1528</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1840</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1106</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">21423</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1418</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2495</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12913</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">119</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">102</span><span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'token_type_ids'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'attention_mask'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'labels'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-100</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-100</span><span style=\"font-weight: bold\">]</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'input_ids'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1;36m101\u001b[0m, \u001b[1;36m7270\u001b[0m, \u001b[1;36m22961\u001b[0m, \u001b[1;36m1528\u001b[0m, \u001b[1;36m1840\u001b[0m, \u001b[1;36m1106\u001b[0m, \u001b[1;36m21423\u001b[0m, \u001b[1;36m1418\u001b[0m, \u001b[1;36m2495\u001b[0m, \u001b[1;36m12913\u001b[0m, \u001b[1;36m119\u001b[0m, \u001b[1;36m102\u001b[0m\u001b[1m]\u001b[0m,\n",
       "    \u001b[32m'token_type_ids'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m,\n",
       "    \u001b[32m'attention_mask'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m,\n",
       "    \u001b[32m'labels'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1;36m-100\u001b[0m, \u001b[1;36m3\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m7\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m7\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m-100\u001b[0m\u001b[1m]\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(tokenized_datasets.get(\"train\")[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning the model with the Trainer API\n",
    "\n",
    "```text\n",
    "- The actual code using the Trainer will be the same as before; the only changes are the way the data is collated into a batch and the metric computation function.\n",
    "\n",
    "\n",
    "Data collation\n",
    "--------------\n",
    "- We canâ€™t just use a DataCollatorWithPadding like in Chapter 3 because that only pads the inputs (input IDs, attention mask, and token type IDs). \n",
    "- Here our labels should be padded the exact same way as the inputs so that they stay the same size, using -100 as a value so that the corresponding predictions are ignored in the loss computation.\n",
    "- This is all done by a DataCollatorForTokenClassification. Like the DataCollatorWithPadding, it takes the tokenizer used to preprocess the inputs\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-26 22:23:56.414954: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "\n",
    "data_collator: DataCollatorForTokenClassification = DataCollatorForTokenClassification(\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-100</span>,    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>,    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>,    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>,    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-100</span><span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-100</span>,    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-100</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-100</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-100</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-100</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-100</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-100</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-100</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-100</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-100</span><span style=\"font-weight: bold\">]])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m-100\u001b[0m,    \u001b[1;36m3\u001b[0m,    \u001b[1;36m0\u001b[0m,    \u001b[1;36m7\u001b[0m,    \u001b[1;36m0\u001b[0m,    \u001b[1;36m0\u001b[0m,    \u001b[1;36m0\u001b[0m,    \u001b[1;36m7\u001b[0m,    \u001b[1;36m0\u001b[0m,    \u001b[1;36m0\u001b[0m,    \u001b[1;36m0\u001b[0m, \u001b[1;36m-100\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m\u001b[1;36m-100\u001b[0m,    \u001b[1;36m1\u001b[0m,    \u001b[1;36m2\u001b[0m, \u001b[1;36m-100\u001b[0m, \u001b[1;36m-100\u001b[0m, \u001b[1;36m-100\u001b[0m, \u001b[1;36m-100\u001b[0m, \u001b[1;36m-100\u001b[0m, \u001b[1;36m-100\u001b[0m, \u001b[1;36m-100\u001b[0m, \u001b[1;36m-100\u001b[0m, \u001b[1;36m-100\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test it on a few samples\n",
    "batch: torch.Tensor = data_collator([tokenized_datasets[\"train\"][i] for i in range(2)])\n",
    "\n",
    "print(batch.get(\"labels\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-100</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-100</span><span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[1;36m-100\u001b[0m, \u001b[1;36m3\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m7\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m7\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m-100\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-100</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-100</span><span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[1;36m-100\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m2\u001b[0m, \u001b[1;36m-100\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Original data (before applying data collator)\n",
    "for i in range(2):\n",
    "    print(tokenized_datasets[\"train\"][i][\"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Metrics\n",
    "\n",
    "```text\n",
    "- To have the Trainer compute a metric every epoch, we will need to define a compute_metrics() function that takes the arrays of predictions and labels, and returns a dictionary with the metric names and values.\n",
    "\n",
    "- The conventional approach for assessing token classification predictions is through the application of the seqeval metric. Before employing this metric, it's essential to install the seqeval library.\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "```python\n",
    "!pip install seqeval\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "\n",
    "metric = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'B-ORG'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'O'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'B-MISC'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'O'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'O'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'O'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'B-MISC'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'O'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'O'</span><span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[32m'B-ORG'\u001b[0m, \u001b[32m'O'\u001b[0m, \u001b[32m'B-MISC'\u001b[0m, \u001b[32m'O'\u001b[0m, \u001b[32m'O'\u001b[0m, \u001b[32m'O'\u001b[0m, \u001b[32m'B-MISC'\u001b[0m, \u001b[32m'O'\u001b[0m, \u001b[32m'O'\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sample\n",
    "labels: list[str] = raw_datasets[\"train\"][0][\"ner_tags\"]\n",
    "labels = [label_names[i] for i in labels]\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'MISC'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'precision'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'recall'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.5</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'f1'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.6666666666666666</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'number'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'ORG'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'precision'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'recall'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'f1'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'number'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'overall_precision'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'overall_recall'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.6666666666666666</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'overall_f1'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.8</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'overall_accuracy'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.8888888888888888</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'MISC'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'precision'\u001b[0m: \u001b[1;36m1.0\u001b[0m, \u001b[32m'recall'\u001b[0m: \u001b[1;36m0.5\u001b[0m, \u001b[32m'f1'\u001b[0m: \u001b[1;36m0.6666666666666666\u001b[0m, \u001b[32m'number'\u001b[0m: \u001b[1;36m2\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[32m'ORG'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'precision'\u001b[0m: \u001b[1;36m1.0\u001b[0m, \u001b[32m'recall'\u001b[0m: \u001b[1;36m1.0\u001b[0m, \u001b[32m'f1'\u001b[0m: \u001b[1;36m1.0\u001b[0m, \u001b[32m'number'\u001b[0m: \u001b[1;36m1\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[32m'overall_precision'\u001b[0m: \u001b[1;36m1.0\u001b[0m,\n",
       "    \u001b[32m'overall_recall'\u001b[0m: \u001b[1;36m0.6666666666666666\u001b[0m,\n",
       "    \u001b[32m'overall_f1'\u001b[0m: \u001b[1;36m0.8\u001b[0m,\n",
       "    \u001b[32m'overall_accuracy'\u001b[0m: \u001b[1;36m0.8888888888888888\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions: list[str] = labels.copy()\n",
    "# Simulate prediction\n",
    "predictions[2] = \"O\"\n",
    "print(metric.compute(predictions=[predictions], references=[labels]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "\n",
    "    return {\n",
    "        \"precision\": all_metrics[\"overall_precision\"],\n",
    "        \"recall\": all_metrics[\"overall_recall\"],\n",
    "        \"f1\": all_metrics[\"overall_f1\"],\n",
    "        \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the model\n",
    "\n",
    "\n",
    "```text\n",
    "- For token classification, the AutoModelForTokenClassification class is used. \n",
    "- It's important to provide information about the number of labels, either through the num_labels argument or by setting id2label and label2id dictionaries for proper inference and mapping of IDs to labels.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'O'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'B-PER'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'I-PER'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'B-ORG'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'I-ORG'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'B-LOC'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'I-LOC'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'B-MISC'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'I-MISC'</span><span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\u001b[1;36m0\u001b[0m: \u001b[32m'O'\u001b[0m, \u001b[1;36m1\u001b[0m: \u001b[32m'B-PER'\u001b[0m, \u001b[1;36m2\u001b[0m: \u001b[32m'I-PER'\u001b[0m, \u001b[1;36m3\u001b[0m: \u001b[32m'B-ORG'\u001b[0m, \u001b[1;36m4\u001b[0m: \u001b[32m'I-ORG'\u001b[0m, \u001b[1;36m5\u001b[0m: \u001b[32m'B-LOC'\u001b[0m, \u001b[1;36m6\u001b[0m: \u001b[32m'I-LOC'\u001b[0m, \u001b[1;36m7\u001b[0m: \u001b[32m'B-MISC'\u001b[0m, \u001b[1;36m8\u001b[0m: \u001b[32m'I-MISC'\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'O'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'B-PER'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'I-PER'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'B-ORG'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'I-ORG'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'B-LOC'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'I-LOC'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'B-MISC'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'I-MISC'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span><span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\u001b[32m'O'\u001b[0m: \u001b[1;36m0\u001b[0m, \u001b[32m'B-PER'\u001b[0m: \u001b[1;36m1\u001b[0m, \u001b[32m'I-PER'\u001b[0m: \u001b[1;36m2\u001b[0m, \u001b[32m'B-ORG'\u001b[0m: \u001b[1;36m3\u001b[0m, \u001b[32m'I-ORG'\u001b[0m: \u001b[1;36m4\u001b[0m, \u001b[32m'B-LOC'\u001b[0m: \u001b[1;36m5\u001b[0m, \u001b[32m'I-LOC'\u001b[0m: \u001b[1;36m6\u001b[0m, \u001b[32m'B-MISC'\u001b[0m: \u001b[1;36m7\u001b[0m, \u001b[32m'I-MISC'\u001b[0m: \u001b[1;36m8\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "id2label: dict[str:Any] = {i: label for i, label in enumerate(label_names)}\n",
    "label2id: dict[str:Any] = {v: k for k, v in id2label.items()}\n",
    "\n",
    "print(id2label)\n",
    "print(label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "\n",
    "# Now we can just pass them to the AutoModelForTokenClassification.from_pretrained() method,\n",
    "# and they will be set in the modelâ€™s configuration and then properly saved and uploaded to the Hub:\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that the model has the right number of labels:\n",
    "model.config.num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_p310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
