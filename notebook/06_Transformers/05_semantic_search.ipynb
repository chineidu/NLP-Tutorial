{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Semantic Search](https://huggingface.co/learn/nlp-course/chapter5/6?fw=pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in library\n",
    "import re\n",
    "import json\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "# Standard imports\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "from rich import print\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Pandas settings\n",
    "pd.options.display.max_rows = 1_000\n",
    "pd.options.display.max_columns = 1_000\n",
    "pd.options.display.max_colwidth = 600\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Black code formatter (Optional)\n",
    "%load_ext lab_black\n",
    "\n",
    "# auto reload imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from datasets import load_dataset\n",
    "from datasets.dataset_dict import DatasetDict, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'pull_request', 'body', 'timeline_url', 'performed_via_github_app', 'is_pull_request'],\n",
       "    num_rows: 3019\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp: str = \"lewtun/github-issues\"\n",
    "\n",
    "issues_dataset: Dataset = load_dataset(fp, split=\"train\")\n",
    "issues_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'url'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://api.github.com/repos/huggingface/datasets/issues/2944'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'repository_url'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://api.github.com/repos/huggingface/datasets'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'labels_url'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://api.github.com/repos/huggingface/datasets/issues/2944/labels{/name}'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'comments_url'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://api.github.com/repos/huggingface/datasets/issues/2944/comments'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'events_url'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://api.github.com/repos/huggingface/datasets/issues/2944/events'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'html_url'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://github.com/huggingface/datasets/issues/2944'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'id'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1000544370</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'node_id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'I_kwDODunzps47oxhy'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'number'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2944</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'title'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Add  `remove_columns` to `IterableDataset ` '</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'user'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'login'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'cccntu'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'id'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">31893406</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'node_id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'MDQ6VXNlcjMxODkzNDA2'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'avatar_url'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://avatars.githubusercontent.com/u/31893406?v=4'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'gravatar_id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'url'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://api.github.com/users/cccntu'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'html_url'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://github.com/cccntu'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'followers_url'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://api.github.com/users/cccntu/followers'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'following_url'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://api.github.com/users/cccntu/following{/other_user}'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'gists_url'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://api.github.com/users/cccntu/gists{/gist_id}'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'starred_url'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://api.github.com/users/cccntu/starred{/owner}{/repo}'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'subscriptions_url'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://api.github.com/users/cccntu/subscriptions'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'organizations_url'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://api.github.com/users/cccntu/orgs'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'repos_url'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://api.github.com/users/cccntu/repos'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'events_url'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://api.github.com/users/cccntu/events{/privacy}'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'received_events_url'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://api.github.com/users/cccntu/received_events'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'User'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'site_admin'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'labels'</span>: <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'id'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1935892871</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'node_id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'MDU6TGFiZWwxOTM1ODkyODcx'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'url'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://api.github.com/repos/huggingface/datasets/labels/enhancement'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'enhancement'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'color'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'a2eeef'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'default'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'description'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'New feature or request'</span>\n",
       "        <span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'state'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'open'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'locked'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'assignee'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'assignees'</span>: <span style=\"font-weight: bold\">[]</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'milestone'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'comments'</span>: <span style=\"font-weight: bold\">[]</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'created_at'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1632110460000</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'updated_at'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1632110460000</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'closed_at'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'author_association'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'CONTRIBUTOR'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'active_lock_reason'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'pull_request'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'body'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'**Is your feature request related to a problem? Please describe.**\\r\\nA clear and concise description </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">of what the problem is.\\r\\n\\r\\n```python\\r\\nfrom datasets import load_dataset\\r\\ndataset = load_dataset(\"c4\", </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\\'realnewslike\\', streaming =True, split=\\'train\\')\\r\\ndataset = </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">dataset.remove_columns(\\'url\\')\\r\\n```\\r\\n```\\r\\nAttributeError: \\'IterableDataset\\' object has no attribute </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\\'remove_columns\\'\\r\\n```\\r\\n\\r\\n**Describe the solution you\\'d like**\\r\\n\\r\\nIt would be nice to have </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">`.remove_columns()` to match the `Datasets` api. \\r\\n\\r\\n\\r\\n**Describe alternatives you\\'ve </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">considered**\\r\\n\\r\\nThis can be done with a single call to `.map()`, \\r\\n\\r\\nI can try to help add this. 🤗'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'timeline_url'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://api.github.com/repos/huggingface/datasets/issues/2944/timeline'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'performed_via_github_app'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'is_pull_request'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'url'\u001b[0m: \u001b[32m'https://api.github.com/repos/huggingface/datasets/issues/2944'\u001b[0m,\n",
       "    \u001b[32m'repository_url'\u001b[0m: \u001b[32m'https://api.github.com/repos/huggingface/datasets'\u001b[0m,\n",
       "    \u001b[32m'labels_url'\u001b[0m: \u001b[32m'https://api.github.com/repos/huggingface/datasets/issues/2944/labels\u001b[0m\u001b[32m{\u001b[0m\u001b[32m/name\u001b[0m\u001b[32m}\u001b[0m\u001b[32m'\u001b[0m,\n",
       "    \u001b[32m'comments_url'\u001b[0m: \u001b[32m'https://api.github.com/repos/huggingface/datasets/issues/2944/comments'\u001b[0m,\n",
       "    \u001b[32m'events_url'\u001b[0m: \u001b[32m'https://api.github.com/repos/huggingface/datasets/issues/2944/events'\u001b[0m,\n",
       "    \u001b[32m'html_url'\u001b[0m: \u001b[32m'https://github.com/huggingface/datasets/issues/2944'\u001b[0m,\n",
       "    \u001b[32m'id'\u001b[0m: \u001b[1;36m1000544370\u001b[0m,\n",
       "    \u001b[32m'node_id'\u001b[0m: \u001b[32m'I_kwDODunzps47oxhy'\u001b[0m,\n",
       "    \u001b[32m'number'\u001b[0m: \u001b[1;36m2944\u001b[0m,\n",
       "    \u001b[32m'title'\u001b[0m: \u001b[32m'Add  `remove_columns` to `IterableDataset ` '\u001b[0m,\n",
       "    \u001b[32m'user'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "        \u001b[32m'login'\u001b[0m: \u001b[32m'cccntu'\u001b[0m,\n",
       "        \u001b[32m'id'\u001b[0m: \u001b[1;36m31893406\u001b[0m,\n",
       "        \u001b[32m'node_id'\u001b[0m: \u001b[32m'MDQ6VXNlcjMxODkzNDA2'\u001b[0m,\n",
       "        \u001b[32m'avatar_url'\u001b[0m: \u001b[32m'https://avatars.githubusercontent.com/u/31893406?\u001b[0m\u001b[32mv\u001b[0m\u001b[32m=\u001b[0m\u001b[32m4\u001b[0m\u001b[32m'\u001b[0m,\n",
       "        \u001b[32m'gravatar_id'\u001b[0m: \u001b[32m''\u001b[0m,\n",
       "        \u001b[32m'url'\u001b[0m: \u001b[32m'https://api.github.com/users/cccntu'\u001b[0m,\n",
       "        \u001b[32m'html_url'\u001b[0m: \u001b[32m'https://github.com/cccntu'\u001b[0m,\n",
       "        \u001b[32m'followers_url'\u001b[0m: \u001b[32m'https://api.github.com/users/cccntu/followers'\u001b[0m,\n",
       "        \u001b[32m'following_url'\u001b[0m: \u001b[32m'https://api.github.com/users/cccntu/following\u001b[0m\u001b[32m{\u001b[0m\u001b[32m/other_user\u001b[0m\u001b[32m}\u001b[0m\u001b[32m'\u001b[0m,\n",
       "        \u001b[32m'gists_url'\u001b[0m: \u001b[32m'https://api.github.com/users/cccntu/gists\u001b[0m\u001b[32m{\u001b[0m\u001b[32m/gist_id\u001b[0m\u001b[32m}\u001b[0m\u001b[32m'\u001b[0m,\n",
       "        \u001b[32m'starred_url'\u001b[0m: \u001b[32m'https://api.github.com/users/cccntu/starred\u001b[0m\u001b[32m{\u001b[0m\u001b[32m/owner\u001b[0m\u001b[32m}\u001b[0m\u001b[32m{\u001b[0m\u001b[32m/repo\u001b[0m\u001b[32m}\u001b[0m\u001b[32m'\u001b[0m,\n",
       "        \u001b[32m'subscriptions_url'\u001b[0m: \u001b[32m'https://api.github.com/users/cccntu/subscriptions'\u001b[0m,\n",
       "        \u001b[32m'organizations_url'\u001b[0m: \u001b[32m'https://api.github.com/users/cccntu/orgs'\u001b[0m,\n",
       "        \u001b[32m'repos_url'\u001b[0m: \u001b[32m'https://api.github.com/users/cccntu/repos'\u001b[0m,\n",
       "        \u001b[32m'events_url'\u001b[0m: \u001b[32m'https://api.github.com/users/cccntu/events\u001b[0m\u001b[32m{\u001b[0m\u001b[32m/privacy\u001b[0m\u001b[32m}\u001b[0m\u001b[32m'\u001b[0m,\n",
       "        \u001b[32m'received_events_url'\u001b[0m: \u001b[32m'https://api.github.com/users/cccntu/received_events'\u001b[0m,\n",
       "        \u001b[32m'type'\u001b[0m: \u001b[32m'User'\u001b[0m,\n",
       "        \u001b[32m'site_admin'\u001b[0m: \u001b[3;91mFalse\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[32m'labels'\u001b[0m: \u001b[1m[\u001b[0m\n",
       "        \u001b[1m{\u001b[0m\n",
       "            \u001b[32m'id'\u001b[0m: \u001b[1;36m1935892871\u001b[0m,\n",
       "            \u001b[32m'node_id'\u001b[0m: \u001b[32m'MDU6TGFiZWwxOTM1ODkyODcx'\u001b[0m,\n",
       "            \u001b[32m'url'\u001b[0m: \u001b[32m'https://api.github.com/repos/huggingface/datasets/labels/enhancement'\u001b[0m,\n",
       "            \u001b[32m'name'\u001b[0m: \u001b[32m'enhancement'\u001b[0m,\n",
       "            \u001b[32m'color'\u001b[0m: \u001b[32m'a2eeef'\u001b[0m,\n",
       "            \u001b[32m'default'\u001b[0m: \u001b[3;92mTrue\u001b[0m,\n",
       "            \u001b[32m'description'\u001b[0m: \u001b[32m'New feature or request'\u001b[0m\n",
       "        \u001b[1m}\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[32m'state'\u001b[0m: \u001b[32m'open'\u001b[0m,\n",
       "    \u001b[32m'locked'\u001b[0m: \u001b[3;91mFalse\u001b[0m,\n",
       "    \u001b[32m'assignee'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       "    \u001b[32m'assignees'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\n",
       "    \u001b[32m'milestone'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       "    \u001b[32m'comments'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\n",
       "    \u001b[32m'created_at'\u001b[0m: \u001b[1;36m1632110460000\u001b[0m,\n",
       "    \u001b[32m'updated_at'\u001b[0m: \u001b[1;36m1632110460000\u001b[0m,\n",
       "    \u001b[32m'closed_at'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       "    \u001b[32m'author_association'\u001b[0m: \u001b[32m'CONTRIBUTOR'\u001b[0m,\n",
       "    \u001b[32m'active_lock_reason'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       "    \u001b[32m'pull_request'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       "    \u001b[32m'body'\u001b[0m: \u001b[32m'**Is your feature request related to a problem? Please describe.**\\r\\nA clear and concise description \u001b[0m\n",
       "\u001b[32mof what the problem is.\\r\\n\\r\\n```python\\r\\nfrom datasets import load_dataset\\r\\ndataset = load_dataset\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\"c4\", \u001b[0m\n",
       "\u001b[32m\\'realnewslike\\', streaming =True, \u001b[0m\u001b[32msplit\u001b[0m\u001b[32m=\\'train\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\r\\ndataset = \u001b[0m\n",
       "\u001b[32mdataset.remove_columns\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'url\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\r\\n```\\r\\n```\\r\\nAttributeError: \\'IterableDataset\\' object has no attribute \u001b[0m\n",
       "\u001b[32m\\'remove_columns\\'\\r\\n```\\r\\n\\r\\n**Describe the solution you\\'d like**\\r\\n\\r\\nIt would be nice to have \u001b[0m\n",
       "\u001b[32m`.remove_columns\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m` to match the `Datasets` api. \\r\\n\\r\\n\\r\\n**Describe alternatives you\\'ve \u001b[0m\n",
       "\u001b[32mconsidered**\\r\\n\\r\\nThis can be done with a single call to `.map\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m`, \\r\\n\\r\\nI can try to help add this. 🤗'\u001b[0m,\n",
       "    \u001b[32m'timeline_url'\u001b[0m: \u001b[32m'https://api.github.com/repos/huggingface/datasets/issues/2944/timeline'\u001b[0m,\n",
       "    \u001b[32m'performed_via_github_app'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       "    \u001b[32m'is_pull_request'\u001b[0m: \u001b[3;91mFalse\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(issues_dataset[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'url'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'repository_url'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'labels_url'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'comments_url'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'events_url'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'html_url'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'id'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'node_id'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'number'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'title'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'user'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'labels'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'state'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'locked'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'assignee'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'assignees'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'milestone'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'comments'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'created_at'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'updated_at'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'closed_at'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'author_association'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'active_lock_reason'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'pull_request'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'body'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'timeline_url'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'performed_via_github_app'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'is_pull_request'</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[32m'url'\u001b[0m,\n",
       "    \u001b[32m'repository_url'\u001b[0m,\n",
       "    \u001b[32m'labels_url'\u001b[0m,\n",
       "    \u001b[32m'comments_url'\u001b[0m,\n",
       "    \u001b[32m'events_url'\u001b[0m,\n",
       "    \u001b[32m'html_url'\u001b[0m,\n",
       "    \u001b[32m'id'\u001b[0m,\n",
       "    \u001b[32m'node_id'\u001b[0m,\n",
       "    \u001b[32m'number'\u001b[0m,\n",
       "    \u001b[32m'title'\u001b[0m,\n",
       "    \u001b[32m'user'\u001b[0m,\n",
       "    \u001b[32m'labels'\u001b[0m,\n",
       "    \u001b[32m'state'\u001b[0m,\n",
       "    \u001b[32m'locked'\u001b[0m,\n",
       "    \u001b[32m'assignee'\u001b[0m,\n",
       "    \u001b[32m'assignees'\u001b[0m,\n",
       "    \u001b[32m'milestone'\u001b[0m,\n",
       "    \u001b[32m'comments'\u001b[0m,\n",
       "    \u001b[32m'created_at'\u001b[0m,\n",
       "    \u001b[32m'updated_at'\u001b[0m,\n",
       "    \u001b[32m'closed_at'\u001b[0m,\n",
       "    \u001b[32m'author_association'\u001b[0m,\n",
       "    \u001b[32m'active_lock_reason'\u001b[0m,\n",
       "    \u001b[32m'pull_request'\u001b[0m,\n",
       "    \u001b[32m'body'\u001b[0m,\n",
       "    \u001b[32m'timeline_url'\u001b[0m,\n",
       "    \u001b[32m'performed_via_github_app'\u001b[0m,\n",
       "    \u001b[32m'is_pull_request'\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(issues_dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Size if data BEFORE: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3019</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Size if data BEFORE: \u001b[1;36m3019\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Size if data AFTER: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">808</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Size if data AFTER: \u001b[1;36m808\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The issues_dataset contains issues and pull requests.\n",
    "# Select ONLY the issues\n",
    "issues_dataset_1: Dataset = issues_dataset.filter(\n",
    "    (lambda x: x.get(\"is_pull_request\") == False and len(x.get(\"comments\")) > 0),\n",
    ")\n",
    "\n",
    "print(f\"Size if data BEFORE: {issues_dataset.num_rows}\\n\")\n",
    "\n",
    "print(f\"Size if data AFTER: {issues_dataset_1.num_rows}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'events_url'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'updated_at'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'user'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'created_at'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'comments_url'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'performed_via_github_app'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'repository_url'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'labels_url'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'assignees'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'author_association'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'id'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'active_lock_reason'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'milestone'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'labels'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'number'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'is_pull_request'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'assignee'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'url'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'locked'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'state'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'pull_request'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'node_id'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'closed_at'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'timeline_url'</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'events_url'\u001b[0m,\n",
       "    \u001b[32m'updated_at'\u001b[0m,\n",
       "    \u001b[32m'user'\u001b[0m,\n",
       "    \u001b[32m'created_at'\u001b[0m,\n",
       "    \u001b[32m'comments_url'\u001b[0m,\n",
       "    \u001b[32m'performed_via_github_app'\u001b[0m,\n",
       "    \u001b[32m'repository_url'\u001b[0m,\n",
       "    \u001b[32m'labels_url'\u001b[0m,\n",
       "    \u001b[32m'assignees'\u001b[0m,\n",
       "    \u001b[32m'author_association'\u001b[0m,\n",
       "    \u001b[32m'id'\u001b[0m,\n",
       "    \u001b[32m'active_lock_reason'\u001b[0m,\n",
       "    \u001b[32m'milestone'\u001b[0m,\n",
       "    \u001b[32m'labels'\u001b[0m,\n",
       "    \u001b[32m'number'\u001b[0m,\n",
       "    \u001b[32m'is_pull_request'\u001b[0m,\n",
       "    \u001b[32m'assignee'\u001b[0m,\n",
       "    \u001b[32m'url'\u001b[0m,\n",
       "    \u001b[32m'locked'\u001b[0m,\n",
       "    \u001b[32m'state'\u001b[0m,\n",
       "    \u001b[32m'pull_request'\u001b[0m,\n",
       "    \u001b[32m'node_id'\u001b[0m,\n",
       "    \u001b[32m'closed_at'\u001b[0m,\n",
       "    \u001b[32m'timeline_url'\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_columns: list[str] = issues_dataset_1.column_names\n",
    "columns_to_keep: list[str] = [\"title\", \"body\", \"html_url\", \"comments\"]\n",
    "# Columns present in ONLY all_columns\n",
    "columns_to_remove: set = set(columns_to_keep).symmetric_difference(set(all_columns))\n",
    "print(columns_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'comments', 'body'],\n",
       "    num_rows: 808\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issues_dataset_1 = issues_dataset_1.remove_columns(column_names=columns_to_remove)\n",
    "issues_dataset_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>html_url</th>\n",
       "      <th>title</th>\n",
       "      <th>comments</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues/2945</td>\n",
       "      <td>Protect master branch</td>\n",
       "      <td>[Cool, I think we can do both :), @lhoestq now the 2 are implemented.\\r\\n\\r\\nPlease note that for the the second protection, finally I have chosen to protect the master branch only from **merge commits** (see update comment above), so no need to disable/re-enable the protection on each release (direct commits, different from merge commits, can be pushed to the remote master branch; and eventually reverted without messing up the repo history).]</td>\n",
       "      <td>After accidental merge commit (91c55355b634d0dc73350a7ddee1a6776dbbdd69) into `datasets` master branch, all commits present in the feature branch were permanently added to `datasets` master branch history, as e.g.:\\r\\n- 00cc036fea7c7745cfe722360036ed306796a3f2\\r\\n- 13ae8c98602bbad8197de3b9b425f4c78f582af1\\r\\n- ...\\r\\n\\r\\nI propose to protect our master branch, so that we avoid we can accidentally make this kind of mistakes in the future:\\r\\n- [x] For Pull Requests using GitHub, allow only squash merging, so that only a single commit per Pull Request is merged into the master branch\\r\\n  - ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues/2943</td>\n",
       "      <td>Backwards compatibility broken for cached datasets that use `.filter()`</td>\n",
       "      <td>[Hi ! I guess the caching mechanism should have considered the new `filter` to be different from the old one, and don't use cached results from the old `filter`.\\r\\nTo avoid other users from having this issue we could make the caching differentiate the two, what do you think ?, If it's easy enough to implement, then yes please 😄  But this issue can be low-priority, since I've only encountered it in a couple of `transformers` CI tests., Well it can cause issue with anyone that updates `datasets` and re-run some code that uses filter, so I'm creating a PR, I just merged a fix, let me know if...</td>\n",
       "      <td>## Describe the bug\\r\\nAfter upgrading to datasets `1.12.0`, some cached `.filter()` steps from `1.11.0` started failing with \\r\\n`ValueError: Keys mismatch: between {'indices': Value(dtype='uint64', id=None)} and {'file': Value(dtype='string', id=None), 'text': Value(dtype='string', id=None), 'speaker_id': Value(dtype='int64', id=None), 'chapter_id': Value(dtype='int64', id=None), 'id': Value(dtype='string', id=None)}`\\r\\n\\r\\nRelated feature: https://github.com/huggingface/datasets/pull/2836\\r\\n\\r\\n:question:  This is probably a `wontfix` bug, since it can be solved by simply cleaning the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues/2941</td>\n",
       "      <td>OSCAR unshuffled_original_ko: NonMatchingSplitsSizesError</td>\n",
       "      <td>[I tried `unshuffled_original_da` and it is also not working]</td>\n",
       "      <td>## Describe the bug\\r\\n\\r\\nCannot download OSCAR `unshuffled_original_ko` due to `NonMatchingSplitsSizesError`.\\r\\n\\r\\n## Steps to reproduce the bug\\r\\n\\r\\n```python\\r\\n&gt;&gt;&gt; dataset = datasets.load_dataset('oscar', 'unshuffled_original_ko')\\r\\nNonMatchingSplitsSizesError: [{'expected': SplitInfo(name='train', num_bytes=25292102197, num_examples=7345075, dataset_name='oscar'), 'recorded': SplitInfo(name='train', num_bytes=25284578514, num_examples=7344907, dataset_name='oscar')}]\\r\\n```\\r\\n\\r\\n## Expected results\\r\\n\\r\\nLoading is successful.\\r\\n\\r\\n## Actual results\\r\\n\\r\\nLoading throws ab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues/2937</td>\n",
       "      <td>load_dataset using default cache on Windows causes PermissionError: [WinError 5] Access is denied</td>\n",
       "      <td>[Hi @daqieq, thanks for reporting.\\r\\n\\r\\nUnfortunately, I was not able to reproduce this bug:\\r\\n```ipython\\r\\nIn [1]: from datasets import load_dataset\\r\\n   ...: ds = load_dataset('wiki_bio')\\r\\nDownloading: 7.58kB [00:00, 26.3kB/s]\\r\\nDownloading: 2.71kB [00:00, ?B/s]\\r\\nUsing custom data configuration default\\r\\nDownloading and preparing dataset wiki_bio/default (download: 318.53 MiB, generated: 736.94 MiB, post-processed: Unknown size, total: 1.03 GiB) to C:\\Users\\username\\.cache\\huggingface\\datasets\\wiki_bio\\default\\\\r\\n1.1.0\\5293ce565954ba965dada626f1e79684e98172d950371d266bf3caaf8...</td>\n",
       "      <td>## Describe the bug\\r\\nStandard process to download and load the wiki_bio dataset causes PermissionError in Windows 10 and 11.\\r\\n\\r\\n## Steps to reproduce the bug\\r\\n```python\\r\\nfrom datasets import load_dataset\\r\\nds = load_dataset('wiki_bio')\\r\\n```\\r\\n\\r\\n## Expected results\\r\\nIt is expected that the dataset downloads without any errors.\\r\\n\\r\\n## Actual results\\r\\nPermissionError see trace below:\\r\\n```\\r\\nUsing custom data configuration default\\r\\nDownloading and preparing dataset wiki_bio/default (download: 318.53 MiB, generated: 736.94 MiB, post-processed: Unknown size, total: 1....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues/2934</td>\n",
       "      <td>to_tf_dataset keeps a reference to the open data somewhere, causing issues on windows</td>\n",
       "      <td>[I did some investigation and, as it seems, the bug stems from [this line](https://github.com/huggingface/datasets/blob/8004d7c3e1d74b29c3e5b0d1660331cd26758363/src/datasets/arrow_dataset.py#L325). The lifecycle of the dataset from the linked line is bound to one of the returned `tf.data.Dataset`. So my (hacky) solution involves wrapping the linked dataset with `weakref.proxy` and adding a custom `__del__` to `tf.python.data.ops.dataset_ops.TensorSliceDataset` (this is the type of a dataset that is returned by `tf.data.Dataset.from_tensor_slices`; this works for TF 2.x, but I'm not sure `t...</td>\n",
       "      <td>To reproduce:\\r\\n```python\\r\\nimport datasets as ds\\r\\nimport weakref\\r\\nimport gc\\r\\n\\r\\nd = ds.load_dataset(\"mnist\", split=\"train\")\\r\\nref = weakref.ref(d._data.table)\\r\\ntfd = d.to_tf_dataset(\"image\", batch_size=1, shuffle=False, label_cols=\"label\")\\r\\ndel tfd, d\\r\\ngc.collect()\\r\\nassert ref() is None, \"Error: there is at least one reference left\"\\r\\n```\\r\\n\\r\\nThis causes issues because the table holds a reference to an open arrow file that should be closed. So on windows it's not possible to delete or move the arrow file afterwards.\\r\\n\\r\\nMoreover the CI test of the `to_tf_dataset` ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              html_url  \\\n",
       "0  https://github.com/huggingface/datasets/issues/2945   \n",
       "1  https://github.com/huggingface/datasets/issues/2943   \n",
       "2  https://github.com/huggingface/datasets/issues/2941   \n",
       "3  https://github.com/huggingface/datasets/issues/2937   \n",
       "4  https://github.com/huggingface/datasets/issues/2934   \n",
       "\n",
       "                                                                                               title  \\\n",
       "0                                                                              Protect master branch   \n",
       "1                            Backwards compatibility broken for cached datasets that use `.filter()`   \n",
       "2                                          OSCAR unshuffled_original_ko: NonMatchingSplitsSizesError   \n",
       "3  load_dataset using default cache on Windows causes PermissionError: [WinError 5] Access is denied   \n",
       "4              to_tf_dataset keeps a reference to the open data somewhere, causing issues on windows   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  comments  \\\n",
       "0                                                                                                                                                          [Cool, I think we can do both :), @lhoestq now the 2 are implemented.\\r\\n\\r\\nPlease note that for the the second protection, finally I have chosen to protect the master branch only from **merge commits** (see update comment above), so no need to disable/re-enable the protection on each release (direct commits, different from merge commits, can be pushed to the remote master branch; and eventually reverted without messing up the repo history).]   \n",
       "1  [Hi ! I guess the caching mechanism should have considered the new `filter` to be different from the old one, and don't use cached results from the old `filter`.\\r\\nTo avoid other users from having this issue we could make the caching differentiate the two, what do you think ?, If it's easy enough to implement, then yes please 😄  But this issue can be low-priority, since I've only encountered it in a couple of `transformers` CI tests., Well it can cause issue with anyone that updates `datasets` and re-run some code that uses filter, so I'm creating a PR, I just merged a fix, let me know if...   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            [I tried `unshuffled_original_da` and it is also not working]   \n",
       "3  [Hi @daqieq, thanks for reporting.\\r\\n\\r\\nUnfortunately, I was not able to reproduce this bug:\\r\\n```ipython\\r\\nIn [1]: from datasets import load_dataset\\r\\n   ...: ds = load_dataset('wiki_bio')\\r\\nDownloading: 7.58kB [00:00, 26.3kB/s]\\r\\nDownloading: 2.71kB [00:00, ?B/s]\\r\\nUsing custom data configuration default\\r\\nDownloading and preparing dataset wiki_bio/default (download: 318.53 MiB, generated: 736.94 MiB, post-processed: Unknown size, total: 1.03 GiB) to C:\\Users\\username\\.cache\\huggingface\\datasets\\wiki_bio\\default\\\\r\\n1.1.0\\5293ce565954ba965dada626f1e79684e98172d950371d266bf3caaf8...   \n",
       "4  [I did some investigation and, as it seems, the bug stems from [this line](https://github.com/huggingface/datasets/blob/8004d7c3e1d74b29c3e5b0d1660331cd26758363/src/datasets/arrow_dataset.py#L325). The lifecycle of the dataset from the linked line is bound to one of the returned `tf.data.Dataset`. So my (hacky) solution involves wrapping the linked dataset with `weakref.proxy` and adding a custom `__del__` to `tf.python.data.ops.dataset_ops.TensorSliceDataset` (this is the type of a dataset that is returned by `tf.data.Dataset.from_tensor_slices`; this works for TF 2.x, but I'm not sure `t...   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      body  \n",
       "0  After accidental merge commit (91c55355b634d0dc73350a7ddee1a6776dbbdd69) into `datasets` master branch, all commits present in the feature branch were permanently added to `datasets` master branch history, as e.g.:\\r\\n- 00cc036fea7c7745cfe722360036ed306796a3f2\\r\\n- 13ae8c98602bbad8197de3b9b425f4c78f582af1\\r\\n- ...\\r\\n\\r\\nI propose to protect our master branch, so that we avoid we can accidentally make this kind of mistakes in the future:\\r\\n- [x] For Pull Requests using GitHub, allow only squash merging, so that only a single commit per Pull Request is merged into the master branch\\r\\n  - ...  \n",
       "1  ## Describe the bug\\r\\nAfter upgrading to datasets `1.12.0`, some cached `.filter()` steps from `1.11.0` started failing with \\r\\n`ValueError: Keys mismatch: between {'indices': Value(dtype='uint64', id=None)} and {'file': Value(dtype='string', id=None), 'text': Value(dtype='string', id=None), 'speaker_id': Value(dtype='int64', id=None), 'chapter_id': Value(dtype='int64', id=None), 'id': Value(dtype='string', id=None)}`\\r\\n\\r\\nRelated feature: https://github.com/huggingface/datasets/pull/2836\\r\\n\\r\\n:question:  This is probably a `wontfix` bug, since it can be solved by simply cleaning the...  \n",
       "2  ## Describe the bug\\r\\n\\r\\nCannot download OSCAR `unshuffled_original_ko` due to `NonMatchingSplitsSizesError`.\\r\\n\\r\\n## Steps to reproduce the bug\\r\\n\\r\\n```python\\r\\n>>> dataset = datasets.load_dataset('oscar', 'unshuffled_original_ko')\\r\\nNonMatchingSplitsSizesError: [{'expected': SplitInfo(name='train', num_bytes=25292102197, num_examples=7345075, dataset_name='oscar'), 'recorded': SplitInfo(name='train', num_bytes=25284578514, num_examples=7344907, dataset_name='oscar')}]\\r\\n```\\r\\n\\r\\n## Expected results\\r\\n\\r\\nLoading is successful.\\r\\n\\r\\n## Actual results\\r\\n\\r\\nLoading throws ab...  \n",
       "3  ## Describe the bug\\r\\nStandard process to download and load the wiki_bio dataset causes PermissionError in Windows 10 and 11.\\r\\n\\r\\n## Steps to reproduce the bug\\r\\n```python\\r\\nfrom datasets import load_dataset\\r\\nds = load_dataset('wiki_bio')\\r\\n```\\r\\n\\r\\n## Expected results\\r\\nIt is expected that the dataset downloads without any errors.\\r\\n\\r\\n## Actual results\\r\\nPermissionError see trace below:\\r\\n```\\r\\nUsing custom data configuration default\\r\\nDownloading and preparing dataset wiki_bio/default (download: 318.53 MiB, generated: 736.94 MiB, post-processed: Unknown size, total: 1....  \n",
       "4  To reproduce:\\r\\n```python\\r\\nimport datasets as ds\\r\\nimport weakref\\r\\nimport gc\\r\\n\\r\\nd = ds.load_dataset(\"mnist\", split=\"train\")\\r\\nref = weakref.ref(d._data.table)\\r\\ntfd = d.to_tf_dataset(\"image\", batch_size=1, shuffle=False, label_cols=\"label\")\\r\\ndel tfd, d\\r\\ngc.collect()\\r\\nassert ref() is None, \"Error: there is at least one reference left\"\\r\\n```\\r\\n\\r\\nThis causes issues because the table holds a reference to an open arrow file that should be closed. So on windows it's not possible to delete or move the arrow file afterwards.\\r\\n\\r\\nMoreover the CI test of the `to_tf_dataset` ...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issues_dataset_1.set_format(\"pandas\")\n",
    "df: pd.DataFrame = issues_dataset_1[:]\n",
    "# OR df = issues_dataset_1.to_pandas()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'Cool, I think we can do both :)'</span>\n",
       " <span style=\"color: #008000; text-decoration-color: #008000\">'@lhoestq now the 2 are implemented.\\r\\n\\r\\nPlease note that for the the second protection, finally I have chosen </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">to protect the master branch only from **merge commits** (see update comment above), so no need to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">disable/re-enable the protection on each release (direct commits, different from merge commits, can be pushed to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the remote master branch; and eventually reverted without messing up the repo history).'</span><span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[32m'Cool, I think we can do both :\u001b[0m\u001b[32m)\u001b[0m\u001b[32m'\u001b[0m\n",
       " \u001b[32m'@lhoestq now the 2 are implemented.\\r\\n\\r\\nPlease note that for the the second protection, finally I have chosen \u001b[0m\n",
       "\u001b[32mto protect the master branch only from **merge commits** \u001b[0m\u001b[32m(\u001b[0m\u001b[32msee update comment above\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, so no need to \u001b[0m\n",
       "\u001b[32mdisable/re-enable the protection on each release \u001b[0m\u001b[32m(\u001b[0m\u001b[32mdirect commits, different from merge commits, can be pushed to \u001b[0m\n",
       "\u001b[32mthe remote master branch; and eventually reverted without messing up the repo history\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.'\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(df[\"comments\"].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# There are two comments in this particular comment index\n",
    "print(len(df[\"comments\"].iloc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>html_url</th>\n",
       "      <th>title</th>\n",
       "      <th>comments</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues/2945</td>\n",
       "      <td>Protect master branch</td>\n",
       "      <td>Cool, I think we can do both :)</td>\n",
       "      <td>After accidental merge commit (91c55355b634d0dc73350a7ddee1a6776dbbdd69) into `datasets` master branch, all commits present in the feature branch were permanently added to `datasets` master branch history, as e.g.:\\r\\n- 00cc036fea7c7745cfe722360036ed306796a3f2\\r\\n- 13ae8c98602bbad8197de3b9b425f4c78f582af1\\r\\n- ...\\r\\n\\r\\nI propose to protect our master branch, so that we avoid we can accidentally make this kind of mistakes in the future:\\r\\n- [x] For Pull Requests using GitHub, allow only squash merging, so that only a single commit per Pull Request is merged into the master branch\\r\\n  - ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues/2945</td>\n",
       "      <td>Protect master branch</td>\n",
       "      <td>@lhoestq now the 2 are implemented.\\r\\n\\r\\nPlease note that for the the second protection, finally I have chosen to protect the master branch only from **merge commits** (see update comment above), so no need to disable/re-enable the protection on each release (direct commits, different from merge commits, can be pushed to the remote master branch; and eventually reverted without messing up the repo history).</td>\n",
       "      <td>After accidental merge commit (91c55355b634d0dc73350a7ddee1a6776dbbdd69) into `datasets` master branch, all commits present in the feature branch were permanently added to `datasets` master branch history, as e.g.:\\r\\n- 00cc036fea7c7745cfe722360036ed306796a3f2\\r\\n- 13ae8c98602bbad8197de3b9b425f4c78f582af1\\r\\n- ...\\r\\n\\r\\nI propose to protect our master branch, so that we avoid we can accidentally make this kind of mistakes in the future:\\r\\n- [x] For Pull Requests using GitHub, allow only squash merging, so that only a single commit per Pull Request is merged into the master branch\\r\\n  - ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues/2943</td>\n",
       "      <td>Backwards compatibility broken for cached datasets that use `.filter()`</td>\n",
       "      <td>Hi ! I guess the caching mechanism should have considered the new `filter` to be different from the old one, and don't use cached results from the old `filter`.\\r\\nTo avoid other users from having this issue we could make the caching differentiate the two, what do you think ?</td>\n",
       "      <td>## Describe the bug\\r\\nAfter upgrading to datasets `1.12.0`, some cached `.filter()` steps from `1.11.0` started failing with \\r\\n`ValueError: Keys mismatch: between {'indices': Value(dtype='uint64', id=None)} and {'file': Value(dtype='string', id=None), 'text': Value(dtype='string', id=None), 'speaker_id': Value(dtype='int64', id=None), 'chapter_id': Value(dtype='int64', id=None), 'id': Value(dtype='string', id=None)}`\\r\\n\\r\\nRelated feature: https://github.com/huggingface/datasets/pull/2836\\r\\n\\r\\n:question:  This is probably a `wontfix` bug, since it can be solved by simply cleaning the...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              html_url  \\\n",
       "0  https://github.com/huggingface/datasets/issues/2945   \n",
       "1  https://github.com/huggingface/datasets/issues/2945   \n",
       "2  https://github.com/huggingface/datasets/issues/2943   \n",
       "\n",
       "                                                                     title  \\\n",
       "0                                                    Protect master branch   \n",
       "1                                                    Protect master branch   \n",
       "2  Backwards compatibility broken for cached datasets that use `.filter()`   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                       comments  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                               Cool, I think we can do both :)   \n",
       "1  @lhoestq now the 2 are implemented.\\r\\n\\r\\nPlease note that for the the second protection, finally I have chosen to protect the master branch only from **merge commits** (see update comment above), so no need to disable/re-enable the protection on each release (direct commits, different from merge commits, can be pushed to the remote master branch; and eventually reverted without messing up the repo history).   \n",
       "2                                                                                                                                          Hi ! I guess the caching mechanism should have considered the new `filter` to be different from the old one, and don't use cached results from the old `filter`.\\r\\nTo avoid other users from having this issue we could make the caching differentiate the two, what do you think ?   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      body  \n",
       "0  After accidental merge commit (91c55355b634d0dc73350a7ddee1a6776dbbdd69) into `datasets` master branch, all commits present in the feature branch were permanently added to `datasets` master branch history, as e.g.:\\r\\n- 00cc036fea7c7745cfe722360036ed306796a3f2\\r\\n- 13ae8c98602bbad8197de3b9b425f4c78f582af1\\r\\n- ...\\r\\n\\r\\nI propose to protect our master branch, so that we avoid we can accidentally make this kind of mistakes in the future:\\r\\n- [x] For Pull Requests using GitHub, allow only squash merging, so that only a single commit per Pull Request is merged into the master branch\\r\\n  - ...  \n",
       "1  After accidental merge commit (91c55355b634d0dc73350a7ddee1a6776dbbdd69) into `datasets` master branch, all commits present in the feature branch were permanently added to `datasets` master branch history, as e.g.:\\r\\n- 00cc036fea7c7745cfe722360036ed306796a3f2\\r\\n- 13ae8c98602bbad8197de3b9b425f4c78f582af1\\r\\n- ...\\r\\n\\r\\nI propose to protect our master branch, so that we avoid we can accidentally make this kind of mistakes in the future:\\r\\n- [x] For Pull Requests using GitHub, allow only squash merging, so that only a single commit per Pull Request is merged into the master branch\\r\\n  - ...  \n",
       "2  ## Describe the bug\\r\\nAfter upgrading to datasets `1.12.0`, some cached `.filter()` steps from `1.11.0` started failing with \\r\\n`ValueError: Keys mismatch: between {'indices': Value(dtype='uint64', id=None)} and {'file': Value(dtype='string', id=None), 'text': Value(dtype='string', id=None), 'speaker_id': Value(dtype='int64', id=None), 'chapter_id': Value(dtype='int64', id=None), 'id': Value(dtype='string', id=None)}`\\r\\n\\r\\nRelated feature: https://github.com/huggingface/datasets/pull/2836\\r\\n\\r\\n:question:  This is probably a `wontfix` bug, since it can be solved by simply cleaning the...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the explode method to create a row for each comment in a given index.\n",
    "# i.e. index 0 with 2 comments creates 2 rows and index 1 with 6 comments creates 6 rows, etc.\n",
    "comments_df: pd.DataFrame = df.explode(\"comments\", ignore_index=True)\n",
    "comments_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'comments', 'body'],\n",
       "    num_rows: 2964\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert back to Dataset\n",
    "comments_dataset: Dataset = Dataset.from_pandas(comments_df)\n",
    "comments_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e05cda94c5754850a0a3c6ff20b87282",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2964 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'comments', 'body', 'comment_length'],\n",
       "    num_rows: 2964\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add n new columns\n",
    "comments_dataset = comments_dataset.map(\n",
    "    lambda x: {\"comment_length\": len(x.get(\"comments\").split())}\n",
    ")\n",
    "\n",
    "comments_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'comments': ['https://github.com/huggingface/datasets/blob/6c766f9115d686182d76b1b937cb27e099c45d68/src/datasets/builder.py#L179-L186',\n",
       "  'https://github.com/huggingface/datasets/blob/6c766f9115d686182d76b1b937cb27e099c45d68/src/datasets/builder.py#L179-L186',\n",
       "  '@albertvillanova ',\n",
       "  'Thanks!',\n",
       "  '#self-assign',\n",
       "  '#take',\n",
       "  '#take',\n",
       "  '#self-assign',\n",
       "  'Resolved',\n",
       "  'Ty!'],\n",
       " 'comment_length': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Short comments\n",
    "comments_dataset.sort(\"comment_length\").select_columns([\"comments\", \"comment_length\"])[\n",
    "    :10\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b22db812c754220b3fae9efebc382a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/2964 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'comments', 'body', 'comment_length'],\n",
       "    num_rows: 2175\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop short comments.\n",
    "# i.e comments like: 'Thanks!', '#self-assign', etc.\n",
    "comments_dataset = comments_dataset.filter(lambda x: x.get(\"comment_length\") > 15)\n",
    "comments_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce810079d1b343039eaab1c3941e83ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2175 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'comments', 'body', 'comment_length', 'text'],\n",
       "    num_rows: 2175\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate the issue title, description (body), and comments together in a new text column.\n",
    "def concat_data(example: dict[str, Any]) -> dict[str, Any]:\n",
    "    \"\"\"This is used to concatenate the title, body and comments together.\"\"\"\n",
    "    title: str = example.get(\"title\")\n",
    "    comments: str = example.get(\"comments\")\n",
    "    body: str = example.get(\"body\")\n",
    "    result = {\"text\": f\"{title} \\n {body} \\n {comments}\"}\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "comments_dataset = comments_dataset.map(concat_data)\n",
    "comments_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'html_url': 'https://github.com/huggingface/datasets/issues/2945',\n",
       " 'title': 'Protect master branch',\n",
       " 'comments': '@lhoestq now the 2 are implemented.\\r\\n\\r\\nPlease note that for the the second protection, finally I have chosen to protect the master branch only from **merge commits** (see update comment above), so no need to disable/re-enable the protection on each release (direct commits, different from merge commits, can be pushed to the remote master branch; and eventually reverted without messing up the repo history).',\n",
       " 'body': 'After accidental merge commit (91c55355b634d0dc73350a7ddee1a6776dbbdd69) into `datasets` master branch, all commits present in the feature branch were permanently added to `datasets` master branch history, as e.g.:\\r\\n- 00cc036fea7c7745cfe722360036ed306796a3f2\\r\\n- 13ae8c98602bbad8197de3b9b425f4c78f582af1\\r\\n- ...\\r\\n\\r\\nI propose to protect our master branch, so that we avoid we can accidentally make this kind of mistakes in the future:\\r\\n- [x] For Pull Requests using GitHub, allow only squash merging, so that only a single commit per Pull Request is merged into the master branch\\r\\n  - Currently, simple merge commits are already disabled\\r\\n  - I propose to disable rebase merging as well\\r\\n- ~~Protect the master branch from direct pushes (to avoid accidentally pushing of merge commits)~~\\r\\n  - ~~This protection would reject direct pushes to master branch~~\\r\\n  - ~~If so, for each release (when we need to commit directly to the master branch), we should previously disable the protection and re-enable it again after the release~~\\r\\n- [x] Protect the master branch only from direct pushing of **merge commits**\\r\\n  - GitHub offers the possibility to protect the master branch only from merge commits (which are the ones that introduce all the commits from the feature branch into the master branch).\\r\\n  - No need to disable/re-enable this protection on each release \\r\\n\\r\\nThis purpose of this Issue is to open a discussion about this problem and to agree in a solution.',\n",
       " 'comment_length': 64,\n",
       " 'text': 'Protect master branch \\n After accidental merge commit (91c55355b634d0dc73350a7ddee1a6776dbbdd69) into `datasets` master branch, all commits present in the feature branch were permanently added to `datasets` master branch history, as e.g.:\\r\\n- 00cc036fea7c7745cfe722360036ed306796a3f2\\r\\n- 13ae8c98602bbad8197de3b9b425f4c78f582af1\\r\\n- ...\\r\\n\\r\\nI propose to protect our master branch, so that we avoid we can accidentally make this kind of mistakes in the future:\\r\\n- [x] For Pull Requests using GitHub, allow only squash merging, so that only a single commit per Pull Request is merged into the master branch\\r\\n  - Currently, simple merge commits are already disabled\\r\\n  - I propose to disable rebase merging as well\\r\\n- ~~Protect the master branch from direct pushes (to avoid accidentally pushing of merge commits)~~\\r\\n  - ~~This protection would reject direct pushes to master branch~~\\r\\n  - ~~If so, for each release (when we need to commit directly to the master branch), we should previously disable the protection and re-enable it again after the release~~\\r\\n- [x] Protect the master branch only from direct pushing of **merge commits**\\r\\n  - GitHub offers the possibility to protect the master branch only from merge commits (which are the ones that introduce all the commits from the feature branch into the master branch).\\r\\n  - No need to disable/re-enable this protection on each release \\r\\n\\r\\nThis purpose of this Issue is to open a discussion about this problem and to agree in a solution. \\n @lhoestq now the 2 are implemented.\\r\\n\\r\\nPlease note that for the the second protection, finally I have chosen to protect the master branch only from **merge commits** (see update comment above), so no need to disable/re-enable the protection on each release (direct commits, different from merge commits, can be pushed to the remote master branch; and eventually reverted without messing up the repo history).'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Text Embeddings\n",
    "\n",
    "- This [table](https://www.sbert.net/docs/pretrained_models.html#model-overview) shows the model overview for open source semantic search.\n",
    "\n",
    "<br>\n",
    "\n",
    "[![image.png](https://i.postimg.cc/MZLLL8TS/image.png)](https://postimg.cc/c6QTK2w9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cbce3988ec94c0a9f7a56507c4fb282",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87aa59cda9f84a7f9e264597bd5a7b1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4080d535bafd4e72ac31db01d13e47a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8a4a7004d594fd6b188c2a29707ee16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0c7cbb73e7445f08560504346a89841",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f977069414e47f8993f8c4611032585",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "\n",
    "checkpoint: str = \"sentence-transformers/multi-qa-mpnet-base-dot-v1\"\n",
    "\n",
    "# Instantiate tokenizer\n",
    "tokenizer: AutoTokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "# Instantiate model\n",
    "model: AutoModel = AutoModel.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# To speed up the processing, push to a GPU\n",
    "device_str: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device(device=device_str)\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MPNetModel(\n",
       "  (embeddings): MPNetEmbeddings(\n",
       "    (word_embeddings): Embedding(30527, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): MPNetEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x MPNetLayer(\n",
       "        (attention): MPNetAttention(\n",
       "          (attn): MPNetSelfAttention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (intermediate): MPNetIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): MPNetOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (relative_attention_bias): Embedding(32, 12)\n",
       "  )\n",
       "  (pooler): MPNetPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To represent each GitHub issue entry as a single vector, we need to pool or\n",
    "# average the token embeddings. One popular approach is CLS pooling, where the last\n",
    "# hidden state for the special [CLS] token is collected.\n",
    "\n",
    "\n",
    "def cls_pooling(model_output):\n",
    "    \"\"\"To represent each GitHub issue entry as a single vector, pool or average the token embeddings.\n",
    "    Using CLS pooling, where the last hidden state for the special [CLS] token is collected.\"\"\"\n",
    "    return model_output.last_hidden_state[:, 0]\n",
    "\n",
    "\n",
    "# Create a helper function that will tokenize a list of documents, place the tensors\n",
    "# on the GPU (if available), feed them to the model, and finally apply CLS pooling to the outputs:\n",
    "\n",
    "\n",
    "def get_embeddings(text_list: list):\n",
    "    \"\"\"This is used to tokenize a list of documents, place the tensors\n",
    "    on the GPU (if available), feed them to the model, and apply CLS pooling to the outputs.\"\"\"\n",
    "    encoded_input = tokenizer(\n",
    "        text_list, padding=True, truncation=True, return_tensors=\"pt\"\n",
    "    )\n",
    "    # Push to encoded input to the GPU (if available)\n",
    "    encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n",
    "    model_output = model(**encoded_input)\n",
    "    return cls_pooling(model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Protect master branch \n",
       " After accidental merge commit <span style=\"font-weight: bold\">(</span>91c55355b634d0dc73350a7ddee1a6776dbbdd69<span style=\"font-weight: bold\">)</span> into `datasets` master branch, all \n",
       "commits present in the feature branch were permanently added to `datasets` master branch history, as e.g.:\n",
       "- 00cc036fea7c7745cfe722360036ed306796a3f2\n",
       "- 13ae8c98602bbad8197de3b9b425f4c78f582af1\n",
       "- <span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "\n",
       "I propose to protect our master branch, so that we avoid we can accidentally make this kind of mistakes in the \n",
       "future:\n",
       "-  For Pull Requests using GitHub, allow only squash merging, so that only a single commit per Pull Request is \n",
       "merged into the master branch\n",
       "  - Currently, simple merge commits are already disabled\n",
       "  - I propose to disable rebase merging as well\n",
       "- ~~Protect the master branch from direct pushes <span style=\"font-weight: bold\">(</span>to avoid accidentally pushing of merge commits<span style=\"font-weight: bold\">)</span>~~\n",
       "  - ~~This protection would reject direct pushes to master branch~~\n",
       "  - ~~If so, for each release <span style=\"font-weight: bold\">(</span>when we need to commit directly to the master branch<span style=\"font-weight: bold\">)</span>, we should previously disable \n",
       "the protection and re-enable it again after the release~~\n",
       "-  Protect the master branch only from direct pushing of **merge commits**\n",
       "  - GitHub offers the possibility to protect the master branch only from merge commits <span style=\"font-weight: bold\">(</span>which are the ones that \n",
       "introduce all the commits from the feature branch into the master branch<span style=\"font-weight: bold\">)</span>.\n",
       "  - No need to disable/re-enable this protection on each release \n",
       "\n",
       "This purpose of this Issue is to open a discussion about this problem and to agree in a solution. \n",
       " @lhoestq now the <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> are implemented.\n",
       "\n",
       "Please note that for the the second protection, finally I have chosen to protect the master branch only from \n",
       "**merge commits** <span style=\"font-weight: bold\">(</span>see update comment above<span style=\"font-weight: bold\">)</span>, so no need to disable/re-enable the protection on each release \n",
       "<span style=\"font-weight: bold\">(</span>direct commits, different from merge commits, can be pushed to the remote master branch; and eventually reverted \n",
       "without messing up the repo history<span style=\"font-weight: bold\">)</span>.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Protect master branch \n",
       " After accidental merge commit \u001b[1m(\u001b[0m91c55355b634d0dc73350a7ddee1a6776dbbdd69\u001b[1m)\u001b[0m into `datasets` master branch, all \n",
       "commits present in the feature branch were permanently added to `datasets` master branch history, as e.g.:\n",
       "- 00cc036fea7c7745cfe722360036ed306796a3f2\n",
       "- 13ae8c98602bbad8197de3b9b425f4c78f582af1\n",
       "- \u001b[33m...\u001b[0m\n",
       "\n",
       "I propose to protect our master branch, so that we avoid we can accidentally make this kind of mistakes in the \n",
       "future:\n",
       "-  For Pull Requests using GitHub, allow only squash merging, so that only a single commit per Pull Request is \n",
       "merged into the master branch\n",
       "  - Currently, simple merge commits are already disabled\n",
       "  - I propose to disable rebase merging as well\n",
       "- ~~Protect the master branch from direct pushes \u001b[1m(\u001b[0mto avoid accidentally pushing of merge commits\u001b[1m)\u001b[0m~~\n",
       "  - ~~This protection would reject direct pushes to master branch~~\n",
       "  - ~~If so, for each release \u001b[1m(\u001b[0mwhen we need to commit directly to the master branch\u001b[1m)\u001b[0m, we should previously disable \n",
       "the protection and re-enable it again after the release~~\n",
       "-  Protect the master branch only from direct pushing of **merge commits**\n",
       "  - GitHub offers the possibility to protect the master branch only from merge commits \u001b[1m(\u001b[0mwhich are the ones that \n",
       "introduce all the commits from the feature branch into the master branch\u001b[1m)\u001b[0m.\n",
       "  - No need to disable/re-enable this protection on each release \n",
       "\n",
       "This purpose of this Issue is to open a discussion about this problem and to agree in a solution. \n",
       " @lhoestq now the \u001b[1;36m2\u001b[0m are implemented.\n",
       "\n",
       "Please note that for the the second protection, finally I have chosen to protect the master branch only from \n",
       "**merge commits** \u001b[1m(\u001b[0msee update comment above\u001b[1m)\u001b[0m, so no need to disable/re-enable the protection on each release \n",
       "\u001b[1m(\u001b[0mdirect commits, different from merge commits, can be pushed to the remote master branch; and eventually reverted \n",
       "without messing up the repo history\u001b[1m)\u001b[0m.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(comments_dataset[\"text\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test that the function works by feeding it the first text entry in our corpus\n",
    "# and inspecting the output shape;\n",
    "embedding = get_embeddings(comments_dataset[\"text\"][0])\n",
    "embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.5532e-01, -1.0023e-01, -7.0321e-02, -7.9816e-02, -1.0425e-01,\n",
       "         -1.8799e-01,  1.0403e-02,  2.7286e-01, -9.8491e-03, -8.4140e-02,\n",
       "          2.9261e-01, -7.7875e-02, -1.3246e-01,  2.1589e-01, -5.6229e-02,\n",
       "          1.7055e-01,  2.0032e-01, -4.0798e-02, -9.2333e-02,  2.7923e-02,\n",
       "          2.2609e-02, -6.8703e-02,  8.3200e-02,  4.3630e-02, -1.8378e-01,\n",
       "          5.4009e-03, -1.4811e-02,  1.5650e-01, -4.3977e-01, -4.6957e-01,\n",
       "          1.4525e-01,  2.3224e-01,  2.5106e-02,  5.7053e-01, -1.0436e-04,\n",
       "         -3.4580e-04,  1.6949e-01, -4.0601e-02, -1.4010e-01, -2.1546e-01,\n",
       "         -4.9011e-01, -4.4480e-01, -7.0063e-02, -8.3547e-03,  1.2881e-01,\n",
       "          5.2868e-02, -1.3250e-01,  2.3846e-02,  3.8485e-01,  1.0555e-01,\n",
       "          2.3861e-01, -1.0285e-01,  1.8124e-01,  1.3770e-02,  3.9201e-01,\n",
       "          5.4745e-01, -6.6635e-02,  3.0320e-01,  5.4018e-02,  6.7668e-04,\n",
       "          6.1106e-02,  1.7411e-01, -8.2373e-02, -2.8667e-01,  2.2702e-01,\n",
       "         -1.7799e-01,  5.1817e-01, -4.2435e-01, -2.4158e-01, -8.3117e-02,\n",
       "          5.2379e-02, -1.7815e-01, -4.0890e-01, -2.4194e-01, -1.4995e-01,\n",
       "          7.6336e-03,  6.7781e-02,  2.4686e-01,  3.9740e-02,  3.6495e-03,\n",
       "         -3.7737e-01, -2.7181e-01, -4.4080e-03,  4.9204e-02,  1.4194e-01,\n",
       "         -1.6263e-01,  6.6426e-02, -1.2704e-01,  2.9422e-01, -4.7328e-02,\n",
       "         -1.4698e-01, -3.6763e-01, -2.4133e-01, -1.2587e-02, -1.0261e-01,\n",
       "         -1.9575e-01, -2.4621e-01, -3.2483e-01,  4.2513e-01,  3.8897e-01,\n",
       "         -3.8327e-01,  1.3740e-02, -3.0489e-02, -8.1016e-02,  2.0718e-01,\n",
       "          1.1786e-01, -1.0490e-02,  5.8489e-02,  5.3071e-01,  3.1444e-01,\n",
       "          5.1579e-03,  3.2559e-01,  3.0157e-01,  3.0475e-03,  3.4447e-02,\n",
       "          5.7412e-01,  4.2894e-01, -2.6292e-01,  1.2765e-01,  1.9597e-01,\n",
       "         -3.4120e-01, -3.8451e-01,  7.1131e-02, -8.6172e-02,  1.1033e-01,\n",
       "          2.7133e-02, -1.1471e-01,  6.6802e-02,  4.5191e-02, -3.8086e-02,\n",
       "         -3.0790e-01, -1.4149e-01, -3.7440e-01, -6.8530e-02,  1.8604e-01,\n",
       "         -4.5710e-01,  1.4917e-01,  4.3252e-01, -6.8038e-02, -1.9929e-01,\n",
       "          4.4552e-02,  1.2923e-01,  5.5586e-02,  2.9612e-01, -1.5380e-01,\n",
       "         -4.5263e-01,  2.2360e-01,  1.3133e-01, -6.6899e-02,  1.9400e-01,\n",
       "         -1.5990e-01, -2.6162e-01, -1.0498e-01,  2.9777e-01, -1.4827e-01,\n",
       "          3.0716e-01, -5.2377e-01,  3.6373e-02, -4.4834e-03,  3.5067e-01,\n",
       "          4.9104e-01,  3.0255e-01,  3.2808e-01, -1.6859e-01,  6.1757e-02,\n",
       "          1.9035e-01,  3.2351e-01, -5.1687e-02,  6.2305e-02, -2.8774e-01,\n",
       "          1.0950e-01,  1.4507e-01, -3.1216e-01,  1.7557e-02, -5.2359e-03,\n",
       "         -1.3128e-01,  1.0777e-02, -7.1407e-02, -1.1199e-01, -9.8930e-02,\n",
       "         -3.6669e-01,  6.9402e-02, -4.0737e-02, -1.0078e-01,  2.0796e-01,\n",
       "         -3.2017e-01, -9.4794e-02,  2.0508e-01, -1.4812e-01,  2.0898e-02,\n",
       "         -2.9199e-01, -6.0614e-01, -1.6366e-01, -6.3439e-02,  1.2491e-01,\n",
       "          2.2756e-01,  3.6077e-01, -5.0310e-03, -2.8362e-02, -1.7140e-01,\n",
       "         -9.3979e-03,  1.1251e-01,  3.4214e-01, -1.4968e-01, -2.4154e-01,\n",
       "          1.3225e-01, -3.5459e-01,  9.1141e-02, -7.8513e-02, -9.7753e-03,\n",
       "          3.4449e-02, -3.1579e-01,  3.4128e-01, -4.4053e-02, -1.2080e-01,\n",
       "         -1.1415e-01,  2.4152e-01,  9.5951e-02, -1.7473e-01, -2.0392e-01,\n",
       "         -2.0741e-01,  2.6019e-01, -2.4083e-01,  3.0187e-01,  1.3001e-01,\n",
       "          1.0264e-01, -1.4118e-01, -3.6454e-02, -2.5096e-02,  3.6194e-01,\n",
       "          2.3630e-01, -8.9917e-02, -5.9159e-02,  1.6876e-01, -4.9063e-02,\n",
       "          2.0357e-01,  2.2469e-01,  4.2524e-01,  3.7362e-01, -1.0268e-01,\n",
       "          6.2601e-02, -9.1793e-02, -7.8725e-02,  2.4594e-02, -3.2913e-01,\n",
       "          3.2804e-01,  7.3694e-02, -1.3946e-01, -1.7403e-01, -1.0333e-01,\n",
       "         -1.7407e-01, -2.2027e-01, -1.7077e-01, -1.2951e-02, -1.6910e-01,\n",
       "          1.9436e-01, -3.3659e-01,  2.2237e-01, -2.7721e-01,  1.4858e-01,\n",
       "          1.5052e-02, -2.5161e-01, -1.0240e-01,  6.8298e-02, -5.9668e-03,\n",
       "         -1.3271e-01,  1.2417e-01,  3.7738e-01,  1.2787e-01,  1.7010e-01,\n",
       "          4.2740e-03,  2.0152e-02,  2.5728e-01, -1.1644e-01,  3.5333e-01,\n",
       "          1.4431e-01, -1.0544e-01,  5.4065e-02,  3.3646e-01,  1.8122e-01,\n",
       "          6.4332e-02,  3.4692e-01, -3.3246e-03,  3.5258e-02, -2.0841e-01,\n",
       "         -4.5985e-02, -2.6021e-01, -5.2834e-01,  2.1924e-01, -1.1064e-01,\n",
       "         -3.5522e-01, -8.5036e-03, -4.5366e-02,  4.7418e-02, -5.2487e-01,\n",
       "          1.3603e-01,  1.8358e-01,  2.9053e-01, -3.1780e-01, -1.1998e-01,\n",
       "          1.2632e-01, -2.2595e-02,  1.2344e-02,  1.2856e-01,  3.6316e-01,\n",
       "         -2.2668e-01,  4.5945e-01,  2.3898e-01, -1.6956e-01, -4.7264e-01,\n",
       "         -3.4341e-01,  1.9130e-02, -2.0394e-01, -1.1797e-01,  9.8188e-03,\n",
       "         -9.8931e-02,  9.2153e-02, -3.5854e-01, -3.2571e-01, -1.8774e-02,\n",
       "         -3.7300e-01, -1.6010e-01,  6.9595e-02,  6.2886e-02, -2.3462e-01,\n",
       "         -1.1404e-01, -3.8659e-01, -2.8215e-01,  4.7936e-01, -1.6433e-02,\n",
       "          1.1566e-01, -4.3174e-02, -2.9238e-01, -4.5918e-02,  8.7482e-02,\n",
       "          1.3664e-01, -4.4778e-05, -3.6093e-01,  3.2567e-02, -4.0660e-02,\n",
       "         -2.9347e-02,  3.5261e-01,  1.8549e-01, -4.3024e-01,  7.1251e-02,\n",
       "         -3.2176e-01, -1.5582e-01, -5.2085e-02,  4.8644e-01,  3.1412e-01,\n",
       "         -9.7374e-02,  7.1080e-02,  1.0355e-01, -1.9190e-01,  5.5089e-02,\n",
       "         -1.8161e-01,  1.4722e-01,  3.4549e-01,  2.8572e-01,  3.7900e-02,\n",
       "          1.4587e-01,  2.5497e-02,  7.1409e-01,  3.7785e-01, -5.9658e-02,\n",
       "          2.1034e-01,  3.1070e-01, -2.7719e-02,  3.9907e-03, -2.6016e-01,\n",
       "          6.5047e-02, -1.0124e-01, -2.0858e-05,  8.6704e-02, -1.2958e-01,\n",
       "         -1.2297e-01, -1.1211e-02, -4.7771e-01,  9.1553e-03, -4.1468e-01,\n",
       "         -2.3966e-01, -3.1818e-01,  1.8425e-01, -2.2526e-02, -4.8549e-01,\n",
       "          1.3138e-01,  5.3771e-02,  2.3058e-01,  2.1597e-01,  2.1421e-01,\n",
       "          1.8122e-01, -2.0259e-02, -5.1085e-02, -1.2007e-01, -6.7726e-02,\n",
       "         -4.3315e-03,  1.4525e-01,  9.0580e-02, -1.6704e-01,  3.1806e-02,\n",
       "          6.9550e-02,  3.2999e-01, -1.4868e-02,  1.4029e-01, -1.7081e-01,\n",
       "         -5.0948e-01,  6.0863e-01, -3.1859e-01,  1.3940e-01,  2.9910e-01,\n",
       "          5.1123e-01,  4.3976e-01, -2.9545e-01, -1.9532e-01,  2.0712e-01,\n",
       "          3.2556e-02,  1.9784e-01,  1.7735e-01,  1.4498e-01,  2.7831e-01,\n",
       "         -4.9038e-01,  2.8877e-01,  1.5483e-01, -2.5048e-01, -1.5518e-01,\n",
       "         -3.9057e-03, -2.7051e-02,  1.2646e-01, -2.6305e-02, -2.4540e-01,\n",
       "          9.1276e-03, -1.0944e-01,  1.7345e-01,  1.3292e-01,  6.9828e-02,\n",
       "          4.1522e-01,  5.9466e-01,  1.3493e-01, -3.8920e-01, -4.8202e-02,\n",
       "         -1.7507e-01, -7.4283e-02,  3.2714e-01,  1.9835e-01,  2.4823e-01,\n",
       "          2.0349e-02,  1.7769e-01, -1.2347e-01, -2.6624e-01, -2.0337e-01,\n",
       "         -3.8491e-02, -7.6742e-02,  7.5898e-03,  8.7330e-02, -4.5898e-02,\n",
       "         -9.0992e-02,  1.3048e-02,  1.4000e-01, -3.6610e-01,  1.6378e-01,\n",
       "          6.1805e-01,  9.8255e-01,  3.9757e-01,  8.9033e-02, -2.5466e-01,\n",
       "         -3.2738e-01,  3.2599e-01, -3.4011e-01, -1.6645e-01, -2.4702e-01,\n",
       "         -1.2359e-01, -1.8425e-01, -3.4031e-01, -3.8694e-02, -4.8985e-01,\n",
       "         -3.5286e-01,  1.3511e-01, -2.1806e-01,  5.2792e-01,  8.1777e-03,\n",
       "          1.2339e-01,  1.9102e-01, -1.2231e-01,  2.5515e-01,  1.9827e-01,\n",
       "          6.1811e-02,  1.0410e-01, -4.0280e-03, -4.6879e-01, -2.2320e-03,\n",
       "          2.0539e-02, -3.7727e-01,  8.9147e-02, -3.4378e-01,  3.8847e-02,\n",
       "         -2.0635e-01, -3.1605e-01,  1.1583e-01, -8.7048e-02,  6.0594e-01,\n",
       "          1.6663e-01, -7.9452e-02, -4.4621e-02,  2.9139e-01,  4.6452e-01,\n",
       "         -2.2885e-01, -2.1953e-01,  5.2237e-01, -2.1336e-01, -1.3022e-01,\n",
       "         -2.0476e-01, -8.0335e-02, -2.3709e-01, -2.6095e-01, -1.2804e-01,\n",
       "         -3.2823e-01,  7.9502e-02,  4.1714e-02,  2.6365e-01, -9.1070e-02,\n",
       "         -1.6761e-01,  1.8599e-01,  1.1549e-01, -1.0757e-01, -1.1858e-01,\n",
       "         -1.6952e-01, -2.6104e-01, -6.7816e-02,  2.0160e-01,  1.6161e-01,\n",
       "         -1.2061e-01, -7.6764e-02,  9.9658e-02, -2.4434e-01, -2.2079e-01,\n",
       "         -2.0729e-01,  1.0599e-01, -1.3947e-01,  9.2981e-02, -5.4868e-01,\n",
       "         -3.2993e-01,  5.1746e-02,  2.4619e-01,  2.7942e-01,  3.3050e-01,\n",
       "          1.1800e-01, -9.9771e-02, -2.2445e-01,  2.1864e-01, -3.2793e-01,\n",
       "          1.6865e-01,  2.3585e-02,  1.5881e-01, -2.6010e-02,  4.0431e-02,\n",
       "         -3.9060e-01,  1.6032e-01, -1.2548e-01, -2.1127e-02, -2.2552e-01,\n",
       "         -1.0175e-01, -6.3097e-04, -3.0819e-01,  3.2487e-02,  2.3434e-02,\n",
       "         -1.9576e-01, -2.8302e-01,  9.2669e-02,  6.5696e-02,  6.4789e-02,\n",
       "         -5.7014e-02, -5.7461e-02, -1.0816e-01,  8.2753e-02,  2.7203e-01,\n",
       "          2.3969e-01, -1.5849e-01,  1.0971e-01, -3.2225e-02,  1.7109e-01,\n",
       "          1.1058e-01, -2.4773e-02,  8.9242e-02,  1.4633e-02, -2.6580e-01,\n",
       "         -3.5375e-02,  2.6716e-01, -1.8175e-01, -2.1714e-01, -2.6854e-01,\n",
       "          1.8883e-01,  1.0563e-01,  5.8409e-01,  2.0288e-01,  1.4422e-01,\n",
       "         -4.0508e-01,  1.7541e-01, -5.3198e-02, -3.6357e-03, -1.6924e-01,\n",
       "         -9.6940e-02, -8.4962e-02,  2.7936e-01, -8.6720e-02, -3.0360e-01,\n",
       "          2.7138e-01,  1.8818e-01,  7.4830e-04,  1.0497e-01,  5.2807e-01,\n",
       "          1.2962e-01, -1.8965e-01,  4.2214e-01,  2.7124e-01, -8.7676e-02,\n",
       "          4.9490e-01,  4.7023e-01, -1.1318e-01, -2.0325e-02, -8.2736e-02,\n",
       "          1.1661e-01,  2.3069e-01, -2.5410e-01,  1.2847e-01,  2.9622e-01,\n",
       "          6.4630e-01, -1.6888e-01,  3.9356e-01, -3.3096e-02,  3.4374e-01,\n",
       "          1.9799e-01,  5.8740e-02, -2.4605e-01,  3.9271e-01,  2.1171e-01,\n",
       "          3.3015e-01, -6.1899e-01,  1.3637e-01,  4.9140e-01,  4.9283e-02,\n",
       "          1.9958e-01, -4.0011e-01, -4.9233e-02, -5.8476e-02, -4.8314e-02,\n",
       "         -7.8183e-02,  1.6350e-01,  1.2374e-01,  2.8297e-01,  6.5844e-03,\n",
       "         -8.8378e-02, -4.3441e-02,  1.8505e-01,  2.4156e-01, -1.1700e-01,\n",
       "         -1.8117e-01,  3.5776e-01, -2.7620e-01,  6.6493e-02,  2.1546e-01,\n",
       "          1.3507e-01, -7.4716e-02, -6.8634e-02,  1.8023e-01,  4.5039e-01,\n",
       "         -1.6993e-01,  2.2512e-02,  5.5749e-02, -1.6257e-01,  1.5772e-01,\n",
       "          8.0147e-02,  2.5803e-01, -5.3341e-02,  1.1280e-01,  3.5427e-02,\n",
       "         -4.1530e-01, -2.5322e-02,  4.3308e-01,  5.5646e-01, -3.3773e-01,\n",
       "          4.6014e-01,  1.1927e-01,  2.8470e-01, -3.2684e-02,  4.3744e-01,\n",
       "         -4.7126e-02,  1.1639e-01, -5.1538e-03,  1.4308e-01,  2.8089e-02,\n",
       "          2.5759e-01,  1.4150e-01, -3.3345e-02, -1.5929e-01,  3.0106e-02,\n",
       "         -1.2154e-01,  1.2744e-01,  2.9684e-01,  1.7307e-01,  4.1989e-01,\n",
       "         -2.9471e-01,  1.2757e-01,  1.2823e-01, -8.7569e-02, -3.4140e-01,\n",
       "          9.5221e-03,  9.4085e-02, -3.6053e-01,  4.2475e-03,  2.2248e-01,\n",
       "         -6.9440e-02, -2.4195e-02,  1.1893e-01,  4.0948e-01,  1.9279e-01,\n",
       "          1.1209e-01,  1.0513e-01, -4.9348e-02,  3.2094e-01, -3.9185e-01,\n",
       "          2.2496e-01,  1.8922e-01, -1.0293e-01, -2.2189e-01, -5.1098e-02,\n",
       "          7.3670e-03,  2.1281e-02, -4.3551e-01, -1.4018e-01, -1.6223e-01,\n",
       "          4.2098e-01, -1.3560e-01,  6.8298e-02, -3.7222e-01,  6.8767e-02,\n",
       "         -3.7135e-02,  2.6836e-01, -3.0010e-01,  8.6408e-02,  5.2548e-02,\n",
       "          1.7338e-02,  8.8371e-02,  1.3058e-01,  6.8434e-02,  2.5285e-01,\n",
       "         -3.5232e-01, -1.2489e-01,  3.5866e-01, -1.9469e-02,  9.1036e-02,\n",
       "         -2.1727e-01,  4.1820e-01,  4.6545e-01, -1.4808e-01, -3.3211e-01,\n",
       "         -2.6352e-01, -1.1156e-02, -2.7983e-01, -5.4209e-01,  2.9864e-01,\n",
       "          1.3787e-01,  1.9744e-01, -1.0587e-01,  3.6607e-02, -7.4327e-02,\n",
       "         -1.5276e-01,  6.1171e-02, -1.9879e-01]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "272de99e447e4ef3a90d808acee6412a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2175 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/neidu/Desktop/Projects/Personal/My_Projects/NLP-Tutorial/notebook/06_Transformers/05_semantic_search.ipynb Cell 30\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/neidu/Desktop/Projects/Personal/My_Projects/NLP-Tutorial/notebook/06_Transformers/05_semantic_search.ipynb#X55sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Get embeddings for the entire dataset\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/neidu/Desktop/Projects/Personal/My_Projects/NLP-Tutorial/notebook/06_Transformers/05_semantic_search.ipynb#X55sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m comments_dataset \u001b[39m=\u001b[39m comments_dataset\u001b[39m.\u001b[39;49mmap(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/neidu/Desktop/Projects/Personal/My_Projects/NLP-Tutorial/notebook/06_Transformers/05_semantic_search.ipynb#X55sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mlambda\u001b[39;49;00m x: {\u001b[39m\"\u001b[39;49m\u001b[39membeddings\u001b[39;49m\u001b[39m\"\u001b[39;49m: get_embeddings(x\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mtext\u001b[39;49m\u001b[39m\"\u001b[39;49m))\u001b[39m.\u001b[39;49mdetach()\u001b[39m.\u001b[39;49mcpu()\u001b[39m.\u001b[39;49mnumpy()[\u001b[39m0\u001b[39;49m]}\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/neidu/Desktop/Projects/Personal/My_Projects/NLP-Tutorial/notebook/06_Transformers/05_semantic_search.ipynb#X55sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m )\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/neidu/Desktop/Projects/Personal/My_Projects/NLP-Tutorial/notebook/06_Transformers/05_semantic_search.ipynb#X55sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m comments_dataset\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp_p310/lib/python3.10/site-packages/datasets/arrow_dataset.py:592\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    590\u001b[0m     \u001b[39mself\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mself\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    591\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 592\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    593\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[1;32m    594\u001b[0m \u001b[39mfor\u001b[39;00m dataset \u001b[39min\u001b[39;00m datasets:\n\u001b[1;32m    595\u001b[0m     \u001b[39m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp_p310/lib/python3.10/site-packages/datasets/arrow_dataset.py:557\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    550\u001b[0m self_format \u001b[39m=\u001b[39m {\n\u001b[1;32m    551\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_type,\n\u001b[1;32m    552\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mformat_kwargs\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_kwargs,\n\u001b[1;32m    553\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_columns,\n\u001b[1;32m    554\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39moutput_all_columns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_all_columns,\n\u001b[1;32m    555\u001b[0m }\n\u001b[1;32m    556\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 557\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    558\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[1;32m    559\u001b[0m \u001b[39m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp_p310/lib/python3.10/site-packages/datasets/arrow_dataset.py:3097\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3090\u001b[0m \u001b[39mif\u001b[39;00m transformed_dataset \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3091\u001b[0m     \u001b[39mwith\u001b[39;00m logging\u001b[39m.\u001b[39mtqdm(\n\u001b[1;32m   3092\u001b[0m         disable\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m logging\u001b[39m.\u001b[39mis_progress_bar_enabled(),\n\u001b[1;32m   3093\u001b[0m         unit\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m examples\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   3094\u001b[0m         total\u001b[39m=\u001b[39mpbar_total,\n\u001b[1;32m   3095\u001b[0m         desc\u001b[39m=\u001b[39mdesc \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mMap\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   3096\u001b[0m     ) \u001b[39mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3097\u001b[0m         \u001b[39mfor\u001b[39;00m rank, done, content \u001b[39min\u001b[39;00m Dataset\u001b[39m.\u001b[39m_map_single(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdataset_kwargs):\n\u001b[1;32m   3098\u001b[0m             \u001b[39mif\u001b[39;00m done:\n\u001b[1;32m   3099\u001b[0m                 shards_done \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp_p310/lib/python3.10/site-packages/datasets/arrow_dataset.py:3450\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3448\u001b[0m _time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m   3449\u001b[0m \u001b[39mfor\u001b[39;00m i, example \u001b[39min\u001b[39;00m shard_iterable:\n\u001b[0;32m-> 3450\u001b[0m     example \u001b[39m=\u001b[39m apply_function_on_filtered_inputs(example, i, offset\u001b[39m=\u001b[39;49moffset)\n\u001b[1;32m   3451\u001b[0m     \u001b[39mif\u001b[39;00m update_data:\n\u001b[1;32m   3452\u001b[0m         \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp_p310/lib/python3.10/site-packages/datasets/arrow_dataset.py:3353\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[0;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   3351\u001b[0m \u001b[39mif\u001b[39;00m with_rank:\n\u001b[1;32m   3352\u001b[0m     additional_args \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (rank,)\n\u001b[0;32m-> 3353\u001b[0m processed_inputs \u001b[39m=\u001b[39m function(\u001b[39m*\u001b[39;49mfn_args, \u001b[39m*\u001b[39;49madditional_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfn_kwargs)\n\u001b[1;32m   3354\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(processed_inputs, LazyDict):\n\u001b[1;32m   3355\u001b[0m     processed_inputs \u001b[39m=\u001b[39m {\n\u001b[1;32m   3356\u001b[0m         k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m processed_inputs\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m processed_inputs\u001b[39m.\u001b[39mkeys_to_format\n\u001b[1;32m   3357\u001b[0m     }\n",
      "\u001b[1;32m/Users/neidu/Desktop/Projects/Personal/My_Projects/NLP-Tutorial/notebook/06_Transformers/05_semantic_search.ipynb Cell 30\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/neidu/Desktop/Projects/Personal/My_Projects/NLP-Tutorial/notebook/06_Transformers/05_semantic_search.ipynb#X55sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Get embeddings for the entire dataset\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/neidu/Desktop/Projects/Personal/My_Projects/NLP-Tutorial/notebook/06_Transformers/05_semantic_search.ipynb#X55sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m comments_dataset \u001b[39m=\u001b[39m comments_dataset\u001b[39m.\u001b[39mmap(\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/neidu/Desktop/Projects/Personal/My_Projects/NLP-Tutorial/notebook/06_Transformers/05_semantic_search.ipynb#X55sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mlambda\u001b[39;00m x: {\u001b[39m\"\u001b[39m\u001b[39membeddings\u001b[39m\u001b[39m\"\u001b[39m: get_embeddings(x\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mtext\u001b[39;49m\u001b[39m\"\u001b[39;49m))\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()[\u001b[39m0\u001b[39m]}\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/neidu/Desktop/Projects/Personal/My_Projects/NLP-Tutorial/notebook/06_Transformers/05_semantic_search.ipynb#X55sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m )\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/neidu/Desktop/Projects/Personal/My_Projects/NLP-Tutorial/notebook/06_Transformers/05_semantic_search.ipynb#X55sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m comments_dataset\n",
      "\u001b[1;32m/Users/neidu/Desktop/Projects/Personal/My_Projects/NLP-Tutorial/notebook/06_Transformers/05_semantic_search.ipynb Cell 30\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/neidu/Desktop/Projects/Personal/My_Projects/NLP-Tutorial/notebook/06_Transformers/05_semantic_search.ipynb#X55sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m encoded_input \u001b[39m=\u001b[39m tokenizer(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/neidu/Desktop/Projects/Personal/My_Projects/NLP-Tutorial/notebook/06_Transformers/05_semantic_search.ipynb#X55sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     text_list, padding\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/neidu/Desktop/Projects/Personal/My_Projects/NLP-Tutorial/notebook/06_Transformers/05_semantic_search.ipynb#X55sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/neidu/Desktop/Projects/Personal/My_Projects/NLP-Tutorial/notebook/06_Transformers/05_semantic_search.ipynb#X55sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m encoded_input \u001b[39m=\u001b[39m {k: v\u001b[39m.\u001b[39mto(device) \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m encoded_input\u001b[39m.\u001b[39mitems()}\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/neidu/Desktop/Projects/Personal/My_Projects/NLP-Tutorial/notebook/06_Transformers/05_semantic_search.ipynb#X55sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m model_output \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mencoded_input)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/neidu/Desktop/Projects/Personal/My_Projects/NLP-Tutorial/notebook/06_Transformers/05_semantic_search.ipynb#X55sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mreturn\u001b[39;00m cls_pooling(model_output)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp_p310/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp_p310/lib/python3.10/site-packages/transformers/models/mpnet/modeling_mpnet.py:551\u001b[0m, in \u001b[0;36mMPNetModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m    549\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    550\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(input_ids\u001b[39m=\u001b[39minput_ids, position_ids\u001b[39m=\u001b[39mposition_ids, inputs_embeds\u001b[39m=\u001b[39minputs_embeds)\n\u001b[0;32m--> 551\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m    552\u001b[0m     embedding_output,\n\u001b[1;32m    553\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m    554\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    555\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    556\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    557\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    558\u001b[0m )\n\u001b[1;32m    559\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    560\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp_p310/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp_p310/lib/python3.10/site-packages/transformers/models/mpnet/modeling_mpnet.py:341\u001b[0m, in \u001b[0;36mMPNetEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[39mif\u001b[39;00m output_hidden_states:\n\u001b[1;32m    339\u001b[0m     all_hidden_states \u001b[39m=\u001b[39m all_hidden_states \u001b[39m+\u001b[39m (hidden_states,)\n\u001b[0;32m--> 341\u001b[0m layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    342\u001b[0m     hidden_states,\n\u001b[1;32m    343\u001b[0m     attention_mask,\n\u001b[1;32m    344\u001b[0m     head_mask[i],\n\u001b[1;32m    345\u001b[0m     position_bias,\n\u001b[1;32m    346\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    347\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    348\u001b[0m )\n\u001b[1;32m    349\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    351\u001b[0m \u001b[39mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp_p310/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp_p310/lib/python3.10/site-packages/transformers/models/mpnet/modeling_mpnet.py:311\u001b[0m, in \u001b[0;36mMPNetLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, position_bias, output_attentions, **kwargs)\u001b[0m\n\u001b[1;32m    308\u001b[0m outputs \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add self attentions if we output attention weights\u001b[39;00m\n\u001b[1;32m    310\u001b[0m intermediate_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintermediate(attention_output)\n\u001b[0;32m--> 311\u001b[0m layer_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput(intermediate_output, attention_output)\n\u001b[1;32m    312\u001b[0m outputs \u001b[39m=\u001b[39m (layer_output,) \u001b[39m+\u001b[39m outputs\n\u001b[1;32m    313\u001b[0m \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp_p310/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp_p310/lib/python3.10/site-packages/transformers/models/mpnet/modeling_mpnet.py:278\u001b[0m, in \u001b[0;36mMPNetOutput.forward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states: torch\u001b[39m.\u001b[39mTensor, input_tensor: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m--> 278\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdense(hidden_states)\n\u001b[1;32m    279\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(hidden_states)\n\u001b[1;32m    280\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mLayerNorm(hidden_states \u001b[39m+\u001b[39m input_tensor)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp_p310/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp_p310/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Get embeddings for the entire dataset\n",
    "comments_dataset = comments_dataset.map(\n",
    "    lambda x: {\"embeddings\": get_embeddings(x.get(\"text\")).detach().cpu().numpy()[0]}\n",
    ")\n",
    "\n",
    "comments_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dataset = comments_dataset.map(\n",
    "    lambda x: {\"embeddings\": get_embeddings(x[\"text\"]).detach().cpu().numpy()[0]}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_p310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
