{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chineidu/NLP-Tutorial/blob/main/notebook/06_Transformers/05_semantic_search.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWpzTikgoMVl"
      },
      "source": [
        "# [Semantic Search](https://huggingface.co/learn/nlp-course/chapter5/6?fw=pt)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import IPython\n",
        "\n",
        "IPython.version_info"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJ4_tFZhqBrf",
        "outputId": "7ddeacdf-6d4f-4f9c-8706-e300268a4804"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7, 34, 0, '')"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rich\n",
        "!pip install transformers[torch]\n",
        "!pip install torch datasets evaluate\n",
        "!pip install black[jupyter]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pLXE61kKpbUX",
        "outputId": "bc7655b9-16b8-4d1e-88e6-96c0c0ec0592"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (13.6.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich) (0.1.2)\n",
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.34.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.12.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.1)\n",
            "Requirement already satisfied: torch!=1.12.0,>=1.10 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.0.1+cu118)\n",
            "Requirement already satisfied: accelerate>=0.20.3 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.23.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.20.3->transformers[torch]) (5.9.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers[torch]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers[torch]) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch!=1.12.0,>=1.10->transformers[torch]) (3.27.6)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch!=1.12.0,>=1.10->transformers[torch]) (17.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=1.12.0,>=1.10->transformers[torch]) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.12.0,>=1.10->transformers[torch]) (1.3.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.14.5)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.27.6)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (17.0.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.17.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.18.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.3.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: black[jupyter] in /usr/local/lib/python3.10/dist-packages (23.9.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from black[jupyter]) (8.1.7)\n",
            "Requirement already satisfied: mypy-extensions>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from black[jupyter]) (1.0.0)\n",
            "Requirement already satisfied: packaging>=22.0 in /usr/local/lib/python3.10/dist-packages (from black[jupyter]) (23.2)\n",
            "Requirement already satisfied: pathspec>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from black[jupyter]) (0.11.2)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.10/dist-packages (from black[jupyter]) (3.11.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from black[jupyter]) (2.0.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from black[jupyter]) (4.5.0)\n",
            "Requirement already satisfied: ipython>=7.8.0 in /usr/local/lib/python3.10/dist-packages (from black[jupyter]) (7.34.0)\n",
            "Requirement already satisfied: tokenize-rt>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from black[jupyter]) (5.2.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.8.0->black[jupyter]) (67.7.2)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.8.0->black[jupyter]) (0.19.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=7.8.0->black[jupyter]) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=7.8.0->black[jupyter]) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.8.0->black[jupyter]) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.8.0->black[jupyter]) (3.0.39)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=7.8.0->black[jupyter]) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=7.8.0->black[jupyter]) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=7.8.0->black[jupyter]) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.8.0->black[jupyter]) (4.8.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=7.8.0->black[jupyter]) (0.8.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=7.8.0->black[jupyter]) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.8.0->black[jupyter]) (0.2.8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Formatter For Colab\n",
        "\n",
        "[Run only once, at startup]\n",
        "\n",
        "```text\n",
        "- Connect to your drive\n",
        "```\n",
        "\n",
        "```python\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "```\n",
        "\n",
        "```sh\n",
        "Install black for jupyter\n",
        "\n",
        "!pip install black[jupyter]\n",
        "```\n",
        "\n",
        "```text\n",
        "- Restart kernel\n",
        "\n",
        "[Then]\n",
        "\n",
        "- Place your .ipynb file somewhere on your drive\n",
        "```\n",
        "\n",
        "```python\n",
        "# Anytime you want format your code run:\n",
        "!black /content/drive/MyDrive/YOUR_PATH/YOUR_NOTEBOOK.ipynb\n",
        "```\n",
        "\n",
        "```text\n",
        "- Don't save your notebook, hit F5 to refresh the page\n",
        "- Now save!\n",
        "```"
      ],
      "metadata": {
        "id": "l_nCGvg1r5Pm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xs3xpJKfrfZt",
        "outputId": "d532c72a-188f-4b70-f3f1-0e84d5941001"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "1aOjiuK_oMVn"
      },
      "outputs": [],
      "source": [
        "# Built-in library\n",
        "import re\n",
        "import json\n",
        "from typing import Any, Dict, List, Optional, Union\n",
        "import logging\n",
        "import warnings\n",
        "\n",
        "# Standard imports\n",
        "import numpy as np\n",
        "from pprint import pprint\n",
        "import pandas as pd\n",
        "from rich import print\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Pandas settings\n",
        "pd.options.display.max_rows = 1_000\n",
        "pd.options.display.max_columns = 1_000\n",
        "pd.options.display.max_colwidth = 600\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Black code formatter (Optional)\n",
        "# %load_ext lab_black\n",
        "\n",
        "# auto reload imports\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ItMOsrqDpcg_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5T6s5J8JoMVo"
      },
      "source": [
        "### Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "fBTViKQ4oMVo"
      },
      "outputs": [],
      "source": [
        "import datasets\n",
        "from datasets import load_dataset\n",
        "from datasets.dataset_dict import DatasetDict, Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xkff8--YoMVp",
        "outputId": "d895c856-b7be-4a73-8dbd-17a04769d5b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Repo card metadata block was not found. Setting CardData to empty.\n",
            "WARNING:huggingface_hub.repocard:Repo card metadata block was not found. Setting CardData to empty.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'pull_request', 'body', 'timeline_url', 'performed_via_github_app', 'is_pull_request'],\n",
              "    num_rows: 3019\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "fp: str = \"lewtun/github-issues\"\n",
        "\n",
        "issues_dataset: Dataset = load_dataset(fp, split=\"train\")\n",
        "issues_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Rw1D10e1oMVp",
        "outputId": "e2ff40fa-da13-45d1-fdbb-40b601decb9f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m{\u001b[0m\n",
              "    \u001b[32m'url'\u001b[0m: \u001b[32m'https://api.github.com/repos/huggingface/datasets/issues/2944'\u001b[0m,\n",
              "    \u001b[32m'repository_url'\u001b[0m: \u001b[32m'https://api.github.com/repos/huggingface/datasets'\u001b[0m,\n",
              "    \u001b[32m'labels_url'\u001b[0m: \u001b[32m'https://api.github.com/repos/huggingface/datasets/issues/2944/labels\u001b[0m\u001b[32m{\u001b[0m\u001b[32m/name\u001b[0m\u001b[32m}\u001b[0m\u001b[32m'\u001b[0m,\n",
              "    \u001b[32m'comments_url'\u001b[0m: \u001b[32m'https://api.github.com/repos/huggingface/datasets/issues/2944/comments'\u001b[0m,\n",
              "    \u001b[32m'events_url'\u001b[0m: \u001b[32m'https://api.github.com/repos/huggingface/datasets/issues/2944/events'\u001b[0m,\n",
              "    \u001b[32m'html_url'\u001b[0m: \u001b[32m'https://github.com/huggingface/datasets/issues/2944'\u001b[0m,\n",
              "    \u001b[32m'id'\u001b[0m: \u001b[1;36m1000544370\u001b[0m,\n",
              "    \u001b[32m'node_id'\u001b[0m: \u001b[32m'I_kwDODunzps47oxhy'\u001b[0m,\n",
              "    \u001b[32m'number'\u001b[0m: \u001b[1;36m2944\u001b[0m,\n",
              "    \u001b[32m'title'\u001b[0m: \u001b[32m'Add  `remove_columns` to `IterableDataset ` '\u001b[0m,\n",
              "    \u001b[32m'user'\u001b[0m: \u001b[1m{\u001b[0m\n",
              "        \u001b[32m'login'\u001b[0m: \u001b[32m'cccntu'\u001b[0m,\n",
              "        \u001b[32m'id'\u001b[0m: \u001b[1;36m31893406\u001b[0m,\n",
              "        \u001b[32m'node_id'\u001b[0m: \u001b[32m'MDQ6VXNlcjMxODkzNDA2'\u001b[0m,\n",
              "        \u001b[32m'avatar_url'\u001b[0m: \u001b[32m'https://avatars.githubusercontent.com/u/31893406?\u001b[0m\u001b[32mv\u001b[0m\u001b[32m=\u001b[0m\u001b[32m4\u001b[0m\u001b[32m'\u001b[0m,\n",
              "        \u001b[32m'gravatar_id'\u001b[0m: \u001b[32m''\u001b[0m,\n",
              "        \u001b[32m'url'\u001b[0m: \u001b[32m'https://api.github.com/users/cccntu'\u001b[0m,\n",
              "        \u001b[32m'html_url'\u001b[0m: \u001b[32m'https://github.com/cccntu'\u001b[0m,\n",
              "        \u001b[32m'followers_url'\u001b[0m: \u001b[32m'https://api.github.com/users/cccntu/followers'\u001b[0m,\n",
              "        \u001b[32m'following_url'\u001b[0m: \u001b[32m'https://api.github.com/users/cccntu/following\u001b[0m\u001b[32m{\u001b[0m\u001b[32m/other_user\u001b[0m\u001b[32m}\u001b[0m\u001b[32m'\u001b[0m,\n",
              "        \u001b[32m'gists_url'\u001b[0m: \u001b[32m'https://api.github.com/users/cccntu/gists\u001b[0m\u001b[32m{\u001b[0m\u001b[32m/gist_id\u001b[0m\u001b[32m}\u001b[0m\u001b[32m'\u001b[0m,\n",
              "        \u001b[32m'starred_url'\u001b[0m: \u001b[32m'https://api.github.com/users/cccntu/starred\u001b[0m\u001b[32m{\u001b[0m\u001b[32m/owner\u001b[0m\u001b[32m}\u001b[0m\u001b[32m{\u001b[0m\u001b[32m/repo\u001b[0m\u001b[32m}\u001b[0m\u001b[32m'\u001b[0m,\n",
              "        \u001b[32m'subscriptions_url'\u001b[0m: \u001b[32m'https://api.github.com/users/cccntu/subscriptions'\u001b[0m,\n",
              "        \u001b[32m'organizations_url'\u001b[0m: \u001b[32m'https://api.github.com/users/cccntu/orgs'\u001b[0m,\n",
              "        \u001b[32m'repos_url'\u001b[0m: \u001b[32m'https://api.github.com/users/cccntu/repos'\u001b[0m,\n",
              "        \u001b[32m'events_url'\u001b[0m: \u001b[32m'https://api.github.com/users/cccntu/events\u001b[0m\u001b[32m{\u001b[0m\u001b[32m/privacy\u001b[0m\u001b[32m}\u001b[0m\u001b[32m'\u001b[0m,\n",
              "        \u001b[32m'received_events_url'\u001b[0m: \u001b[32m'https://api.github.com/users/cccntu/received_events'\u001b[0m,\n",
              "        \u001b[32m'type'\u001b[0m: \u001b[32m'User'\u001b[0m,\n",
              "        \u001b[32m'site_admin'\u001b[0m: \u001b[3;91mFalse\u001b[0m\n",
              "    \u001b[1m}\u001b[0m,\n",
              "    \u001b[32m'labels'\u001b[0m: \u001b[1m[\u001b[0m\n",
              "        \u001b[1m{\u001b[0m\n",
              "            \u001b[32m'id'\u001b[0m: \u001b[1;36m1935892871\u001b[0m,\n",
              "            \u001b[32m'node_id'\u001b[0m: \u001b[32m'MDU6TGFiZWwxOTM1ODkyODcx'\u001b[0m,\n",
              "            \u001b[32m'url'\u001b[0m: \u001b[32m'https://api.github.com/repos/huggingface/datasets/labels/enhancement'\u001b[0m,\n",
              "            \u001b[32m'name'\u001b[0m: \u001b[32m'enhancement'\u001b[0m,\n",
              "            \u001b[32m'color'\u001b[0m: \u001b[32m'a2eeef'\u001b[0m,\n",
              "            \u001b[32m'default'\u001b[0m: \u001b[3;92mTrue\u001b[0m,\n",
              "            \u001b[32m'description'\u001b[0m: \u001b[32m'New feature or request'\u001b[0m\n",
              "        \u001b[1m}\u001b[0m\n",
              "    \u001b[1m]\u001b[0m,\n",
              "    \u001b[32m'state'\u001b[0m: \u001b[32m'open'\u001b[0m,\n",
              "    \u001b[32m'locked'\u001b[0m: \u001b[3;91mFalse\u001b[0m,\n",
              "    \u001b[32m'assignee'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
              "    \u001b[32m'assignees'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\n",
              "    \u001b[32m'milestone'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
              "    \u001b[32m'comments'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\n",
              "    \u001b[32m'created_at'\u001b[0m: \u001b[1;36m1632110460000\u001b[0m,\n",
              "    \u001b[32m'updated_at'\u001b[0m: \u001b[1;36m1632110460000\u001b[0m,\n",
              "    \u001b[32m'closed_at'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
              "    \u001b[32m'author_association'\u001b[0m: \u001b[32m'CONTRIBUTOR'\u001b[0m,\n",
              "    \u001b[32m'active_lock_reason'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
              "    \u001b[32m'pull_request'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
              "    \u001b[32m'body'\u001b[0m: \u001b[32m'**Is your feature request related to a problem? Please describe.**\\r\\nA clear and concise description \u001b[0m\n",
              "\u001b[32mof what the problem is.\\r\\n\\r\\n```python\\r\\nfrom datasets import load_dataset\\r\\ndataset = load_dataset\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\"c4\", \u001b[0m\n",
              "\u001b[32m\\'realnewslike\\', streaming =True, \u001b[0m\u001b[32msplit\u001b[0m\u001b[32m=\\'train\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\r\\ndataset = \u001b[0m\n",
              "\u001b[32mdataset.remove_columns\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'url\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\r\\n```\\r\\n```\\r\\nAttributeError: \\'IterableDataset\\' object has no attribute \u001b[0m\n",
              "\u001b[32m\\'remove_columns\\'\\r\\n```\\r\\n\\r\\n**Describe the solution you\\'d like**\\r\\n\\r\\nIt would be nice to have \u001b[0m\n",
              "\u001b[32m`.remove_columns\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m` to match the `Datasets` api. \\r\\n\\r\\n\\r\\n**Describe alternatives you\\'ve \u001b[0m\n",
              "\u001b[32mconsidered**\\r\\n\\r\\nThis can be done with a single call to `.map\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m`, \\r\\n\\r\\nI can try to help add this. 🤗'\u001b[0m,\n",
              "    \u001b[32m'timeline_url'\u001b[0m: \u001b[32m'https://api.github.com/repos/huggingface/datasets/issues/2944/timeline'\u001b[0m,\n",
              "    \u001b[32m'performed_via_github_app'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
              "    \u001b[32m'is_pull_request'\u001b[0m: \u001b[3;91mFalse\u001b[0m\n",
              "\u001b[1m}\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'url'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://api.github.com/repos/huggingface/datasets/issues/2944'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'repository_url'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://api.github.com/repos/huggingface/datasets'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'labels_url'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://api.github.com/repos/huggingface/datasets/issues/2944/labels{/name}'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'comments_url'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://api.github.com/repos/huggingface/datasets/issues/2944/comments'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'events_url'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://api.github.com/repos/huggingface/datasets/issues/2944/events'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'html_url'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://github.com/huggingface/datasets/issues/2944'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'id'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1000544370</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'node_id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'I_kwDODunzps47oxhy'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'number'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2944</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'title'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Add  `remove_columns` to `IterableDataset ` '</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'user'</span>: <span style=\"font-weight: bold\">{</span>\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">'login'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'cccntu'</span>,\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">'id'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">31893406</span>,\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">'node_id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'MDQ6VXNlcjMxODkzNDA2'</span>,\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">'avatar_url'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://avatars.githubusercontent.com/u/31893406?v=4'</span>,\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">'gravatar_id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">'url'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://api.github.com/users/cccntu'</span>,\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">'html_url'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://github.com/cccntu'</span>,\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">'followers_url'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://api.github.com/users/cccntu/followers'</span>,\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">'following_url'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://api.github.com/users/cccntu/following{/other_user}'</span>,\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">'gists_url'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://api.github.com/users/cccntu/gists{/gist_id}'</span>,\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">'starred_url'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://api.github.com/users/cccntu/starred{/owner}{/repo}'</span>,\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">'subscriptions_url'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://api.github.com/users/cccntu/subscriptions'</span>,\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">'organizations_url'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://api.github.com/users/cccntu/orgs'</span>,\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">'repos_url'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://api.github.com/users/cccntu/repos'</span>,\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">'events_url'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://api.github.com/users/cccntu/events{/privacy}'</span>,\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">'received_events_url'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://api.github.com/users/cccntu/received_events'</span>,\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">'type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'User'</span>,\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">'site_admin'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>\n",
              "    <span style=\"font-weight: bold\">}</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'labels'</span>: <span style=\"font-weight: bold\">[</span>\n",
              "        <span style=\"font-weight: bold\">{</span>\n",
              "            <span style=\"color: #008000; text-decoration-color: #008000\">'id'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1935892871</span>,\n",
              "            <span style=\"color: #008000; text-decoration-color: #008000\">'node_id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'MDU6TGFiZWwxOTM1ODkyODcx'</span>,\n",
              "            <span style=\"color: #008000; text-decoration-color: #008000\">'url'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://api.github.com/repos/huggingface/datasets/labels/enhancement'</span>,\n",
              "            <span style=\"color: #008000; text-decoration-color: #008000\">'name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'enhancement'</span>,\n",
              "            <span style=\"color: #008000; text-decoration-color: #008000\">'color'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'a2eeef'</span>,\n",
              "            <span style=\"color: #008000; text-decoration-color: #008000\">'default'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
              "            <span style=\"color: #008000; text-decoration-color: #008000\">'description'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'New feature or request'</span>\n",
              "        <span style=\"font-weight: bold\">}</span>\n",
              "    <span style=\"font-weight: bold\">]</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'state'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'open'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'locked'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'assignee'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'assignees'</span>: <span style=\"font-weight: bold\">[]</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'milestone'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'comments'</span>: <span style=\"font-weight: bold\">[]</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'created_at'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1632110460000</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'updated_at'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1632110460000</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'closed_at'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'author_association'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'CONTRIBUTOR'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'active_lock_reason'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'pull_request'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'body'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'**Is your feature request related to a problem? Please describe.**\\r\\nA clear and concise description </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">of what the problem is.\\r\\n\\r\\n```python\\r\\nfrom datasets import load_dataset\\r\\ndataset = load_dataset(\"c4\", </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">\\'realnewslike\\', streaming =True, split=\\'train\\')\\r\\ndataset = </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">dataset.remove_columns(\\'url\\')\\r\\n```\\r\\n```\\r\\nAttributeError: \\'IterableDataset\\' object has no attribute </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">\\'remove_columns\\'\\r\\n```\\r\\n\\r\\n**Describe the solution you\\'d like**\\r\\n\\r\\nIt would be nice to have </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">`.remove_columns()` to match the `Datasets` api. \\r\\n\\r\\n\\r\\n**Describe alternatives you\\'ve </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">considered**\\r\\n\\r\\nThis can be done with a single call to `.map()`, \\r\\n\\r\\nI can try to help add this. 🤗'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'timeline_url'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://api.github.com/repos/huggingface/datasets/issues/2944/timeline'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'performed_via_github_app'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'is_pull_request'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>\n",
              "<span style=\"font-weight: bold\">}</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "print(issues_dataset[10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "id": "p8UYbHNKoMVq",
        "outputId": "3be2fcc7-e11f-4145-fbcd-93eb9dd1a68f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m[\u001b[0m\n",
              "    \u001b[32m'url'\u001b[0m,\n",
              "    \u001b[32m'repository_url'\u001b[0m,\n",
              "    \u001b[32m'labels_url'\u001b[0m,\n",
              "    \u001b[32m'comments_url'\u001b[0m,\n",
              "    \u001b[32m'events_url'\u001b[0m,\n",
              "    \u001b[32m'html_url'\u001b[0m,\n",
              "    \u001b[32m'id'\u001b[0m,\n",
              "    \u001b[32m'node_id'\u001b[0m,\n",
              "    \u001b[32m'number'\u001b[0m,\n",
              "    \u001b[32m'title'\u001b[0m,\n",
              "    \u001b[32m'user'\u001b[0m,\n",
              "    \u001b[32m'labels'\u001b[0m,\n",
              "    \u001b[32m'state'\u001b[0m,\n",
              "    \u001b[32m'locked'\u001b[0m,\n",
              "    \u001b[32m'assignee'\u001b[0m,\n",
              "    \u001b[32m'assignees'\u001b[0m,\n",
              "    \u001b[32m'milestone'\u001b[0m,\n",
              "    \u001b[32m'comments'\u001b[0m,\n",
              "    \u001b[32m'created_at'\u001b[0m,\n",
              "    \u001b[32m'updated_at'\u001b[0m,\n",
              "    \u001b[32m'closed_at'\u001b[0m,\n",
              "    \u001b[32m'author_association'\u001b[0m,\n",
              "    \u001b[32m'active_lock_reason'\u001b[0m,\n",
              "    \u001b[32m'pull_request'\u001b[0m,\n",
              "    \u001b[32m'body'\u001b[0m,\n",
              "    \u001b[32m'timeline_url'\u001b[0m,\n",
              "    \u001b[32m'performed_via_github_app'\u001b[0m,\n",
              "    \u001b[32m'is_pull_request'\u001b[0m\n",
              "\u001b[1m]\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'url'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'repository_url'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'labels_url'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'comments_url'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'events_url'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'html_url'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'id'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'node_id'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'number'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'title'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'user'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'labels'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'state'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'locked'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'assignee'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'assignees'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'milestone'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'comments'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'created_at'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'updated_at'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'closed_at'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'author_association'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'active_lock_reason'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'pull_request'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'body'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'timeline_url'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'performed_via_github_app'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'is_pull_request'</span>\n",
              "<span style=\"font-weight: bold\">]</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "print(issues_dataset.column_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        },
        "id": "qIYsiTxVoMVq",
        "outputId": "b6867363-5546-4692-fe08-7780f8deddc3"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Size if data BEFORE: \u001b[1;36m3019\u001b[0m\n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Size if data BEFORE: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3019</span>\n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Size if data AFTER: \u001b[1;36m808\u001b[0m\n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Size if data AFTER: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">808</span>\n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# The issues_dataset contains issues and pull requests.\n",
        "# Select ONLY the issues\n",
        "issues_dataset_1: Dataset = issues_dataset.filter(\n",
        "    (lambda x: x.get(\"is_pull_request\") == False and len(x.get(\"comments\")) > 0),\n",
        ")\n",
        "\n",
        "print(f\"Size if data BEFORE: {issues_dataset.num_rows}\\n\")\n",
        "\n",
        "print(f\"Size if data AFTER: {issues_dataset_1.num_rows}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "id": "XIfqpPNDoMVq",
        "outputId": "e4d13a3c-331b-47b7-80a1-e6517906881e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m{\u001b[0m\n",
              "    \u001b[32m'node_id'\u001b[0m,\n",
              "    \u001b[32m'user'\u001b[0m,\n",
              "    \u001b[32m'milestone'\u001b[0m,\n",
              "    \u001b[32m'repository_url'\u001b[0m,\n",
              "    \u001b[32m'id'\u001b[0m,\n",
              "    \u001b[32m'events_url'\u001b[0m,\n",
              "    \u001b[32m'labels_url'\u001b[0m,\n",
              "    \u001b[32m'locked'\u001b[0m,\n",
              "    \u001b[32m'number'\u001b[0m,\n",
              "    \u001b[32m'pull_request'\u001b[0m,\n",
              "    \u001b[32m'assignees'\u001b[0m,\n",
              "    \u001b[32m'state'\u001b[0m,\n",
              "    \u001b[32m'performed_via_github_app'\u001b[0m,\n",
              "    \u001b[32m'updated_at'\u001b[0m,\n",
              "    \u001b[32m'labels'\u001b[0m,\n",
              "    \u001b[32m'assignee'\u001b[0m,\n",
              "    \u001b[32m'created_at'\u001b[0m,\n",
              "    \u001b[32m'active_lock_reason'\u001b[0m,\n",
              "    \u001b[32m'url'\u001b[0m,\n",
              "    \u001b[32m'comments_url'\u001b[0m,\n",
              "    \u001b[32m'closed_at'\u001b[0m,\n",
              "    \u001b[32m'timeline_url'\u001b[0m,\n",
              "    \u001b[32m'is_pull_request'\u001b[0m,\n",
              "    \u001b[32m'author_association'\u001b[0m\n",
              "\u001b[1m}\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'node_id'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'user'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'milestone'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'repository_url'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'id'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'events_url'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'labels_url'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'locked'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'number'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'pull_request'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'assignees'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'state'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'performed_via_github_app'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'updated_at'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'labels'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'assignee'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'created_at'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'active_lock_reason'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'url'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'comments_url'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'closed_at'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'timeline_url'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'is_pull_request'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'author_association'</span>\n",
              "<span style=\"font-weight: bold\">}</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "all_columns: list[str] = issues_dataset_1.column_names\n",
        "columns_to_keep: list[str] = [\"title\", \"body\", \"html_url\", \"comments\"]\n",
        "# Columns present in ONLY all_columns\n",
        "columns_to_remove: set = set(columns_to_keep).symmetric_difference(set(all_columns))\n",
        "print(columns_to_remove)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mf7NZ4dioMVr",
        "outputId": "3ba9b525-9f8f-4926-8c1d-3613c1ae936d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['html_url', 'title', 'comments', 'body'],\n",
              "    num_rows: 808\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "issues_dataset_1 = issues_dataset_1.remove_columns(column_names=columns_to_remove)\n",
        "issues_dataset_1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 883
        },
        "id": "Gv7yetVIoMVr",
        "outputId": "ef1258af-eeb6-48a1-a9fa-49274fc3cb1c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              html_url  \\\n",
              "0  https://github.com/huggingface/datasets/issues/2945   \n",
              "1  https://github.com/huggingface/datasets/issues/2943   \n",
              "2  https://github.com/huggingface/datasets/issues/2941   \n",
              "3  https://github.com/huggingface/datasets/issues/2937   \n",
              "4  https://github.com/huggingface/datasets/issues/2934   \n",
              "\n",
              "                                                                                               title  \\\n",
              "0                                                                              Protect master branch   \n",
              "1                            Backwards compatibility broken for cached datasets that use `.filter()`   \n",
              "2                                          OSCAR unshuffled_original_ko: NonMatchingSplitsSizesError   \n",
              "3  load_dataset using default cache on Windows causes PermissionError: [WinError 5] Access is denied   \n",
              "4              to_tf_dataset keeps a reference to the open data somewhere, causing issues on windows   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  comments  \\\n",
              "0                                                                                                                                                          [Cool, I think we can do both :), @lhoestq now the 2 are implemented.\\r\\n\\r\\nPlease note that for the the second protection, finally I have chosen to protect the master branch only from **merge commits** (see update comment above), so no need to disable/re-enable the protection on each release (direct commits, different from merge commits, can be pushed to the remote master branch; and eventually reverted without messing up the repo history).]   \n",
              "1  [Hi ! I guess the caching mechanism should have considered the new `filter` to be different from the old one, and don't use cached results from the old `filter`.\\r\\nTo avoid other users from having this issue we could make the caching differentiate the two, what do you think ?, If it's easy enough to implement, then yes please 😄  But this issue can be low-priority, since I've only encountered it in a couple of `transformers` CI tests., Well it can cause issue with anyone that updates `datasets` and re-run some code that uses filter, so I'm creating a PR, I just merged a fix, let me know if...   \n",
              "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            [I tried `unshuffled_original_da` and it is also not working]   \n",
              "3  [Hi @daqieq, thanks for reporting.\\r\\n\\r\\nUnfortunately, I was not able to reproduce this bug:\\r\\n```ipython\\r\\nIn [1]: from datasets import load_dataset\\r\\n   ...: ds = load_dataset('wiki_bio')\\r\\nDownloading: 7.58kB [00:00, 26.3kB/s]\\r\\nDownloading: 2.71kB [00:00, ?B/s]\\r\\nUsing custom data configuration default\\r\\nDownloading and preparing dataset wiki_bio/default (download: 318.53 MiB, generated: 736.94 MiB, post-processed: Unknown size, total: 1.03 GiB) to C:\\Users\\username\\.cache\\huggingface\\datasets\\wiki_bio\\default\\\\r\\n1.1.0\\5293ce565954ba965dada626f1e79684e98172d950371d266bf3caaf8...   \n",
              "4  [I did some investigation and, as it seems, the bug stems from [this line](https://github.com/huggingface/datasets/blob/8004d7c3e1d74b29c3e5b0d1660331cd26758363/src/datasets/arrow_dataset.py#L325). The lifecycle of the dataset from the linked line is bound to one of the returned `tf.data.Dataset`. So my (hacky) solution involves wrapping the linked dataset with `weakref.proxy` and adding a custom `__del__` to `tf.python.data.ops.dataset_ops.TensorSliceDataset` (this is the type of a dataset that is returned by `tf.data.Dataset.from_tensor_slices`; this works for TF 2.x, but I'm not sure `t...   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      body  \n",
              "0  After accidental merge commit (91c55355b634d0dc73350a7ddee1a6776dbbdd69) into `datasets` master branch, all commits present in the feature branch were permanently added to `datasets` master branch history, as e.g.:\\r\\n- 00cc036fea7c7745cfe722360036ed306796a3f2\\r\\n- 13ae8c98602bbad8197de3b9b425f4c78f582af1\\r\\n- ...\\r\\n\\r\\nI propose to protect our master branch, so that we avoid we can accidentally make this kind of mistakes in the future:\\r\\n- [x] For Pull Requests using GitHub, allow only squash merging, so that only a single commit per Pull Request is merged into the master branch\\r\\n  - ...  \n",
              "1  ## Describe the bug\\r\\nAfter upgrading to datasets `1.12.0`, some cached `.filter()` steps from `1.11.0` started failing with \\r\\n`ValueError: Keys mismatch: between {'indices': Value(dtype='uint64', id=None)} and {'file': Value(dtype='string', id=None), 'text': Value(dtype='string', id=None), 'speaker_id': Value(dtype='int64', id=None), 'chapter_id': Value(dtype='int64', id=None), 'id': Value(dtype='string', id=None)}`\\r\\n\\r\\nRelated feature: https://github.com/huggingface/datasets/pull/2836\\r\\n\\r\\n:question:  This is probably a `wontfix` bug, since it can be solved by simply cleaning the...  \n",
              "2  ## Describe the bug\\r\\n\\r\\nCannot download OSCAR `unshuffled_original_ko` due to `NonMatchingSplitsSizesError`.\\r\\n\\r\\n## Steps to reproduce the bug\\r\\n\\r\\n```python\\r\\n>>> dataset = datasets.load_dataset('oscar', 'unshuffled_original_ko')\\r\\nNonMatchingSplitsSizesError: [{'expected': SplitInfo(name='train', num_bytes=25292102197, num_examples=7345075, dataset_name='oscar'), 'recorded': SplitInfo(name='train', num_bytes=25284578514, num_examples=7344907, dataset_name='oscar')}]\\r\\n```\\r\\n\\r\\n## Expected results\\r\\n\\r\\nLoading is successful.\\r\\n\\r\\n## Actual results\\r\\n\\r\\nLoading throws ab...  \n",
              "3  ## Describe the bug\\r\\nStandard process to download and load the wiki_bio dataset causes PermissionError in Windows 10 and 11.\\r\\n\\r\\n## Steps to reproduce the bug\\r\\n```python\\r\\nfrom datasets import load_dataset\\r\\nds = load_dataset('wiki_bio')\\r\\n```\\r\\n\\r\\n## Expected results\\r\\nIt is expected that the dataset downloads without any errors.\\r\\n\\r\\n## Actual results\\r\\nPermissionError see trace below:\\r\\n```\\r\\nUsing custom data configuration default\\r\\nDownloading and preparing dataset wiki_bio/default (download: 318.53 MiB, generated: 736.94 MiB, post-processed: Unknown size, total: 1....  \n",
              "4  To reproduce:\\r\\n```python\\r\\nimport datasets as ds\\r\\nimport weakref\\r\\nimport gc\\r\\n\\r\\nd = ds.load_dataset(\"mnist\", split=\"train\")\\r\\nref = weakref.ref(d._data.table)\\r\\ntfd = d.to_tf_dataset(\"image\", batch_size=1, shuffle=False, label_cols=\"label\")\\r\\ndel tfd, d\\r\\ngc.collect()\\r\\nassert ref() is None, \"Error: there is at least one reference left\"\\r\\n```\\r\\n\\r\\nThis causes issues because the table holds a reference to an open arrow file that should be closed. So on windows it's not possible to delete or move the arrow file afterwards.\\r\\n\\r\\nMoreover the CI test of the `to_tf_dataset` ...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-02871d02-6572-4560-8106-b53918f06947\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>html_url</th>\n",
              "      <th>title</th>\n",
              "      <th>comments</th>\n",
              "      <th>body</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>https://github.com/huggingface/datasets/issues/2945</td>\n",
              "      <td>Protect master branch</td>\n",
              "      <td>[Cool, I think we can do both :), @lhoestq now the 2 are implemented.\\r\\n\\r\\nPlease note that for the the second protection, finally I have chosen to protect the master branch only from **merge commits** (see update comment above), so no need to disable/re-enable the protection on each release (direct commits, different from merge commits, can be pushed to the remote master branch; and eventually reverted without messing up the repo history).]</td>\n",
              "      <td>After accidental merge commit (91c55355b634d0dc73350a7ddee1a6776dbbdd69) into `datasets` master branch, all commits present in the feature branch were permanently added to `datasets` master branch history, as e.g.:\\r\\n- 00cc036fea7c7745cfe722360036ed306796a3f2\\r\\n- 13ae8c98602bbad8197de3b9b425f4c78f582af1\\r\\n- ...\\r\\n\\r\\nI propose to protect our master branch, so that we avoid we can accidentally make this kind of mistakes in the future:\\r\\n- [x] For Pull Requests using GitHub, allow only squash merging, so that only a single commit per Pull Request is merged into the master branch\\r\\n  - ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>https://github.com/huggingface/datasets/issues/2943</td>\n",
              "      <td>Backwards compatibility broken for cached datasets that use `.filter()`</td>\n",
              "      <td>[Hi ! I guess the caching mechanism should have considered the new `filter` to be different from the old one, and don't use cached results from the old `filter`.\\r\\nTo avoid other users from having this issue we could make the caching differentiate the two, what do you think ?, If it's easy enough to implement, then yes please 😄  But this issue can be low-priority, since I've only encountered it in a couple of `transformers` CI tests., Well it can cause issue with anyone that updates `datasets` and re-run some code that uses filter, so I'm creating a PR, I just merged a fix, let me know if...</td>\n",
              "      <td>## Describe the bug\\r\\nAfter upgrading to datasets `1.12.0`, some cached `.filter()` steps from `1.11.0` started failing with \\r\\n`ValueError: Keys mismatch: between {'indices': Value(dtype='uint64', id=None)} and {'file': Value(dtype='string', id=None), 'text': Value(dtype='string', id=None), 'speaker_id': Value(dtype='int64', id=None), 'chapter_id': Value(dtype='int64', id=None), 'id': Value(dtype='string', id=None)}`\\r\\n\\r\\nRelated feature: https://github.com/huggingface/datasets/pull/2836\\r\\n\\r\\n:question:  This is probably a `wontfix` bug, since it can be solved by simply cleaning the...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>https://github.com/huggingface/datasets/issues/2941</td>\n",
              "      <td>OSCAR unshuffled_original_ko: NonMatchingSplitsSizesError</td>\n",
              "      <td>[I tried `unshuffled_original_da` and it is also not working]</td>\n",
              "      <td>## Describe the bug\\r\\n\\r\\nCannot download OSCAR `unshuffled_original_ko` due to `NonMatchingSplitsSizesError`.\\r\\n\\r\\n## Steps to reproduce the bug\\r\\n\\r\\n```python\\r\\n&gt;&gt;&gt; dataset = datasets.load_dataset('oscar', 'unshuffled_original_ko')\\r\\nNonMatchingSplitsSizesError: [{'expected': SplitInfo(name='train', num_bytes=25292102197, num_examples=7345075, dataset_name='oscar'), 'recorded': SplitInfo(name='train', num_bytes=25284578514, num_examples=7344907, dataset_name='oscar')}]\\r\\n```\\r\\n\\r\\n## Expected results\\r\\n\\r\\nLoading is successful.\\r\\n\\r\\n## Actual results\\r\\n\\r\\nLoading throws ab...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>https://github.com/huggingface/datasets/issues/2937</td>\n",
              "      <td>load_dataset using default cache on Windows causes PermissionError: [WinError 5] Access is denied</td>\n",
              "      <td>[Hi @daqieq, thanks for reporting.\\r\\n\\r\\nUnfortunately, I was not able to reproduce this bug:\\r\\n```ipython\\r\\nIn [1]: from datasets import load_dataset\\r\\n   ...: ds = load_dataset('wiki_bio')\\r\\nDownloading: 7.58kB [00:00, 26.3kB/s]\\r\\nDownloading: 2.71kB [00:00, ?B/s]\\r\\nUsing custom data configuration default\\r\\nDownloading and preparing dataset wiki_bio/default (download: 318.53 MiB, generated: 736.94 MiB, post-processed: Unknown size, total: 1.03 GiB) to C:\\Users\\username\\.cache\\huggingface\\datasets\\wiki_bio\\default\\\\r\\n1.1.0\\5293ce565954ba965dada626f1e79684e98172d950371d266bf3caaf8...</td>\n",
              "      <td>## Describe the bug\\r\\nStandard process to download and load the wiki_bio dataset causes PermissionError in Windows 10 and 11.\\r\\n\\r\\n## Steps to reproduce the bug\\r\\n```python\\r\\nfrom datasets import load_dataset\\r\\nds = load_dataset('wiki_bio')\\r\\n```\\r\\n\\r\\n## Expected results\\r\\nIt is expected that the dataset downloads without any errors.\\r\\n\\r\\n## Actual results\\r\\nPermissionError see trace below:\\r\\n```\\r\\nUsing custom data configuration default\\r\\nDownloading and preparing dataset wiki_bio/default (download: 318.53 MiB, generated: 736.94 MiB, post-processed: Unknown size, total: 1....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>https://github.com/huggingface/datasets/issues/2934</td>\n",
              "      <td>to_tf_dataset keeps a reference to the open data somewhere, causing issues on windows</td>\n",
              "      <td>[I did some investigation and, as it seems, the bug stems from [this line](https://github.com/huggingface/datasets/blob/8004d7c3e1d74b29c3e5b0d1660331cd26758363/src/datasets/arrow_dataset.py#L325). The lifecycle of the dataset from the linked line is bound to one of the returned `tf.data.Dataset`. So my (hacky) solution involves wrapping the linked dataset with `weakref.proxy` and adding a custom `__del__` to `tf.python.data.ops.dataset_ops.TensorSliceDataset` (this is the type of a dataset that is returned by `tf.data.Dataset.from_tensor_slices`; this works for TF 2.x, but I'm not sure `t...</td>\n",
              "      <td>To reproduce:\\r\\n```python\\r\\nimport datasets as ds\\r\\nimport weakref\\r\\nimport gc\\r\\n\\r\\nd = ds.load_dataset(\"mnist\", split=\"train\")\\r\\nref = weakref.ref(d._data.table)\\r\\ntfd = d.to_tf_dataset(\"image\", batch_size=1, shuffle=False, label_cols=\"label\")\\r\\ndel tfd, d\\r\\ngc.collect()\\r\\nassert ref() is None, \"Error: there is at least one reference left\"\\r\\n```\\r\\n\\r\\nThis causes issues because the table holds a reference to an open arrow file that should be closed. So on windows it's not possible to delete or move the arrow file afterwards.\\r\\n\\r\\nMoreover the CI test of the `to_tf_dataset` ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-02871d02-6572-4560-8106-b53918f06947')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-02871d02-6572-4560-8106-b53918f06947 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-02871d02-6572-4560-8106-b53918f06947');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-eabfb226-c4db-4943-89fa-33537d8ace2c\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-eabfb226-c4db-4943-89fa-33537d8ace2c')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-eabfb226-c4db-4943-89fa-33537d8ace2c button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "issues_dataset_1.set_format(\"pandas\")\n",
        "df: pd.DataFrame = issues_dataset_1[:]\n",
        "# OR df = issues_dataset_1.to_pandas()\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        },
        "id": "s4JalQxJoMVr",
        "outputId": "339ca288-465d-449b-82f3-03dcf67189bd"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m[\u001b[0m\u001b[32m'Cool, I think we can do both :\u001b[0m\u001b[32m)\u001b[0m\u001b[32m'\u001b[0m\n",
              " \u001b[32m'@lhoestq now the 2 are implemented.\\r\\n\\r\\nPlease note that for the the second protection, finally I have chosen \u001b[0m\n",
              "\u001b[32mto protect the master branch only from **merge commits** \u001b[0m\u001b[32m(\u001b[0m\u001b[32msee update comment above\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, so no need to \u001b[0m\n",
              "\u001b[32mdisable/re-enable the protection on each release \u001b[0m\u001b[32m(\u001b[0m\u001b[32mdirect commits, different from merge commits, can be pushed to \u001b[0m\n",
              "\u001b[32mthe remote master branch; and eventually reverted without messing up the repo history\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.'\u001b[0m\u001b[1m]\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'Cool, I think we can do both :)'</span>\n",
              " <span style=\"color: #008000; text-decoration-color: #008000\">'@lhoestq now the 2 are implemented.\\r\\n\\r\\nPlease note that for the the second protection, finally I have chosen </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">to protect the master branch only from **merge commits** (see update comment above), so no need to </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">disable/re-enable the protection on each release (direct commits, different from merge commits, can be pushed to </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">the remote master branch; and eventually reverted without messing up the repo history).'</span><span style=\"font-weight: bold\">]</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "print(df[\"comments\"].iloc[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "id": "i3ev7Bt6oMVs",
        "outputId": "c6db107c-42c5-4902-ca05-f69d895896d3"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;36m2\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# There are two comments in this particular comment index\n",
        "print(len(df[\"comments\"].iloc[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "id": "m3zJHXY-oMVs",
        "outputId": "399b2a8b-1f0e-4d8f-bf7b-3fa48a38e710"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              html_url  \\\n",
              "0  https://github.com/huggingface/datasets/issues/2945   \n",
              "1  https://github.com/huggingface/datasets/issues/2945   \n",
              "2  https://github.com/huggingface/datasets/issues/2943   \n",
              "\n",
              "                                                                     title  \\\n",
              "0                                                    Protect master branch   \n",
              "1                                                    Protect master branch   \n",
              "2  Backwards compatibility broken for cached datasets that use `.filter()`   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                       comments  \\\n",
              "0                                                                                                                                                                                                                                                                                                                                                                                               Cool, I think we can do both :)   \n",
              "1  @lhoestq now the 2 are implemented.\\r\\n\\r\\nPlease note that for the the second protection, finally I have chosen to protect the master branch only from **merge commits** (see update comment above), so no need to disable/re-enable the protection on each release (direct commits, different from merge commits, can be pushed to the remote master branch; and eventually reverted without messing up the repo history).   \n",
              "2                                                                                                                                          Hi ! I guess the caching mechanism should have considered the new `filter` to be different from the old one, and don't use cached results from the old `filter`.\\r\\nTo avoid other users from having this issue we could make the caching differentiate the two, what do you think ?   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      body  \n",
              "0  After accidental merge commit (91c55355b634d0dc73350a7ddee1a6776dbbdd69) into `datasets` master branch, all commits present in the feature branch were permanently added to `datasets` master branch history, as e.g.:\\r\\n- 00cc036fea7c7745cfe722360036ed306796a3f2\\r\\n- 13ae8c98602bbad8197de3b9b425f4c78f582af1\\r\\n- ...\\r\\n\\r\\nI propose to protect our master branch, so that we avoid we can accidentally make this kind of mistakes in the future:\\r\\n- [x] For Pull Requests using GitHub, allow only squash merging, so that only a single commit per Pull Request is merged into the master branch\\r\\n  - ...  \n",
              "1  After accidental merge commit (91c55355b634d0dc73350a7ddee1a6776dbbdd69) into `datasets` master branch, all commits present in the feature branch were permanently added to `datasets` master branch history, as e.g.:\\r\\n- 00cc036fea7c7745cfe722360036ed306796a3f2\\r\\n- 13ae8c98602bbad8197de3b9b425f4c78f582af1\\r\\n- ...\\r\\n\\r\\nI propose to protect our master branch, so that we avoid we can accidentally make this kind of mistakes in the future:\\r\\n- [x] For Pull Requests using GitHub, allow only squash merging, so that only a single commit per Pull Request is merged into the master branch\\r\\n  - ...  \n",
              "2  ## Describe the bug\\r\\nAfter upgrading to datasets `1.12.0`, some cached `.filter()` steps from `1.11.0` started failing with \\r\\n`ValueError: Keys mismatch: between {'indices': Value(dtype='uint64', id=None)} and {'file': Value(dtype='string', id=None), 'text': Value(dtype='string', id=None), 'speaker_id': Value(dtype='int64', id=None), 'chapter_id': Value(dtype='int64', id=None), 'id': Value(dtype='string', id=None)}`\\r\\n\\r\\nRelated feature: https://github.com/huggingface/datasets/pull/2836\\r\\n\\r\\n:question:  This is probably a `wontfix` bug, since it can be solved by simply cleaning the...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-337c12b0-dc6d-4185-a7ba-26e7b95ab1f4\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>html_url</th>\n",
              "      <th>title</th>\n",
              "      <th>comments</th>\n",
              "      <th>body</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>https://github.com/huggingface/datasets/issues/2945</td>\n",
              "      <td>Protect master branch</td>\n",
              "      <td>Cool, I think we can do both :)</td>\n",
              "      <td>After accidental merge commit (91c55355b634d0dc73350a7ddee1a6776dbbdd69) into `datasets` master branch, all commits present in the feature branch were permanently added to `datasets` master branch history, as e.g.:\\r\\n- 00cc036fea7c7745cfe722360036ed306796a3f2\\r\\n- 13ae8c98602bbad8197de3b9b425f4c78f582af1\\r\\n- ...\\r\\n\\r\\nI propose to protect our master branch, so that we avoid we can accidentally make this kind of mistakes in the future:\\r\\n- [x] For Pull Requests using GitHub, allow only squash merging, so that only a single commit per Pull Request is merged into the master branch\\r\\n  - ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>https://github.com/huggingface/datasets/issues/2945</td>\n",
              "      <td>Protect master branch</td>\n",
              "      <td>@lhoestq now the 2 are implemented.\\r\\n\\r\\nPlease note that for the the second protection, finally I have chosen to protect the master branch only from **merge commits** (see update comment above), so no need to disable/re-enable the protection on each release (direct commits, different from merge commits, can be pushed to the remote master branch; and eventually reverted without messing up the repo history).</td>\n",
              "      <td>After accidental merge commit (91c55355b634d0dc73350a7ddee1a6776dbbdd69) into `datasets` master branch, all commits present in the feature branch were permanently added to `datasets` master branch history, as e.g.:\\r\\n- 00cc036fea7c7745cfe722360036ed306796a3f2\\r\\n- 13ae8c98602bbad8197de3b9b425f4c78f582af1\\r\\n- ...\\r\\n\\r\\nI propose to protect our master branch, so that we avoid we can accidentally make this kind of mistakes in the future:\\r\\n- [x] For Pull Requests using GitHub, allow only squash merging, so that only a single commit per Pull Request is merged into the master branch\\r\\n  - ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>https://github.com/huggingface/datasets/issues/2943</td>\n",
              "      <td>Backwards compatibility broken for cached datasets that use `.filter()`</td>\n",
              "      <td>Hi ! I guess the caching mechanism should have considered the new `filter` to be different from the old one, and don't use cached results from the old `filter`.\\r\\nTo avoid other users from having this issue we could make the caching differentiate the two, what do you think ?</td>\n",
              "      <td>## Describe the bug\\r\\nAfter upgrading to datasets `1.12.0`, some cached `.filter()` steps from `1.11.0` started failing with \\r\\n`ValueError: Keys mismatch: between {'indices': Value(dtype='uint64', id=None)} and {'file': Value(dtype='string', id=None), 'text': Value(dtype='string', id=None), 'speaker_id': Value(dtype='int64', id=None), 'chapter_id': Value(dtype='int64', id=None), 'id': Value(dtype='string', id=None)}`\\r\\n\\r\\nRelated feature: https://github.com/huggingface/datasets/pull/2836\\r\\n\\r\\n:question:  This is probably a `wontfix` bug, since it can be solved by simply cleaning the...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-337c12b0-dc6d-4185-a7ba-26e7b95ab1f4')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-337c12b0-dc6d-4185-a7ba-26e7b95ab1f4 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-337c12b0-dc6d-4185-a7ba-26e7b95ab1f4');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-4f77120b-b2b1-4266-b3f7-db76cbb262ae\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4f77120b-b2b1-4266-b3f7-db76cbb262ae')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-4f77120b-b2b1-4266-b3f7-db76cbb262ae button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "# Use the explode method to create a row for each comment in a given index.\n",
        "# i.e. index 0 with 2 comments creates 2 rows and index 1 with 6 comments creates 6 rows, etc.\n",
        "comments_df: pd.DataFrame = df.explode(\"comments\", ignore_index=True)\n",
        "comments_df.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qnpu3oAfoMVs",
        "outputId": "13f9181b-844e-41d5-e51d-ff4445a3967a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['html_url', 'title', 'comments', 'body'],\n",
              "    num_rows: 2964\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "# Convert back to Dataset\n",
        "comments_dataset: Dataset = Dataset.from_pandas(comments_df)\n",
        "comments_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121,
          "referenced_widgets": [
            "6c2175ce28e442e6902f9687e58ae986",
            "167df158f70045cd9eb8b0e202e103e9",
            "3d955237041f468299e8fc0133eeaec0",
            "a71a5912e5374130b569c4a66c39660b",
            "ccea5ae0a4974543866a0c2e459d789c",
            "d0d1f4a2eca54c649c8cbbfc0e6dec4c",
            "b8e9a1417e5f4d57b48959bf7962240d",
            "6a38b8c03f464fea8e645f915ff4734a",
            "21ea0b39c54d403686f06e9e91952397",
            "4a3bb9165b1c4d469d47a08c4926e140",
            "3add2b6262934223a16d6dd1f629be51"
          ]
        },
        "id": "B0KWGLntoMVs",
        "outputId": "460d007b-9d58-4c47-c9c0-75d03e7c445f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/2964 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6c2175ce28e442e6902f9687e58ae986"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['html_url', 'title', 'comments', 'body', 'comment_length'],\n",
              "    num_rows: 2964\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "# Add n new columns\n",
        "comments_dataset = comments_dataset.map(\n",
        "    lambda x: {\"comment_length\": len(x.get(\"comments\").split())}\n",
        ")\n",
        "\n",
        "comments_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtYvxD4UoMVs",
        "outputId": "3e40cf6f-dcf4-4a8b-9c0e-b35453198b18"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'comments': ['https://github.com/huggingface/datasets/blob/6c766f9115d686182d76b1b937cb27e099c45d68/src/datasets/builder.py#L179-L186',\n",
              "  'https://github.com/huggingface/datasets/blob/6c766f9115d686182d76b1b937cb27e099c45d68/src/datasets/builder.py#L179-L186',\n",
              "  '@albertvillanova ',\n",
              "  'Thanks!',\n",
              "  '#self-assign',\n",
              "  '#take',\n",
              "  '#take',\n",
              "  '#self-assign',\n",
              "  'Resolved',\n",
              "  'Ty!'],\n",
              " 'comment_length': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "# Short comments\n",
        "comments_dataset.sort(\"comment_length\").select_columns([\"comments\", \"comment_length\"])[\n",
        "    :10\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121,
          "referenced_widgets": [
            "5a1fce2be658465294e0e3a5dc947794",
            "63b5fc821e9245dab54377e613ed1c93",
            "236e9fb75c1047769e044e46e1291196",
            "59b6d437f5544442b68b2d00e94acf6b",
            "5f73f21ed89c458ab461af6d82b8dc06",
            "6718c739c1114293b52c7fcd0f3063f0",
            "321755f964f4412f9c2c0a25d376dd54",
            "582d3c99e3b44808a4344c4a02c4684f",
            "f219ec180e8641c1a9b906bd0154b94e",
            "c2d45934112649aaafca4981d2bc0ac4",
            "9c0949fd85f0450fa388b869d48641c5"
          ]
        },
        "id": "VxzUA4GxoMVt",
        "outputId": "f78a1eda-e2a8-4c04-dcfe-74bdfb6d1d3f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Filter:   0%|          | 0/2964 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5a1fce2be658465294e0e3a5dc947794"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['html_url', 'title', 'comments', 'body', 'comment_length'],\n",
              "    num_rows: 2175\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "# Drop short comments.\n",
        "# i.e comments like: 'Thanks!', '#self-assign', etc.\n",
        "comments_dataset = comments_dataset.filter(lambda x: x.get(\"comment_length\") > 15)\n",
        "comments_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121,
          "referenced_widgets": [
            "a52ffcfdff274a91be5afc951706e8ee",
            "ff89d769a93e46ada746e9c3242fc6d3",
            "72b54539198a4bc8a1640a744af78040",
            "391d57b656d94abda6167c3cdfc6a480",
            "59f465b12ee94587a44d1e4438010a32",
            "3f485adf1b18427ca5412e3b72e89f70",
            "415d368d9c874eb39417ff656c64b282",
            "55e3ec1ba9564bee86ead59279c7c188",
            "97b2b48bceb54c659cb840dfd0d62e7c",
            "c8de28812c4140eb925b1c734250e5fe",
            "5b5e77718e874fc1a068c83c1536abaa"
          ]
        },
        "id": "OKu3VPRsoMVt",
        "outputId": "fa3bea1c-d970-4d72-fc8d-1c7d36337e42"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/2175 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a52ffcfdff274a91be5afc951706e8ee"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['html_url', 'title', 'comments', 'body', 'comment_length', 'text'],\n",
              "    num_rows: 2175\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "# Concatenate the issue title, description (body), and comments together in a new text column.\n",
        "def concat_data(example: dict[str, Any]) -> dict[str, Any]:\n",
        "    \"\"\"This is used to concatenate the title, body and comments together.\"\"\"\n",
        "    title: str = example.get(\"title\")\n",
        "    comments: str = example.get(\"comments\")\n",
        "    body: str = example.get(\"body\")\n",
        "    result = {\"text\": f\"{title} \\n {body} \\n {comments}\"}\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "comments_dataset = comments_dataset.map(concat_data)\n",
        "comments_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2uNfHgA6oMVu",
        "outputId": "2e203d83-84f3-4d21-9c40-84f7f896439f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'html_url': 'https://github.com/huggingface/datasets/issues/2945',\n",
              " 'title': 'Protect master branch',\n",
              " 'comments': '@lhoestq now the 2 are implemented.\\r\\n\\r\\nPlease note that for the the second protection, finally I have chosen to protect the master branch only from **merge commits** (see update comment above), so no need to disable/re-enable the protection on each release (direct commits, different from merge commits, can be pushed to the remote master branch; and eventually reverted without messing up the repo history).',\n",
              " 'body': 'After accidental merge commit (91c55355b634d0dc73350a7ddee1a6776dbbdd69) into `datasets` master branch, all commits present in the feature branch were permanently added to `datasets` master branch history, as e.g.:\\r\\n- 00cc036fea7c7745cfe722360036ed306796a3f2\\r\\n- 13ae8c98602bbad8197de3b9b425f4c78f582af1\\r\\n- ...\\r\\n\\r\\nI propose to protect our master branch, so that we avoid we can accidentally make this kind of mistakes in the future:\\r\\n- [x] For Pull Requests using GitHub, allow only squash merging, so that only a single commit per Pull Request is merged into the master branch\\r\\n  - Currently, simple merge commits are already disabled\\r\\n  - I propose to disable rebase merging as well\\r\\n- ~~Protect the master branch from direct pushes (to avoid accidentally pushing of merge commits)~~\\r\\n  - ~~This protection would reject direct pushes to master branch~~\\r\\n  - ~~If so, for each release (when we need to commit directly to the master branch), we should previously disable the protection and re-enable it again after the release~~\\r\\n- [x] Protect the master branch only from direct pushing of **merge commits**\\r\\n  - GitHub offers the possibility to protect the master branch only from merge commits (which are the ones that introduce all the commits from the feature branch into the master branch).\\r\\n  - No need to disable/re-enable this protection on each release \\r\\n\\r\\nThis purpose of this Issue is to open a discussion about this problem and to agree in a solution.',\n",
              " 'comment_length': 64,\n",
              " 'text': 'Protect master branch \\n After accidental merge commit (91c55355b634d0dc73350a7ddee1a6776dbbdd69) into `datasets` master branch, all commits present in the feature branch were permanently added to `datasets` master branch history, as e.g.:\\r\\n- 00cc036fea7c7745cfe722360036ed306796a3f2\\r\\n- 13ae8c98602bbad8197de3b9b425f4c78f582af1\\r\\n- ...\\r\\n\\r\\nI propose to protect our master branch, so that we avoid we can accidentally make this kind of mistakes in the future:\\r\\n- [x] For Pull Requests using GitHub, allow only squash merging, so that only a single commit per Pull Request is merged into the master branch\\r\\n  - Currently, simple merge commits are already disabled\\r\\n  - I propose to disable rebase merging as well\\r\\n- ~~Protect the master branch from direct pushes (to avoid accidentally pushing of merge commits)~~\\r\\n  - ~~This protection would reject direct pushes to master branch~~\\r\\n  - ~~If so, for each release (when we need to commit directly to the master branch), we should previously disable the protection and re-enable it again after the release~~\\r\\n- [x] Protect the master branch only from direct pushing of **merge commits**\\r\\n  - GitHub offers the possibility to protect the master branch only from merge commits (which are the ones that introduce all the commits from the feature branch into the master branch).\\r\\n  - No need to disable/re-enable this protection on each release \\r\\n\\r\\nThis purpose of this Issue is to open a discussion about this problem and to agree in a solution. \\n @lhoestq now the 2 are implemented.\\r\\n\\r\\nPlease note that for the the second protection, finally I have chosen to protect the master branch only from **merge commits** (see update comment above), so no need to disable/re-enable the protection on each release (direct commits, different from merge commits, can be pushed to the remote master branch; and eventually reverted without messing up the repo history).'}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "comments_dataset[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9gJYeesoMVu"
      },
      "source": [
        "## Create Text Embeddings\n",
        "\n",
        "- This [table](https://www.sbert.net/docs/pretrained_models.html#model-overview) shows the model overview for sentence-transformers (open source) semantic search.\n",
        "\n",
        "<br>\n",
        "\n",
        "[![image.png](https://i.postimg.cc/MZLLL8TS/image.png)](https://postimg.cc/c6QTK2w9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "tOwpSTp4oMVu"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "repo_name: str = \"sentence-transformers\"\n",
        "checkpoint: str = f\"{repo_name}/multi-qa-mpnet-base-dot-v1\"\n",
        "\n",
        "# Instantiate tokenizer\n",
        "tokenizer: AutoTokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "# Instantiate model\n",
        "model: AutoModel = AutoModel.from_pretrained(checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5mOkXq9foMVv",
        "outputId": "6005b2c0-3e74-413e-a06d-c6b3d6d5622a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# To speed up the processing, push to a GPU\n",
        "device_str: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device = torch.device(device=device_str)\n",
        "\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RArfeI3RoMVv",
        "outputId": "f31233d1-3d5c-4ae1-dcc9-a78b2020b544"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MPNetModel(\n",
              "  (embeddings): MPNetEmbeddings(\n",
              "    (word_embeddings): Embedding(30527, 768, padding_idx=1)\n",
              "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): MPNetEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0-11): 12 x MPNetLayer(\n",
              "        (attention): MPNetAttention(\n",
              "          (attn): MPNetSelfAttention(\n",
              "            (q): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (o): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (intermediate): MPNetIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): MPNetOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (relative_attention_bias): Embedding(32, 12)\n",
              "  )\n",
              "  (pooler): MPNetPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Ohe51gUeoMVv"
      },
      "outputs": [],
      "source": [
        "# To represent each GitHub issue entry as a single vector, we need to pool or\n",
        "# average the token embeddings. One popular approach is CLS pooling, where the last\n",
        "# hidden state for the special [CLS] token is collected.\n",
        "\n",
        "\n",
        "def cls_pooling(model_output):\n",
        "    \"\"\"To represent each GitHub issue entry as a single vector, pool or average the token embeddings.\n",
        "    Using CLS pooling, where the last hidden state for the special [CLS] token is collected.\"\"\"\n",
        "    return model_output.last_hidden_state[:, 0]\n",
        "\n",
        "\n",
        "# Create a helper function that will tokenize a list of documents, place the tensors\n",
        "# on the GPU (if available), feed them to the model, and finally apply CLS pooling to the outputs:\n",
        "\n",
        "\n",
        "def get_embeddings(text_list: list):\n",
        "    \"\"\"This is used to tokenize a list of documents, place the tensors\n",
        "    on the GPU (if available), feed them to the model, and apply CLS pooling to the outputs.\"\"\"\n",
        "    encoded_input = tokenizer(\n",
        "        text_list, padding=True, truncation=True, return_tensors=\"pt\"\n",
        "    )\n",
        "    # Push to encoded input to the GPU (if available)\n",
        "    encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n",
        "    model_output = model(**encoded_input)\n",
        "    return cls_pooling(model_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "WUB_DCaMoMVv",
        "outputId": "bc29b4cf-41f1-4f4f-8c99-08a4097faf5e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Protect master branch \n",
              " After accidental merge commit \u001b[1m(\u001b[0m91c55355b634d0dc73350a7ddee1a6776dbbdd69\u001b[1m)\u001b[0m into `datasets` master branch, all \n",
              "commits present in the feature branch were permanently added to `datasets` master branch history, as e.g.:\n",
              "- 00cc036fea7c7745cfe722360036ed306796a3f2\n",
              "- 13ae8c98602bbad8197de3b9b425f4c78f582af1\n",
              "- \u001b[33m...\u001b[0m\n",
              "\n",
              "I propose to protect our master branch, so that we avoid we can accidentally make this kind of mistakes in the \n",
              "future:\n",
              "-  For Pull Requests using GitHub, allow only squash merging, so that only a single commit per Pull Request is \n",
              "merged into the master branch\n",
              "  - Currently, simple merge commits are already disabled\n",
              "  - I propose to disable rebase merging as well\n",
              "- ~~Protect the master branch from direct pushes \u001b[1m(\u001b[0mto avoid accidentally pushing of merge commits\u001b[1m)\u001b[0m~~\n",
              "  - ~~This protection would reject direct pushes to master branch~~\n",
              "  - ~~If so, for each release \u001b[1m(\u001b[0mwhen we need to commit directly to the master branch\u001b[1m)\u001b[0m, we should previously disable \n",
              "the protection and re-enable it again after the release~~\n",
              "-  Protect the master branch only from direct pushing of **merge commits**\n",
              "  - GitHub offers the possibility to protect the master branch only from merge commits \u001b[1m(\u001b[0mwhich are the ones that \n",
              "introduce all the commits from the feature branch into the master branch\u001b[1m)\u001b[0m.\n",
              "  - No need to disable/re-enable this protection on each release \n",
              "\n",
              "This purpose of this Issue is to open a discussion about this problem and to agree in a solution. \n",
              " @lhoestq now the \u001b[1;36m2\u001b[0m are implemented.\n",
              "\n",
              "Please note that for the the second protection, finally I have chosen to protect the master branch only from \n",
              "**merge commits** \u001b[1m(\u001b[0msee update comment above\u001b[1m)\u001b[0m, so no need to disable/re-enable the protection on each release \n",
              "\u001b[1m(\u001b[0mdirect commits, different from merge commits, can be pushed to the remote master branch; and eventually reverted \n",
              "without messing up the repo history\u001b[1m)\u001b[0m.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Protect master branch \n",
              " After accidental merge commit <span style=\"font-weight: bold\">(</span>91c55355b634d0dc73350a7ddee1a6776dbbdd69<span style=\"font-weight: bold\">)</span> into `datasets` master branch, all \n",
              "commits present in the feature branch were permanently added to `datasets` master branch history, as e.g.:\n",
              "- 00cc036fea7c7745cfe722360036ed306796a3f2\n",
              "- 13ae8c98602bbad8197de3b9b425f4c78f582af1\n",
              "- <span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
              "\n",
              "I propose to protect our master branch, so that we avoid we can accidentally make this kind of mistakes in the \n",
              "future:\n",
              "-  For Pull Requests using GitHub, allow only squash merging, so that only a single commit per Pull Request is \n",
              "merged into the master branch\n",
              "  - Currently, simple merge commits are already disabled\n",
              "  - I propose to disable rebase merging as well\n",
              "- ~~Protect the master branch from direct pushes <span style=\"font-weight: bold\">(</span>to avoid accidentally pushing of merge commits<span style=\"font-weight: bold\">)</span>~~\n",
              "  - ~~This protection would reject direct pushes to master branch~~\n",
              "  - ~~If so, for each release <span style=\"font-weight: bold\">(</span>when we need to commit directly to the master branch<span style=\"font-weight: bold\">)</span>, we should previously disable \n",
              "the protection and re-enable it again after the release~~\n",
              "-  Protect the master branch only from direct pushing of **merge commits**\n",
              "  - GitHub offers the possibility to protect the master branch only from merge commits <span style=\"font-weight: bold\">(</span>which are the ones that \n",
              "introduce all the commits from the feature branch into the master branch<span style=\"font-weight: bold\">)</span>.\n",
              "  - No need to disable/re-enable this protection on each release \n",
              "\n",
              "This purpose of this Issue is to open a discussion about this problem and to agree in a solution. \n",
              " @lhoestq now the <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> are implemented.\n",
              "\n",
              "Please note that for the the second protection, finally I have chosen to protect the master branch only from \n",
              "**merge commits** <span style=\"font-weight: bold\">(</span>see update comment above<span style=\"font-weight: bold\">)</span>, so no need to disable/re-enable the protection on each release \n",
              "<span style=\"font-weight: bold\">(</span>direct commits, different from merge commits, can be pushed to the remote master branch; and eventually reverted \n",
              "without messing up the repo history<span style=\"font-weight: bold\">)</span>.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "print(comments_dataset[\"text\"][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JE6NPnLGoMVv",
        "outputId": "18d08f60-ee24-4b11-ba0d-41e7daf9cead"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "# Test that the function works by feeding it the first text entry in our corpus\n",
        "# and inspecting the output shape;\n",
        "embedding = get_embeddings(comments_dataset[\"text\"][0])\n",
        "embedding.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLWofuq-oMVw",
        "outputId": "1c72cc00-1607-4e39-bc37-41ca1a5d9704"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-1.5532e-01, -1.0023e-01, -7.0321e-02, -7.9817e-02, -1.0425e-01,\n",
              "         -1.8799e-01,  1.0403e-02,  2.7286e-01, -9.8488e-03, -8.4140e-02,\n",
              "          2.9261e-01, -7.7875e-02, -1.3246e-01,  2.1589e-01, -5.6229e-02,\n",
              "          1.7055e-01,  2.0032e-01, -4.0798e-02, -9.2333e-02,  2.7923e-02,\n",
              "          2.2608e-02, -6.8703e-02,  8.3200e-02,  4.3630e-02, -1.8378e-01,\n",
              "          5.4012e-03, -1.4812e-02,  1.5650e-01, -4.3977e-01, -4.6957e-01,\n",
              "          1.4525e-01,  2.3224e-01,  2.5106e-02,  5.7053e-01, -1.0436e-04,\n",
              "         -3.4550e-04,  1.6949e-01, -4.0601e-02, -1.4010e-01, -2.1546e-01,\n",
              "         -4.9011e-01, -4.4480e-01, -7.0063e-02, -8.3543e-03,  1.2881e-01,\n",
              "          5.2868e-02, -1.3250e-01,  2.3846e-02,  3.8485e-01,  1.0555e-01,\n",
              "          2.3861e-01, -1.0285e-01,  1.8124e-01,  1.3769e-02,  3.9201e-01,\n",
              "          5.4745e-01, -6.6634e-02,  3.0320e-01,  5.4018e-02,  6.7586e-04,\n",
              "          6.1106e-02,  1.7411e-01, -8.2372e-02, -2.8667e-01,  2.2702e-01,\n",
              "         -1.7799e-01,  5.1817e-01, -4.2435e-01, -2.4158e-01, -8.3118e-02,\n",
              "          5.2379e-02, -1.7815e-01, -4.0890e-01, -2.4194e-01, -1.4995e-01,\n",
              "          7.6337e-03,  6.7780e-02,  2.4686e-01,  3.9740e-02,  3.6497e-03,\n",
              "         -3.7737e-01, -2.7181e-01, -4.4078e-03,  4.9204e-02,  1.4194e-01,\n",
              "         -1.6263e-01,  6.6426e-02, -1.2704e-01,  2.9422e-01, -4.7328e-02,\n",
              "         -1.4698e-01, -3.6763e-01, -2.4133e-01, -1.2587e-02, -1.0261e-01,\n",
              "         -1.9575e-01, -2.4621e-01, -3.2483e-01,  4.2513e-01,  3.8897e-01,\n",
              "         -3.8327e-01,  1.3740e-02, -3.0489e-02, -8.1016e-02,  2.0719e-01,\n",
              "          1.1786e-01, -1.0490e-02,  5.8488e-02,  5.3071e-01,  3.1444e-01,\n",
              "          5.1585e-03,  3.2559e-01,  3.0157e-01,  3.0472e-03,  3.4446e-02,\n",
              "          5.7412e-01,  4.2894e-01, -2.6292e-01,  1.2765e-01,  1.9597e-01,\n",
              "         -3.4120e-01, -3.8451e-01,  7.1131e-02, -8.6172e-02,  1.1033e-01,\n",
              "          2.7134e-02, -1.1471e-01,  6.6802e-02,  4.5191e-02, -3.8086e-02,\n",
              "         -3.0790e-01, -1.4149e-01, -3.7440e-01, -6.8531e-02,  1.8604e-01,\n",
              "         -4.5710e-01,  1.4917e-01,  4.3252e-01, -6.8037e-02, -1.9928e-01,\n",
              "          4.4552e-02,  1.2923e-01,  5.5587e-02,  2.9612e-01, -1.5380e-01,\n",
              "         -4.5263e-01,  2.2360e-01,  1.3133e-01, -6.6900e-02,  1.9400e-01,\n",
              "         -1.5990e-01, -2.6162e-01, -1.0498e-01,  2.9777e-01, -1.4827e-01,\n",
              "          3.0716e-01, -5.2377e-01,  3.6373e-02, -4.4837e-03,  3.5067e-01,\n",
              "          4.9104e-01,  3.0255e-01,  3.2808e-01, -1.6859e-01,  6.1757e-02,\n",
              "          1.9035e-01,  3.2351e-01, -5.1686e-02,  6.2304e-02, -2.8774e-01,\n",
              "          1.0950e-01,  1.4507e-01, -3.1216e-01,  1.7556e-02, -5.2361e-03,\n",
              "         -1.3128e-01,  1.0776e-02, -7.1407e-02, -1.1199e-01, -9.8930e-02,\n",
              "         -3.6669e-01,  6.9403e-02, -4.0737e-02, -1.0078e-01,  2.0796e-01,\n",
              "         -3.2018e-01, -9.4794e-02,  2.0508e-01, -1.4812e-01,  2.0899e-02,\n",
              "         -2.9199e-01, -6.0614e-01, -1.6366e-01, -6.3439e-02,  1.2491e-01,\n",
              "          2.2756e-01,  3.6077e-01, -5.0309e-03, -2.8362e-02, -1.7140e-01,\n",
              "         -9.3973e-03,  1.1251e-01,  3.4214e-01, -1.4968e-01, -2.4154e-01,\n",
              "          1.3225e-01, -3.5459e-01,  9.1141e-02, -7.8512e-02, -9.7749e-03,\n",
              "          3.4449e-02, -3.1579e-01,  3.4128e-01, -4.4053e-02, -1.2080e-01,\n",
              "         -1.1415e-01,  2.4152e-01,  9.5952e-02, -1.7473e-01, -2.0392e-01,\n",
              "         -2.0741e-01,  2.6019e-01, -2.4083e-01,  3.0187e-01,  1.3001e-01,\n",
              "          1.0264e-01, -1.4118e-01, -3.6453e-02, -2.5096e-02,  3.6194e-01,\n",
              "          2.3630e-01, -8.9917e-02, -5.9158e-02,  1.6876e-01, -4.9063e-02,\n",
              "          2.0357e-01,  2.2469e-01,  4.2524e-01,  3.7362e-01, -1.0268e-01,\n",
              "          6.2601e-02, -9.1793e-02, -7.8724e-02,  2.4595e-02, -3.2913e-01,\n",
              "          3.2804e-01,  7.3694e-02, -1.3946e-01, -1.7403e-01, -1.0333e-01,\n",
              "         -1.7407e-01, -2.2027e-01, -1.7077e-01, -1.2951e-02, -1.6910e-01,\n",
              "          1.9436e-01, -3.3659e-01,  2.2237e-01, -2.7721e-01,  1.4858e-01,\n",
              "          1.5053e-02, -2.5161e-01, -1.0240e-01,  6.8298e-02, -5.9672e-03,\n",
              "         -1.3272e-01,  1.2417e-01,  3.7738e-01,  1.2787e-01,  1.7010e-01,\n",
              "          4.2744e-03,  2.0151e-02,  2.5728e-01, -1.1644e-01,  3.5333e-01,\n",
              "          1.4431e-01, -1.0545e-01,  5.4064e-02,  3.3646e-01,  1.8122e-01,\n",
              "          6.4332e-02,  3.4692e-01, -3.3245e-03,  3.5258e-02, -2.0841e-01,\n",
              "         -4.5985e-02, -2.6021e-01, -5.2834e-01,  2.1924e-01, -1.1064e-01,\n",
              "         -3.5522e-01, -8.5043e-03, -4.5366e-02,  4.7418e-02, -5.2487e-01,\n",
              "          1.3603e-01,  1.8358e-01,  2.9053e-01, -3.1780e-01, -1.1998e-01,\n",
              "          1.2632e-01, -2.2595e-02,  1.2344e-02,  1.2856e-01,  3.6316e-01,\n",
              "         -2.2668e-01,  4.5945e-01,  2.3898e-01, -1.6956e-01, -4.7264e-01,\n",
              "         -3.4341e-01,  1.9130e-02, -2.0394e-01, -1.1797e-01,  9.8186e-03,\n",
              "         -9.8930e-02,  9.2154e-02, -3.5854e-01, -3.2571e-01, -1.8774e-02,\n",
              "         -3.7300e-01, -1.6010e-01,  6.9596e-02,  6.2886e-02, -2.3462e-01,\n",
              "         -1.1404e-01, -3.8659e-01, -2.8215e-01,  4.7936e-01, -1.6433e-02,\n",
              "          1.1566e-01, -4.3173e-02, -2.9238e-01, -4.5918e-02,  8.7483e-02,\n",
              "          1.3664e-01, -4.5056e-05, -3.6093e-01,  3.2567e-02, -4.0660e-02,\n",
              "         -2.9347e-02,  3.5261e-01,  1.8549e-01, -4.3024e-01,  7.1252e-02,\n",
              "         -3.2176e-01, -1.5582e-01, -5.2085e-02,  4.8644e-01,  3.1412e-01,\n",
              "         -9.7374e-02,  7.1080e-02,  1.0355e-01, -1.9190e-01,  5.5089e-02,\n",
              "         -1.8161e-01,  1.4722e-01,  3.4549e-01,  2.8572e-01,  3.7900e-02,\n",
              "          1.4587e-01,  2.5497e-02,  7.1409e-01,  3.7785e-01, -5.9657e-02,\n",
              "          2.1034e-01,  3.1070e-01, -2.7718e-02,  3.9905e-03, -2.6016e-01,\n",
              "          6.5047e-02, -1.0124e-01, -2.0868e-05,  8.6704e-02, -1.2958e-01,\n",
              "         -1.2297e-01, -1.1211e-02, -4.7771e-01,  9.1558e-03, -4.1468e-01,\n",
              "         -2.3966e-01, -3.1818e-01,  1.8425e-01, -2.2526e-02, -4.8549e-01,\n",
              "          1.3138e-01,  5.3771e-02,  2.3058e-01,  2.1597e-01,  2.1421e-01,\n",
              "          1.8122e-01, -2.0259e-02, -5.1084e-02, -1.2007e-01, -6.7726e-02,\n",
              "         -4.3311e-03,  1.4525e-01,  9.0580e-02, -1.6704e-01,  3.1805e-02,\n",
              "          6.9550e-02,  3.2999e-01, -1.4869e-02,  1.4029e-01, -1.7081e-01,\n",
              "         -5.0948e-01,  6.0863e-01, -3.1859e-01,  1.3940e-01,  2.9910e-01,\n",
              "          5.1122e-01,  4.3976e-01, -2.9545e-01, -1.9532e-01,  2.0712e-01,\n",
              "          3.2556e-02,  1.9784e-01,  1.7735e-01,  1.4498e-01,  2.7831e-01,\n",
              "         -4.9038e-01,  2.8877e-01,  1.5483e-01, -2.5048e-01, -1.5518e-01,\n",
              "         -3.9055e-03, -2.7050e-02,  1.2646e-01, -2.6304e-02, -2.4540e-01,\n",
              "          9.1276e-03, -1.0944e-01,  1.7345e-01,  1.3292e-01,  6.9828e-02,\n",
              "          4.1522e-01,  5.9466e-01,  1.3493e-01, -3.8920e-01, -4.8201e-02,\n",
              "         -1.7507e-01, -7.4283e-02,  3.2714e-01,  1.9835e-01,  2.4823e-01,\n",
              "          2.0348e-02,  1.7769e-01, -1.2347e-01, -2.6624e-01, -2.0337e-01,\n",
              "         -3.8491e-02, -7.6743e-02,  7.5892e-03,  8.7329e-02, -4.5898e-02,\n",
              "         -9.0992e-02,  1.3049e-02,  1.4000e-01, -3.6610e-01,  1.6378e-01,\n",
              "          6.1805e-01,  9.8254e-01,  3.9757e-01,  8.9034e-02, -2.5466e-01,\n",
              "         -3.2738e-01,  3.2599e-01, -3.4011e-01, -1.6645e-01, -2.4702e-01,\n",
              "         -1.2359e-01, -1.8425e-01, -3.4031e-01, -3.8694e-02, -4.8985e-01,\n",
              "         -3.5286e-01,  1.3511e-01, -2.1806e-01,  5.2792e-01,  8.1777e-03,\n",
              "          1.2339e-01,  1.9102e-01, -1.2231e-01,  2.5515e-01,  1.9827e-01,\n",
              "          6.1811e-02,  1.0410e-01, -4.0275e-03, -4.6879e-01, -2.2328e-03,\n",
              "          2.0539e-02, -3.7727e-01,  8.9147e-02, -3.4378e-01,  3.8847e-02,\n",
              "         -2.0635e-01, -3.1605e-01,  1.1583e-01, -8.7048e-02,  6.0594e-01,\n",
              "          1.6663e-01, -7.9452e-02, -4.4621e-02,  2.9139e-01,  4.6453e-01,\n",
              "         -2.2885e-01, -2.1953e-01,  5.2237e-01, -2.1336e-01, -1.3022e-01,\n",
              "         -2.0476e-01, -8.0334e-02, -2.3709e-01, -2.6095e-01, -1.2804e-01,\n",
              "         -3.2823e-01,  7.9502e-02,  4.1714e-02,  2.6365e-01, -9.1070e-02,\n",
              "         -1.6761e-01,  1.8599e-01,  1.1549e-01, -1.0757e-01, -1.1858e-01,\n",
              "         -1.6952e-01, -2.6104e-01, -6.7816e-02,  2.0160e-01,  1.6161e-01,\n",
              "         -1.2061e-01, -7.6764e-02,  9.9657e-02, -2.4434e-01, -2.2079e-01,\n",
              "         -2.0729e-01,  1.0599e-01, -1.3947e-01,  9.2981e-02, -5.4868e-01,\n",
              "         -3.2993e-01,  5.1747e-02,  2.4619e-01,  2.7942e-01,  3.3050e-01,\n",
              "          1.1800e-01, -9.9771e-02, -2.2445e-01,  2.1864e-01, -3.2793e-01,\n",
              "          1.6865e-01,  2.3584e-02,  1.5881e-01, -2.6010e-02,  4.0430e-02,\n",
              "         -3.9060e-01,  1.6032e-01, -1.2548e-01, -2.1126e-02, -2.2552e-01,\n",
              "         -1.0175e-01, -6.3019e-04, -3.0819e-01,  3.2488e-02,  2.3433e-02,\n",
              "         -1.9576e-01, -2.8302e-01,  9.2669e-02,  6.5696e-02,  6.4790e-02,\n",
              "         -5.7014e-02, -5.7461e-02, -1.0816e-01,  8.2753e-02,  2.7203e-01,\n",
              "          2.3969e-01, -1.5849e-01,  1.0971e-01, -3.2225e-02,  1.7109e-01,\n",
              "          1.1059e-01, -2.4773e-02,  8.9242e-02,  1.4632e-02, -2.6580e-01,\n",
              "         -3.5375e-02,  2.6716e-01, -1.8175e-01, -2.1714e-01, -2.6854e-01,\n",
              "          1.8884e-01,  1.0563e-01,  5.8409e-01,  2.0288e-01,  1.4422e-01,\n",
              "         -4.0508e-01,  1.7541e-01, -5.3198e-02, -3.6355e-03, -1.6924e-01,\n",
              "         -9.6940e-02, -8.4962e-02,  2.7936e-01, -8.6719e-02, -3.0360e-01,\n",
              "          2.7138e-01,  1.8818e-01,  7.4871e-04,  1.0497e-01,  5.2808e-01,\n",
              "          1.2962e-01, -1.8965e-01,  4.2214e-01,  2.7124e-01, -8.7676e-02,\n",
              "          4.9490e-01,  4.7023e-01, -1.1318e-01, -2.0325e-02, -8.2736e-02,\n",
              "          1.1661e-01,  2.3069e-01, -2.5410e-01,  1.2847e-01,  2.9622e-01,\n",
              "          6.4630e-01, -1.6888e-01,  3.9355e-01, -3.3096e-02,  3.4374e-01,\n",
              "          1.9799e-01,  5.8740e-02, -2.4605e-01,  3.9271e-01,  2.1171e-01,\n",
              "          3.3015e-01, -6.1899e-01,  1.3637e-01,  4.9140e-01,  4.9283e-02,\n",
              "          1.9958e-01, -4.0011e-01, -4.9233e-02, -5.8476e-02, -4.8313e-02,\n",
              "         -7.8183e-02,  1.6350e-01,  1.2374e-01,  2.8297e-01,  6.5844e-03,\n",
              "         -8.8378e-02, -4.3440e-02,  1.8505e-01,  2.4156e-01, -1.1700e-01,\n",
              "         -1.8117e-01,  3.5775e-01, -2.7620e-01,  6.6494e-02,  2.1546e-01,\n",
              "          1.3507e-01, -7.4716e-02, -6.8634e-02,  1.8023e-01,  4.5039e-01,\n",
              "         -1.6993e-01,  2.2511e-02,  5.5750e-02, -1.6257e-01,  1.5772e-01,\n",
              "          8.0147e-02,  2.5803e-01, -5.3341e-02,  1.1280e-01,  3.5427e-02,\n",
              "         -4.1530e-01, -2.5322e-02,  4.3308e-01,  5.5646e-01, -3.3773e-01,\n",
              "          4.6014e-01,  1.1927e-01,  2.8470e-01, -3.2685e-02,  4.3744e-01,\n",
              "         -4.7126e-02,  1.1639e-01, -5.1535e-03,  1.4308e-01,  2.8090e-02,\n",
              "          2.5759e-01,  1.4150e-01, -3.3345e-02, -1.5929e-01,  3.0106e-02,\n",
              "         -1.2154e-01,  1.2744e-01,  2.9684e-01,  1.7307e-01,  4.1989e-01,\n",
              "         -2.9471e-01,  1.2757e-01,  1.2823e-01, -8.7570e-02, -3.4140e-01,\n",
              "          9.5218e-03,  9.4085e-02, -3.6053e-01,  4.2475e-03,  2.2248e-01,\n",
              "         -6.9440e-02, -2.4194e-02,  1.1893e-01,  4.0948e-01,  1.9279e-01,\n",
              "          1.1209e-01,  1.0513e-01, -4.9348e-02,  3.2094e-01, -3.9185e-01,\n",
              "          2.2496e-01,  1.8922e-01, -1.0293e-01, -2.2189e-01, -5.1099e-02,\n",
              "          7.3661e-03,  2.1280e-02, -4.3551e-01, -1.4018e-01, -1.6223e-01,\n",
              "          4.2098e-01, -1.3560e-01,  6.8298e-02, -3.7222e-01,  6.8767e-02,\n",
              "         -3.7135e-02,  2.6836e-01, -3.0010e-01,  8.6409e-02,  5.2547e-02,\n",
              "          1.7338e-02,  8.8370e-02,  1.3058e-01,  6.8434e-02,  2.5285e-01,\n",
              "         -3.5232e-01, -1.2489e-01,  3.5866e-01, -1.9470e-02,  9.1036e-02,\n",
              "         -2.1727e-01,  4.1820e-01,  4.6545e-01, -1.4808e-01, -3.3211e-01,\n",
              "         -2.6352e-01, -1.1157e-02, -2.7983e-01, -5.4209e-01,  2.9864e-01,\n",
              "          1.3787e-01,  1.9744e-01, -1.0587e-01,  3.6607e-02, -7.4326e-02,\n",
              "         -1.5276e-01,  6.1172e-02, -1.9879e-01]], device='cuda:0',\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121,
          "referenced_widgets": [
            "547f511233094f7fa6966e447f973226",
            "e366f7e012044aa0908403d3b63f5013",
            "ce6be0f780954dc1a19b195fc96212c5",
            "019f1e2ca3ed42dbb5232b00a14367eb",
            "117fb75a71cb4bcfb87a450d6b623d13",
            "29c2c23adff049f3a8097c7e4d040c87",
            "c4e506d0ce044207b1147fdd6d0c647a",
            "3f8c9c6dbad8442cb651ac40a4e0bbaa",
            "4b7c9f11871a448c8868987a3b9d3f8c",
            "3b17fd629ef641b7983566d63f27cbd2",
            "faf57a1cc51645828914d730fc6933c8"
          ]
        },
        "id": "3xZHBAahoMVw",
        "outputId": "910074fa-a764-4052-8e54-fafc86a8160a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/2175 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "547f511233094f7fa6966e447f973226"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['html_url', 'title', 'comments', 'body', 'comment_length', 'text', 'embeddings'],\n",
              "    num_rows: 2175\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "# Get embeddings for the entire dataset\n",
        "\n",
        "# The embeddings have been converted to NumPy arrays — that’s because 🤗 Datasets\n",
        "# requires this format inorder to index them with FAISS.\n",
        "embeddings_dataset = comments_dataset.map(\n",
        "    lambda x: {\"embeddings\": get_embeddings(x.get(\"text\")).detach().cpu().numpy()[0]}\n",
        ")\n",
        "\n",
        "embeddings_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### FAISS For Efficient Similarity Search\n",
        "\n",
        "```text\n",
        "FAISS: Facebook AI Similarity Search.\n",
        "\n",
        "- Dataset of embeddings needs a search mechanism.\n",
        "- 🤗 Datasets offers a FAISS index for this purpose.\n",
        "- FAISS is a library providing efficient algorithms for searching and clustering embedding vectors.\n",
        "- FAISS creates an index data structure to find similar embeddings.\n",
        "- Creating a FAISS index in 🤗 Datasets is straightforward using Dataset.add_faiss_index() function.\n",
        "- Specify the desired column to index in the dataset.\n",
        "```"
      ],
      "metadata": {
        "id": "jD1jomw1oMVw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-gpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OohKYSXo0EmN",
        "outputId": "5d5f6b39-70b0-49e8-9c63-3dc30ff50b93"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faiss-gpu in /usr/local/lib/python3.10/dist-packages (1.7.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings_dataset.add_faiss_index(column=\"embeddings\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121,
          "referenced_widgets": [
            "564da74a96fd4f15a10a1bbcb1f52c41",
            "ed3b8a7624634799882a9fd039de55b7",
            "47f28503912c481db809f3365e7cf9d9",
            "d1f9a369f78847c4a44ce7a7a70f5e42",
            "02935cf35342426a9f7280dddc4900a9",
            "b218ed5d69a742bfaf099ec15706967c",
            "7b8ad99f581943fdb27c75f170f0a706",
            "58741deb948d426cab0a471e13c6da5e",
            "85849c7d679a4aefa688957823a0a5ec",
            "20bb6758cbe44918806f3b4b689c2a90",
            "527c514ebb9a4735a12fdaa73ea69118"
          ]
        },
        "id": "c7OqWih3z4AU",
        "outputId": "35895e92-af72-4bd7-c5bd-abb6ff1c2ec9"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "564da74a96fd4f15a10a1bbcb1f52c41"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['html_url', 'title', 'comments', 'body', 'comment_length', 'text', 'embeddings'],\n",
              "    num_rows: 2175\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnz5Xr9ioMVw",
        "outputId": "15cdc3a7-7ac5-4085-8312-7cff609822d3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 768)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "# - Queries can be performed on the FAISS index using Dataset.get_nearest_examples() function.\n",
        "# - Nearest neighbor lookup enables finding similar examples based on embeddings.\n",
        "# - Embedding a question is the first step to test the functionality.\n",
        "\n",
        "question: str = \"How can I load a dataset offline?\"\n",
        "question_embedding: np.ndarray = get_embeddings([question]).cpu().detach().numpy()\n",
        "question_embedding.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scores, samples = embeddings_dataset.get_nearest_examples(\n",
        "    \"embeddings\", question_embedding, k=5\n",
        ")\n",
        "\n",
        "scores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XT42eNiHzd51",
        "outputId": "198b465d-f2f6-44f3-905c-200558eed76a"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([22.40664 , 22.893995, 24.148972, 24.555553, 25.505049],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "samples_df: pd.DataFrame = pd.DataFrame.from_dict(samples)\n",
        "samples_df[\"scores\"] = scores\n",
        "samples_df.sort_values(\"scores\", ascending=False, inplace=True)"
      ],
      "metadata": {
        "id": "x7DxnxVc1-qK"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Shape of data: {samples_df.shape}\\n\")\n",
        "samples_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SRHIaXUb1-Pe",
        "outputId": "d08086fb-95fb-4ddf-a94b-ebe4bc2c265d"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Shpae of data: \u001b[1m(\u001b[0m\u001b[1;36m5\u001b[0m, \u001b[1;36m8\u001b[0m\u001b[1m)\u001b[0m\n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Shpae of data: <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span><span style=\"font-weight: bold\">)</span>\n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             html_url  \\\n",
              "4  https://github.com/huggingface/datasets/issues/824   \n",
              "3  https://github.com/huggingface/datasets/issues/824   \n",
              "2  https://github.com/huggingface/datasets/issues/824   \n",
              "1  https://github.com/huggingface/datasets/issues/824   \n",
              "0  https://github.com/huggingface/datasets/issues/824   \n",
              "\n",
              "                                       title  \\\n",
              "4  Discussion using datasets in offline mode   \n",
              "3  Discussion using datasets in offline mode   \n",
              "2  Discussion using datasets in offline mode   \n",
              "1  Discussion using datasets in offline mode   \n",
              "0  Discussion using datasets in offline mode   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  comments  \\\n",
              "4                                                                                                                                                                                                                                     Requiring online connection is a deal breaker in some cases unfortunately so it'd be great if offline mode is added similar to how `transformers` loads models offline fine.\\r\\n\\r\\n@mandubian's second bullet point suggests that there's a workaround allowing you to use your offline (custom?) dataset with `datasets`. Could you please elaborate on how that should look like?   \n",
              "3                                                                                                                                                                                                                                                                                                                                                          The local dataset builders (csv, text , json and pandas) are now part of the `datasets` package since #1726 :)\\r\\nYou can now use them offline\\r\\n```python\\r\\ndatasets = load_dataset('text', data_files=data_files)\\r\\n```\\r\\n\\r\\nWe'll do a new release soon   \n",
              "2  I opened a PR that allows to reload modules that have already been loaded once even if there's no internet.\\r\\n\\r\\nLet me know if you know other ways that can make the offline mode experience better. I'd be happy to add them :) \\r\\n\\r\\nI already note the \"freeze\" modules option, to prevent local modules updates. It would be a cool feature.\\r\\n\\r\\n----------\\r\\n\\r\\n> @mandubian's second bullet point suggests that there's a workaround allowing you to use your offline (custom?) dataset with `datasets`. Could you please elaborate on how that should look like?\\r\\n\\r\\nIndeed `load_dataset` allow...   \n",
              "1                                                                                                                                                         > here is my way to load a dataset offline, but it **requires** an online machine\\n> \\n> 1. (online machine)\\n> \\n> ```\\n> \\n> import datasets\\n> \\n> data = datasets.load_dataset(...)\\n> \\n> data.save_to_disk(/YOUR/DATASET/DIR)\\n> \\n> ```\\n> \\n> 2. copy the dir from online to the offline machine\\n> \\n> 3. (offline machine)\\n> \\n> ```\\n> \\n> import datasets\\n> \\n> data = datasets.load_from_disk(/SAVED/DATA/DIR)\\n> \\n> ```\\n> \\n> \\n> \\n> HTH.\\n\\n   \n",
              "0                                                                                                                                                                                                                       here is my way to load a dataset offline, but it **requires** an online machine\\r\\n1. (online machine)\\r\\n```\\r\\nimport datasets\\r\\ndata = datasets.load_dataset(...)\\r\\ndata.save_to_disk(/YOUR/DATASET/DIR)\\r\\n```\\r\\n2. copy the dir from online to the offline machine\\r\\n3. (offline machine)\\r\\n```\\r\\nimport datasets\\r\\ndata = datasets.load_from_disk(/SAVED/DATA/DIR)\\r\\n```\\r\\n\\r\\nHTH.   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      body  \\\n",
              "4  `datasets.load_dataset(\"csv\", ...)` breaks if you have no connection (There is already this issue https://github.com/huggingface/datasets/issues/761 about it). It seems to be the same for metrics too.\\r\\n\\r\\nI create this ticket to discuss a bit and gather what you have in mind or other propositions.\\r\\n\\r\\nHere are some points to open discussion:\\r\\n- if you want to prepare your code/datasets on your machine (having internet connexion) but run it on another offline machine (not having internet connexion), it won't work as is, even if you have all files locally on this machine.\\r\\n- AFAIK,...   \n",
              "3  `datasets.load_dataset(\"csv\", ...)` breaks if you have no connection (There is already this issue https://github.com/huggingface/datasets/issues/761 about it). It seems to be the same for metrics too.\\r\\n\\r\\nI create this ticket to discuss a bit and gather what you have in mind or other propositions.\\r\\n\\r\\nHere are some points to open discussion:\\r\\n- if you want to prepare your code/datasets on your machine (having internet connexion) but run it on another offline machine (not having internet connexion), it won't work as is, even if you have all files locally on this machine.\\r\\n- AFAIK,...   \n",
              "2  `datasets.load_dataset(\"csv\", ...)` breaks if you have no connection (There is already this issue https://github.com/huggingface/datasets/issues/761 about it). It seems to be the same for metrics too.\\r\\n\\r\\nI create this ticket to discuss a bit and gather what you have in mind or other propositions.\\r\\n\\r\\nHere are some points to open discussion:\\r\\n- if you want to prepare your code/datasets on your machine (having internet connexion) but run it on another offline machine (not having internet connexion), it won't work as is, even if you have all files locally on this machine.\\r\\n- AFAIK,...   \n",
              "1  `datasets.load_dataset(\"csv\", ...)` breaks if you have no connection (There is already this issue https://github.com/huggingface/datasets/issues/761 about it). It seems to be the same for metrics too.\\r\\n\\r\\nI create this ticket to discuss a bit and gather what you have in mind or other propositions.\\r\\n\\r\\nHere are some points to open discussion:\\r\\n- if you want to prepare your code/datasets on your machine (having internet connexion) but run it on another offline machine (not having internet connexion), it won't work as is, even if you have all files locally on this machine.\\r\\n- AFAIK,...   \n",
              "0  `datasets.load_dataset(\"csv\", ...)` breaks if you have no connection (There is already this issue https://github.com/huggingface/datasets/issues/761 about it). It seems to be the same for metrics too.\\r\\n\\r\\nI create this ticket to discuss a bit and gather what you have in mind or other propositions.\\r\\n\\r\\nHere are some points to open discussion:\\r\\n- if you want to prepare your code/datasets on your machine (having internet connexion) but run it on another offline machine (not having internet connexion), it won't work as is, even if you have all files locally on this machine.\\r\\n- AFAIK,...   \n",
              "\n",
              "   comment_length  \\\n",
              "4              57   \n",
              "3              38   \n",
              "2             179   \n",
              "1              76   \n",
              "0              47   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      text  \\\n",
              "4  Discussion using datasets in offline mode \\n `datasets.load_dataset(\"csv\", ...)` breaks if you have no connection (There is already this issue https://github.com/huggingface/datasets/issues/761 about it). It seems to be the same for metrics too.\\r\\n\\r\\nI create this ticket to discuss a bit and gather what you have in mind or other propositions.\\r\\n\\r\\nHere are some points to open discussion:\\r\\n- if you want to prepare your code/datasets on your machine (having internet connexion) but run it on another offline machine (not having internet connexion), it won't work as is, even if you have a...   \n",
              "3  Discussion using datasets in offline mode \\n `datasets.load_dataset(\"csv\", ...)` breaks if you have no connection (There is already this issue https://github.com/huggingface/datasets/issues/761 about it). It seems to be the same for metrics too.\\r\\n\\r\\nI create this ticket to discuss a bit and gather what you have in mind or other propositions.\\r\\n\\r\\nHere are some points to open discussion:\\r\\n- if you want to prepare your code/datasets on your machine (having internet connexion) but run it on another offline machine (not having internet connexion), it won't work as is, even if you have a...   \n",
              "2  Discussion using datasets in offline mode \\n `datasets.load_dataset(\"csv\", ...)` breaks if you have no connection (There is already this issue https://github.com/huggingface/datasets/issues/761 about it). It seems to be the same for metrics too.\\r\\n\\r\\nI create this ticket to discuss a bit and gather what you have in mind or other propositions.\\r\\n\\r\\nHere are some points to open discussion:\\r\\n- if you want to prepare your code/datasets on your machine (having internet connexion) but run it on another offline machine (not having internet connexion), it won't work as is, even if you have a...   \n",
              "1  Discussion using datasets in offline mode \\n `datasets.load_dataset(\"csv\", ...)` breaks if you have no connection (There is already this issue https://github.com/huggingface/datasets/issues/761 about it). It seems to be the same for metrics too.\\r\\n\\r\\nI create this ticket to discuss a bit and gather what you have in mind or other propositions.\\r\\n\\r\\nHere are some points to open discussion:\\r\\n- if you want to prepare your code/datasets on your machine (having internet connexion) but run it on another offline machine (not having internet connexion), it won't work as is, even if you have a...   \n",
              "0  Discussion using datasets in offline mode \\n `datasets.load_dataset(\"csv\", ...)` breaks if you have no connection (There is already this issue https://github.com/huggingface/datasets/issues/761 about it). It seems to be the same for metrics too.\\r\\n\\r\\nI create this ticket to discuss a bit and gather what you have in mind or other propositions.\\r\\n\\r\\nHere are some points to open discussion:\\r\\n- if you want to prepare your code/datasets on your machine (having internet connexion) but run it on another offline machine (not having internet connexion), it won't work as is, even if you have a...   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                embeddings  \\\n",
              "4  [-0.47318071126937866, 0.24578367173671722, -0.012630763463675976, 0.14121408760547638, 0.28335559368133545, -0.1470210701227188, 0.6012278199195862, 0.012776073068380356, 0.2687809467315674, 0.13127808272838593, -0.02403687685728073, -0.023985162377357483, -0.024662325158715248, 0.40210670232772827, -0.09124023467302322, -0.10213788598775864, -0.1722334325313568, 0.038877222687006, -0.1540050506591797, 0.13259492814540863, -0.14081068336963654, -0.12811940908432007, -0.3727093040943146, -0.028236284852027893, -0.16281437873840332, -0.1865873783826828, 0.009142007678747177, 0.1420215219259...   \n",
              "3  [-0.44908538460731506, 0.20950642228126526, -0.05981910973787308, 0.12935049831867218, 0.26219233870506287, -0.13128556311130524, 0.5469644069671631, 0.0949263647198677, 0.3154381513595581, 0.22943100333213806, 0.05104105919599533, -0.0031593164894729853, 0.047941938042640686, 0.40367624163627625, -0.1032216027379036, -0.11917857080698013, -0.10968127101659775, 0.08062367141246796, -0.17092695832252502, 0.09250368177890778, -0.18166573345661163, -0.0897308811545372, -0.37507691979408264, -0.025563210248947144, -0.10498439520597458, -0.17466020584106445, -0.10713899880647659, 0.156695634126...   \n",
              "2  [-0.47164806723594666, 0.2902272641658783, -0.04767205938696861, 0.13344231247901917, 0.2106887549161911, -0.21222515404224396, 0.5858257412910461, 0.05341650918126106, 0.28334110975265503, 0.18411293625831604, 0.03105962462723255, 0.032638560980558395, 0.0011812117882072926, 0.3333257734775543, -0.07214410603046417, -0.05854179710149765, -0.19054260849952698, -0.005105196963995695, -0.16585849225521088, 0.08279171586036682, -0.08703479170799255, -0.17246849834918976, -0.4247152805328369, -0.11953136324882507, -0.06756021082401276, -0.16881543397903442, -0.024211646988987923, 0.21167261898...   \n",
              "1  [-0.499260276556015, 0.22699761390686035, -0.0324692539870739, 0.14187218248844147, 0.23695068061351776, -0.10291724652051926, 0.5442944169044495, 0.07441112399101257, 0.2753629684448242, 0.24428822100162506, -0.008833910338580608, -0.06653957813978195, 0.02808590605854988, 0.3756229281425476, -0.0988999605178833, -0.04195521026849747, -0.1623331606388092, 0.056354790925979614, -0.18303634226322174, 0.1076541319489479, -0.16337788105010986, -0.08695139735937119, -0.433300256729126, -0.06250766664743423, -0.015938540920615196, -0.1606818586587906, -0.07989782840013504, 0.21157224476337433, ...   \n",
              "0  [-0.49025776982307434, 0.2288963347673416, -0.03322095796465874, 0.13887320458889008, 0.23637260496616364, -0.08911527693271637, 0.5481941103935242, 0.06737261265516281, 0.2955958843231201, 0.24505114555358887, -0.01017451286315918, -0.06949253380298615, 0.027625715360045433, 0.3827284574508667, -0.10571841895580292, -0.021846573799848557, -0.15303955972194672, 0.0536612905561924, -0.17722827196121216, 0.08962270617485046, -0.16522905230522156, -0.09213365614414215, -0.43372341990470886, -0.07040619850158691, -0.018133236095309258, -0.1504259705543518, -0.08321218192577362, 0.2194387912750...   \n",
              "\n",
              "      scores  \n",
              "4  25.505049  \n",
              "3  24.555553  \n",
              "2  24.148972  \n",
              "1  22.893995  \n",
              "0  22.406639  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-672100b9-27a8-4483-b26f-7d98d0c1a659\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>html_url</th>\n",
              "      <th>title</th>\n",
              "      <th>comments</th>\n",
              "      <th>body</th>\n",
              "      <th>comment_length</th>\n",
              "      <th>text</th>\n",
              "      <th>embeddings</th>\n",
              "      <th>scores</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>https://github.com/huggingface/datasets/issues/824</td>\n",
              "      <td>Discussion using datasets in offline mode</td>\n",
              "      <td>Requiring online connection is a deal breaker in some cases unfortunately so it'd be great if offline mode is added similar to how `transformers` loads models offline fine.\\r\\n\\r\\n@mandubian's second bullet point suggests that there's a workaround allowing you to use your offline (custom?) dataset with `datasets`. Could you please elaborate on how that should look like?</td>\n",
              "      <td>`datasets.load_dataset(\"csv\", ...)` breaks if you have no connection (There is already this issue https://github.com/huggingface/datasets/issues/761 about it). It seems to be the same for metrics too.\\r\\n\\r\\nI create this ticket to discuss a bit and gather what you have in mind or other propositions.\\r\\n\\r\\nHere are some points to open discussion:\\r\\n- if you want to prepare your code/datasets on your machine (having internet connexion) but run it on another offline machine (not having internet connexion), it won't work as is, even if you have all files locally on this machine.\\r\\n- AFAIK,...</td>\n",
              "      <td>57</td>\n",
              "      <td>Discussion using datasets in offline mode \\n `datasets.load_dataset(\"csv\", ...)` breaks if you have no connection (There is already this issue https://github.com/huggingface/datasets/issues/761 about it). It seems to be the same for metrics too.\\r\\n\\r\\nI create this ticket to discuss a bit and gather what you have in mind or other propositions.\\r\\n\\r\\nHere are some points to open discussion:\\r\\n- if you want to prepare your code/datasets on your machine (having internet connexion) but run it on another offline machine (not having internet connexion), it won't work as is, even if you have a...</td>\n",
              "      <td>[-0.47318071126937866, 0.24578367173671722, -0.012630763463675976, 0.14121408760547638, 0.28335559368133545, -0.1470210701227188, 0.6012278199195862, 0.012776073068380356, 0.2687809467315674, 0.13127808272838593, -0.02403687685728073, -0.023985162377357483, -0.024662325158715248, 0.40210670232772827, -0.09124023467302322, -0.10213788598775864, -0.1722334325313568, 0.038877222687006, -0.1540050506591797, 0.13259492814540863, -0.14081068336963654, -0.12811940908432007, -0.3727093040943146, -0.028236284852027893, -0.16281437873840332, -0.1865873783826828, 0.009142007678747177, 0.1420215219259...</td>\n",
              "      <td>25.505049</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>https://github.com/huggingface/datasets/issues/824</td>\n",
              "      <td>Discussion using datasets in offline mode</td>\n",
              "      <td>The local dataset builders (csv, text , json and pandas) are now part of the `datasets` package since #1726 :)\\r\\nYou can now use them offline\\r\\n```python\\r\\ndatasets = load_dataset('text', data_files=data_files)\\r\\n```\\r\\n\\r\\nWe'll do a new release soon</td>\n",
              "      <td>`datasets.load_dataset(\"csv\", ...)` breaks if you have no connection (There is already this issue https://github.com/huggingface/datasets/issues/761 about it). It seems to be the same for metrics too.\\r\\n\\r\\nI create this ticket to discuss a bit and gather what you have in mind or other propositions.\\r\\n\\r\\nHere are some points to open discussion:\\r\\n- if you want to prepare your code/datasets on your machine (having internet connexion) but run it on another offline machine (not having internet connexion), it won't work as is, even if you have all files locally on this machine.\\r\\n- AFAIK,...</td>\n",
              "      <td>38</td>\n",
              "      <td>Discussion using datasets in offline mode \\n `datasets.load_dataset(\"csv\", ...)` breaks if you have no connection (There is already this issue https://github.com/huggingface/datasets/issues/761 about it). It seems to be the same for metrics too.\\r\\n\\r\\nI create this ticket to discuss a bit and gather what you have in mind or other propositions.\\r\\n\\r\\nHere are some points to open discussion:\\r\\n- if you want to prepare your code/datasets on your machine (having internet connexion) but run it on another offline machine (not having internet connexion), it won't work as is, even if you have a...</td>\n",
              "      <td>[-0.44908538460731506, 0.20950642228126526, -0.05981910973787308, 0.12935049831867218, 0.26219233870506287, -0.13128556311130524, 0.5469644069671631, 0.0949263647198677, 0.3154381513595581, 0.22943100333213806, 0.05104105919599533, -0.0031593164894729853, 0.047941938042640686, 0.40367624163627625, -0.1032216027379036, -0.11917857080698013, -0.10968127101659775, 0.08062367141246796, -0.17092695832252502, 0.09250368177890778, -0.18166573345661163, -0.0897308811545372, -0.37507691979408264, -0.025563210248947144, -0.10498439520597458, -0.17466020584106445, -0.10713899880647659, 0.156695634126...</td>\n",
              "      <td>24.555553</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>https://github.com/huggingface/datasets/issues/824</td>\n",
              "      <td>Discussion using datasets in offline mode</td>\n",
              "      <td>I opened a PR that allows to reload modules that have already been loaded once even if there's no internet.\\r\\n\\r\\nLet me know if you know other ways that can make the offline mode experience better. I'd be happy to add them :) \\r\\n\\r\\nI already note the \"freeze\" modules option, to prevent local modules updates. It would be a cool feature.\\r\\n\\r\\n----------\\r\\n\\r\\n&gt; @mandubian's second bullet point suggests that there's a workaround allowing you to use your offline (custom?) dataset with `datasets`. Could you please elaborate on how that should look like?\\r\\n\\r\\nIndeed `load_dataset` allow...</td>\n",
              "      <td>`datasets.load_dataset(\"csv\", ...)` breaks if you have no connection (There is already this issue https://github.com/huggingface/datasets/issues/761 about it). It seems to be the same for metrics too.\\r\\n\\r\\nI create this ticket to discuss a bit and gather what you have in mind or other propositions.\\r\\n\\r\\nHere are some points to open discussion:\\r\\n- if you want to prepare your code/datasets on your machine (having internet connexion) but run it on another offline machine (not having internet connexion), it won't work as is, even if you have all files locally on this machine.\\r\\n- AFAIK,...</td>\n",
              "      <td>179</td>\n",
              "      <td>Discussion using datasets in offline mode \\n `datasets.load_dataset(\"csv\", ...)` breaks if you have no connection (There is already this issue https://github.com/huggingface/datasets/issues/761 about it). It seems to be the same for metrics too.\\r\\n\\r\\nI create this ticket to discuss a bit and gather what you have in mind or other propositions.\\r\\n\\r\\nHere are some points to open discussion:\\r\\n- if you want to prepare your code/datasets on your machine (having internet connexion) but run it on another offline machine (not having internet connexion), it won't work as is, even if you have a...</td>\n",
              "      <td>[-0.47164806723594666, 0.2902272641658783, -0.04767205938696861, 0.13344231247901917, 0.2106887549161911, -0.21222515404224396, 0.5858257412910461, 0.05341650918126106, 0.28334110975265503, 0.18411293625831604, 0.03105962462723255, 0.032638560980558395, 0.0011812117882072926, 0.3333257734775543, -0.07214410603046417, -0.05854179710149765, -0.19054260849952698, -0.005105196963995695, -0.16585849225521088, 0.08279171586036682, -0.08703479170799255, -0.17246849834918976, -0.4247152805328369, -0.11953136324882507, -0.06756021082401276, -0.16881543397903442, -0.024211646988987923, 0.21167261898...</td>\n",
              "      <td>24.148972</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>https://github.com/huggingface/datasets/issues/824</td>\n",
              "      <td>Discussion using datasets in offline mode</td>\n",
              "      <td>&gt; here is my way to load a dataset offline, but it **requires** an online machine\\n&gt; \\n&gt; 1. (online machine)\\n&gt; \\n&gt; ```\\n&gt; \\n&gt; import datasets\\n&gt; \\n&gt; data = datasets.load_dataset(...)\\n&gt; \\n&gt; data.save_to_disk(/YOUR/DATASET/DIR)\\n&gt; \\n&gt; ```\\n&gt; \\n&gt; 2. copy the dir from online to the offline machine\\n&gt; \\n&gt; 3. (offline machine)\\n&gt; \\n&gt; ```\\n&gt; \\n&gt; import datasets\\n&gt; \\n&gt; data = datasets.load_from_disk(/SAVED/DATA/DIR)\\n&gt; \\n&gt; ```\\n&gt; \\n&gt; \\n&gt; \\n&gt; HTH.\\n\\n</td>\n",
              "      <td>`datasets.load_dataset(\"csv\", ...)` breaks if you have no connection (There is already this issue https://github.com/huggingface/datasets/issues/761 about it). It seems to be the same for metrics too.\\r\\n\\r\\nI create this ticket to discuss a bit and gather what you have in mind or other propositions.\\r\\n\\r\\nHere are some points to open discussion:\\r\\n- if you want to prepare your code/datasets on your machine (having internet connexion) but run it on another offline machine (not having internet connexion), it won't work as is, even if you have all files locally on this machine.\\r\\n- AFAIK,...</td>\n",
              "      <td>76</td>\n",
              "      <td>Discussion using datasets in offline mode \\n `datasets.load_dataset(\"csv\", ...)` breaks if you have no connection (There is already this issue https://github.com/huggingface/datasets/issues/761 about it). It seems to be the same for metrics too.\\r\\n\\r\\nI create this ticket to discuss a bit and gather what you have in mind or other propositions.\\r\\n\\r\\nHere are some points to open discussion:\\r\\n- if you want to prepare your code/datasets on your machine (having internet connexion) but run it on another offline machine (not having internet connexion), it won't work as is, even if you have a...</td>\n",
              "      <td>[-0.499260276556015, 0.22699761390686035, -0.0324692539870739, 0.14187218248844147, 0.23695068061351776, -0.10291724652051926, 0.5442944169044495, 0.07441112399101257, 0.2753629684448242, 0.24428822100162506, -0.008833910338580608, -0.06653957813978195, 0.02808590605854988, 0.3756229281425476, -0.0988999605178833, -0.04195521026849747, -0.1623331606388092, 0.056354790925979614, -0.18303634226322174, 0.1076541319489479, -0.16337788105010986, -0.08695139735937119, -0.433300256729126, -0.06250766664743423, -0.015938540920615196, -0.1606818586587906, -0.07989782840013504, 0.21157224476337433, ...</td>\n",
              "      <td>22.893995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>https://github.com/huggingface/datasets/issues/824</td>\n",
              "      <td>Discussion using datasets in offline mode</td>\n",
              "      <td>here is my way to load a dataset offline, but it **requires** an online machine\\r\\n1. (online machine)\\r\\n```\\r\\nimport datasets\\r\\ndata = datasets.load_dataset(...)\\r\\ndata.save_to_disk(/YOUR/DATASET/DIR)\\r\\n```\\r\\n2. copy the dir from online to the offline machine\\r\\n3. (offline machine)\\r\\n```\\r\\nimport datasets\\r\\ndata = datasets.load_from_disk(/SAVED/DATA/DIR)\\r\\n```\\r\\n\\r\\nHTH.</td>\n",
              "      <td>`datasets.load_dataset(\"csv\", ...)` breaks if you have no connection (There is already this issue https://github.com/huggingface/datasets/issues/761 about it). It seems to be the same for metrics too.\\r\\n\\r\\nI create this ticket to discuss a bit and gather what you have in mind or other propositions.\\r\\n\\r\\nHere are some points to open discussion:\\r\\n- if you want to prepare your code/datasets on your machine (having internet connexion) but run it on another offline machine (not having internet connexion), it won't work as is, even if you have all files locally on this machine.\\r\\n- AFAIK,...</td>\n",
              "      <td>47</td>\n",
              "      <td>Discussion using datasets in offline mode \\n `datasets.load_dataset(\"csv\", ...)` breaks if you have no connection (There is already this issue https://github.com/huggingface/datasets/issues/761 about it). It seems to be the same for metrics too.\\r\\n\\r\\nI create this ticket to discuss a bit and gather what you have in mind or other propositions.\\r\\n\\r\\nHere are some points to open discussion:\\r\\n- if you want to prepare your code/datasets on your machine (having internet connexion) but run it on another offline machine (not having internet connexion), it won't work as is, even if you have a...</td>\n",
              "      <td>[-0.49025776982307434, 0.2288963347673416, -0.03322095796465874, 0.13887320458889008, 0.23637260496616364, -0.08911527693271637, 0.5481941103935242, 0.06737261265516281, 0.2955958843231201, 0.24505114555358887, -0.01017451286315918, -0.06949253380298615, 0.027625715360045433, 0.3827284574508667, -0.10571841895580292, -0.021846573799848557, -0.15303955972194672, 0.0536612905561924, -0.17722827196121216, 0.08962270617485046, -0.16522905230522156, -0.09213365614414215, -0.43372341990470886, -0.07040619850158691, -0.018133236095309258, -0.1504259705543518, -0.08321218192577362, 0.2194387912750...</td>\n",
              "      <td>22.406639</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-672100b9-27a8-4483-b26f-7d98d0c1a659')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-672100b9-27a8-4483-b26f-7d98d0c1a659 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-672100b9-27a8-4483-b26f-7d98d0c1a659');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-48482f0a-6344-49ce-bc3a-d2b83eb57cb9\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-48482f0a-6344-49ce-bc3a-d2b83eb57cb9')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-48482f0a-6344-49ce-bc3a-d2b83eb57cb9 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for _, row in samples_df.iterrows():\n",
        "    print(f\"COMMENT: {row.comments}\")\n",
        "    print(f\"SCORE: {row.scores}\")\n",
        "    print(f\"TITLE: {row.title}\")\n",
        "    print(f\"URL: {row.html_url}\")\n",
        "    print(\"=\" * 50)\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3aP15gK_2ya6",
        "outputId": "a64ef6a2-6af2-48c5-a0c0-5e73bc8012ba"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "COMMENT: Requiring online connection is a deal breaker in some cases unfortunately so it'd be great if offline mode\n",
              "is added similar to how `transformers` loads models offline fine.\n",
              "\n",
              "@mandubian's second bullet point suggests that there's a workaround allowing you to use your offline \u001b[1m(\u001b[0mcustom?\u001b[1m)\u001b[0m \n",
              "dataset with `datasets`. Could you please elaborate on how that should look like?\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">COMMENT: Requiring online connection is a deal breaker in some cases unfortunately so it'd be great if offline mode\n",
              "is added similar to how `transformers` loads models offline fine.\n",
              "\n",
              "@mandubian's second bullet point suggests that there's a workaround allowing you to use your offline <span style=\"font-weight: bold\">(</span>custom?<span style=\"font-weight: bold\">)</span> \n",
              "dataset with `datasets`. Could you please elaborate on how that should look like?\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "SCORE: \u001b[1;36m25.505048751831055\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">SCORE: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">25.505048751831055</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "TITLE: Discussion using datasets in offline mode\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">TITLE: Discussion using datasets in offline mode\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "URL: \u001b[4;94mhttps://github.com/huggingface/datasets/issues/824\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">URL: <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/huggingface/datasets/issues/824</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "==================================================\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">==================================================\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "COMMENT: The local dataset builders \u001b[1m(\u001b[0mcsv, text , json and pandas\u001b[1m)\u001b[0m are now part of the `datasets` package since \n",
              "#\u001b[1;36m1726\u001b[0m :\u001b[1m)\u001b[0m\n",
              "You can now use them offline\n",
              "```python\n",
              "datasets = \u001b[1;35mload_dataset\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'text'\u001b[0m, \u001b[33mdata_files\u001b[0m=\u001b[35mdata_files\u001b[0m\u001b[1m)\u001b[0m\n",
              "```\n",
              "\n",
              "We'll do a new release soon\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">COMMENT: The local dataset builders <span style=\"font-weight: bold\">(</span>csv, text , json and pandas<span style=\"font-weight: bold\">)</span> are now part of the `datasets` package since \n",
              "#<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1726</span> :<span style=\"font-weight: bold\">)</span>\n",
              "You can now use them offline\n",
              "```python\n",
              "datasets = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">load_dataset</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'text'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">data_files</span>=<span style=\"color: #800080; text-decoration-color: #800080\">data_files</span><span style=\"font-weight: bold\">)</span>\n",
              "```\n",
              "\n",
              "We'll do a new release soon\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "SCORE: \u001b[1;36m24.555553436279297\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">SCORE: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">24.555553436279297</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "TITLE: Discussion using datasets in offline mode\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">TITLE: Discussion using datasets in offline mode\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "URL: \u001b[4;94mhttps://github.com/huggingface/datasets/issues/824\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">URL: <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/huggingface/datasets/issues/824</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "==================================================\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">==================================================\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "COMMENT: I opened a PR that allows to reload modules that have already been loaded once even if there's no \n",
              "internet.\n",
              "\n",
              "Let me know if you know other ways that can make the offline mode experience better. I'd be happy to add them :\u001b[1m)\u001b[0m \n",
              "\n",
              "I already note the \u001b[32m\"freeze\"\u001b[0m modules option, to prevent local modules updates. It would be a cool feature.\n",
              "\n",
              "----------\n",
              "\n",
              "> @mandubian's second bullet point suggests that there's a workaround allowing you to use your offline \u001b[1m(\u001b[0mcustom?\u001b[1m)\u001b[0m \n",
              "dataset with `datasets`. Could you please elaborate on how that should look like?\n",
              "\n",
              "Indeed `load_dataset` allows to load remote dataset script \u001b[1m(\u001b[0msquad, glue, etc.\u001b[1m)\u001b[0m but also you own local ones.\n",
              "For example if you have a dataset script at `.\u001b[35m/my_dataset/\u001b[0m\u001b[95mmy_dataset.py\u001b[0m` then you can do\n",
              "```python\n",
              "\u001b[1;35mload_dataset\u001b[0m\u001b[1m(\u001b[0m\u001b[32m\"./my_dataset\"\u001b[0m\u001b[1m)\u001b[0m\n",
              "```\n",
              "and the dataset script will generate your dataset once and for all.\n",
              "\n",
              "----------\n",
              "\n",
              "About I'm looking into having `csv`, `json`, `text`, `pandas` dataset builders already included in the `datasets` \n",
              "package, so that they are available offline by default, as opposed to the other datasets that require the script to\n",
              "be downloaded.\n",
              "cf #\u001b[1;36m1724\u001b[0m \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">COMMENT: I opened a PR that allows to reload modules that have already been loaded once even if there's no \n",
              "internet.\n",
              "\n",
              "Let me know if you know other ways that can make the offline mode experience better. I'd be happy to add them :<span style=\"font-weight: bold\">)</span> \n",
              "\n",
              "I already note the <span style=\"color: #008000; text-decoration-color: #008000\">\"freeze\"</span> modules option, to prevent local modules updates. It would be a cool feature.\n",
              "\n",
              "----------\n",
              "\n",
              "&gt; @mandubian's second bullet point suggests that there's a workaround allowing you to use your offline <span style=\"font-weight: bold\">(</span>custom?<span style=\"font-weight: bold\">)</span> \n",
              "dataset with `datasets`. Could you please elaborate on how that should look like?\n",
              "\n",
              "Indeed `load_dataset` allows to load remote dataset script <span style=\"font-weight: bold\">(</span>squad, glue, etc.<span style=\"font-weight: bold\">)</span> but also you own local ones.\n",
              "For example if you have a dataset script at `.<span style=\"color: #800080; text-decoration-color: #800080\">/my_dataset/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">my_dataset.py</span>` then you can do\n",
              "```python\n",
              "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">load_dataset</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"./my_dataset\"</span><span style=\"font-weight: bold\">)</span>\n",
              "```\n",
              "and the dataset script will generate your dataset once and for all.\n",
              "\n",
              "----------\n",
              "\n",
              "About I'm looking into having `csv`, `json`, `text`, `pandas` dataset builders already included in the `datasets` \n",
              "package, so that they are available offline by default, as opposed to the other datasets that require the script to\n",
              "be downloaded.\n",
              "cf #<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1724</span> \n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "SCORE: \u001b[1;36m24.148971557617188\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">SCORE: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">24.148971557617188</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "TITLE: Discussion using datasets in offline mode\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">TITLE: Discussion using datasets in offline mode\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "URL: \u001b[4;94mhttps://github.com/huggingface/datasets/issues/824\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">URL: <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/huggingface/datasets/issues/824</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "==================================================\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">==================================================\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "COMMENT: > here is my way to load a dataset offline, but it **requires** an online machine\n",
              "> \n",
              "> \u001b[1;36m1\u001b[0m. \u001b[1m(\u001b[0monline machine\u001b[1m)\u001b[0m\n",
              "> \n",
              "> ```\n",
              "> \n",
              "> import datasets\n",
              "> \n",
              "> data = \u001b[1;35mdatasets.load_dataset\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m\n",
              "> \n",
              "> \u001b[1;35mdata.save_to_disk\u001b[0m\u001b[1m(\u001b[0m\u001b[35m/YOUR/DATASET/\u001b[0m\u001b[95mDIR\u001b[0m\u001b[1m)\u001b[0m\n",
              "> \n",
              "> ```\n",
              "> \n",
              "> \u001b[1;36m2\u001b[0m. copy the dir from online to the offline machine\n",
              "> \n",
              "> \u001b[1;36m3\u001b[0m. \u001b[1m(\u001b[0moffline machine\u001b[1m)\u001b[0m\n",
              "> \n",
              "> ```\n",
              "> \n",
              "> import datasets\n",
              "> \n",
              "> data = \u001b[1;35mdatasets.load_from_disk\u001b[0m\u001b[1m(\u001b[0m\u001b[35m/SAVED/DATA/\u001b[0m\u001b[95mDIR\u001b[0m\u001b[1m)\u001b[0m\n",
              "> \n",
              "> ```\n",
              "> \n",
              "> \n",
              "> \n",
              "> HTH.\n",
              "\n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">COMMENT: &gt; here is my way to load a dataset offline, but it **requires** an online machine\n",
              "&gt; \n",
              "&gt; <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. <span style=\"font-weight: bold\">(</span>online machine<span style=\"font-weight: bold\">)</span>\n",
              "&gt; \n",
              "&gt; ```\n",
              "&gt; \n",
              "&gt; import datasets\n",
              "&gt; \n",
              "&gt; data = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">datasets.load_dataset</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>\n",
              "&gt; \n",
              "&gt; <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">data.save_to_disk</span><span style=\"font-weight: bold\">(</span><span style=\"color: #800080; text-decoration-color: #800080\">/YOUR/DATASET/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">DIR</span><span style=\"font-weight: bold\">)</span>\n",
              "&gt; \n",
              "&gt; ```\n",
              "&gt; \n",
              "&gt; <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>. copy the dir from online to the offline machine\n",
              "&gt; \n",
              "&gt; <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>. <span style=\"font-weight: bold\">(</span>offline machine<span style=\"font-weight: bold\">)</span>\n",
              "&gt; \n",
              "&gt; ```\n",
              "&gt; \n",
              "&gt; import datasets\n",
              "&gt; \n",
              "&gt; data = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">datasets.load_from_disk</span><span style=\"font-weight: bold\">(</span><span style=\"color: #800080; text-decoration-color: #800080\">/SAVED/DATA/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">DIR</span><span style=\"font-weight: bold\">)</span>\n",
              "&gt; \n",
              "&gt; ```\n",
              "&gt; \n",
              "&gt; \n",
              "&gt; \n",
              "&gt; HTH.\n",
              "\n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "SCORE: \u001b[1;36m22.89399528503418\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">SCORE: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">22.89399528503418</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "TITLE: Discussion using datasets in offline mode\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">TITLE: Discussion using datasets in offline mode\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "URL: \u001b[4;94mhttps://github.com/huggingface/datasets/issues/824\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">URL: <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/huggingface/datasets/issues/824</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "==================================================\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">==================================================\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "COMMENT: here is my way to load a dataset offline, but it **requires** an online machine\n",
              "\u001b[1;36m1\u001b[0m. \u001b[1m(\u001b[0monline machine\u001b[1m)\u001b[0m\n",
              "```\n",
              "import datasets\n",
              "data = \u001b[1;35mdatasets.load_dataset\u001b[0m\u001b[1m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1m)\u001b[0m\n",
              "\u001b[1;35mdata.save_to_disk\u001b[0m\u001b[1m(\u001b[0m\u001b[35m/YOUR/DATASET/\u001b[0m\u001b[95mDIR\u001b[0m\u001b[1m)\u001b[0m\n",
              "```\n",
              "\u001b[1;36m2\u001b[0m. copy the dir from online to the offline machine\n",
              "\u001b[1;36m3\u001b[0m. \u001b[1m(\u001b[0moffline machine\u001b[1m)\u001b[0m\n",
              "```\n",
              "import datasets\n",
              "data = \u001b[1;35mdatasets.load_from_disk\u001b[0m\u001b[1m(\u001b[0m\u001b[35m/SAVED/DATA/\u001b[0m\u001b[95mDIR\u001b[0m\u001b[1m)\u001b[0m\n",
              "```\n",
              "\n",
              "HTH.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">COMMENT: here is my way to load a dataset offline, but it **requires** an online machine\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. <span style=\"font-weight: bold\">(</span>online machine<span style=\"font-weight: bold\">)</span>\n",
              "```\n",
              "import datasets\n",
              "data = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">datasets.load_dataset</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>\n",
              "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">data.save_to_disk</span><span style=\"font-weight: bold\">(</span><span style=\"color: #800080; text-decoration-color: #800080\">/YOUR/DATASET/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">DIR</span><span style=\"font-weight: bold\">)</span>\n",
              "```\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>. copy the dir from online to the offline machine\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>. <span style=\"font-weight: bold\">(</span>offline machine<span style=\"font-weight: bold\">)</span>\n",
              "```\n",
              "import datasets\n",
              "data = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">datasets.load_from_disk</span><span style=\"font-weight: bold\">(</span><span style=\"color: #800080; text-decoration-color: #800080\">/SAVED/DATA/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">DIR</span><span style=\"font-weight: bold\">)</span>\n",
              "```\n",
              "\n",
              "HTH.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "SCORE: \u001b[1;36m22.406639099121094\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">SCORE: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">22.406639099121094</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "TITLE: Discussion using datasets in offline mode\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">TITLE: Discussion using datasets in offline mode\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "URL: \u001b[4;94mhttps://github.com/huggingface/datasets/issues/824\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">URL: <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/huggingface/datasets/issues/824</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "==================================================\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">==================================================\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_query_matches(*, query: str, K: int) -> pd.DataFrame:\n",
        "    \"\"\"This is used to to display the top K matches to a query.\"\"\"\n",
        "    query_embedding: np.ndarray = get_embeddings([query]).cpu().detach().numpy()\n",
        "\n",
        "    scores, results = embeddings_dataset.get_nearest_examples(\n",
        "        \"embeddings\", query_embedding, k=K\n",
        "    )\n",
        "    results_df: pd.DataFrame = pd.DataFrame.from_dict(results)\n",
        "    results_df[\"scores\"] = scores\n",
        "    results_df.sort_values(\"scores\", ascending=False, inplace=True)\n",
        "\n",
        "    for _, row in results_df.iterrows():\n",
        "        print(f\"COMMENT: {row.comments}\")\n",
        "        print(f\"SCORE: {row.scores}\")\n",
        "        print(f\"TITLE: {row.title}\")\n",
        "        print(f\"URL: {row.html_url}\")\n",
        "        print(\"=\" * 70)\n",
        "        print()\n",
        "\n",
        "    return results_df"
      ],
      "metadata": {
        "id": "XPc9pObO6Q0x"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.iloc[255:260, [1, 2]]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "_8xpN5yy8HEE",
        "outputId": "eb9d53bb-e43a-4500-c02d-6a86a95a46a8"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                        title  \\\n",
              "255                        visualization for cc100 is broken    \n",
              "256  any possibility to download part of large datasets only?   \n",
              "257       data_args.preprocessing_num_workers almost freezes    \n",
              "258                                      adding ccnet dataset   \n",
              "259                          viewer \"fake_news_english\" error   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    comments  \n",
              "255  [This looks like an issue with the cc100 dataset itself but not sure\\r\\nDid you try loading cc100 on your machine ?, Hi\\nloading works fine, but the viewer only is broken\\nthanks\\n\\nOn Wed, Apr 7, 2021 at 12:17 PM Quentin Lhoest ***@***.***>\\nwrote:\\n\\n> This looks like an issue with the cc100 dataset itself but not sure\\n> Did you try loading cc100 on your machine ?\\n>\\n> —\\n> You are receiving this because you authored the thread.\\n> Reply to this email directly, view it on GitHub\\n> <https://github.com/huggingface/datasets/issues/2162#issuecomment-814793809>,\\n> or unsubscribe\\n> <https...  \n",
              "256  [Not yet but it’s on the short/mid-term roadmap (requested by many indeed)., oh, great, really awesome feature to have, thank you very much for the great, fabulous work, We'll work on dataset streaming soon. This should allow you to only load the examples you need ;), thanks a lot Quentin, this would be really really a great feature to have\\n\\nOn Wed, Apr 7, 2021 at 12:14 PM Quentin Lhoest ***@***.***>\\nwrote:\\n\\n> We'll work on dataset streaming soon. This should allow you to only load\\n> the examples you need ;)\\n>\\n> —\\n> You are receiving this because you authored the thread.\\n> Reply ...  \n",
              "257  [Hi.\\r\\nI cannot always reproduce this issue, and on later runs I did not see it so far. Sometimes also I set 8 processes but I see less being showed, is this normal, here only 5 are shown for 8 being set, thanks\\r\\n\\r\\n```\\r\\n#3:  11%|███████████████▊                                                                                                                                  | 172/1583 [00:46<06:21,  3.70ba/s]\\r\\n#4:   9%|█████████████▏                                                                                                                                    | 143/1583 [00:46<07...  \n",
              "258                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           [closing since I think this is cc100, just the name has been changed. thanks ]  \n",
              "259                                                                                                                                                                                                                                                                                                                                                                                                                                                                   [Thanks for reporting !\\r\\nThe viewer doesn't have all the dependencies of the datasets. We may add openpyxl to be able to show this dataset properly]  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b14c5173-d2ca-4251-b502-4a55eb554aaa\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>comments</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>255</th>\n",
              "      <td>visualization for cc100 is broken</td>\n",
              "      <td>[This looks like an issue with the cc100 dataset itself but not sure\\r\\nDid you try loading cc100 on your machine ?, Hi\\nloading works fine, but the viewer only is broken\\nthanks\\n\\nOn Wed, Apr 7, 2021 at 12:17 PM Quentin Lhoest ***@***.***&gt;\\nwrote:\\n\\n&gt; This looks like an issue with the cc100 dataset itself but not sure\\n&gt; Did you try loading cc100 on your machine ?\\n&gt;\\n&gt; —\\n&gt; You are receiving this because you authored the thread.\\n&gt; Reply to this email directly, view it on GitHub\\n&gt; &lt;https://github.com/huggingface/datasets/issues/2162#issuecomment-814793809&gt;,\\n&gt; or unsubscribe\\n&gt; &lt;https...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>256</th>\n",
              "      <td>any possibility to download part of large datasets only?</td>\n",
              "      <td>[Not yet but it’s on the short/mid-term roadmap (requested by many indeed)., oh, great, really awesome feature to have, thank you very much for the great, fabulous work, We'll work on dataset streaming soon. This should allow you to only load the examples you need ;), thanks a lot Quentin, this would be really really a great feature to have\\n\\nOn Wed, Apr 7, 2021 at 12:14 PM Quentin Lhoest ***@***.***&gt;\\nwrote:\\n\\n&gt; We'll work on dataset streaming soon. This should allow you to only load\\n&gt; the examples you need ;)\\n&gt;\\n&gt; —\\n&gt; You are receiving this because you authored the thread.\\n&gt; Reply ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>257</th>\n",
              "      <td>data_args.preprocessing_num_workers almost freezes</td>\n",
              "      <td>[Hi.\\r\\nI cannot always reproduce this issue, and on later runs I did not see it so far. Sometimes also I set 8 processes but I see less being showed, is this normal, here only 5 are shown for 8 being set, thanks\\r\\n\\r\\n```\\r\\n#3:  11%|███████████████▊                                                                                                                                  | 172/1583 [00:46&lt;06:21,  3.70ba/s]\\r\\n#4:   9%|█████████████▏                                                                                                                                    | 143/1583 [00:46&lt;07...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>258</th>\n",
              "      <td>adding ccnet dataset</td>\n",
              "      <td>[closing since I think this is cc100, just the name has been changed. thanks ]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>259</th>\n",
              "      <td>viewer \"fake_news_english\" error</td>\n",
              "      <td>[Thanks for reporting !\\r\\nThe viewer doesn't have all the dependencies of the datasets. We may add openpyxl to be able to show this dataset properly]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b14c5173-d2ca-4251-b502-4a55eb554aaa')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b14c5173-d2ca-4251-b502-4a55eb554aaa button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b14c5173-d2ca-4251-b502-4a55eb554aaa');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-dbc8114e-6d3c-47ef-8b04-2e12f9661943\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-dbc8114e-6d3c-47ef-8b04-2e12f9661943')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-dbc8114e-6d3c-47ef-8b04-2e12f9661943 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"title\"].value_counts(normalize=True)"
      ],
      "metadata": {
        "id": "m3_KSU2h-tCx",
        "outputId": "260db2c9-4e2b-4085-d015-acb792c73cfb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Protect master branch                                                                                                                                                                                                                                                           0.002475\n",
              "Add WIT Dataset                                                                                                                                                                                                                                                                 0.002475\n",
              "Backwards compatibility broken for cached datasets that use `.filter()`                                                                                                                                                                                                         0.002475\n",
              "wmt datasets fail to load                                                                                                                                                                                                                                                       0.002475\n",
              "Loading allenai/c4 in streaming mode does too many HEAD requests                                                                                                                                                                                                                0.002475\n",
              "\"counter\" dataset raises an error in normal mode, but not in streaming mode                                                                                                                                                                                                     0.002475\n",
              "TypeError: 'NoneType' object is not callable                                                                                                                                                                                                                                    0.002475\n",
              "datasets.config.PYARROW_VERSION has no attribute 'major'                                                                                                                                                                                                                        0.002475\n",
              "In v1.4.1, all TIMIT train transcripts are \"Would such an act of refusal be useful?\"                                                                                                                                                                                            0.002475\n",
              "`load_dataset('docred')` results in a `NonMatchingChecksumError`                                                                                                                                                                                                                0.002475\n",
              "Adding an Elastic Search index to a Dataset                                                                                                                                                                                                                                     0.002475\n",
              "v1.11.1 release date                                                                                                                                                                                                                                                            0.002475\n",
              "Error when encoding a dataset with None objects with a Sequence feature                                                                                                                                                                                                         0.002475\n",
              "Incompatibility with pytest                                                                                                                                                                                                                                                     0.002475\n",
              "Cannot download TOTTO dataset                                                                                                                                                                                                                                                   0.002475\n",
              "FORCE_REDOWNLOAD does not work                                                                                                                                                                                                                                                  0.002475\n",
              "Datasets 1.12 dataset.filter TypeError: get_indices_from_mask_function() got an unexpected keyword argument                                                                                                                                                                     0.002475\n",
              "timit_asr dataset only includes one text phrase                                                                                                                                                                                                                                 0.002475\n",
              "load_dataset using default cache on Windows causes PermissionError: [WinError 5] Access is denied                                                                                                                                                                               0.002475\n",
              "OSCAR unshuffled_original_ko: NonMatchingSplitsSizesError                                                                                                                                                                                                                       0.002475\n",
              "Conda build fails                                                                                                                                                                                                                                                               0.002475\n",
              "Mutable columns argument breaks set_format                                                                                                                                                                                                                                      0.002475\n",
              "to_tf_dataset keeps a reference to the open data somewhere, causing issues on windows                                                                                                                                                                                           0.002475\n",
              "\"File name too long\" error for file locks                                                                                                                                                                                                                                       0.002475\n",
              "Unwanted progress bars when accessing examples                                                                                                                                                                                                                                  0.002475\n",
              "`Can not decode content-encoding: gzip` when loading `scitldr` dataset with streaming                                                                                                                                                                                           0.002475\n",
              "windows download abnormal                                                                                                                                                                                                                                                       0.002475\n",
              "Having a dependency defining fsspec entrypoint raises an AttributeError when importing datasets                                                                                                                                                                                 0.002475\n",
              "Token classification labels are strings and we don't have the list of labels                                                                                                                                                                                                    0.001238\n",
              "Seq2Seq Metrics QOL: Bleu, Rouge                                                                                                                                                                                                                                                0.001238\n",
              "Descriptions of raw and processed versions of wikitext are inverted                                                                                                                                                                                                             0.001238\n",
              "self.options cannot be converted to a Python object for pickling                                                                                                                                                                                                                0.001238\n",
              "KILT dataset: empty string in triviaqa input field                                                                                                                                                                                                                              0.001238\n",
              "Error running pip install -e \".[dev]\" on MacOS 10.13.6: faiss/python does not exist                                                                                                                                                                                             0.001238\n",
              "feat(dataset): multiprocessing _generate_examples                                                                                                                                                                                                                               0.001238\n",
              "Issue with downloading Wikipedia data for low resource language                                                                                                                                                                                                                 0.001238\n",
              "Unexpected behavior when loading cached csv file?                                                                                                                                                                                                                               0.001238\n",
              "Adding CC-100: Monolingual Datasets from Web Crawl Data                                                                                                                                                                                                                         0.001238\n",
              "Downloaded datasets are not usable offline                                                                                                                                                                                                                                      0.001238\n",
              "Using `Dataset.map` with `n_proc>1` print multiple progress bars                                                                                                                                                                                                                0.001238\n",
              "How to choose proper download_mode in function load_dataset?                                                                                                                                                                                                                    0.001238\n",
              "Add a `lazy_map` method to `Dataset` and `DatasetDict`                                                                                                                                                                                                                          0.001238\n",
              "Add option for named splits when using ds.train_test_split                                                                                                                                                                                                                      0.001238\n",
              "[GEM] add DART data-to-text generation dataset                                                                                                                                                                                                                                  0.001238\n",
              "(Load dataset failure) ConnectionError: Couldn’t reach https://raw.githubusercontent.com/huggingface/datasets/1.1.2/datasets/cnn_dailymail/cnn_dailymail.py                                                                                                                     0.001238\n",
              "Process 0 very slow when using num_procs with map to tokenizer                                                                                                                                                                                                                  0.001238\n",
              "CUDA out of memory                                                                                                                                                                                                                                                              0.001238\n",
              "Clicking on a metric in the search page points to datasets page giving \"Missing dataset\" warning                                                                                                                                                                                0.001238\n",
              "How to join two datasets?                                                                                                                                                                                                                                                       0.001238\n",
              "Cannot load TREC dataset: ConnectionError                                                                                                                                                                                                                                       0.001238\n",
              "datasets freezes                                                                                                                                                                                                                                                                0.001238\n",
              "Empty output/answer in TriviaQA test set (both in 'kilt_tasks' and 'trivia_qa')                                                                                                                                                                                                 0.001238\n",
              "On loading a metric from datasets, I get the following error                                                                                                                                                                                                                    0.001238\n",
              "Can not reuse datasets already downloaded                                                                                                                                                                                                                                       0.001238\n",
              "load_dataset with 'csv' is not working. while the same file is loading with 'text' mode or with pandas                                                                                                                                                                          0.001238\n",
              "Wikipedia postprocessing                                                                                                                                                                                                                                                        0.001238\n",
              "[GEM] add WikiLingua cross-lingual abstractive summarization dataset                                                                                                                                                                                                            0.001238\n",
              "[GEM] add ToTTo Table-to-text dataset                                                                                                                                                                                                                                           0.001238\n",
              "[GEM] MultiWOZ dialogue dataset                                                                                                                                                                                                                                                 0.001238\n",
              "Discussion using datasets in offline mode                                                                                                                                                                                                                                       0.001238\n",
              "how processing in batch works in datasets                                                                                                                                                                                                                                       0.001238\n",
              "[XGLUE] Adding new dataset                                                                                                                                                                                                                                                      0.001238\n",
              "Add MRQA dataset                                                                                                                                                                                                                                                                0.001238\n",
              "[Caching] Dill globalvars() output order is not deterministic and can cause cache issues.                                                                                                                                                                                       0.001238\n",
              "Is dataset iterative or not?                                                                                                                                                                                                                                                    0.001238\n",
              "Joining multiple datasets                                                                                                                                                                                                                                                       0.001238\n",
              "How to implement DistributedSampler with datasets                                                                                                                                                                                                                               0.001238\n",
              "Too much logging                                                                                                                                                                                                                                                                0.001238\n",
              "nlp viewer error                                                                                                                                                                                                                                                                0.001238\n",
              "Add Google Taskmaster dataset                                                                                                                                                                                                                                                   0.001238\n",
              "load_dataset for LOCAL CSV files report CONNECTION ERROR                                                                                                                                                                                                                        0.001238\n",
              "Quail dataset urls are out of date                                                                                                                                                                                                                                              0.001238\n",
              "Error loading ms_marco v2.1 using load_dataset()                                                                                                                                                                                                                                0.001238\n",
              "How to use similarity settings  other then \"BM25\" in Elasticsearch index ?                                                                                                                                                                                                      0.001238\n",
              "Dataset Explorer Doesn't Work for squad_es and squad_it                                                                                                                                                                                                                         0.001238\n",
              "Inconsistent behavior in map                                                                                                                                                                                                                                                    0.001238\n",
              "Questions about XSUM                                                                                                                                                                                                                                                            0.001238\n",
              "How to skip a example when running dataset.map                                                                                                                                                                                                                                  0.001238\n",
              "Loss not decrease with Datasets and Transformers                                                                                                                                                                                                                                0.001238\n",
              "Does both 'bookcorpus' and 'wikipedia' belong to the same datasets which Google used for pretraining BERT?                                                                                                                                                                      0.001238\n",
              "runing dataset.map, it raises TypeError: can't pickle Tokenizer objects                                                                                                                                                                                                         0.001238\n",
              "load_dataset from local squad.py, raise error: TypeError: 'NoneType' object is not callable                                                                                                                                                                                     0.001238\n",
              "Squad Metric Description & Feature Mismatch                                                                                                                                                                                                                                     0.001238\n",
              "Problem with JSON dataset format                                                                                                                                                                                                                                                0.001238\n",
              "dummy data testing can't test datasets using `dl_manager.extract` in `_split_generators`                                                                                                                                                                                        0.001238\n",
              "offset overflow when multiprocessing batched map on large datasets.                                                                                                                                                                                                             0.001238\n",
              "load_dataset for CSV files not working                                                                                                                                                                                                                                          0.001238\n",
              "Cannot download dataset_info.json                                                                                                                                                                                                                                               0.001238\n",
              "Caching processed dataset at wrong folder                                                                                                                                                                                                                                       0.001238\n",
              "GLUE/QQP dataset: NonMatchingChecksumError                                                                                                                                                                                                                                      0.001238\n",
              "Load large text file for LM pre-training resulting in OOM                                                                                                                                                                                                                       0.001238\n",
              "Text dataset not working with large files                                                                                                                                                                                                                                       0.001238\n",
              "straddling object straddles two block boundaries                                                                                                                                                                                                                                0.001238\n",
              "dtype of tensors should be preserved                                                                                                                                                                                                                                            0.001238\n",
              "Custom feature types in `load_dataset` from CSV                                                                                                                                                                                                                                 0.001238\n",
              "load_dataset for text files not working                                                                                                                                                                                                                                         0.001238\n",
              "blog_authorship_corpus crashed                                                                                                                                                                                                                                                  0.001238\n",
              "load_dataset() won't download in Windows                                                                                                                                                                                                                                        0.001238\n",
              "Add custom dataset to NLP?                                                                                                                                                                                                                                                      0.001238\n",
              "train_test_split returns empty dataset item                                                                                                                                                                                                                                     0.001238\n",
              "Creating dataset consumes too much memory                                                                                                                                                                                                                                       0.001238\n",
              "Trec Dataset Connection Error                                                                                                                                                                                                                                                   0.001238\n",
              "Throw error when an unexpected key is used in data_files                                                                                                                                                                                                                        0.001238\n",
              "Possible caching bug                                                                                                                                                                                                                                                            0.001238\n",
              "\"Checksums didn't match for dataset source files\" error while loading openwebtext dataset                                                                                                                                                                                       0.001238\n",
              "need to redirect /nlp to /datasets and remove outdated info                                                                                                                                                                                                                     0.001238\n",
              "Adding pseudo-labels to datasets                                                                                                                                                                                                                                                0.001238\n",
              "feat(dl_manager): add support for ftp downloads                                                                                                                                                                                                                                 0.001238\n",
              "OSError: Cannot find data file when not using the dummy dataset in RAG                                                                                                                                                                                                          0.001238\n",
              "Error in the notebooks/Overview.ipynb notebook                                                                                                                                                                                                                                  0.001238\n",
              "use_custom_baseline still produces errors for bertscore                                                                                                                                                                                                                         0.001238\n",
              "Datasets performance slow? - 6.4x slower than in memory dataset                                                                                                                                                                                                                 0.001238\n",
              "Requirements should specify pyarrow<1                                                                                                                                                                                                                                           0.001238\n",
              "TypeError: '<' not supported between instances of 'NamedSplit' and 'NamedSplit'                                                                                                                                                                                                 0.001238\n",
              "XNLI dataset is not loading                                                                                                                                                                                                                                                     0.001238\n",
              "XNLI dataset: NonMatchingChecksumError                                                                                                                                                                                                                                          0.001238\n",
              "`ArrowInvalid` occurs while running `Dataset.map()` function                                                                                                                                                                                                                    0.001238\n",
              "Dataset browser url is still https://huggingface.co/nlp/viewer/                                                                                                                                                                                                                 0.001238\n",
              "The download instructions for c4 datasets are not contained in the error message                                                                                                                                                                                                0.001238\n",
              "How to enable `.map()` pre-processing pipelines to support multi-node parallelism?                                                                                                                                                                                              0.001238\n",
              "terminate called after throwing an instance of 'google::protobuf::FatalException'                                                                                                                                                                                               0.001238\n",
              "Add HoVer multi-hop fact verification dataset                                                                                                                                                                                                                                   0.001238\n",
              "Access to key in DatasetDict map                                                                                                                                                                                                                                                0.001238\n",
              "Cannot download ade_corpus_v2                                                                                                                                                                                                                                                   0.001238\n",
              "Can't call shape on the output of select()                                                                                                                                                                                                                                      0.001238\n",
              "Can't filter language:EN on https://huggingface.co/datasets                                                                                                                                                                                                                     0.001238\n",
              "Bug: Can't download TriviaQA with `load_dataset` - custom `cache_dir`                                                                                                                                                                                                           0.001238\n",
              "shuffle with torch generator                                                                                                                                                                                                                                                    0.001238\n",
              "shuffle does not accept seed                                                                                                                                                                                                                                                    0.001238\n",
              "Not able to use 'jigsaw_toxicity_pred' dataset                                                                                                                                                                                                                                  0.001238\n",
              "AttributeError: 'DatasetDict' object has no attribute 'train_test_split'                                                                                                                                                                                                        0.001238\n",
              "connection error                                                                                                                                                                                                                                                                0.001238\n",
              "Using datasets.Metric with Trainer()                                                                                                                                                                                                                                            0.001238\n",
              "❓ Sharing ElasticSearch indexed dataset                                                                                                                                                                                                                                         0.001238\n",
              "IWSLT-17 Link Broken                                                                                                                                                                                                                                                            0.001238\n",
              "Add helper to resolve namespace collision                                                                                                                                                                                                                                       0.001238\n",
              "FileNotFoundError for `amazon_polarity`                                                                                                                                                                                                                                         0.001238\n",
              "Installing datasets and transformers in a tensorflow docker image throws Permission Error on 'import transformers'                                                                                                                                                              0.001238\n",
              "connection issue while downloading data                                                                                                                                                                                                                                         0.001238\n",
              "how to get all the options of a property in datasets                                                                                                                                                                                                                            0.001238\n",
              "Inconsistent argument names.                                                                                                                                                                                                                                                    0.001238\n",
              "SNLI dataset contains labels with value -1                                                                                                                                                                                                                                      0.001238\n",
              "FileNotFound remotly, can't load a dataset                                                                                                                                                                                                                                      0.001238\n",
              "`Dataset.map` disable progress bar                                                                                                                                                                                                                                              0.001238\n",
              "Adding UKP Argument Aspect Similarity Corpus                                                                                                                                                                                                                                    0.001238\n",
              "social_i_qa wrong format of labels                                                                                                                                                                                                                                              0.001238\n",
              "Inspecting datasets per category                                                                                                                                                                                                                                                0.001238\n",
              "Question: Shouldn't .info be a part of DatasetDict?                                                                                                                                                                                                                             0.001238\n",
              "Dataset Error: DaNE contains empty samples at the end                                                                                                                                                                                                                           0.001238\n",
              "`ArrowInvalid` occurs while running `Dataset.map()` function for DPRContext                                                                                                                                                                                                     0.001238\n",
              "Dataset \"dane\" missing                                                                                                                                                                                                                                                          0.001238\n",
              "Can't import cc100 dataset                                                                                                                                                                                                                                                      0.001238\n",
              "Add the 800GB Pile dataset?                                                                                                                                                                                                                                                     0.001238\n",
              "dutch_social can't be loaded                                                                                                                                                                                                                                                    0.001238\n",
              "Unable to Download Hindi Wikipedia Dataset                                                                                                                                                                                                                                      0.001238\n",
              "load_dataset hang on file_lock                                                                                                                                                                                                                                                  0.001238\n",
              "connection issue                                                                                                                                                                                                                                                                0.001238\n",
              "wiki_dpr pre-processing performance                                                                                                                                                                                                                                             0.001238\n",
              "wiki_dpr dataset pre-processesing performance                                                                                                                                                                                                                                   0.001238\n",
              "Arrow file is too large when saving vector data                                                                                                                                                                                                                                 0.001238\n",
              "NarrativeQA fails to load with `load_dataset`                                                                                                                                                                                                                                   0.001238\n",
              "HoVeR dataset fails to load                                                                                                                                                                                                                                                     0.001238\n",
              "Dataset social_bias_frames 404                                                                                                                                                                                                                                                  0.001238\n",
              "muchocine dataset cannot be dowloaded                                                                                                                                                                                                                                           0.001238\n",
              "bug with sst2 in glue                                                                                                                                                                                                                                                           0.001238\n",
              "winogrande cannot be dowloaded                                                                                                                                                                                                                                                  0.001238\n",
              "Can't map dataset (loaded from csv)                                                                                                                                                                                                                                             0.001238\n",
              "can't load \"german_legal_entity_recognition\" dataset                                                                                                                                                                                                                            0.001238\n",
              "multiprocessing in dataset map \"can only test a child process\"                                                                                                                                                                                                                  0.001238\n",
              "Mistakes in MLQA features names                                                                                                                                                                                                                                                 0.001238\n",
              "Downloading/caching only a part of a datasets' dataset.                                                                                                                                                                                                                         0.001238\n",
              "Add SQA                                                                                                                                                                                                                                                                         0.001238\n",
              "boolq does not load                                                                                                                                                                                                                                                             0.001238\n",
              "Loading Data From S3 Path in Sagemaker                                                                                                                                                                                                                                          0.001238\n",
              "DataLoader(datasets) become more and more slowly within iterations                                                                                                                                                                                                              0.001238\n",
              "imdb dataset cannot be loaded                                                                                                                                                                                                                                                   0.001238\n",
              "bug in boolq dataset loading                                                                                                                                                                                                                                                    0.001238\n",
              "trec dataset unavailable                                                                                                                                                                                                                                                        0.001238\n",
              "load_dataset('cnn_dalymail', '3.0.0') gives a 'Not a directory' error                                                                                                                                                                                                           0.001238\n",
              "[Feature Request] Add optional parameter in text loading script to preserve linebreaks                                                                                                                                                                                          0.001238\n",
              "imdb dataset cannot be downloaded                                                                                                                                                                                                                                               0.001238\n",
              "OSCAR from Inria group                                                                                                                                                                                                                                                          0.001238\n",
              "Have Trouble importing `datasets`                                                                                                                                                                                                                                               0.001238\n",
              "Unable to download cnn_dailymail dataset                                                                                                                                                                                                                                        0.001238\n",
              "Possible Bug: Small training/dataset file creates gigantic output                                                                                                                                                                                                               0.001238\n",
              "wmt16 does not download                                                                                                                                                                                                                                                         0.001238\n",
              "concatenate_datasets support axis=0 or 1 ？                                                                                                                                                                                                                                      0.001238\n",
              "Add support for other languages for rouge                                                                                                                                                                                                                                       0.001238\n",
              "Load amazon dataset                                                                                                                                                                                                                                                             0.001238\n",
              "Error when concatenate_datasets                                                                                                                                                                                                                                                 0.001238\n",
              "Very slow cold-start                                                                                                                                                                                                                                                            0.001238\n",
              "pyarrow.lib.ArrowNotImplementedError: MakeBuilder: cannot construct builder for type extension<arrow.py_extension_type>                                                                                                                                                         0.001238\n",
              "Nested lists are zipped unexpectedly                                                                                                                                                                                                                                            0.001238\n",
              "Dataset viewer issues                                                                                                                                                                                                                                                           0.001238\n",
              "'iwslt2017-ro-nl', cannot be downloaded                                                                                                                                                                                                                                         0.001238\n",
              "[libprotobuf FATAL /sentencepiece/src/../third_party/protobuf-lite/google/protobuf/repeated_field.h:1505] CHECK failed: (index) >= (0):  terminate called after throwing an instance of 'google::protobuf::FatalException'   what():  CHECK failed: (index) >= (0):  Aborted    0.001238\n",
              "boolq does not work                                                                                                                                                                                                                                                             0.001238\n",
              "❓ On-the-fly tokenization with datasets, tokenizers, and torch Datasets and Dataloaders                                                                                                                                                                                         0.001238\n",
              "Incorrect URL for MRQA SQuAD train subset                                                                                                                                                                                                                                       0.001238\n",
              "Using a feature named \"_type\" fails with certain operations                                                                                                                                                                                                                     0.001238\n",
              "Add support to download kaggle datasets                                                                                                                                                                                                                                         0.001238\n",
              "Not support links with 302 redirect                                                                                                                                                                                                                                             0.001238\n",
              "Dataset.map() turns tensors into lists?                                                                                                                                                                                                                                         0.001238\n",
              "how large datasets are handled under the hood                                                                                                                                                                                                                                   0.001238\n",
              "NotADirectoryError while loading the CNN/Dailymail dataset                                                                                                                                                                                                                      0.001238\n",
              "Problem downloading amazon_reviews_multi                                                                                                                                                                                                                                        0.001238\n",
              "making sure datasets are not loaded in memory and distributed training of them                                                                                                                                                                                                  0.001238\n",
              "sample multiple datasets                                                                                                                                                                                                                                                        0.001238\n",
              "Local machine/cluster Beam Datasets example/tutorial                                                                                                                                                                                                                            0.001238\n",
              "wrong length with datasets                                                                                                                                                                                                                                                      0.001238\n",
              "Shall we change the hashing to encoding to reduce potential replicated cache files?                                                                                                                                                                                             0.001238\n",
              "datasets module not found                                                                                                                                                                                                                                                       0.001238\n",
              "datasets.load_dataset() custom chaching directory bug                                                                                                                                                                                                                           0.001238\n",
              "map/filter multiprocessing raises errors and corrupts datasets                                                                                                                                                                                                                  0.001238\n",
              "Some languages in wikipedia dataset are not loading                                                                                                                                                                                                                             0.001238\n",
              "Compare different Rouge implementations                                                                                                                                                                                                                                         0.001238\n",
              "Should we still have to force to install apache_beam to download wikipedia ?                                                                                                                                                                                                    0.001238\n",
              "SST-2 test labels are all -1                                                                                                                                                                                                                                                    0.001238\n",
              "UnicodeDecodeError when downloading GLUE-MNLI                                                                                                                                                                                                                                   0.001238\n",
              "Deterministic dataset loading                                                                                                                                                                                                                                                   0.001238\n",
              "[Creating new dataset] Not found dataset_info.json                                                                                                                                                                                                                              0.001238\n",
              "[Metric] Bertscore : Warning : Empty candidate sentence; Setting recall to be 0.                                                                                                                                                                                                0.001238\n",
              "Can't download MultiNLI                                                                                                                                                                                                                                                         0.001238\n",
              "Huggingface NLP, Uploading custom dataset                                                                                                                                                                                                                                       0.001238\n",
              "Fail to download c4 english corpus                                                                                                                                                                                                                                              0.001238\n",
              "Not able to access the XNLI dataset                                                                                                                                                                                                                                             0.001238\n",
              "[ROUGE] Different scores with `files2rouge`                                                                                                                                                                                                                                     0.001238\n",
              "[Dataset created] some critical small issues when I was creating a dataset                                                                                                                                                                                                      0.001238\n",
              "[Feature Request/Help] BLEURT model -> PyTorch                                                                                                                                                                                                                                  0.001238\n",
              "[Feature request] Add FLUE dataset                                                                                                                                                                                                                                              0.001238\n",
              "Colab Notebook breaks when downloading the squad dataset                                                                                                                                                                                                                        0.001238\n",
              "Multi-task dataset mixing                                                                                                                                                                                                                                                       0.001238\n",
              "❓ How to get ROUGE-2 with the ROUGE metric ?                                                                                                                                                                                                                                    0.001238\n",
              "NonMatchingSplitsSizesError when loading blog_authorship_corpus                                                                                                                                                                                                                 0.001238\n",
              "[Arrow writer, Trivia_qa] Could not convert TagMe with type str: converting to null type                                                                                                                                                                                        0.001238\n",
              "Remove test set from NLP viewer                                                                                                                                                                                                                                                 0.001238\n",
              "[Question] Combine 2 datasets which have the same columns                                                                                                                                                                                                                       0.001238\n",
              "What is the best way to cache a dataset?                                                                                                                                                                                                                                        0.001238\n",
              "NonMatchingSplitsSizesError error when reading the IMDB dataset                                                                                                                                                                                                                 0.001238\n",
              "UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors                                                                                                                                                                         0.001238\n",
              "Empty samples in glue/qqp                                                                                                                                                                                                                                                       0.001238\n",
              "Setting cache_dir gives error on wikipedia download                                                                                                                                                                                                                             0.001238\n",
              "Error in Demo for Specific Datasets                                                                                                                                                                                                                                             0.001238\n",
              "snli -1 labels                                                                                                                                                                                                                                                                  0.001238\n",
              "Cannot load arxiv dataset on MacOS?                                                                                                                                                                                                                                             0.001238\n",
              "ConnectionError - Eli5 dataset download                                                                                                                                                                                                                                         0.001238\n",
              "Error at the first example in README: AttributeError: module 'dill' has no attribute '_dill'                                                                                                                                                                                    0.001238\n",
              "Private/sensitive data                                                                                                                                                                                                                                                          0.001238\n",
              "Dataset Preprocessing Cache with .map() function not working as expected                                                                                                                                                                                                        0.001238\n",
              "MemoryError when loading German Wikipedia                                                                                                                                                                                                                                       0.001238\n",
              "NonMatchingChecksumError when loading pubmed dataset                                                                                                                                                                                                                            0.001238\n",
              "[Feature request] Be able to remove a specific sample of the dataset                                                                                                                                                                                                            0.001238\n",
              "PG-19                                                                                                                                                                                                                                                                           0.001238\n",
              "c4 dataset is not viewable in nlpviewer demo                                                                                                                                                                                                                                    0.001238\n",
              "How can I load/find WMT en-romanian?                                                                                                                                                                                                                                            0.001238\n",
              "[Feature request] Support for external modality for language datasets                                                                                                                                                                                                           0.001238\n",
              "Downloading dataset error with pyarrow.lib.RecordBatch                                                                                                                                                                                                                          0.001238\n",
              "documentation missing how to split a dataset                                                                                                                                                                                                                                    0.001238\n",
              "Why is dataset after tokenization far more larger than the orginal one ?                                                                                                                                                                                                        0.001238\n",
              "Tokenizer pickling issue fix not landed in `nlp` yet?                                                                                                                                                                                                                           0.001238\n",
              "[Feature request] Add a feature to dataset                                                                                                                                                                                                                                      0.001238\n",
              "Mistaken `_KWARGS_DESCRIPTION` for XNLI metric                                                                                                                                                                                                                                  0.001238\n",
              "Index outside of table length                                                                                                                                                                                                                                                   0.001238\n",
              "Scientific Papers only downloading Pubmed                                                                                                                                                                                                                                       0.001238\n",
              "🐛 Colab : type object 'pyarrow.lib.RecordBatch' has no attribute 'from_struct_array'                                                                                                                                                                                            0.001238\n",
              "Consider renaming to nld                                                                                                                                                                                                                                                        0.001238\n",
              "Tokenized BLEU considered harmful - Discussion on community-based process                                                                                                                                                                                                       0.001238\n",
              "[Question] Using/adding a local dataset                                                                                                                                                                                                                                         0.001238\n",
              "[Feature Request] Add the OpenWebText dataset                                                                                                                                                                                                                                   0.001238\n",
              "[Feature request] Add Toronto BookCorpus dataset                                                                                                                                                                                                                                0.001238\n",
              "Loading GLUE dataset loads CoLA by default                                                                                                                                                                                                                                      0.001238\n",
              "[Feature request] Add Google Natural Question dataset                                                                                                                                                                                                                           0.001238\n",
              "Some error inside nlp.load_dataset()                                                                                                                                                                                                                                            0.001238\n",
              "🐛 `map` not working                                                                                                                                                                                                                                                             0.001238\n",
              "❓ How to apply a map to all subsets ?                                                                                                                                                                                                                                           0.001238\n",
              "[Tensorflow] Use something else than `from_tensor_slices()`                                                                                                                                                                                                                     0.001238\n",
              "❓ How to remove specific rows of a dataset ?                                                                                                                                                                                                                                    0.001238\n",
              "🐛 Trying to use ROUGE metric : pyarrow.lib.ArrowInvalid: Column 1 named references expected length 534 but got length 323                                                                                                                                                       0.001238\n",
              "AttributeError: 'dict' object has no attribute 'info'                                                                                                                                                                                                                           0.001238\n",
              "Couldn't reach CNN/DM dataset                                                                                                                                                                                                                                                   0.001238\n",
              "[Checksums] Error for some datasets                                                                                                                                                                                                                                             0.001238\n",
              "Error when citation is not given in the DatasetInfo                                                                                                                                                                                                                             0.001238\n",
              "ValueError when a split is empty                                                                                                                                                                                                                                                0.001238\n",
              "[Feature] Keep the list of labels of a dataset as metadata                                                                                                                                                                                                                      0.001238\n",
              "[Feature] More dataset outputs                                                                                                                                                                                                                                                  0.001238\n",
              "ArrowTypeError in squad metrics                                                                                                                                                                                                                                                 0.001238\n",
              "Error with sklearn train_test_split                                                                                                                                                                                                                                             0.001238\n",
              "_download_and_prepare() got an unexpected keyword argument 'verify_infos'                                                                                                                                                                                                       0.001238\n",
              "[Feature request] Add Ubuntu Dialogue Corpus dataset                                                                                                                                                                                                                            0.001238\n",
              "[Question] Create Apache Arrow dataset from raw text file                                                                                                                                                                                                                       0.001238\n",
              "[Question] BERT-style multiple choice formatting                                                                                                                                                                                                                                0.001238\n",
              "When will the remaining math_dataset modules be added as dataset objects                                                                                                                                                                                                        0.001238\n",
              "[Question] How to load wikipedia ? Beam runner ?                                                                                                                                                                                                                                0.001238\n",
              "Weird-ish: Not creating unique caches for different phases                                                                                                                                                                                                                      0.001238\n",
              "[Bug] labels of glue/ax are all -1                                                                                                                                                                                                                                              0.001238\n",
              "Cannot upload my own dataset                                                                                                                                                                                                                                                    0.001238\n",
              "[Feature request] separate split name and split instructions                                                                                                                                                                                                                    0.001238\n",
              "Clone not working on Windows environment                                                                                                                                                                                                                                        0.001238\n",
              "Loading 'wikitext' dataset fails                                                                                                                                                                                                                                                0.001238\n",
              "Add a method to shuffle a dataset                                                                                                                                                                                                                                               0.001238\n",
              "Add Spanish POR and NER Datasets                                                                                                                                                                                                                                                0.001238\n",
              "[Feature request] Add cos-e v1.0                                                                                                                                                                                                                                                0.001238\n",
              "Discussion on version identifier & MockDataLoaderManager for test data                                                                                                                                                                                                          0.001238\n",
              "caching in map causes same result to be returned for train, validation and test                                                                                                                                                                                                 0.001238\n",
              "How can we add more datasets to nlp library?                                                                                                                                                                                                                                    0.001238\n",
              "nlp.load_dataset() gives \"TypeError: list_() takes exactly one argument (2 given)\"                                                                                                                                                                                              0.001238\n",
              "SyntaxError with WMT datasets                                                                                                                                                                                                                                                   0.001238\n",
              "Meta-datasets (GLUE/XTREME/...) – Special care to attributions and citations                                                                                                                                                                                                    0.001238\n",
              "Question - Sign Language Datasets                                                                                                                                                                                                                                               0.001238\n",
              "[Feature request] Add `shard()` method to dataset                                                                                                                                                                                                                               0.001238\n",
              "[Question] Best way to batch a large dataset?                                                                                                                                                                                                                                   0.001238\n",
              "TypeError: Receiver() takes no arguments                                                                                                                                                                                                                                        0.001238\n",
              "wmt download speed example                                                                                                                                                                                                                                                      0.001238\n",
              "Some docs are missing parameter names                                                                                                                                                                                                                                           0.001238\n",
              "dictionnary typo in docs                                                                                                                                                                                                                                                        0.001238\n",
              "[BUG] Metrics throwing new error on master since 0.4.0                                                                                                                                                                                                                          0.001238\n",
              "add MLDoc dataset                                                                                                                                                                                                                                                               0.001238\n",
              "dataset.shuffle(keep_in_memory=True) is never allowed                                                                                                                                                                                                                           0.001238\n",
              "dataset.shuffle() and select() resets format. Intended?                                                                                                                                                                                                                         0.001238\n",
              "Version of numpy to use the library                                                                                                                                                                                                                                             0.001238\n",
              "Converting TensorFlow dataset example                                                                                                                                                                                                                                           0.001238\n",
              "Errors when I use                                                                                                                                                                                                                                                               0.001238\n",
              "test_load_real_dataset when config has BUILDER_CONFIGS that matter                                                                                                                                                                                                              0.001238\n",
              "Caching doesn't work for map (non-deterministic)                                                                                                                                                                                                                                0.001238\n",
              "nlp.Features does not distinguish between nullable and non-nullable types in PyArrow schema                                                                                                                                                                                     0.001238\n",
              "No 0.4.0 release on GitHub                                                                                                                                                                                                                                                      0.001238\n",
              "ug                                                                                                                                                                                                                                                                              0.001238\n",
              "issues with downloading datasets for wmt16 and wmt19                                                                                                                                                                                                                            0.001238\n",
              "Bookcorpus data contains pretokenized text                                                                                                                                                                                                                                      0.001238\n",
              "rotten tomatoes movie review dataset taken down                                                                                                                                                                                                                                 0.001238\n",
              "Bugs : dataset.map() is frozen on ELI5                                                                                                                                                                                                                                          0.001238\n",
              "Export TFRecord to GCP bucket                                                                                                                                                                                                                                                   0.001238\n",
              "File exists error when used with TPU                                                                                                                                                                                                                                            0.001238\n",
              "`list_datasets()` is broken.                                                                                                                                                                                                                                                    0.001238\n",
              "[Dataset] RACE dataset Checksums error                                                                                                                                                                                                                                          0.001238\n",
              "[Dataset] `NonMatchingChecksumError` due to an update in the LinCE benchmark data                                                                                                                                                                                               0.001238\n",
              "Offset overflow when slicing a big dataset with an array of indices in Pyarrow >= 1.0.0                                                                                                                                                                                         0.001238\n",
              "ArrowCapacityError: List array cannot contain more than 2147483646 child elements, have 2147483648                                                                                                                                                                              0.001238\n",
              "Load text file for RoBERTa pre-training.                                                                                                                                                                                                                                        0.001238\n",
              "Don't use the old NYU GLUE dataset URLs                                                                                                                                                                                                                                         0.001238\n",
              "Pickling error when loading dataset                                                                                                                                                                                                                                             0.001238\n",
              "The current version of the package on github has an error when loading dataset                                                                                                                                                                                                  0.001238\n",
              "Indices incorrect with multiprocessing                                                                                                                                                                                                                                          0.001238\n",
              "`Dataset`/`DatasetDict` has no attribute 'save_to_disk'                                                                                                                                                                                                                         0.001238\n",
              "The process cannot access the file because it is being used by another process (windows)                                                                                                                                                                                        0.001238\n",
              "nlp re-creates already-there caches when using a script, but not within a shell                                                                                                                                                                                                 0.001238\n",
              "Some datasets miss dataset_infos.json or dummy_data.zip                                                                                                                                                                                                                         0.001238\n",
              "Couldn't reach certain URLs and for the ones that can be reached, code just blocks after downloading.                                                                                                                                                                           0.001238\n",
              "`metric.compute` throws `ArrowInvalid` error                                                                                                                                                                                                                                    0.001238\n",
              "No module named 'nlp.logging'                                                                                                                                                                                                                                                   0.001238\n",
              "Using custom DownloadConfig results in an error                                                                                                                                                                                                                                 0.001238\n",
              "nlp downloads to its module path                                                                                                                                                                                                                                                0.001238\n",
              "Very slow data loading on large dataset                                                                                                                                                                                                                                         0.001238\n",
              "New release coming up for this library                                                                                                                                                                                                                                          0.001238\n",
              "nlp.load_dataset is not safe for multi processes when loading from local files                                                                                                                                                                                                  0.001238\n",
              "Overview.ipynb throws exceptions with nlp 0.4.0                                                                                                                                                                                                                                 0.001238\n",
              "invalid data type 'str' at _convert_outputs in arrow_dataset.py                                                                                                                                                                                                                 0.001238\n",
              "Adding a dataset with multiple subtasks                                                                                                                                                                                                                                         0.001238\n",
              "'cp950' codec error from load_dataset('xtreme', 'tydiqa')                                                                                                                                                                                                                       0.001238\n",
              "can't load local dataset: pyarrow.lib.ArrowInvalid: straddling object straddles two block boundaries                                                                                                                                                                            0.001238\n",
              "load_metric can't acquire lock anymore                                                                                                                                                                                                                                          0.001238\n",
              "How to augment data ?                                                                                                                                                                                                                                                           0.001238\n",
              "[dateset subset missing]  xtreme paws-x                                                                                                                                                                                                                                         0.001238\n",
              "🐛 [Metrics] ROUGE is non-deterministic                                                                                                                                                                                                                                          0.001238\n",
              "[Feature request] Add dataset.ragged_map() function for many-to-many transformations                                                                                                                                                                                            0.001238\n",
              "ArrowBasedBuilder _prepare_split parse_schema breaks on nested structures                                                                                                                                                                                                       0.001238\n",
              "can't load SNLI dataset                                                                                                                                                                                                                                                         0.001238\n",
              "[Dataset requests] New datasets for Text Classification                                                                                                                                                                                                                         0.001238\n",
              "Supporting documents in ELI5                                                                                                                                                                                                                                                    0.001238\n",
              "UnicodeDecodeError while loading PAN-X task of XTREME dataset                                                                                                                                                                                                                   0.001238\n",
              "Features should be updated when `map()` changes schema                                                                                                                                                                                                                          0.001238\n",
              "Loading CNN/Daily Mail dataset produces `nlp.utils.info_utils.NonMatchingSplitsSizesError`                                                                                                                                                                                      0.001238\n",
              "[Bug] FileLock dependency incompatible with filesystem                                                                                                                                                                                                                          0.001238\n",
              "Fork dataset                                                                                                                                                                                                                                                                    0.001238\n",
              "Large dataset in Squad2-format                                                                                                                                                                                                                                                  0.001238\n",
              "Error when calculating glue score                                                                                                                                                                                                                                               0.001238\n",
              "ERROR:root:mwparserfromhell                                                                                                                                                                                                                                                     0.001238\n",
              "Blog Authorship Corpus, Non Matching Splits Sizes Error, nlp viewer                                                                                                                                                                                                             0.001238\n",
              "Nested sequences with dicts                                                                                                                                                                                                                                                     0.001238\n",
              "Segmentation fault when loading local JSON dataset as of #372                                                                                                                                                                                                                   0.001238\n",
              "TypeError when computing bertscore                                                                                                                                                                                                                                              0.001238\n",
              "to_pandas conversion doesn't always work                                                                                                                                                                                                                                        0.001238\n",
              "[dataset] Structure of MLQA seems unecessary nested                                                                                                                                                                                                                             0.001238\n",
              "DEFAULT_TOKENIZER import error in sacrebleu                                                                                                                                                                                                                                     0.001238\n",
              "Keep loading old file even I specify a new file in load_dataset                                                                                                                                                                                                                 0.001238\n",
              "Cannot unpickle saved .pt dataset with torch.save()/load()                                                                                                                                                                                                                      0.001238\n",
              "Issues: Adding a FAISS or Elastic Search index to a Dataset                                                                                                                                                                                                                     0.001238\n",
              "New Datasets: IWSLT15+, ITTB                                                                                                                                                                                                                                                    0.001238\n",
              "Google Colab - load_dataset - PyArrow exception                                                                                                                                                                                                                                 0.001238\n",
              "ImportWarning for pyarrow 1.0.0                                                                                                                                                                                                                                                 0.001238\n",
              "How to reuse functionality of a (generic) dataset?                                                                                                                                                                                                                              0.001238\n",
              "[FEATURE REQUEST] Multiprocessing with for dataset.map, dataset.filter                                                                                                                                                                                                          0.001238\n",
              "Correct data structure for PAN-X task in XTREME dataset?                                                                                                                                                                                                                        0.001238\n",
              "Addition of google drive links to dl_manager                                                                                                                                                                                                                                    0.001238\n",
              "from_dict delete?                                                                                                                                                                                                                                                               0.001238\n",
              "Is there a way to download only NQ dev?                                                                                                                                                                                                                                         0.001238\n",
              "Unable to load XTREME dataset from disk                                                                                                                                                                                                                                         0.001238\n",
              "train_test_split error: 'dict' object has no attribute 'deepcopy'                                                                                                                                                                                                               0.001238\n",
              "MissingBeamOptions for Wikipedia 20200501.en                                                                                                                                                                                                                                    0.001238\n",
              "Faster Shuffling?                                                                                                                                                                                                                                                               0.001238\n",
              "🐛 [Dataset] Cannot download wmt14, wmt15 and wmt17                                                                                                                                                                                                                              0.001238\n",
              "Conversion through to_pandas output numpy arrays for lists instead of python objects                                                                                                                                                                                            0.001238\n",
              "Unable to install datasets                                                                                                                                                                                                                                                      0.001238\n",
              "Issue while Creating Custom Metric                                                                                                                                                                                                                                              0.001238\n",
              "Error when downloading a large dataset on slow connection.                                                                                                                                                                                                                      0.001238\n",
              "load_from_disk and save_to_disk are not compatible with each other                                                                                                                                                                                                              0.001238\n",
              "dataset adversarial_qa has no answers in the \"test\" set                                                                                                                                                                                                                         0.001238\n",
              "`yelp_polarity` is broken                                                                                                                                                                                                                                                       0.001238\n",
              "Sentence Boundaries missing in Dataset: xtreme / udpos                                                                                                                                                                                                                          0.001238\n",
              "Some tests hang on Windows                                                                                                                                                                                                                                                      0.001238\n",
              "DuplicatedKeysError on personal dataset                                                                                                                                                                                                                                         0.001238\n",
              "Remove `extended` field from dataset tagger                                                                                                                                                                                                                                     0.001238\n",
              "Extend QuestionAnsweringExtractive template to handle nested columns                                                                                                                                                                                                            0.001238\n",
              "DuplicatedKeysError when trying to load adversarial_qa                                                                                                                                                                                                                          0.001238\n",
              "Saving Graph/Structured Data in Datasets                                                                                                                                                                                                                                        0.001238\n",
              "Cached dataset not loaded                                                                                                                                                                                                                                                       0.001238\n",
              "MRPC test set differences between torch and tensorflow datasets                                                                                                                                                                                                                 0.001238\n",
              "AttributeError: 'DatasetInfo' object has no attribute 'task_templates'                                                                                                                                                                                                          0.001238\n",
              "Docstring mistake: dataset vs. metric                                                                                                                                                                                                                                           0.001238\n",
              ".map() function got an unexpected keyword argument 'cache_file_name'                                                                                                                                                                                                            0.001238\n",
              "load_dataset('natural_questions') fails with \"ValueError: External features info don't match the dataset\"                                                                                                                                                                       0.001238\n",
              "Concatenate several datasets with removed columns is not working.                                                                                                                                                                                                               0.001238\n",
              "strange datasets from OSCAR corpus                                                                                                                                                                                                                                              0.001238\n",
              "Missing original answers in kilt-TriviaQA                                                                                                                                                                                                                                       0.001238\n",
              "datasets 1.6 ignores cache                                                                                                                                                                                                                                                      0.001238\n",
              "Accessing Arrow dataset cache_files                                                                                                                                                                                                                                             0.001238\n",
              "BLUE file not found                                                                                                                                                                                                                                                             0.001238\n",
              "Revert default in-memory for small datasets                                                                                                                                                                                                                                     0.001238\n",
              "IsADirectoryError when trying to download C4                                                                                                                                                                                                                                    0.001238\n",
              "Load Image Classification Dataset from Local                                                                                                                                                                                                                                    0.001238\n",
              "Use `Audio` features for `AutomaticSpeechRecognition` task template                                                                                                                                                                                                             0.001238\n",
              "Tokenizer's normalization preprocessor cause misalignment in return_offsets_mapping for tokenizer classification task                                                                                                                                                           0.001238\n",
              "Logging cannot be set to NOTSET similar to transformers                                                                                                                                                                                                                         0.001238\n",
              "Documentation Mistakes in Dataset: emotion                                                                                                                                                                                                                                      0.001238\n",
              "datasets.map pickle issue resulting in invalid mapping function                                                                                                                                                                                                                 0.001238\n",
              "Can datasets remove duplicated rows?                                                                                                                                                                                                                                            0.001238\n",
              "Corelation should be Correlation                                                                                                                                                                                                                                                0.001238\n",
              "seqeval metric does not work with a recent version of sklearn: classification_report() got an unexpected keyword argument 'output_dict'                                                                                                                                         0.001238\n",
              "Add C4                                                                                                                                                                                                                                                                          0.001238\n",
              "SubjQA wrong boolean values in entries                                                                                                                                                                                                                                          0.001238\n",
              "`Proto_qa` hosting seems to be broken                                                                                                                                                                                                                                           0.001238\n",
              " Python Programming Puzzles                                                                                                                                                                                                                                                     0.001238\n",
              "Improve torch formatting performance                                                                                                                                                                                                                                            0.001238\n",
              "Implement loading a dataset builder                                                                                                                                                                                                                                             0.001238\n",
              "Delete extracted files to save disk space                                                                                                                                                                                                                                       0.001238\n",
              "Set download/extracted paths configurable                                                                                                                                                                                                                                       0.001238\n",
              "Issue in timit_asr database                                                                                                                                                                                                                                                     0.001238\n",
              "cache_dir parameter for load_from_disk ?                                                                                                                                                                                                                                        0.001238\n",
              "Fix automatic generation of Zenodo DOI                                                                                                                                                                                                                                          0.001238\n",
              "Crash when `num_proc` > dataset length for `map()` on a `datasets.Dataset`.                                                                                                                                                                                                     0.001238\n",
              "ArrowDataset.save_to_disk produces files that cannot be read using pyarrow.feather                                                                                                                                                                                              0.001238\n",
              "Loading dataset from local path                                                                                                                                                                                                                                                 0.001238\n",
              "Trying to use metric.compute but get OSError                                                                                                                                                                                                                                    0.001238\n",
              "Update Dataset.dataset_size after transformed with map                                                                                                                                                                                                                          0.001238\n",
              "Synchronize table metadata with features                                                                                                                                                                                                                                        0.001238\n",
              "DatasetDict save load Failing test in 1.6 not in 1.5                                                                                                                                                                                                                            0.001238\n",
              "NewsPH NLI dataset script fails to access test data.                                                                                                                                                                                                                            0.001238\n",
              "Running `datase.map` with `num_proc > 1` uses a lot of memory                                                                                                                                                                                                                   0.001238\n",
              "Slow dataloading with big datasets issue persists                                                                                                                                                                                                                               0.001238\n",
              "some issue in loading local txt file as Dataset for run_mlm.py                                                                                                                                                                                                                  0.001238\n",
              "Map is slow and processes batches one after another                                                                                                                                                                                                                             0.001238\n",
              "Link to datasets viwer on Quick Tour page returns \"502 Bad Gateway\"                                                                                                                                                                                                             0.001238\n",
              "Error loading wikihow dataset                                                                                                                                                                                                                                                   0.001238\n",
              "Keys yielded while generating dataset are not being checked                                                                                                                                                                                                                     0.001238\n",
              "How to Add New Metrics Guide                                                                                                                                                                                                                                                    0.001238\n",
              "`xnli` dataset creating a tuple key while yielding instead of `str` or `int`                                                                                                                                                                                                    0.001238\n",
              "Batched map fails when removing all columns                                                                                                                                                                                                                                     0.001238\n",
              "Duplicates in the LAMA dataset                                                                                                                                                                                                                                                  0.001238\n",
              "load_metric error: module 'datasets.utils.file_utils' has no attribute 'add_start_docstrings'                                                                                                                                                                                   0.001238\n",
              "Can't reach \"https://storage.googleapis.com/illuin/fquad/train.json.zip\" when trying to load fquad dataset                                                                                                                                                                      0.001238\n",
              "Getting checksum error when trying to load lc_quad dataset                                                                                                                                                                                                                      0.001238\n",
              "dataloading slow when using HUGE dataset                                                                                                                                                                                                                                        0.001238\n",
              "making labels consistent across the datasets                                                                                                                                                                                                                                    0.001238\n",
              "Got pyarrow error when loading a dataset while adding special tokens into the tokenizer                                                                                                                                                                                         0.001238\n",
              "Bug in Dataset.class_encode_column                                                                                                                                                                                                                                              0.001238\n",
              "SNLI dataset has labels of -1                                                                                                                                                                                                                                                   0.001238\n",
              "concatenate_datasets loads all the data into memory                                                                                                                                                                                                                             0.001238\n",
              "Loss result inGptNeoForCasual                                                                                                                                                                                                                                                   0.001238\n",
              "`FaissIndex.save` throws error on GPU                                                                                                                                                                                                                                           0.001238\n",
              "Add an API to access the language and pretty name of a dataset                                                                                                                                                                                                                  0.001238\n",
              "[Question] How to move and reuse preprocessed dataset?                                                                                                                                                                                                                          0.001238\n",
              "Is there a way to join multiple datasets in one?                                                                                                                                                                                                                                0.001238\n",
              "NonMatchingChecksumError for web_of_science dataset                                                                                                                                                                                                                             0.001238\n",
              "Allow passing `desc` to `tqdm` in `Dataset.map()`                                                                                                                                                                                                                               0.001238\n",
              "A syntax error in example                                                                                                                                                                                                                                                       0.001238\n",
              "load_dataset(\"timit_asr\") gives back duplicates of just one sample text                                                                                                                                                                                                         0.001238\n",
              "Calls to map are not cached.                                                                                                                                                                                                                                                    0.001238\n",
              "UnicodeDecodeError for OSCAR (Afrikaans)                                                                                                                                                                                                                                        0.001238\n",
              "[api request] API to obtain \"dataset_module\" dynamic path?                                                                                                                                                                                                                      0.001238\n",
              "Incorrect version specification for pyarrow                                                                                                                                                                                                                                     0.001238\n",
              "Add COCO evaluation metrics                                                                                                                                                                                                                                                     0.001238\n",
              "Unable to setup dev env on Windows                                                                                                                                                                                                                                              0.001238\n",
              "Add VoxPopuli                                                                                                                                                                                                                                                                   0.001238\n",
              "Slow #0 when using map to tokenize.                                                                                                                                                                                                                                             0.001238\n",
              "Load_dataset for local CSV files                                                                                                                                                                                                                                                0.001238\n",
              "Help understanding how to build a dataset for language modeling as with the old TextDataset                                                                                                                                                                                     0.001238\n",
              "Compatibility with Ubuntu 18 and GLIBC 2.27?                                                                                                                                                                                                                                    0.001238\n",
              "Loading partial dataset when debugging                                                                                                                                                                                                                                          0.001238\n",
              "`datasets.keyhash.DuplicatedKeysError` for `drop` and `adversarial_qa/adversarialQA`                                                                                                                                                                                            0.001238\n",
              "switching some low-level log.info's to log.debug?                                                                                                                                                                                                                               0.001238\n",
              "Update CommonVoice with new release                                                                                                                                                                                                                                             0.001238\n",
              "the meteor metric seems not consist with the official version                                                                                                                                                                                                                   0.001238\n",
              "Unexpected type after `concatenate_datasets`                                                                                                                                                                                                                                    0.001238\n",
              "Second concatenation of datasets produces errors                                                                                                                                                                                                                                0.001238\n",
              "Raise a proper exception when trying to stream a dataset that requires to manually download files                                                                                                                                                                               0.001238\n",
              "Cannot load `few-nerd` dataset                                                                                                                                                                                                                                                  0.001238\n",
              "Dataset JSON is incorrect                                                                                                                                                                                                                                                       0.001238\n",
              "Improve detection of streamable file types                                                                                                                                                                                                                                      0.001238\n",
              "SacreBLEU update                                                                                                                                                                                                                                                                0.001238\n",
              "Add Microsoft Building Footprints dataset                                                                                                                                                                                                                                       0.001238\n",
              "Concurrent use of same dataset (already downloaded)                                                                                                                                                                                                                             0.001238\n",
              "cannot combine splits merging and streaming?                                                                                                                                                                                                                                    0.001238\n",
              "Error in loading the Arabic Billion Words Corpus                                                                                                                                                                                                                                0.001238\n",
              "404 Error when loading remote data files from private repo                                                                                                                                                                                                                      0.001238\n",
              "Missing cache file                                                                                                                                                                                                                                                              0.001238\n",
              "Calling shuffle on IterableDataset will disable batching in case any functions were mapped                                                                                                                                                                                      0.001238\n",
              "add more precise information for size                                                                                                                                                                                                                                           0.001238\n",
              "Missing documentation for wnut_17 (ner_tags)                                                                                                                                                                                                                                    0.001238\n",
              "QASC: incomplete training set                                                                                                                                                                                                                                                   0.001238\n",
              "404 Not Found Error when loading LAMA dataset                                                                                                                                                                                                                                   0.001238\n",
              "404 not found error on loading WIKIANN dataset                                                                                                                                                                                                                                  0.001238\n",
              "Error loading C4 realnewslike dataset                                                                                                                                                                                                                                           0.001238\n",
              "Add RVL-CDIP dataset                                                                                                                                                                                                                                                            0.001238\n",
              "English wikipedia datasets is not clean                                                                                                                                                                                                                                         0.001238\n",
              "BERTScore Error                                                                                                                                                                                                                                                                 0.001238\n",
              "Negative timezone                                                                                                                                                                                                                                                               0.001238\n",
              "always requiring the username in the dataset name when there is one                                                                                                                                                                                                             0.001238\n",
              "OpenWebText: NonMatchingSplitsSizesError                                                                                                                                                                                                                                        0.001238\n",
              "prepare_module issue when loading from read-only fs                                                                                                                                                                                                                             0.001238\n",
              "ArrowInvalid when mapping dataset with missing values                                                                                                                                                                                                                           0.001238\n",
              "Add a Text Classification dataset: KanHope                                                                                                                                                                                                                                      0.001238\n",
              "The datasets.map function does not load cached dataset after moving python script                                                                                                                                                                                               0.001238\n",
              "HF_DATASETS_CACHE variable in Windows                                                                                                                                                                                                                                           0.001238\n",
              "Cannot load linnaeus dataset                                                                                                                                                                                                                                                    0.001238\n",
              "Downloading “reddit” dataset keeps timing out.                                                                                                                                                                                                                                  0.001238\n",
              "cannot load data from my loacal path                                                                                                                                                                                                                                            0.001238\n",
              "Add Mostly Basic Python Problems Dataset                                                                                                                                                                                                                                        0.001238\n",
              "Remove compression from xopen                                                                                                                                                                                                                                                   0.001238\n",
              "Loading JSON throws ArrowNotImplementedError                                                                                                                                                                                                                                    0.001238\n",
              "How to sample every file in a list of files making up a split in a dataset when loading?                                                                                                                                                                                        0.001238\n",
              "ConnectionError: Couldn't reach https://raw.githubusercontent.com                                                                                                                                                                                                               0.001238\n",
              "`generate_random_fingerprint()` deterministic with 🤗Transformers' `set_seed()`                                                                                                                                                                                                  0.001238\n",
              "`ArrowInvalid: Added column's length must match table's length.` after using `select`                                                                                                                                                                                           0.001238\n",
              "equal operation to perform unbatch for huggingface datasets                                                                                                                                                                                                                     0.001238\n",
              "from datasets import Dataset is failing                                                                                                                                                                                                                                         0.001238\n",
              "Cannot import load_dataset on Colab                                                                                                                                                                                                                                             0.001238\n",
              "Dataset load_from_disk is too slow                                                                                                                                                                                                                                              0.001238\n",
              "sqaud_v2 dataset contains misalignment between the answer text and the context value at the answer index                                                                                                                                                                        0.001238\n",
              "`filelock.py` Error                                                                                                                                                                                                                                                             0.001238\n",
              "Jsonlines export error                                                                                                                                                                                                                                                          0.001238\n",
              "Streaming local gzip compressed JSON line files is not working                                                                                                                                                                                                                  0.001238\n",
              "[Metrics] addition of wiki_split metrics                                                                                                                                                                                                                                        0.001238\n",
              "Add option to delete temporary files (e.g. extracted files) when loading dataset                                                                                                                                                                                                0.001238\n",
              "Unable to download omp dataset                                                                                                                                                                                                                                                  0.001238\n",
              "Transformer Class on dataset                                                                                                                                                                                                                                                    0.001238\n",
              "ModuleNotFoundError: No module named 'datasets.tasks' while importing common voice datasets                                                                                                                                                                                     0.001238\n",
              "Cached dataset overflowing disk space                                                                                                                                                                                                                                           0.001238\n",
              "Error iteration over IterableDataset using Torch DataLoader                                                                                                                                                                                                                     0.001238\n",
              "xtreme / pan-x cannot be downloaded                                                                                                                                                                                                                                             0.001238\n",
              "Finding right block-size with JSON loading difficult for user                                                                                                                                                                                                                   0.001238\n",
              "Weights of model checkpoint not initialized for RobertaModel for Bertscore                                                                                                                                                                                                      0.001238\n",
              "Existing cache for local dataset builder file updates is ignored with `ignore_verifications=True`                                                                                                                                                                               0.001238\n",
              "Memory usage consistently increases when processing a dataset with `.map`                                                                                                                                                                                                       0.001238\n",
              "Multilabel metrics not supported                                                                                                                                                                                                                                                0.001238\n",
              "load_dataset(\"web_nlg\") NonMatchingChecksumError                                                                                                                                                                                                                                0.001238\n",
              "Keys should be unique error on code_search_net                                                                                                                                                                                                                                  0.001238\n",
              "Handling unlabeled datasets                                                                                                                                                                                                                                                     0.001238\n",
              "Field order issue in loading json                                                                                                                                                                                                                                               0.001238\n",
              "Integration with AugLy                                                                                                                                                                                                                                                          0.001238\n",
              "can't set verbosity for `metric.py`                                                                                                                                                                                                                                             0.001238\n",
              "Load datasets from the Hub without requiring a dataset script                                                                                                                                                                                                                   0.001238\n",
              "Progress bars are not properly rendered in Jupyter notebook                                                                                                                                                                                                                     0.001238\n",
              "cannot save the dataset to disk after rename_column                                                                                                                                                                                                                             0.001238\n",
              "hebrew language codes he and iw should be treated as aliases                                                                                                                                                                                                                    0.001238\n",
              "5 duplicate datasets                                                                                                                                                                                                                                                            0.001238\n",
              "Cannot load the blog_authorship_corpus due to codec errors                                                                                                                                                                                                                      0.001238\n",
              "Import Error in Kaggle notebook                                                                                                                                                                                                                                                 0.001238\n",
              "Error when downloading C4                                                                                                                                                                                                                                                       0.001238\n",
              "Metric kwargs are not passed to underlying external metric f1_score                                                                                                                                                                                                             0.001238\n",
              "[`to_json`] add multi-proc sharding support                                                                                                                                                                                                                                     0.001238\n",
              "Allow the selection of multiple columns at once                                                                                                                                                                                                                                 0.001238\n",
              "Give a user feedback if the dataset he loads is streamable or not                                                                                                                                                                                                               0.001238\n",
              "Add SD task for SUPERB                                                                                                                                                                                                                                                          0.001238\n",
              "Setting log level higher than warning does not suppress progress bar                                                                                                                                                                                                            0.001238\n",
              "Add web_split dataset for Paraphase and Rephrase benchmark                                                                                                                                                                                                                      0.001238\n",
              "downloading of yahoo_answers_topics dataset failed                                                                                                                                                                                                                              0.001238\n",
              "load_dataset processing failed with OS error after downloading a dataset                                                                                                                                                                                                        0.001238\n",
              "Batched `map` not allowed to return 0 items                                                                                                                                                                                                                                     0.001238\n",
              "Enum used in map functions will raise a RecursionError with dill.                                                                                                                                                                                                               0.001238\n",
              "Support multi-worker with streaming dataset (IterableDataset).                                                                                                                                                                                                                  0.001238\n",
              "load_dataset(\"financial_phrasebank\") NonMatchingChecksumError                                                                                                                                                                                                                   0.001238\n",
              "_prepare_split will overwrite DatasetBuilder.info.features                                                                                                                                                                                                                      0.001238\n",
              "`load_dataset` caches two arrow files?                                                                                                                                                                                                                                          0.001238\n",
              "KeyError: '_indices_files' in `arrow_dataset.py`                                                                                                                                                                                                                                0.001238\n",
              "Error \"in void don't know how to serialize this type of index\" when saving index to disk when device=0 (GPU)                                                                                                                                                                    0.001238\n",
              "benchmarking against MMapIndexedDataset                                                                                                                                                                                                                                         0.001238\n",
              "wmt19 is broken                                                                                                                                                                                                                                                                 0.001238\n",
              "request to mirror wmt datasets, as they are really slow to download                                                                                                                                                                                                             0.001238\n",
              "Allow concatenation of both in-memory and on-disk datasets                                                                                                                                                                                                                      0.001238\n",
              " load_dataset(\"multi_woz_v22\") NonMatchingChecksumError                                                                                                                                                                                                                         0.001238\n",
              "Adding a new column to the dataset after set_format was called                                                                                                                                                                                                                  0.001238\n",
              "ERROR WHEN USING SET_TRANSFORM()                                                                                                                                                                                                                                                0.001238\n",
              "Add Winogender Schemas                                                                                                                                                                                                                                                          0.001238\n",
              "Add WikiCREM                                                                                                                                                                                                                                                                    0.001238\n",
              "Unable to upload \"community provided\" dataset - 400 Client Error                                                                                                                                                                                                                0.001238\n",
              "Regarding On-the-fly Data Loading                                                                                                                                                                                                                                               0.001238\n",
              "load_dataset(\"amazon_polarity\") NonMatchingChecksumError                                                                                                                                                                                                                        0.001238\n",
              "Feature Request: Dataset.add_item                                                                                                                                                                                                                                               0.001238\n",
              "Add TIMIT                                                                                                                                                                                                                                                                       0.001238\n",
              "Update Open Subtitles corpus with original sentence IDs                                                                                                                                                                                                                         0.001238\n",
              "MustC Speech Translation                                                                                                                                                                                                                                                        0.001238\n",
              "Add common voice                                                                                                                                                                                                                                                                0.001238\n",
              "Add tedlium                                                                                                                                                                                                                                                                     0.001238\n",
              "test.json has been removed from the limit dataset repo (breaks dataset)                                                                                                                                                                                                         0.001238\n",
              "Some question about raw dataset download info in the project .                                                                                                                                                                                                                  0.001238\n",
              "Bug Report: timestamp[ns] not recognized                                                                                                                                                                                                                                        0.001238\n",
              "ALT dataset has repeating instances in all splits                                                                                                                                                                                                                               0.001238\n",
              "Feature Request: Support for Pandas `Categorical`                                                                                                                                                                                                                               0.001238\n",
              "DBPedia14 Dataset Checksum bug?                                                                                                                                                                                                                                                 0.001238\n",
              "Bug in skip_rows argument of load_dataset function ?                                                                                                                                                                                                                            0.001238\n",
              "XSum dataset download link broken                                                                                                                                                                                                                                               0.001238\n",
              "[distributed env] potentially unsafe parallel execution                                                                                                                                                                                                                         0.001238\n",
              "add a new column                                                                                                                                                                                                                                                                0.001238\n",
              "Enable Fast Filtering using Arrow Dataset                                                                                                                                                                                                                                       0.001238\n",
              "dataset loading logger level                                                                                                                                                                                                                                                    0.001238\n",
              "AttributeError: 'DatasetDict' object has no attribute 'concatenate_datasets'                                                                                                                                                                                                    0.001238\n",
              "[experiment] missing default_experiment-1-0.arrow                                                                                                                                                                                                                               0.001238\n",
              "Loading of FAISS index fails for index_name = 'exact'                                                                                                                                                                                                                           0.001238\n",
              "Side effect when filtering data due to `does_function_return_dict` call in `Dataset.map()`                                                                                                                                                                                      0.001238\n",
              "[firewalled env] OFFLINE mode                                                                                                                                                                                                                                                   0.001238\n",
              "CommonGen dataset page shows an error OSError: [Errno 28] No space left on device                                                                                                                                                                                               0.001238\n",
              "Add Stanford Sentiment Treebank (SST)                                                                                                                                                                                                                                           0.001238\n",
              "Anonymous Dataset Addition (i.e Anonymous PR?)                                                                                                                                                                                                                                  0.001238\n",
              "How to update the \"wino_bias\" dataset                                                                                                                                                                                                                                           0.001238\n",
              "Failure to save with save_to_disk                                                                                                                                                                                                                                               0.001238\n",
              "UnicodeDecodeError: windows 10 machine                                                                                                                                                                                                                                          0.001238\n",
              "Unable to download `wiki_dpr`                                                                                                                                                                                                                                                   0.001238\n",
              "Saving processed dataset running infinitely                                                                                                                                                                                                                                     0.001238\n",
              "using map on loaded Tokenizer 10x - 100x slower than default Tokenizer?                                                                                                                                                                                                         0.001238\n",
              "Datasets library not suitable for huge text datasets.                                                                                                                                                                                                                           0.001238\n",
              "Datasets.py function load_dataset does not match squad dataset                                                                                                                                                                                                                  0.001238\n",
              "error when run fine_tuning on text_classification                                                                                                                                                                                                                               0.001238\n",
              "Error iterating over Dataset with DataLoader                                                                                                                                                                                                                                    0.001238\n",
              "Connection Issues                                                                                                                                                                                                                                                               0.001238\n",
              "Unable to format dataset to CUDA Tensors                                                                                                                                                                                                                                        0.001238\n",
              "wikipedia dataset incomplete                                                                                                                                                                                                                                                    0.001238\n",
              "dataset.search() (elastic) cannot reliably retrieve search results                                                                                                                                                                                                              0.001238\n",
              "FewRel                                                                                                                                                                                                                                                                          0.001238\n",
              "Using select/reordering datasets slows operations down immensely                                                                                                                                                                                                                0.001238\n",
              "datasets slicing with seed                                                                                                                                                                                                                                                      0.001238\n",
              "difference between wsc and wsc.fixed for superglue                                                                                                                                                                                                                              0.001238\n",
              "connection issue with glue, what is the data url for glue?                                                                                                                                                                                                                      0.001238\n",
              "Provide better exception message when one of many files results in an exception                                                                                                                                                                                                 0.001238\n",
              "Couldn't reach swda.py                                                                                                                                                                                                                                                          0.001238\n",
              "Is there support for Deep learning datasets?                                                                                                                                                                                                                                    0.001238\n",
              "Add an entry to an arrow dataset                                                                                                                                                                                                                                                0.001238\n",
              "BLEURT score calculation raises UnrecognizedFlagError                                                                                                                                                                                                                           0.001238\n",
              "load the local dataset                                                                                                                                                                                                                                                          0.001238\n",
              "could not run models on a offline server successfully                                                                                                                                                                                                                           0.001238\n",
              "Possible cache miss in datasets                                                                                                                                                                                                                                                 0.001238\n",
              "SciFact dataset - minor changes                                                                                                                                                                                                                                                 0.001238\n",
              "Installation using conda                                                                                                                                                                                                                                                        0.001238\n",
              "Issues when run two programs compute the same metrics                                                                                                                                                                                                                           0.001238\n",
              "_pickle.PicklingError: Can't pickle typing.Union[str, NoneType]: it's not the same object as typing.Union when calling datasets.map with num_proc=2                                                                                                                             0.001238\n",
              "how can I combine 2 dataset with different/same features?                                                                                                                                                                                                                       0.001238\n",
              "Couldn't reach https://raw.githubusercontent.com/huggingface/datasets/1.2.1/datasets/csv/csv.py                                                                                                                                                                                 0.001238\n",
              "Loading local dataset raise requests.exceptions.ConnectTimeout                                                                                                                                                                                                                  0.001238\n",
              "pyarrow.lib.ArrowInvalid: Column 1 named input_ids expected length 599 but got length 1500                                                                                                                                                                                      0.001238\n",
              "Unable to add Multi-label Datasets                                                                                                                                                                                                                                              0.001238\n",
              "Add Hateful Memes Dataset                                                                                                                                                                                                                                                       0.001238\n",
              "writing Datasets in a human readable format                                                                                                                                                                                                                                     0.001238\n",
              "can't pickle SwigPyObject objects when calling dataset.get_nearest_examples from FAISS index                                                                                                                                                                                    0.001238\n",
              "Querying examples from big datasets is slower than small datasets                                                                                                                                                                                                               0.001238\n",
              "Connection error                                                                                                                                                                                                                                                                0.001238\n",
              "Filter on dataset too much slowww                                                                                                                                                                                                                                               0.001238\n",
              "ModuleNotFoundError: No module named 'apache_beam', when specific languages.                                                                                                                                                                                                    0.001238\n",
              "How to use split dataset                                                                                                                                                                                                                                                        0.001238\n",
              "Not enough disk space (Needed: Unknown size) when caching on a cluster                                                                                                                                                                                                          0.001238\n",
              "JSONDecodeError on JSON with multiple lines                                                                                                                                                                                                                                     0.001238\n",
              "Dataset Examples Explorer                                                                                                                                                                                                                                                       0.001238\n",
              "AttributeError: module 'pyarrow' has no attribute 'PyExtensionType' during import                                                                                                                                                                                               0.001238\n",
              "[Question & Bug Report] Can we preprocess a dataset on the fly?                                                                                                                                                                                                                 0.001238\n",
              "Efficient ways to iterate the dataset                                                                                                                                                                                                                                           0.001238\n",
              "is it possible to make slice to be more compatible like python list and numpy?                                                                                                                                                                                                  0.001238\n",
              "bug in loading datasets                                                                                                                                                                                                                                                         0.001238\n",
              "bug in SNLI dataset                                                                                                                                                                                                                                                             0.001238\n",
              "Can we parallelized the add_faiss_index process over dataset shards ?                                                                                                                                                                                                           0.001238\n",
              "py3.7: TypeError: can't pickle _LazyModule objects                                                                                                                                                                                                                              0.001238\n",
              "Adding ScaNN library to do MIPS?                                                                                                                                                                                                                                                0.001238\n",
              "en language data from MLQA dataset is missing                                                                                                                                                                                                                                   0.001238\n",
              "Saving large in-memory datasets with save_to_disk crashes because of pickling                                                                                                                                                                                                   0.001238\n",
              "bug in mlqa dataset                                                                                                                                                                                                                                                             0.001238\n",
              "TydiQA dataset is mixed and is not split per language                                                                                                                                                                                                                           0.001238\n",
              "When training with Multi-Node Multi-GPU the worker 2 has TypeError: 'NoneType' object                                                                                                                                                                                           0.001238\n",
              "wikiann dataset is missing columns                                                                                                                                                                                                                                              0.001238\n",
              "How to train BERT model with next sentence prediction?                                                                                                                                                                                                                          0.001238\n",
              "Dialogue action slot name and value are reversed in MultiWoZ 2.2                                                                                                                                                                                                                0.001238\n",
              "Is dataset timit_asr broken?                                                                                                                                                                                                                                                    0.001238\n",
              "Problem downloading GEM wiki_auto_asset_turk dataset                                                                                                                                                                                                                            0.001238\n",
              "Add documentaton for dataset README.md files                                                                                                                                                                                                                                    0.001238\n",
              "dataset viewer does not work anymore                                                                                                                                                                                                                                            0.001238\n",
              "load_metric from local \"glue.py\" meet error 'NoneType' object is not callable                                                                                                                                                                                                   0.001238\n",
              "Creating custom dataset results in error while calling the map() function                                                                                                                                                                                                       0.001238\n",
              "WMT19 Dataset for Kazakh-English is not formatted correctly                                                                                                                                                                                                                     0.001238\n",
              "Request to remove S2ORC dataset                                                                                                                                                                                                                                                 0.001238\n",
              "Trouble loading wiki_movies                                                                                                                                                                                                                                                     0.001238\n",
              "citation, homepage, and license fields of `dataset_info.json` are duplicated many times                                                                                                                                                                                         0.001238\n",
              "load_from_disk takes a long time to load local dataset                                                                                                                                                                                                                          0.001238\n",
              "SQuAD version                                                                                                                                                                                                                                                                   0.001238\n",
              "TypeError when using save_to_disk in a dataset loaded with ReadInstruction split                                                                                                                                                                                                0.001238\n",
              "Loading wikipedia 20200501.en throws pyarrow related error                                                                                                                                                                                                                      0.001238\n",
              "Dataset file size on disk is very large with 3D Array                                                                                                                                                                                                                           0.001238\n",
              "Add configurable options to `seqeval` metric                                                                                                                                                                                                                                    0.001238\n",
              "Filtering/mapping on one column is very slow                                                                                                                                                                                                                                    0.001238\n",
              "News_commentary Dataset Translation Pairs are of Incorrect Language Specified Pairs                                                                                                                                                                                             0.001238\n",
              "save_to_disk doesn't work when we use concatenate_datasets function before creating the final dataset_object.                                                                                                                                                                   0.001238\n",
              "Duplicate data in Timit dataset                                                                                                                                                                                                                                                 0.001238\n",
              "Question (potential issue?) related to datasets caching                                                                                                                                                                                                                         0.001238\n",
              ".map() and distributed training                                                                                                                                                                                                                                                 0.001238\n",
              "Error when loading a HUGE json file (pyarrow.lib.ArrowInvalid: straddling object straddles two block boundaries)                                                                                                                                                                0.001238\n",
              "Converting a Value to a ClassLabel                                                                                                                                                                                                                                              0.001238\n",
              "dataset.search_batch() function outputs all -1 indices sometime.                                                                                                                                                                                                                0.001238\n",
              "Wikipedia historic dumps are deleted but hf/datasets hardcodes dump date                                                                                                                                                                                                        0.001238\n",
              "Regarding Test Sets for the GEM datasets                                                                                                                                                                                                                                        0.001238\n",
              "How to convert datasets.arrow_dataset.Dataset to torch.utils.data.Dataset                                                                                                                                                                                                       0.001238\n",
              "visualization for cc100 is broken                                                                                                                                                                                                                                               0.001238\n",
              "any possibility to download part of large datasets only?                                                                                                                                                                                                                        0.001238\n",
              "data_args.preprocessing_num_workers almost freezes                                                                                                                                                                                                                              0.001238\n",
              "adding ccnet dataset                                                                                                                                                                                                                                                            0.001238\n",
              "viewer \"fake_news_english\" error                                                                                                                                                                                                                                                0.001238\n",
              "load_dataset ignoring features                                                                                                                                                                                                                                                  0.001238\n",
              "Telugu subset missing for xtreme tatoeba dataset                                                                                                                                                                                                                                0.001238\n",
              "How to disable making arrow tables in load_dataset ?                                                                                                                                                                                                                            0.001238\n",
              "CUAD - Contract Understanding Atticus Dataset                                                                                                                                                                                                                                   0.001238\n",
              "'Dataset' object has no attribute 'rename_column'                                                                                                                                                                                                                               0.001238\n",
              "Windows Permission Error (most recent version of datasets)                                                                                                                                                                                                                      0.001238\n",
              "KeyError on using map after renaming a column                                                                                                                                                                                                                                   0.001238\n",
              "ValueError when rename_column on splitted dataset                                                                                                                                                                                                                               0.001238\n",
              "Interactively doing  save_to_disk and load_from_disk corrupts the datasets object?                                                                                                                                                                                              0.001238\n",
              "No upstream branch                                                                                                                                                                                                                                                              0.001238\n",
              "Local testing fails                                                                                                                                                                                                                                                             0.001238\n",
              "Ambiguous documentation                                                                                                                                                                                                                                                         0.001238\n",
              "How to not load huggingface datasets into memory                                                                                                                                                                                                                                0.001238\n",
              "Setting to torch format not working with torchvision and MNIST                                                                                                                                                                                                                  0.001238\n",
              "Messages are being printed to the `stdout`                                                                                                                                                                                                                                      0.001238\n",
              "Error when exploring `arabic_speech_corpus`                                                                                                                                                                                                                                     0.001238\n",
              "`concatenate_datasets` throws error when changing the order of datasets to concatenate                                                                                                                                                                                          0.001238\n",
              "not being able to get wikipedia es language                                                                                                                                                                                                                                     0.001238\n",
              "How to load a dataset with load_from disk and save it again after doing transformations without changing the original?                                                                                                                                                          0.001238\n",
              "`datasets.map` multi processing much slower than single processing                                                                                                                                                                                                              0.001238\n",
              "OSError: Memory mapping file failed: Cannot allocate memory                                                                                                                                                                                                                     0.001238\n",
              "Question/problem with dataset labels                                                                                                                                                                                                                                            0.001238\n",
              "Readme.md is misleading about kinds of datasets?                                                                                                                                                                                                                                0.001238\n",
              "The size of CoNLL-2003 is not consistant with the official release.                                                                                                                                                                                                             0.001238\n",
              "ModuleNotFoundError: No module named 'apache_beam' for wikipedia datasets                                                                                                                                                                                                       0.001238\n",
              "Question: what gets stored in the datasets cache and why is it so huge?                                                                                                                                                                                                         0.001238\n",
              "Loading a faiss index KeyError                                                                                                                                                                                                                                                  0.001238\n",
              "wikipedia.py generator that extracts XML doesn't release memory                                                                                                                                                                                                                 0.001238\n",
              "wiki40b/wikipedia for almost all languages cannot be downloaded                                                                                                                                                                                                                 0.001238\n",
              "Cannot load wikitext                                                                                                                                                                                                                                                            0.001238\n",
              "Multidimensional arrays in a Dataset                                                                                                                                                                                                                                            0.001238\n",
              "MemoryError when computing WER metric                                                                                                                                                                                                                                           0.001238\n",
              "Issue: Dataset download error                                                                                                                                                                                                                                                   0.001238\n",
              "ConnectionError: Couldn't reach common_voice.py                                                                                                                                                                                                                                 0.001238\n",
              "Multiprocessing is slower than single process                                                                                                                                                                                                                                   0.001238\n",
              "ArrowInvalid issue for squad v2 dataset                                                                                                                                                                                                                                         0.001238\n",
              "PyTorch not available error on SageMaker GPU docker though it is installed                                                                                                                                                                                                      0.001238\n",
              "Multiprocessing windows error                                                                                                                                                                                                                                                   0.001238\n",
              "Only user permission of saved cache files, not group                                                                                                                                                                                                                            0.001238\n",
              "Cannot load udpos subsets from xtreme dataset using load_dataset()                                                                                                                                                                                                              0.001238\n",
              "Error while following docs to load the `ted_talks_iwslt` dataset                                                                                                                                                                                                                0.001238\n",
              "issue with opus100/en-fr dataset                                                                                                                                                                                                                                                0.001238\n",
              "is there a way to override a dataset object saved with save_to_disk?                                                                                                                                                                                                            0.001238\n",
              "Could not find file for ZEST dataset                                                                                                                                                                                                                                            0.001238\n",
              "Timit_asr dataset repeats examples                                                                                                                                                                                                                                              0.001238\n",
              "Build custom dataset to fine-tune Wav2Vec2                                                                                                                                                                                                                                      0.001238\n",
              "add_faisis_index  gets very slow when doing it interatively                                                                                                                                                                                                                     0.001238\n",
              "ValueError: datasets' indices [1] come from memory and datasets' indices [0] come from disk                                                                                                                                                                                     0.001238\n",
              "outdated dataset_infos.json might fail verifications                                                                                                                                                                                                                            0.001238\n",
              "Issue to read a local dataset                                                                                                                                                                                                                                                   0.001238\n",
              "Name: title, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query: str = \"How does processing in a batch work\"\n",
        "get_query_matches(query=query, K=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "IkCI1Gbu2yQu",
        "outputId": "827f8652-7930-4b8d-8e78-6771f5455925"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "COMMENT: Hi Thomas,\n",
              "what I do not get from documentation is that why when you set \u001b[33mbatched\u001b[0m=\u001b[3;92mTrue\u001b[0m,\n",
              "this is processed in batch, while data is not divided to batched\n",
              "beforehand, basically this is a question on the documentation and I do not\n",
              "get the \u001b[33mbatched\u001b[0m=\u001b[3;92mTrue\u001b[0m, but sure, if you think this is more appropriate in\n",
              "forum I will post it there.\n",
              "thanks\n",
              "Best\n",
              "Rabeeh\n",
              "\n",
              "On Tue, Nov \u001b[1;36m10\u001b[0m, \u001b[1;36m2020\u001b[0m at \u001b[1;92m12:21\u001b[0m PM Thomas Wolf \u001b[1m<\u001b[0m\u001b[1;95mnotifications\u001b[0m\u001b[39m@github.com>\u001b[0m\n",
              "\u001b[39mwrote:\u001b[0m\n",
              "\n",
              "\u001b[39m> Hi I don’t think this is a request for a dataset like you labeled it.\u001b[0m\n",
              "\u001b[39m>\u001b[0m\n",
              "\u001b[39m> I also think this would be better suited for the forum at\u001b[0m\n",
              "\u001b[39m> \u001b[0m\u001b[4;94mhttps://discuss.huggingface.co.\u001b[0m\u001b[39m we try to keep the issue for the repo for\u001b[0m\n",
              "\u001b[39m> bug reports and new features/dataset requests and have usage questions\u001b[0m\n",
              "\u001b[39m> discussed on the forum. Thanks.\u001b[0m\n",
              "\u001b[39m>\u001b[0m\n",
              "\u001b[39m> —\u001b[0m\n",
              "\u001b[39m> You are receiving this because you authored the thread.\u001b[0m\n",
              "\u001b[39m> Reply to this email directly, view it on GitHub\u001b[0m\n",
              "\u001b[39m> <\u001b[0m\u001b[4;94mhttps://github.com/huggingface/datasets/issues/823#issuecomment-724639476\u001b[0m\u001b[39m>,\u001b[0m\n",
              "\u001b[39m> or unsubscribe\u001b[0m\n",
              "\u001b[39m> <\u001b[0m\u001b[4;94mhttps://github.com/notifications/unsubscribe-auth/ARPXHH4FIPFHVVUHANAE4F3SPEO2JANCNFSM4TQQVEXQ\u001b[0m\u001b[39m>\u001b[0m\n",
              "\u001b[39m> .\u001b[0m\n",
              "\u001b[1m>\u001b[0m\n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">COMMENT: Hi Thomas,\n",
              "what I do not get from documentation is that why when you set <span style=\"color: #808000; text-decoration-color: #808000\">batched</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
              "this is processed in batch, while data is not divided to batched\n",
              "beforehand, basically this is a question on the documentation and I do not\n",
              "get the <span style=\"color: #808000; text-decoration-color: #808000\">batched</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>, but sure, if you think this is more appropriate in\n",
              "forum I will post it there.\n",
              "thanks\n",
              "Best\n",
              "Rabeeh\n",
              "\n",
              "On Tue, Nov <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2020</span> at <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:21</span> PM Thomas Wolf <span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">notifications</span><span style=\"color: #000000; text-decoration-color: #000000\">@github.com&gt;</span>\n",
              "<span style=\"color: #000000; text-decoration-color: #000000\">wrote:</span>\n",
              "\n",
              "<span style=\"color: #000000; text-decoration-color: #000000\">&gt; Hi I don’t think this is a request for a dataset like you labeled it.</span>\n",
              "<span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
              "<span style=\"color: #000000; text-decoration-color: #000000\">&gt; I also think this would be better suited for the forum at</span>\n",
              "<span style=\"color: #000000; text-decoration-color: #000000\">&gt; </span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://discuss.huggingface.co.</span><span style=\"color: #000000; text-decoration-color: #000000\"> we try to keep the issue for the repo for</span>\n",
              "<span style=\"color: #000000; text-decoration-color: #000000\">&gt; bug reports and new features/dataset requests and have usage questions</span>\n",
              "<span style=\"color: #000000; text-decoration-color: #000000\">&gt; discussed on the forum. Thanks.</span>\n",
              "<span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
              "<span style=\"color: #000000; text-decoration-color: #000000\">&gt; —</span>\n",
              "<span style=\"color: #000000; text-decoration-color: #000000\">&gt; You are receiving this because you authored the thread.</span>\n",
              "<span style=\"color: #000000; text-decoration-color: #000000\">&gt; Reply to this email directly, view it on GitHub</span>\n",
              "<span style=\"color: #000000; text-decoration-color: #000000\">&gt; &lt;</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/huggingface/datasets/issues/823#issuecomment-724639476</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;,</span>\n",
              "<span style=\"color: #000000; text-decoration-color: #000000\">&gt; or unsubscribe</span>\n",
              "<span style=\"color: #000000; text-decoration-color: #000000\">&gt; &lt;</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/notifications/unsubscribe-auth/ARPXHH4FIPFHVVUHANAE4F3SPEO2JANCNFSM4TQQVEXQ</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
              "<span style=\"color: #000000; text-decoration-color: #000000\">&gt; .</span>\n",
              "<span style=\"font-weight: bold\">&gt;</span>\n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "SCORE: \u001b[1;36m30.65512466430664\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">SCORE: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">30.65512466430664</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "TITLE: how processing in batch works in datasets \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">TITLE: how processing in batch works in datasets \n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "URL: \u001b[4;94mhttps://github.com/huggingface/datasets/issues/823\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">URL: <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/huggingface/datasets/issues/823</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "======================================================================\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">======================================================================\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "COMMENT: Yes the forum is perfect for that. You can post in the `datasets` section.\n",
              "Thanks a lot!\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">COMMENT: Yes the forum is perfect for that. You can post in the `datasets` section.\n",
              "Thanks a lot!\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "SCORE: \u001b[1;36m30.65512466430664\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">SCORE: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">30.65512466430664</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "TITLE: how processing in batch works in datasets \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">TITLE: how processing in batch works in datasets \n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "URL: \u001b[4;94mhttps://github.com/huggingface/datasets/issues/823\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">URL: <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/huggingface/datasets/issues/823</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "======================================================================\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">======================================================================\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "COMMENT: Hi I don’t think this is a request for a dataset like you labeled it.\n",
              "\n",
              "I also think this would be better suited for the forum at \u001b[4;94mhttps://discuss.huggingface.co.\u001b[0m we try to keep the issue \n",
              "for the repo for bug reports and new features/dataset requests and have usage questions discussed on the forum. \n",
              "Thanks.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">COMMENT: Hi I don’t think this is a request for a dataset like you labeled it.\n",
              "\n",
              "I also think this would be better suited for the forum at <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://discuss.huggingface.co.</span> we try to keep the issue \n",
              "for the repo for bug reports and new features/dataset requests and have usage questions discussed on the forum. \n",
              "Thanks.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "SCORE: \u001b[1;36m30.65512466430664\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">SCORE: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">30.65512466430664</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "TITLE: how processing in batch works in datasets \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">TITLE: how processing in batch works in datasets \n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "URL: \u001b[4;94mhttps://github.com/huggingface/datasets/issues/823\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">URL: <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/huggingface/datasets/issues/823</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "======================================================================\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">======================================================================\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             html_url  \\\n",
              "0  https://github.com/huggingface/datasets/issues/823   \n",
              "1  https://github.com/huggingface/datasets/issues/823   \n",
              "2  https://github.com/huggingface/datasets/issues/823   \n",
              "\n",
              "                                        title  \\\n",
              "0  how processing in batch works in datasets    \n",
              "1  how processing in batch works in datasets    \n",
              "2  how processing in batch works in datasets    \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  comments  \\\n",
              "0  Hi Thomas,\\nwhat I do not get from documentation is that why when you set batched=True,\\nthis is processed in batch, while data is not divided to batched\\nbeforehand, basically this is a question on the documentation and I do not\\nget the batched=True, but sure, if you think this is more appropriate in\\nforum I will post it there.\\nthanks\\nBest\\nRabeeh\\n\\nOn Tue, Nov 10, 2020 at 12:21 PM Thomas Wolf <notifications@github.com>\\nwrote:\\n\\n> Hi I don’t think this is a request for a dataset like you labeled it.\\n>\\n> I also think this would be better suited for the forum at\\n> https://discuss....   \n",
              "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Yes the forum is perfect for that. You can post in the `datasets` section.\\r\\nThanks a lot!   \n",
              "2                                                                                                                                                                                                                                                                                                  Hi I don’t think this is a request for a dataset like you labeled it.\\r\\n\\r\\nI also think this would be better suited for the forum at https://discuss.huggingface.co. we try to keep the issue for the repo for bug reports and new features/dataset requests and have usage questions discussed on the forum. Thanks.   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      body  \\\n",
              "0  Hi,\\r\\nI need to process my datasets before it is passed to dataloader in batch, \\r\\nhere is my codes \\r\\n\\r\\n```\\r\\nclass AbstractTask(ABC):\\r\\n    task_name: str = NotImplemented\\r\\n    preprocessor: Callable = NotImplemented\\r\\n    split_to_data_split: Mapping[str, str] = NotImplemented\\r\\n    tokenizer: Callable = NotImplemented\\r\\n    max_source_length: str = NotImplemented\\r\\n    max_target_length: str = NotImplemented\\r\\n    # TODO: should not be a task item, but cannot see other ways.\\r\\n    tpu_num_cores: int = None\\r\\n\\r\\n    # The arguments set are for all tasks and needs to be ...   \n",
              "1  Hi,\\r\\nI need to process my datasets before it is passed to dataloader in batch, \\r\\nhere is my codes \\r\\n\\r\\n```\\r\\nclass AbstractTask(ABC):\\r\\n    task_name: str = NotImplemented\\r\\n    preprocessor: Callable = NotImplemented\\r\\n    split_to_data_split: Mapping[str, str] = NotImplemented\\r\\n    tokenizer: Callable = NotImplemented\\r\\n    max_source_length: str = NotImplemented\\r\\n    max_target_length: str = NotImplemented\\r\\n    # TODO: should not be a task item, but cannot see other ways.\\r\\n    tpu_num_cores: int = None\\r\\n\\r\\n    # The arguments set are for all tasks and needs to be ...   \n",
              "2  Hi,\\r\\nI need to process my datasets before it is passed to dataloader in batch, \\r\\nhere is my codes \\r\\n\\r\\n```\\r\\nclass AbstractTask(ABC):\\r\\n    task_name: str = NotImplemented\\r\\n    preprocessor: Callable = NotImplemented\\r\\n    split_to_data_split: Mapping[str, str] = NotImplemented\\r\\n    tokenizer: Callable = NotImplemented\\r\\n    max_source_length: str = NotImplemented\\r\\n    max_target_length: str = NotImplemented\\r\\n    # TODO: should not be a task item, but cannot see other ways.\\r\\n    tpu_num_cores: int = None\\r\\n\\r\\n    # The arguments set are for all tasks and needs to be ...   \n",
              "\n",
              "   comment_length  \\\n",
              "0             167   \n",
              "1              17   \n",
              "2              53   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      text  \\\n",
              "0  how processing in batch works in datasets  \\n Hi,\\r\\nI need to process my datasets before it is passed to dataloader in batch, \\r\\nhere is my codes \\r\\n\\r\\n```\\r\\nclass AbstractTask(ABC):\\r\\n    task_name: str = NotImplemented\\r\\n    preprocessor: Callable = NotImplemented\\r\\n    split_to_data_split: Mapping[str, str] = NotImplemented\\r\\n    tokenizer: Callable = NotImplemented\\r\\n    max_source_length: str = NotImplemented\\r\\n    max_target_length: str = NotImplemented\\r\\n    # TODO: should not be a task item, but cannot see other ways.\\r\\n    tpu_num_cores: int = None\\r\\n\\r\\n    # The ar...   \n",
              "1  how processing in batch works in datasets  \\n Hi,\\r\\nI need to process my datasets before it is passed to dataloader in batch, \\r\\nhere is my codes \\r\\n\\r\\n```\\r\\nclass AbstractTask(ABC):\\r\\n    task_name: str = NotImplemented\\r\\n    preprocessor: Callable = NotImplemented\\r\\n    split_to_data_split: Mapping[str, str] = NotImplemented\\r\\n    tokenizer: Callable = NotImplemented\\r\\n    max_source_length: str = NotImplemented\\r\\n    max_target_length: str = NotImplemented\\r\\n    # TODO: should not be a task item, but cannot see other ways.\\r\\n    tpu_num_cores: int = None\\r\\n\\r\\n    # The ar...   \n",
              "2  how processing in batch works in datasets  \\n Hi,\\r\\nI need to process my datasets before it is passed to dataloader in batch, \\r\\nhere is my codes \\r\\n\\r\\n```\\r\\nclass AbstractTask(ABC):\\r\\n    task_name: str = NotImplemented\\r\\n    preprocessor: Callable = NotImplemented\\r\\n    split_to_data_split: Mapping[str, str] = NotImplemented\\r\\n    tokenizer: Callable = NotImplemented\\r\\n    max_source_length: str = NotImplemented\\r\\n    max_target_length: str = NotImplemented\\r\\n    # TODO: should not be a task item, but cannot see other ways.\\r\\n    tpu_num_cores: int = None\\r\\n\\r\\n    # The ar...   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                embeddings  \\\n",
              "0  [-0.5023795366287231, -0.17439508438110352, -0.22950220108032227, 0.130157470703125, 0.1780354231595993, 0.03179476410150528, 0.29556161165237427, 0.1713045984506607, -0.1484704315662384, 0.1780766397714615, 0.009121731854975224, 0.12478089332580566, 0.06411994248628616, 0.24562396109104156, -0.056165214627981186, -0.03589145094156265, -0.07459743320941925, 0.048377472907304764, 0.1096092090010643, 0.007234200835227966, -0.36760956048965454, 0.06899745017290115, -0.4043073058128357, 0.05223430320620537, -0.22304658591747284, -0.26935648918151855, -0.061245325952768326, -0.03925704210996628...   \n",
              "1  [-0.5023795366287231, -0.17439508438110352, -0.22950220108032227, 0.130157470703125, 0.1780354231595993, 0.03179476410150528, 0.29556161165237427, 0.1713045984506607, -0.1484704315662384, 0.1780766397714615, 0.009121731854975224, 0.12478089332580566, 0.06411994248628616, 0.24562396109104156, -0.056165214627981186, -0.03589145094156265, -0.07459743320941925, 0.048377472907304764, 0.1096092090010643, 0.007234200835227966, -0.36760956048965454, 0.06899745017290115, -0.4043073058128357, 0.05223430320620537, -0.22304658591747284, -0.26935648918151855, -0.061245325952768326, -0.03925704210996628...   \n",
              "2  [-0.5023795366287231, -0.17439508438110352, -0.22950220108032227, 0.130157470703125, 0.1780354231595993, 0.03179476410150528, 0.29556161165237427, 0.1713045984506607, -0.1484704315662384, 0.1780766397714615, 0.009121731854975224, 0.12478089332580566, 0.06411994248628616, 0.24562396109104156, -0.056165214627981186, -0.03589145094156265, -0.07459743320941925, 0.048377472907304764, 0.1096092090010643, 0.007234200835227966, -0.36760956048965454, 0.06899745017290115, -0.4043073058128357, 0.05223430320620537, -0.22304658591747284, -0.26935648918151855, -0.061245325952768326, -0.03925704210996628...   \n",
              "\n",
              "      scores  \n",
              "0  30.655125  \n",
              "1  30.655125  \n",
              "2  30.655125  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-200f7b03-6cd5-49e9-a160-063493543cfb\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>html_url</th>\n",
              "      <th>title</th>\n",
              "      <th>comments</th>\n",
              "      <th>body</th>\n",
              "      <th>comment_length</th>\n",
              "      <th>text</th>\n",
              "      <th>embeddings</th>\n",
              "      <th>scores</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>https://github.com/huggingface/datasets/issues/823</td>\n",
              "      <td>how processing in batch works in datasets</td>\n",
              "      <td>Hi Thomas,\\nwhat I do not get from documentation is that why when you set batched=True,\\nthis is processed in batch, while data is not divided to batched\\nbeforehand, basically this is a question on the documentation and I do not\\nget the batched=True, but sure, if you think this is more appropriate in\\nforum I will post it there.\\nthanks\\nBest\\nRabeeh\\n\\nOn Tue, Nov 10, 2020 at 12:21 PM Thomas Wolf &lt;notifications@github.com&gt;\\nwrote:\\n\\n&gt; Hi I don’t think this is a request for a dataset like you labeled it.\\n&gt;\\n&gt; I also think this would be better suited for the forum at\\n&gt; https://discuss....</td>\n",
              "      <td>Hi,\\r\\nI need to process my datasets before it is passed to dataloader in batch, \\r\\nhere is my codes \\r\\n\\r\\n```\\r\\nclass AbstractTask(ABC):\\r\\n    task_name: str = NotImplemented\\r\\n    preprocessor: Callable = NotImplemented\\r\\n    split_to_data_split: Mapping[str, str] = NotImplemented\\r\\n    tokenizer: Callable = NotImplemented\\r\\n    max_source_length: str = NotImplemented\\r\\n    max_target_length: str = NotImplemented\\r\\n    # TODO: should not be a task item, but cannot see other ways.\\r\\n    tpu_num_cores: int = None\\r\\n\\r\\n    # The arguments set are for all tasks and needs to be ...</td>\n",
              "      <td>167</td>\n",
              "      <td>how processing in batch works in datasets  \\n Hi,\\r\\nI need to process my datasets before it is passed to dataloader in batch, \\r\\nhere is my codes \\r\\n\\r\\n```\\r\\nclass AbstractTask(ABC):\\r\\n    task_name: str = NotImplemented\\r\\n    preprocessor: Callable = NotImplemented\\r\\n    split_to_data_split: Mapping[str, str] = NotImplemented\\r\\n    tokenizer: Callable = NotImplemented\\r\\n    max_source_length: str = NotImplemented\\r\\n    max_target_length: str = NotImplemented\\r\\n    # TODO: should not be a task item, but cannot see other ways.\\r\\n    tpu_num_cores: int = None\\r\\n\\r\\n    # The ar...</td>\n",
              "      <td>[-0.5023795366287231, -0.17439508438110352, -0.22950220108032227, 0.130157470703125, 0.1780354231595993, 0.03179476410150528, 0.29556161165237427, 0.1713045984506607, -0.1484704315662384, 0.1780766397714615, 0.009121731854975224, 0.12478089332580566, 0.06411994248628616, 0.24562396109104156, -0.056165214627981186, -0.03589145094156265, -0.07459743320941925, 0.048377472907304764, 0.1096092090010643, 0.007234200835227966, -0.36760956048965454, 0.06899745017290115, -0.4043073058128357, 0.05223430320620537, -0.22304658591747284, -0.26935648918151855, -0.061245325952768326, -0.03925704210996628...</td>\n",
              "      <td>30.655125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>https://github.com/huggingface/datasets/issues/823</td>\n",
              "      <td>how processing in batch works in datasets</td>\n",
              "      <td>Yes the forum is perfect for that. You can post in the `datasets` section.\\r\\nThanks a lot!</td>\n",
              "      <td>Hi,\\r\\nI need to process my datasets before it is passed to dataloader in batch, \\r\\nhere is my codes \\r\\n\\r\\n```\\r\\nclass AbstractTask(ABC):\\r\\n    task_name: str = NotImplemented\\r\\n    preprocessor: Callable = NotImplemented\\r\\n    split_to_data_split: Mapping[str, str] = NotImplemented\\r\\n    tokenizer: Callable = NotImplemented\\r\\n    max_source_length: str = NotImplemented\\r\\n    max_target_length: str = NotImplemented\\r\\n    # TODO: should not be a task item, but cannot see other ways.\\r\\n    tpu_num_cores: int = None\\r\\n\\r\\n    # The arguments set are for all tasks and needs to be ...</td>\n",
              "      <td>17</td>\n",
              "      <td>how processing in batch works in datasets  \\n Hi,\\r\\nI need to process my datasets before it is passed to dataloader in batch, \\r\\nhere is my codes \\r\\n\\r\\n```\\r\\nclass AbstractTask(ABC):\\r\\n    task_name: str = NotImplemented\\r\\n    preprocessor: Callable = NotImplemented\\r\\n    split_to_data_split: Mapping[str, str] = NotImplemented\\r\\n    tokenizer: Callable = NotImplemented\\r\\n    max_source_length: str = NotImplemented\\r\\n    max_target_length: str = NotImplemented\\r\\n    # TODO: should not be a task item, but cannot see other ways.\\r\\n    tpu_num_cores: int = None\\r\\n\\r\\n    # The ar...</td>\n",
              "      <td>[-0.5023795366287231, -0.17439508438110352, -0.22950220108032227, 0.130157470703125, 0.1780354231595993, 0.03179476410150528, 0.29556161165237427, 0.1713045984506607, -0.1484704315662384, 0.1780766397714615, 0.009121731854975224, 0.12478089332580566, 0.06411994248628616, 0.24562396109104156, -0.056165214627981186, -0.03589145094156265, -0.07459743320941925, 0.048377472907304764, 0.1096092090010643, 0.007234200835227966, -0.36760956048965454, 0.06899745017290115, -0.4043073058128357, 0.05223430320620537, -0.22304658591747284, -0.26935648918151855, -0.061245325952768326, -0.03925704210996628...</td>\n",
              "      <td>30.655125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>https://github.com/huggingface/datasets/issues/823</td>\n",
              "      <td>how processing in batch works in datasets</td>\n",
              "      <td>Hi I don’t think this is a request for a dataset like you labeled it.\\r\\n\\r\\nI also think this would be better suited for the forum at https://discuss.huggingface.co. we try to keep the issue for the repo for bug reports and new features/dataset requests and have usage questions discussed on the forum. Thanks.</td>\n",
              "      <td>Hi,\\r\\nI need to process my datasets before it is passed to dataloader in batch, \\r\\nhere is my codes \\r\\n\\r\\n```\\r\\nclass AbstractTask(ABC):\\r\\n    task_name: str = NotImplemented\\r\\n    preprocessor: Callable = NotImplemented\\r\\n    split_to_data_split: Mapping[str, str] = NotImplemented\\r\\n    tokenizer: Callable = NotImplemented\\r\\n    max_source_length: str = NotImplemented\\r\\n    max_target_length: str = NotImplemented\\r\\n    # TODO: should not be a task item, but cannot see other ways.\\r\\n    tpu_num_cores: int = None\\r\\n\\r\\n    # The arguments set are for all tasks and needs to be ...</td>\n",
              "      <td>53</td>\n",
              "      <td>how processing in batch works in datasets  \\n Hi,\\r\\nI need to process my datasets before it is passed to dataloader in batch, \\r\\nhere is my codes \\r\\n\\r\\n```\\r\\nclass AbstractTask(ABC):\\r\\n    task_name: str = NotImplemented\\r\\n    preprocessor: Callable = NotImplemented\\r\\n    split_to_data_split: Mapping[str, str] = NotImplemented\\r\\n    tokenizer: Callable = NotImplemented\\r\\n    max_source_length: str = NotImplemented\\r\\n    max_target_length: str = NotImplemented\\r\\n    # TODO: should not be a task item, but cannot see other ways.\\r\\n    tpu_num_cores: int = None\\r\\n\\r\\n    # The ar...</td>\n",
              "      <td>[-0.5023795366287231, -0.17439508438110352, -0.22950220108032227, 0.130157470703125, 0.1780354231595993, 0.03179476410150528, 0.29556161165237427, 0.1713045984506607, -0.1484704315662384, 0.1780766397714615, 0.009121731854975224, 0.12478089332580566, 0.06411994248628616, 0.24562396109104156, -0.056165214627981186, -0.03589145094156265, -0.07459743320941925, 0.048377472907304764, 0.1096092090010643, 0.007234200835227966, -0.36760956048965454, 0.06899745017290115, -0.4043073058128357, 0.05223430320620537, -0.22304658591747284, -0.26935648918151855, -0.061245325952768326, -0.03925704210996628...</td>\n",
              "      <td>30.655125</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-200f7b03-6cd5-49e9-a160-063493543cfb')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-200f7b03-6cd5-49e9-a160-063493543cfb button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-200f7b03-6cd5-49e9-a160-063493543cfb');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-e5814ca6-5e44-4f90-beb4-5ce9ac609dc6\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e5814ca6-5e44-4f90-beb4-5ce9ac609dc6')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-e5814ca6-5e44-4f90-beb4-5ce9ac609dc6 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 732
        },
        "id": "sybyuSCJoMVw",
        "outputId": "b8d98591-9162-4046-b61f-d6614444ca29"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "COMMENT: Yes we are working on the doc right now, should be in the next release quite soon.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">COMMENT: Yes we are working on the doc right now, should be in the next release quite soon.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "SCORE: \u001b[1;36m30.260379791259766\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">SCORE: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">30.260379791259766</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "TITLE: \u001b[1m[\u001b[0mFeature\u001b[1m]\u001b[0m Keep the list of labels of a dataset as metadata\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">TITLE: <span style=\"font-weight: bold\">[</span>Feature<span style=\"font-weight: bold\">]</span> Keep the list of labels of a dataset as metadata\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "URL: \u001b[4;94mhttps://github.com/huggingface/datasets/issues/4\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">URL: <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/huggingface/datasets/issues/4</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "======================================================================\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">======================================================================\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "COMMENT: Hi,\n",
              "I hope we could get a better documentation.\n",
              "It took me more than \u001b[1;36m1\u001b[0m hour to found this way to get the label information.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">COMMENT: Hi,\n",
              "I hope we could get a better documentation.\n",
              "It took me more than <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> hour to found this way to get the label information.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "SCORE: \u001b[1;36m28.216829299926758\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">SCORE: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">28.216829299926758</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "TITLE: \u001b[1m[\u001b[0mFeature\u001b[1m]\u001b[0m Keep the list of labels of a dataset as metadata\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">TITLE: <span style=\"font-weight: bold\">[</span>Feature<span style=\"font-weight: bold\">]</span> Keep the list of labels of a dataset as metadata\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "URL: \u001b[4;94mhttps://github.com/huggingface/datasets/issues/4\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">URL: <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/huggingface/datasets/issues/4</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "======================================================================\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">======================================================================\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "COMMENT: This should be accessible now as a feature in dataset.info.features \u001b[1m(\u001b[0mand even have the mapping methods\u001b[1m)\u001b[0m.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">COMMENT: This should be accessible now as a feature in dataset.info.features <span style=\"font-weight: bold\">(</span>and even have the mapping methods<span style=\"font-weight: bold\">)</span>.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "SCORE: \u001b[1;36m25.969202041625977\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">SCORE: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">25.969202041625977</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "TITLE: \u001b[1m[\u001b[0mFeature\u001b[1m]\u001b[0m Keep the list of labels of a dataset as metadata\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">TITLE: <span style=\"font-weight: bold\">[</span>Feature<span style=\"font-weight: bold\">]</span> Keep the list of labels of a dataset as metadata\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "URL: \u001b[4;94mhttps://github.com/huggingface/datasets/issues/4\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">URL: <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/huggingface/datasets/issues/4</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "======================================================================\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">======================================================================\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                           html_url  \\\n",
              "2  https://github.com/huggingface/datasets/issues/4   \n",
              "1  https://github.com/huggingface/datasets/issues/4   \n",
              "0  https://github.com/huggingface/datasets/issues/4   \n",
              "\n",
              "                                                        title  \\\n",
              "2  [Feature] Keep the list of labels of a dataset as metadata   \n",
              "1  [Feature] Keep the list of labels of a dataset as metadata   \n",
              "0  [Feature] Keep the list of labels of a dataset as metadata   \n",
              "\n",
              "                                                                                                                            comments  \\\n",
              "2                                                 Yes we are working on the doc right now, should be in the next release quite soon.   \n",
              "1  Hi,\\r\\nI hope we could get a better documentation.\\r\\nIt took me more than 1 hour to found this way to get the label information.   \n",
              "0                           This should be accessible now as a feature in dataset.info.features (and even have the mapping methods).   \n",
              "\n",
              "                                                                                                                                         body  \\\n",
              "2  It would be useful to keep the list of the labels of a dataset as metadata. Either directly in the `DatasetInfo` or in the Arrow metadata.   \n",
              "1  It would be useful to keep the list of the labels of a dataset as metadata. Either directly in the `DatasetInfo` or in the Arrow metadata.   \n",
              "0  It would be useful to keep the list of the labels of a dataset as metadata. Either directly in the `DatasetInfo` or in the Arrow metadata.   \n",
              "\n",
              "   comment_length  \\\n",
              "2              17   \n",
              "1              25   \n",
              "0              16   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                            text  \\\n",
              "2                                                 [Feature] Keep the list of labels of a dataset as metadata \\n It would be useful to keep the list of the labels of a dataset as metadata. Either directly in the `DatasetInfo` or in the Arrow metadata. \\n Yes we are working on the doc right now, should be in the next release quite soon.   \n",
              "1  [Feature] Keep the list of labels of a dataset as metadata \\n It would be useful to keep the list of the labels of a dataset as metadata. Either directly in the `DatasetInfo` or in the Arrow metadata. \\n Hi,\\r\\nI hope we could get a better documentation.\\r\\nIt took me more than 1 hour to found this way to get the label information.   \n",
              "0                           [Feature] Keep the list of labels of a dataset as metadata \\n It would be useful to keep the list of the labels of a dataset as metadata. Either directly in the `DatasetInfo` or in the Arrow metadata. \\n This should be accessible now as a feature in dataset.info.features (and even have the mapping methods).   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                embeddings  \\\n",
              "2  [0.04343941807746887, -0.036206990480422974, -0.15638597309589386, -0.052454523742198944, 0.12230357527732849, 0.2208477407693863, 0.2759694457054138, 0.17885375022888184, 0.00874372012913227, 0.22673830389976501, -0.157405287027359, 0.5626072287559509, -0.27078744769096375, 0.23665167391300201, -0.023262230679392815, 0.016595318913459778, -0.04746421426534653, 0.020963408052921295, 0.13064606487751007, -0.033688608556985855, -0.24152983725070953, -0.16987986862659454, 0.15129399299621582, -0.05320284143090248, -0.23283235728740692, -0.03940807282924652, 0.1785152107477188, -0.291641861200...   \n",
              "1  [-0.0012372644850984216, -0.16487111151218414, -0.1444353461265564, 0.08951263874769211, 0.16622760891914368, 0.18495865166187286, 0.31819161772727966, 0.08626117557287216, -0.07907035201787949, 0.31925591826438904, -0.16471421718597412, 0.547156572341919, -0.20061591267585754, 0.24912923574447632, -0.10227081179618835, 0.005949097219854593, -0.15736478567123413, 0.07841548323631287, 0.158057302236557, -0.12615589797496796, -0.18062910437583923, -0.1693413257598877, 0.025151681154966354, 0.07299867272377014, -0.35703474283218384, -0.08059339225292206, 0.21164342761039734, -0.20556977391242...   \n",
              "0  [-0.049605146050453186, -0.09847134351730347, -0.17237889766693115, -0.02733141928911209, 0.16801317036151886, 0.16547058522701263, 0.32426437735557556, 0.23050767183303833, -0.04784256964921951, 0.277883917093277, -0.23027127981185913, 0.49288830161094666, -0.18443314731121063, 0.2717520594596863, -0.07220461964607239, 0.0264095701277256, -0.10095421969890594, 0.05305645242333412, 0.04141265153884888, -0.03478182852268219, -0.30807143449783325, -0.16353321075439453, 0.1349450945854187, 0.06438116729259491, -0.23750022053718567, -0.04607643187046051, 0.14126580953598022, -0.195450693368911...   \n",
              "\n",
              "      scores  \n",
              "2  30.260380  \n",
              "1  28.216829  \n",
              "0  25.969202  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-aa181ff4-486d-44fa-9170-91a65d751b46\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>html_url</th>\n",
              "      <th>title</th>\n",
              "      <th>comments</th>\n",
              "      <th>body</th>\n",
              "      <th>comment_length</th>\n",
              "      <th>text</th>\n",
              "      <th>embeddings</th>\n",
              "      <th>scores</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>https://github.com/huggingface/datasets/issues/4</td>\n",
              "      <td>[Feature] Keep the list of labels of a dataset as metadata</td>\n",
              "      <td>Yes we are working on the doc right now, should be in the next release quite soon.</td>\n",
              "      <td>It would be useful to keep the list of the labels of a dataset as metadata. Either directly in the `DatasetInfo` or in the Arrow metadata.</td>\n",
              "      <td>17</td>\n",
              "      <td>[Feature] Keep the list of labels of a dataset as metadata \\n It would be useful to keep the list of the labels of a dataset as metadata. Either directly in the `DatasetInfo` or in the Arrow metadata. \\n Yes we are working on the doc right now, should be in the next release quite soon.</td>\n",
              "      <td>[0.04343941807746887, -0.036206990480422974, -0.15638597309589386, -0.052454523742198944, 0.12230357527732849, 0.2208477407693863, 0.2759694457054138, 0.17885375022888184, 0.00874372012913227, 0.22673830389976501, -0.157405287027359, 0.5626072287559509, -0.27078744769096375, 0.23665167391300201, -0.023262230679392815, 0.016595318913459778, -0.04746421426534653, 0.020963408052921295, 0.13064606487751007, -0.033688608556985855, -0.24152983725070953, -0.16987986862659454, 0.15129399299621582, -0.05320284143090248, -0.23283235728740692, -0.03940807282924652, 0.1785152107477188, -0.291641861200...</td>\n",
              "      <td>30.260380</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>https://github.com/huggingface/datasets/issues/4</td>\n",
              "      <td>[Feature] Keep the list of labels of a dataset as metadata</td>\n",
              "      <td>Hi,\\r\\nI hope we could get a better documentation.\\r\\nIt took me more than 1 hour to found this way to get the label information.</td>\n",
              "      <td>It would be useful to keep the list of the labels of a dataset as metadata. Either directly in the `DatasetInfo` or in the Arrow metadata.</td>\n",
              "      <td>25</td>\n",
              "      <td>[Feature] Keep the list of labels of a dataset as metadata \\n It would be useful to keep the list of the labels of a dataset as metadata. Either directly in the `DatasetInfo` or in the Arrow metadata. \\n Hi,\\r\\nI hope we could get a better documentation.\\r\\nIt took me more than 1 hour to found this way to get the label information.</td>\n",
              "      <td>[-0.0012372644850984216, -0.16487111151218414, -0.1444353461265564, 0.08951263874769211, 0.16622760891914368, 0.18495865166187286, 0.31819161772727966, 0.08626117557287216, -0.07907035201787949, 0.31925591826438904, -0.16471421718597412, 0.547156572341919, -0.20061591267585754, 0.24912923574447632, -0.10227081179618835, 0.005949097219854593, -0.15736478567123413, 0.07841548323631287, 0.158057302236557, -0.12615589797496796, -0.18062910437583923, -0.1693413257598877, 0.025151681154966354, 0.07299867272377014, -0.35703474283218384, -0.08059339225292206, 0.21164342761039734, -0.20556977391242...</td>\n",
              "      <td>28.216829</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>https://github.com/huggingface/datasets/issues/4</td>\n",
              "      <td>[Feature] Keep the list of labels of a dataset as metadata</td>\n",
              "      <td>This should be accessible now as a feature in dataset.info.features (and even have the mapping methods).</td>\n",
              "      <td>It would be useful to keep the list of the labels of a dataset as metadata. Either directly in the `DatasetInfo` or in the Arrow metadata.</td>\n",
              "      <td>16</td>\n",
              "      <td>[Feature] Keep the list of labels of a dataset as metadata \\n It would be useful to keep the list of the labels of a dataset as metadata. Either directly in the `DatasetInfo` or in the Arrow metadata. \\n This should be accessible now as a feature in dataset.info.features (and even have the mapping methods).</td>\n",
              "      <td>[-0.049605146050453186, -0.09847134351730347, -0.17237889766693115, -0.02733141928911209, 0.16801317036151886, 0.16547058522701263, 0.32426437735557556, 0.23050767183303833, -0.04784256964921951, 0.277883917093277, -0.23027127981185913, 0.49288830161094666, -0.18443314731121063, 0.2717520594596863, -0.07220461964607239, 0.0264095701277256, -0.10095421969890594, 0.05305645242333412, 0.04141265153884888, -0.03478182852268219, -0.30807143449783325, -0.16353321075439453, 0.1349450945854187, 0.06438116729259491, -0.23750022053718567, -0.04607643187046051, 0.14126580953598022, -0.195450693368911...</td>\n",
              "      <td>25.969202</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-aa181ff4-486d-44fa-9170-91a65d751b46')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-aa181ff4-486d-44fa-9170-91a65d751b46 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-aa181ff4-486d-44fa-9170-91a65d751b46');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-0701eafd-b325-4307-a5e5-914559a14fa9\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0701eafd-b325-4307-a5e5-914559a14fa9')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-0701eafd-b325-4307-a5e5-914559a14fa9 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ],
      "source": [
        "query: str = \"How can improve the documentation of datasets\"\n",
        "get_query_matches(query=query, K=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "73PV0YdXoMVx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "vIDVZ98GoMVx"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6c2175ce28e442e6902f9687e58ae986": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_167df158f70045cd9eb8b0e202e103e9",
              "IPY_MODEL_3d955237041f468299e8fc0133eeaec0",
              "IPY_MODEL_a71a5912e5374130b569c4a66c39660b"
            ],
            "layout": "IPY_MODEL_ccea5ae0a4974543866a0c2e459d789c"
          }
        },
        "167df158f70045cd9eb8b0e202e103e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d0d1f4a2eca54c649c8cbbfc0e6dec4c",
            "placeholder": "​",
            "style": "IPY_MODEL_b8e9a1417e5f4d57b48959bf7962240d",
            "value": "Map: 100%"
          }
        },
        "3d955237041f468299e8fc0133eeaec0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6a38b8c03f464fea8e645f915ff4734a",
            "max": 2964,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_21ea0b39c54d403686f06e9e91952397",
            "value": 2964
          }
        },
        "a71a5912e5374130b569c4a66c39660b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4a3bb9165b1c4d469d47a08c4926e140",
            "placeholder": "​",
            "style": "IPY_MODEL_3add2b6262934223a16d6dd1f629be51",
            "value": " 2964/2964 [00:00&lt;00:00, 8640.60 examples/s]"
          }
        },
        "ccea5ae0a4974543866a0c2e459d789c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d0d1f4a2eca54c649c8cbbfc0e6dec4c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b8e9a1417e5f4d57b48959bf7962240d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6a38b8c03f464fea8e645f915ff4734a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "21ea0b39c54d403686f06e9e91952397": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4a3bb9165b1c4d469d47a08c4926e140": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3add2b6262934223a16d6dd1f629be51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5a1fce2be658465294e0e3a5dc947794": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_63b5fc821e9245dab54377e613ed1c93",
              "IPY_MODEL_236e9fb75c1047769e044e46e1291196",
              "IPY_MODEL_59b6d437f5544442b68b2d00e94acf6b"
            ],
            "layout": "IPY_MODEL_5f73f21ed89c458ab461af6d82b8dc06"
          }
        },
        "63b5fc821e9245dab54377e613ed1c93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6718c739c1114293b52c7fcd0f3063f0",
            "placeholder": "​",
            "style": "IPY_MODEL_321755f964f4412f9c2c0a25d376dd54",
            "value": "Filter: 100%"
          }
        },
        "236e9fb75c1047769e044e46e1291196": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_582d3c99e3b44808a4344c4a02c4684f",
            "max": 2964,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f219ec180e8641c1a9b906bd0154b94e",
            "value": 2964
          }
        },
        "59b6d437f5544442b68b2d00e94acf6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c2d45934112649aaafca4981d2bc0ac4",
            "placeholder": "​",
            "style": "IPY_MODEL_9c0949fd85f0450fa388b869d48641c5",
            "value": " 2964/2964 [00:00&lt;00:00, 38887.78 examples/s]"
          }
        },
        "5f73f21ed89c458ab461af6d82b8dc06": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6718c739c1114293b52c7fcd0f3063f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "321755f964f4412f9c2c0a25d376dd54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "582d3c99e3b44808a4344c4a02c4684f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f219ec180e8641c1a9b906bd0154b94e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c2d45934112649aaafca4981d2bc0ac4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c0949fd85f0450fa388b869d48641c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a52ffcfdff274a91be5afc951706e8ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ff89d769a93e46ada746e9c3242fc6d3",
              "IPY_MODEL_72b54539198a4bc8a1640a744af78040",
              "IPY_MODEL_391d57b656d94abda6167c3cdfc6a480"
            ],
            "layout": "IPY_MODEL_59f465b12ee94587a44d1e4438010a32"
          }
        },
        "ff89d769a93e46ada746e9c3242fc6d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3f485adf1b18427ca5412e3b72e89f70",
            "placeholder": "​",
            "style": "IPY_MODEL_415d368d9c874eb39417ff656c64b282",
            "value": "Map: 100%"
          }
        },
        "72b54539198a4bc8a1640a744af78040": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_55e3ec1ba9564bee86ead59279c7c188",
            "max": 2175,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_97b2b48bceb54c659cb840dfd0d62e7c",
            "value": 2175
          }
        },
        "391d57b656d94abda6167c3cdfc6a480": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c8de28812c4140eb925b1c734250e5fe",
            "placeholder": "​",
            "style": "IPY_MODEL_5b5e77718e874fc1a068c83c1536abaa",
            "value": " 2175/2175 [00:00&lt;00:00, 6672.10 examples/s]"
          }
        },
        "59f465b12ee94587a44d1e4438010a32": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f485adf1b18427ca5412e3b72e89f70": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "415d368d9c874eb39417ff656c64b282": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "55e3ec1ba9564bee86ead59279c7c188": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "97b2b48bceb54c659cb840dfd0d62e7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c8de28812c4140eb925b1c734250e5fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b5e77718e874fc1a068c83c1536abaa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "547f511233094f7fa6966e447f973226": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e366f7e012044aa0908403d3b63f5013",
              "IPY_MODEL_ce6be0f780954dc1a19b195fc96212c5",
              "IPY_MODEL_019f1e2ca3ed42dbb5232b00a14367eb"
            ],
            "layout": "IPY_MODEL_117fb75a71cb4bcfb87a450d6b623d13"
          }
        },
        "e366f7e012044aa0908403d3b63f5013": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_29c2c23adff049f3a8097c7e4d040c87",
            "placeholder": "​",
            "style": "IPY_MODEL_c4e506d0ce044207b1147fdd6d0c647a",
            "value": "Map: 100%"
          }
        },
        "ce6be0f780954dc1a19b195fc96212c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3f8c9c6dbad8442cb651ac40a4e0bbaa",
            "max": 2175,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4b7c9f11871a448c8868987a3b9d3f8c",
            "value": 2175
          }
        },
        "019f1e2ca3ed42dbb5232b00a14367eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b17fd629ef641b7983566d63f27cbd2",
            "placeholder": "​",
            "style": "IPY_MODEL_faf57a1cc51645828914d730fc6933c8",
            "value": " 2175/2175 [01:23&lt;00:00, 34.13 examples/s]"
          }
        },
        "117fb75a71cb4bcfb87a450d6b623d13": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29c2c23adff049f3a8097c7e4d040c87": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c4e506d0ce044207b1147fdd6d0c647a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3f8c9c6dbad8442cb651ac40a4e0bbaa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b7c9f11871a448c8868987a3b9d3f8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3b17fd629ef641b7983566d63f27cbd2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "faf57a1cc51645828914d730fc6933c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "564da74a96fd4f15a10a1bbcb1f52c41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ed3b8a7624634799882a9fd039de55b7",
              "IPY_MODEL_47f28503912c481db809f3365e7cf9d9",
              "IPY_MODEL_d1f9a369f78847c4a44ce7a7a70f5e42"
            ],
            "layout": "IPY_MODEL_02935cf35342426a9f7280dddc4900a9"
          }
        },
        "ed3b8a7624634799882a9fd039de55b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b218ed5d69a742bfaf099ec15706967c",
            "placeholder": "​",
            "style": "IPY_MODEL_7b8ad99f581943fdb27c75f170f0a706",
            "value": "100%"
          }
        },
        "47f28503912c481db809f3365e7cf9d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_58741deb948d426cab0a471e13c6da5e",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_85849c7d679a4aefa688957823a0a5ec",
            "value": 3
          }
        },
        "d1f9a369f78847c4a44ce7a7a70f5e42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_20bb6758cbe44918806f3b4b689c2a90",
            "placeholder": "​",
            "style": "IPY_MODEL_527c514ebb9a4735a12fdaa73ea69118",
            "value": " 3/3 [00:00&lt;00:00, 97.16it/s]"
          }
        },
        "02935cf35342426a9f7280dddc4900a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b218ed5d69a742bfaf099ec15706967c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b8ad99f581943fdb27c75f170f0a706": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "58741deb948d426cab0a471e13c6da5e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "85849c7d679a4aefa688957823a0a5ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "20bb6758cbe44918806f3b4b689c2a90": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "527c514ebb9a4735a12fdaa73ea69118": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}