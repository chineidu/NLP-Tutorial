{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Semantic Search](https://huggingface.co/learn/nlp-course/chapter5/6?fw=pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in library\n",
    "import re\n",
    "import json\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "# Standard imports\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "from rich import print\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Pandas settings\n",
    "pd.options.display.max_rows = 1_000\n",
    "pd.options.display.max_columns = 1_000\n",
    "pd.options.display.max_colwidth = 600\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Black code formatter (Optional)\n",
    "%load_ext lab_black\n",
    "\n",
    "# auto reload imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from datasets import load_dataset\n",
    "from datasets.dataset_dict import DatasetDict, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'pull_request', 'body', 'timeline_url', 'performed_via_github_app', 'is_pull_request'],\n",
       "    num_rows: 3019\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp: str = \"lewtun/github-issues\"\n",
    "\n",
    "issues_dataset: Dataset = load_dataset(fp, split=\"train\")\n",
    "issues_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'url'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://api.github.com/repos/huggingface/datasets/issues/2944'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'repository_url'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://api.github.com/repos/huggingface/datasets'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'labels_url'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://api.github.com/repos/huggingface/datasets/issues/2944/labels{/name}'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'comments_url'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://api.github.com/repos/huggingface/datasets/issues/2944/comments'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'events_url'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://api.github.com/repos/huggingface/datasets/issues/2944/events'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'html_url'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://github.com/huggingface/datasets/issues/2944'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'id'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1000544370</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'node_id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'I_kwDODunzps47oxhy'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'number'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2944</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'title'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Add  `remove_columns` to `IterableDataset ` '</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'user'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'login'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'cccntu'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'id'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">31893406</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'node_id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'MDQ6VXNlcjMxODkzNDA2'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'avatar_url'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://avatars.githubusercontent.com/u/31893406?v=4'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'gravatar_id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'url'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://api.github.com/users/cccntu'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'html_url'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://github.com/cccntu'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'followers_url'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://api.github.com/users/cccntu/followers'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'following_url'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://api.github.com/users/cccntu/following{/other_user}'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'gists_url'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://api.github.com/users/cccntu/gists{/gist_id}'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'starred_url'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://api.github.com/users/cccntu/starred{/owner}{/repo}'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'subscriptions_url'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://api.github.com/users/cccntu/subscriptions'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'organizations_url'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://api.github.com/users/cccntu/orgs'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'repos_url'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://api.github.com/users/cccntu/repos'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'events_url'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://api.github.com/users/cccntu/events{/privacy}'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'received_events_url'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://api.github.com/users/cccntu/received_events'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'User'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'site_admin'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'labels'</span>: <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'id'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1935892871</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'node_id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'MDU6TGFiZWwxOTM1ODkyODcx'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'url'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://api.github.com/repos/huggingface/datasets/labels/enhancement'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'enhancement'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'color'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'a2eeef'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'default'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'description'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'New feature or request'</span>\n",
       "        <span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'state'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'open'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'locked'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'assignee'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'assignees'</span>: <span style=\"font-weight: bold\">[]</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'milestone'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'comments'</span>: <span style=\"font-weight: bold\">[]</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'created_at'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1632110460000</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'updated_at'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1632110460000</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'closed_at'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'author_association'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'CONTRIBUTOR'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'active_lock_reason'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'pull_request'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'body'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'**Is your feature request related to a problem? Please describe.**\\r\\nA clear and concise description </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">of what the problem is.\\r\\n\\r\\n```python\\r\\nfrom datasets import load_dataset\\r\\ndataset = load_dataset(\"c4\", </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\\'realnewslike\\', streaming =True, split=\\'train\\')\\r\\ndataset = </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">dataset.remove_columns(\\'url\\')\\r\\n```\\r\\n```\\r\\nAttributeError: \\'IterableDataset\\' object has no attribute </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\\'remove_columns\\'\\r\\n```\\r\\n\\r\\n**Describe the solution you\\'d like**\\r\\n\\r\\nIt would be nice to have </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">`.remove_columns()` to match the `Datasets` api. \\r\\n\\r\\n\\r\\n**Describe alternatives you\\'ve </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">considered**\\r\\n\\r\\nThis can be done with a single call to `.map()`, \\r\\n\\r\\nI can try to help add this. ðŸ¤—'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'timeline_url'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://api.github.com/repos/huggingface/datasets/issues/2944/timeline'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'performed_via_github_app'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'is_pull_request'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'url'\u001b[0m: \u001b[32m'https://api.github.com/repos/huggingface/datasets/issues/2944'\u001b[0m,\n",
       "    \u001b[32m'repository_url'\u001b[0m: \u001b[32m'https://api.github.com/repos/huggingface/datasets'\u001b[0m,\n",
       "    \u001b[32m'labels_url'\u001b[0m: \u001b[32m'https://api.github.com/repos/huggingface/datasets/issues/2944/labels\u001b[0m\u001b[32m{\u001b[0m\u001b[32m/name\u001b[0m\u001b[32m}\u001b[0m\u001b[32m'\u001b[0m,\n",
       "    \u001b[32m'comments_url'\u001b[0m: \u001b[32m'https://api.github.com/repos/huggingface/datasets/issues/2944/comments'\u001b[0m,\n",
       "    \u001b[32m'events_url'\u001b[0m: \u001b[32m'https://api.github.com/repos/huggingface/datasets/issues/2944/events'\u001b[0m,\n",
       "    \u001b[32m'html_url'\u001b[0m: \u001b[32m'https://github.com/huggingface/datasets/issues/2944'\u001b[0m,\n",
       "    \u001b[32m'id'\u001b[0m: \u001b[1;36m1000544370\u001b[0m,\n",
       "    \u001b[32m'node_id'\u001b[0m: \u001b[32m'I_kwDODunzps47oxhy'\u001b[0m,\n",
       "    \u001b[32m'number'\u001b[0m: \u001b[1;36m2944\u001b[0m,\n",
       "    \u001b[32m'title'\u001b[0m: \u001b[32m'Add  `remove_columns` to `IterableDataset ` '\u001b[0m,\n",
       "    \u001b[32m'user'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "        \u001b[32m'login'\u001b[0m: \u001b[32m'cccntu'\u001b[0m,\n",
       "        \u001b[32m'id'\u001b[0m: \u001b[1;36m31893406\u001b[0m,\n",
       "        \u001b[32m'node_id'\u001b[0m: \u001b[32m'MDQ6VXNlcjMxODkzNDA2'\u001b[0m,\n",
       "        \u001b[32m'avatar_url'\u001b[0m: \u001b[32m'https://avatars.githubusercontent.com/u/31893406?\u001b[0m\u001b[32mv\u001b[0m\u001b[32m=\u001b[0m\u001b[32m4\u001b[0m\u001b[32m'\u001b[0m,\n",
       "        \u001b[32m'gravatar_id'\u001b[0m: \u001b[32m''\u001b[0m,\n",
       "        \u001b[32m'url'\u001b[0m: \u001b[32m'https://api.github.com/users/cccntu'\u001b[0m,\n",
       "        \u001b[32m'html_url'\u001b[0m: \u001b[32m'https://github.com/cccntu'\u001b[0m,\n",
       "        \u001b[32m'followers_url'\u001b[0m: \u001b[32m'https://api.github.com/users/cccntu/followers'\u001b[0m,\n",
       "        \u001b[32m'following_url'\u001b[0m: \u001b[32m'https://api.github.com/users/cccntu/following\u001b[0m\u001b[32m{\u001b[0m\u001b[32m/other_user\u001b[0m\u001b[32m}\u001b[0m\u001b[32m'\u001b[0m,\n",
       "        \u001b[32m'gists_url'\u001b[0m: \u001b[32m'https://api.github.com/users/cccntu/gists\u001b[0m\u001b[32m{\u001b[0m\u001b[32m/gist_id\u001b[0m\u001b[32m}\u001b[0m\u001b[32m'\u001b[0m,\n",
       "        \u001b[32m'starred_url'\u001b[0m: \u001b[32m'https://api.github.com/users/cccntu/starred\u001b[0m\u001b[32m{\u001b[0m\u001b[32m/owner\u001b[0m\u001b[32m}\u001b[0m\u001b[32m{\u001b[0m\u001b[32m/repo\u001b[0m\u001b[32m}\u001b[0m\u001b[32m'\u001b[0m,\n",
       "        \u001b[32m'subscriptions_url'\u001b[0m: \u001b[32m'https://api.github.com/users/cccntu/subscriptions'\u001b[0m,\n",
       "        \u001b[32m'organizations_url'\u001b[0m: \u001b[32m'https://api.github.com/users/cccntu/orgs'\u001b[0m,\n",
       "        \u001b[32m'repos_url'\u001b[0m: \u001b[32m'https://api.github.com/users/cccntu/repos'\u001b[0m,\n",
       "        \u001b[32m'events_url'\u001b[0m: \u001b[32m'https://api.github.com/users/cccntu/events\u001b[0m\u001b[32m{\u001b[0m\u001b[32m/privacy\u001b[0m\u001b[32m}\u001b[0m\u001b[32m'\u001b[0m,\n",
       "        \u001b[32m'received_events_url'\u001b[0m: \u001b[32m'https://api.github.com/users/cccntu/received_events'\u001b[0m,\n",
       "        \u001b[32m'type'\u001b[0m: \u001b[32m'User'\u001b[0m,\n",
       "        \u001b[32m'site_admin'\u001b[0m: \u001b[3;91mFalse\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[32m'labels'\u001b[0m: \u001b[1m[\u001b[0m\n",
       "        \u001b[1m{\u001b[0m\n",
       "            \u001b[32m'id'\u001b[0m: \u001b[1;36m1935892871\u001b[0m,\n",
       "            \u001b[32m'node_id'\u001b[0m: \u001b[32m'MDU6TGFiZWwxOTM1ODkyODcx'\u001b[0m,\n",
       "            \u001b[32m'url'\u001b[0m: \u001b[32m'https://api.github.com/repos/huggingface/datasets/labels/enhancement'\u001b[0m,\n",
       "            \u001b[32m'name'\u001b[0m: \u001b[32m'enhancement'\u001b[0m,\n",
       "            \u001b[32m'color'\u001b[0m: \u001b[32m'a2eeef'\u001b[0m,\n",
       "            \u001b[32m'default'\u001b[0m: \u001b[3;92mTrue\u001b[0m,\n",
       "            \u001b[32m'description'\u001b[0m: \u001b[32m'New feature or request'\u001b[0m\n",
       "        \u001b[1m}\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[32m'state'\u001b[0m: \u001b[32m'open'\u001b[0m,\n",
       "    \u001b[32m'locked'\u001b[0m: \u001b[3;91mFalse\u001b[0m,\n",
       "    \u001b[32m'assignee'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       "    \u001b[32m'assignees'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\n",
       "    \u001b[32m'milestone'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       "    \u001b[32m'comments'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\n",
       "    \u001b[32m'created_at'\u001b[0m: \u001b[1;36m1632110460000\u001b[0m,\n",
       "    \u001b[32m'updated_at'\u001b[0m: \u001b[1;36m1632110460000\u001b[0m,\n",
       "    \u001b[32m'closed_at'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       "    \u001b[32m'author_association'\u001b[0m: \u001b[32m'CONTRIBUTOR'\u001b[0m,\n",
       "    \u001b[32m'active_lock_reason'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       "    \u001b[32m'pull_request'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       "    \u001b[32m'body'\u001b[0m: \u001b[32m'**Is your feature request related to a problem? Please describe.**\\r\\nA clear and concise description \u001b[0m\n",
       "\u001b[32mof what the problem is.\\r\\n\\r\\n```python\\r\\nfrom datasets import load_dataset\\r\\ndataset = load_dataset\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\"c4\", \u001b[0m\n",
       "\u001b[32m\\'realnewslike\\', streaming =True, \u001b[0m\u001b[32msplit\u001b[0m\u001b[32m=\\'train\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\r\\ndataset = \u001b[0m\n",
       "\u001b[32mdataset.remove_columns\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\'url\\'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\r\\n```\\r\\n```\\r\\nAttributeError: \\'IterableDataset\\' object has no attribute \u001b[0m\n",
       "\u001b[32m\\'remove_columns\\'\\r\\n```\\r\\n\\r\\n**Describe the solution you\\'d like**\\r\\n\\r\\nIt would be nice to have \u001b[0m\n",
       "\u001b[32m`.remove_columns\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m` to match the `Datasets` api. \\r\\n\\r\\n\\r\\n**Describe alternatives you\\'ve \u001b[0m\n",
       "\u001b[32mconsidered**\\r\\n\\r\\nThis can be done with a single call to `.map\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m`, \\r\\n\\r\\nI can try to help add this. ðŸ¤—'\u001b[0m,\n",
       "    \u001b[32m'timeline_url'\u001b[0m: \u001b[32m'https://api.github.com/repos/huggingface/datasets/issues/2944/timeline'\u001b[0m,\n",
       "    \u001b[32m'performed_via_github_app'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       "    \u001b[32m'is_pull_request'\u001b[0m: \u001b[3;91mFalse\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(issues_dataset[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'url'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'repository_url'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'labels_url'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'comments_url'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'events_url'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'html_url'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'id'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'node_id'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'number'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'title'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'user'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'labels'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'state'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'locked'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'assignee'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'assignees'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'milestone'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'comments'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'created_at'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'updated_at'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'closed_at'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'author_association'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'active_lock_reason'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'pull_request'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'body'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'timeline_url'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'performed_via_github_app'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'is_pull_request'</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[32m'url'\u001b[0m,\n",
       "    \u001b[32m'repository_url'\u001b[0m,\n",
       "    \u001b[32m'labels_url'\u001b[0m,\n",
       "    \u001b[32m'comments_url'\u001b[0m,\n",
       "    \u001b[32m'events_url'\u001b[0m,\n",
       "    \u001b[32m'html_url'\u001b[0m,\n",
       "    \u001b[32m'id'\u001b[0m,\n",
       "    \u001b[32m'node_id'\u001b[0m,\n",
       "    \u001b[32m'number'\u001b[0m,\n",
       "    \u001b[32m'title'\u001b[0m,\n",
       "    \u001b[32m'user'\u001b[0m,\n",
       "    \u001b[32m'labels'\u001b[0m,\n",
       "    \u001b[32m'state'\u001b[0m,\n",
       "    \u001b[32m'locked'\u001b[0m,\n",
       "    \u001b[32m'assignee'\u001b[0m,\n",
       "    \u001b[32m'assignees'\u001b[0m,\n",
       "    \u001b[32m'milestone'\u001b[0m,\n",
       "    \u001b[32m'comments'\u001b[0m,\n",
       "    \u001b[32m'created_at'\u001b[0m,\n",
       "    \u001b[32m'updated_at'\u001b[0m,\n",
       "    \u001b[32m'closed_at'\u001b[0m,\n",
       "    \u001b[32m'author_association'\u001b[0m,\n",
       "    \u001b[32m'active_lock_reason'\u001b[0m,\n",
       "    \u001b[32m'pull_request'\u001b[0m,\n",
       "    \u001b[32m'body'\u001b[0m,\n",
       "    \u001b[32m'timeline_url'\u001b[0m,\n",
       "    \u001b[32m'performed_via_github_app'\u001b[0m,\n",
       "    \u001b[32m'is_pull_request'\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(issues_dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Size if data BEFORE: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3019</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Size if data BEFORE: \u001b[1;36m3019\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Size if data AFTER: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">808</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Size if data AFTER: \u001b[1;36m808\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The issues_dataset contains issues and pull requests.\n",
    "# Select ONLY the issues\n",
    "issues_dataset_1: Dataset = issues_dataset.filter(\n",
    "    (lambda x: x.get(\"is_pull_request\") == False and len(x.get(\"comments\")) > 0),\n",
    ")\n",
    "\n",
    "print(f\"Size if data BEFORE: {issues_dataset.num_rows}\\n\")\n",
    "\n",
    "print(f\"Size if data AFTER: {issues_dataset_1.num_rows}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'events_url'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'updated_at'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'user'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'created_at'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'comments_url'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'performed_via_github_app'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'repository_url'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'labels_url'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'assignees'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'author_association'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'id'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'active_lock_reason'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'milestone'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'labels'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'number'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'is_pull_request'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'assignee'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'url'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'locked'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'state'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'pull_request'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'node_id'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'closed_at'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'timeline_url'</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'events_url'\u001b[0m,\n",
       "    \u001b[32m'updated_at'\u001b[0m,\n",
       "    \u001b[32m'user'\u001b[0m,\n",
       "    \u001b[32m'created_at'\u001b[0m,\n",
       "    \u001b[32m'comments_url'\u001b[0m,\n",
       "    \u001b[32m'performed_via_github_app'\u001b[0m,\n",
       "    \u001b[32m'repository_url'\u001b[0m,\n",
       "    \u001b[32m'labels_url'\u001b[0m,\n",
       "    \u001b[32m'assignees'\u001b[0m,\n",
       "    \u001b[32m'author_association'\u001b[0m,\n",
       "    \u001b[32m'id'\u001b[0m,\n",
       "    \u001b[32m'active_lock_reason'\u001b[0m,\n",
       "    \u001b[32m'milestone'\u001b[0m,\n",
       "    \u001b[32m'labels'\u001b[0m,\n",
       "    \u001b[32m'number'\u001b[0m,\n",
       "    \u001b[32m'is_pull_request'\u001b[0m,\n",
       "    \u001b[32m'assignee'\u001b[0m,\n",
       "    \u001b[32m'url'\u001b[0m,\n",
       "    \u001b[32m'locked'\u001b[0m,\n",
       "    \u001b[32m'state'\u001b[0m,\n",
       "    \u001b[32m'pull_request'\u001b[0m,\n",
       "    \u001b[32m'node_id'\u001b[0m,\n",
       "    \u001b[32m'closed_at'\u001b[0m,\n",
       "    \u001b[32m'timeline_url'\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_columns: list[str] = issues_dataset_1.column_names\n",
    "columns_to_keep: list[str] = [\"title\", \"body\", \"html_url\", \"comments\"]\n",
    "# Columns present in ONLY all_columns\n",
    "columns_to_remove: set = set(columns_to_keep).symmetric_difference(set(all_columns))\n",
    "print(columns_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'comments', 'body'],\n",
       "    num_rows: 808\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issues_dataset_1 = issues_dataset_1.remove_columns(column_names=columns_to_remove)\n",
    "issues_dataset_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>html_url</th>\n",
       "      <th>title</th>\n",
       "      <th>comments</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues/2945</td>\n",
       "      <td>Protect master branch</td>\n",
       "      <td>[Cool, I think we can do both :), @lhoestq now the 2 are implemented.\\r\\n\\r\\nPlease note that for the the second protection, finally I have chosen to protect the master branch only from **merge commits** (see update comment above), so no need to disable/re-enable the protection on each release (direct commits, different from merge commits, can be pushed to the remote master branch; and eventually reverted without messing up the repo history).]</td>\n",
       "      <td>After accidental merge commit (91c55355b634d0dc73350a7ddee1a6776dbbdd69) into `datasets` master branch, all commits present in the feature branch were permanently added to `datasets` master branch history, as e.g.:\\r\\n- 00cc036fea7c7745cfe722360036ed306796a3f2\\r\\n- 13ae8c98602bbad8197de3b9b425f4c78f582af1\\r\\n- ...\\r\\n\\r\\nI propose to protect our master branch, so that we avoid we can accidentally make this kind of mistakes in the future:\\r\\n- [x] For Pull Requests using GitHub, allow only squash merging, so that only a single commit per Pull Request is merged into the master branch\\r\\n  - ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues/2943</td>\n",
       "      <td>Backwards compatibility broken for cached datasets that use `.filter()`</td>\n",
       "      <td>[Hi ! I guess the caching mechanism should have considered the new `filter` to be different from the old one, and don't use cached results from the old `filter`.\\r\\nTo avoid other users from having this issue we could make the caching differentiate the two, what do you think ?, If it's easy enough to implement, then yes please ðŸ˜„  But this issue can be low-priority, since I've only encountered it in a couple of `transformers` CI tests., Well it can cause issue with anyone that updates `datasets` and re-run some code that uses filter, so I'm creating a PR, I just merged a fix, let me know if...</td>\n",
       "      <td>## Describe the bug\\r\\nAfter upgrading to datasets `1.12.0`, some cached `.filter()` steps from `1.11.0` started failing with \\r\\n`ValueError: Keys mismatch: between {'indices': Value(dtype='uint64', id=None)} and {'file': Value(dtype='string', id=None), 'text': Value(dtype='string', id=None), 'speaker_id': Value(dtype='int64', id=None), 'chapter_id': Value(dtype='int64', id=None), 'id': Value(dtype='string', id=None)}`\\r\\n\\r\\nRelated feature: https://github.com/huggingface/datasets/pull/2836\\r\\n\\r\\n:question:  This is probably a `wontfix` bug, since it can be solved by simply cleaning the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues/2941</td>\n",
       "      <td>OSCAR unshuffled_original_ko: NonMatchingSplitsSizesError</td>\n",
       "      <td>[I tried `unshuffled_original_da` and it is also not working]</td>\n",
       "      <td>## Describe the bug\\r\\n\\r\\nCannot download OSCAR `unshuffled_original_ko` due to `NonMatchingSplitsSizesError`.\\r\\n\\r\\n## Steps to reproduce the bug\\r\\n\\r\\n```python\\r\\n&gt;&gt;&gt; dataset = datasets.load_dataset('oscar', 'unshuffled_original_ko')\\r\\nNonMatchingSplitsSizesError: [{'expected': SplitInfo(name='train', num_bytes=25292102197, num_examples=7345075, dataset_name='oscar'), 'recorded': SplitInfo(name='train', num_bytes=25284578514, num_examples=7344907, dataset_name='oscar')}]\\r\\n```\\r\\n\\r\\n## Expected results\\r\\n\\r\\nLoading is successful.\\r\\n\\r\\n## Actual results\\r\\n\\r\\nLoading throws ab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues/2937</td>\n",
       "      <td>load_dataset using default cache on Windows causes PermissionError: [WinError 5] Access is denied</td>\n",
       "      <td>[Hi @daqieq, thanks for reporting.\\r\\n\\r\\nUnfortunately, I was not able to reproduce this bug:\\r\\n```ipython\\r\\nIn [1]: from datasets import load_dataset\\r\\n   ...: ds = load_dataset('wiki_bio')\\r\\nDownloading: 7.58kB [00:00, 26.3kB/s]\\r\\nDownloading: 2.71kB [00:00, ?B/s]\\r\\nUsing custom data configuration default\\r\\nDownloading and preparing dataset wiki_bio/default (download: 318.53 MiB, generated: 736.94 MiB, post-processed: Unknown size, total: 1.03 GiB) to C:\\Users\\username\\.cache\\huggingface\\datasets\\wiki_bio\\default\\\\r\\n1.1.0\\5293ce565954ba965dada626f1e79684e98172d950371d266bf3caaf8...</td>\n",
       "      <td>## Describe the bug\\r\\nStandard process to download and load the wiki_bio dataset causes PermissionError in Windows 10 and 11.\\r\\n\\r\\n## Steps to reproduce the bug\\r\\n```python\\r\\nfrom datasets import load_dataset\\r\\nds = load_dataset('wiki_bio')\\r\\n```\\r\\n\\r\\n## Expected results\\r\\nIt is expected that the dataset downloads without any errors.\\r\\n\\r\\n## Actual results\\r\\nPermissionError see trace below:\\r\\n```\\r\\nUsing custom data configuration default\\r\\nDownloading and preparing dataset wiki_bio/default (download: 318.53 MiB, generated: 736.94 MiB, post-processed: Unknown size, total: 1....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues/2934</td>\n",
       "      <td>to_tf_dataset keeps a reference to the open data somewhere, causing issues on windows</td>\n",
       "      <td>[I did some investigation and, as it seems, the bug stems from [this line](https://github.com/huggingface/datasets/blob/8004d7c3e1d74b29c3e5b0d1660331cd26758363/src/datasets/arrow_dataset.py#L325). The lifecycle of the dataset from the linked line is bound to one of the returned `tf.data.Dataset`. So my (hacky) solution involves wrapping the linked dataset with `weakref.proxy` and adding a custom `__del__` to `tf.python.data.ops.dataset_ops.TensorSliceDataset` (this is the type of a dataset that is returned by `tf.data.Dataset.from_tensor_slices`; this works for TF 2.x, but I'm not sure `t...</td>\n",
       "      <td>To reproduce:\\r\\n```python\\r\\nimport datasets as ds\\r\\nimport weakref\\r\\nimport gc\\r\\n\\r\\nd = ds.load_dataset(\"mnist\", split=\"train\")\\r\\nref = weakref.ref(d._data.table)\\r\\ntfd = d.to_tf_dataset(\"image\", batch_size=1, shuffle=False, label_cols=\"label\")\\r\\ndel tfd, d\\r\\ngc.collect()\\r\\nassert ref() is None, \"Error: there is at least one reference left\"\\r\\n```\\r\\n\\r\\nThis causes issues because the table holds a reference to an open arrow file that should be closed. So on windows it's not possible to delete or move the arrow file afterwards.\\r\\n\\r\\nMoreover the CI test of the `to_tf_dataset` ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              html_url  \\\n",
       "0  https://github.com/huggingface/datasets/issues/2945   \n",
       "1  https://github.com/huggingface/datasets/issues/2943   \n",
       "2  https://github.com/huggingface/datasets/issues/2941   \n",
       "3  https://github.com/huggingface/datasets/issues/2937   \n",
       "4  https://github.com/huggingface/datasets/issues/2934   \n",
       "\n",
       "                                                                                               title  \\\n",
       "0                                                                              Protect master branch   \n",
       "1                            Backwards compatibility broken for cached datasets that use `.filter()`   \n",
       "2                                          OSCAR unshuffled_original_ko: NonMatchingSplitsSizesError   \n",
       "3  load_dataset using default cache on Windows causes PermissionError: [WinError 5] Access is denied   \n",
       "4              to_tf_dataset keeps a reference to the open data somewhere, causing issues on windows   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  comments  \\\n",
       "0                                                                                                                                                          [Cool, I think we can do both :), @lhoestq now the 2 are implemented.\\r\\n\\r\\nPlease note that for the the second protection, finally I have chosen to protect the master branch only from **merge commits** (see update comment above), so no need to disable/re-enable the protection on each release (direct commits, different from merge commits, can be pushed to the remote master branch; and eventually reverted without messing up the repo history).]   \n",
       "1  [Hi ! I guess the caching mechanism should have considered the new `filter` to be different from the old one, and don't use cached results from the old `filter`.\\r\\nTo avoid other users from having this issue we could make the caching differentiate the two, what do you think ?, If it's easy enough to implement, then yes please ðŸ˜„  But this issue can be low-priority, since I've only encountered it in a couple of `transformers` CI tests., Well it can cause issue with anyone that updates `datasets` and re-run some code that uses filter, so I'm creating a PR, I just merged a fix, let me know if...   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            [I tried `unshuffled_original_da` and it is also not working]   \n",
       "3  [Hi @daqieq, thanks for reporting.\\r\\n\\r\\nUnfortunately, I was not able to reproduce this bug:\\r\\n```ipython\\r\\nIn [1]: from datasets import load_dataset\\r\\n   ...: ds = load_dataset('wiki_bio')\\r\\nDownloading: 7.58kB [00:00, 26.3kB/s]\\r\\nDownloading: 2.71kB [00:00, ?B/s]\\r\\nUsing custom data configuration default\\r\\nDownloading and preparing dataset wiki_bio/default (download: 318.53 MiB, generated: 736.94 MiB, post-processed: Unknown size, total: 1.03 GiB) to C:\\Users\\username\\.cache\\huggingface\\datasets\\wiki_bio\\default\\\\r\\n1.1.0\\5293ce565954ba965dada626f1e79684e98172d950371d266bf3caaf8...   \n",
       "4  [I did some investigation and, as it seems, the bug stems from [this line](https://github.com/huggingface/datasets/blob/8004d7c3e1d74b29c3e5b0d1660331cd26758363/src/datasets/arrow_dataset.py#L325). The lifecycle of the dataset from the linked line is bound to one of the returned `tf.data.Dataset`. So my (hacky) solution involves wrapping the linked dataset with `weakref.proxy` and adding a custom `__del__` to `tf.python.data.ops.dataset_ops.TensorSliceDataset` (this is the type of a dataset that is returned by `tf.data.Dataset.from_tensor_slices`; this works for TF 2.x, but I'm not sure `t...   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      body  \n",
       "0  After accidental merge commit (91c55355b634d0dc73350a7ddee1a6776dbbdd69) into `datasets` master branch, all commits present in the feature branch were permanently added to `datasets` master branch history, as e.g.:\\r\\n- 00cc036fea7c7745cfe722360036ed306796a3f2\\r\\n- 13ae8c98602bbad8197de3b9b425f4c78f582af1\\r\\n- ...\\r\\n\\r\\nI propose to protect our master branch, so that we avoid we can accidentally make this kind of mistakes in the future:\\r\\n- [x] For Pull Requests using GitHub, allow only squash merging, so that only a single commit per Pull Request is merged into the master branch\\r\\n  - ...  \n",
       "1  ## Describe the bug\\r\\nAfter upgrading to datasets `1.12.0`, some cached `.filter()` steps from `1.11.0` started failing with \\r\\n`ValueError: Keys mismatch: between {'indices': Value(dtype='uint64', id=None)} and {'file': Value(dtype='string', id=None), 'text': Value(dtype='string', id=None), 'speaker_id': Value(dtype='int64', id=None), 'chapter_id': Value(dtype='int64', id=None), 'id': Value(dtype='string', id=None)}`\\r\\n\\r\\nRelated feature: https://github.com/huggingface/datasets/pull/2836\\r\\n\\r\\n:question:  This is probably a `wontfix` bug, since it can be solved by simply cleaning the...  \n",
       "2  ## Describe the bug\\r\\n\\r\\nCannot download OSCAR `unshuffled_original_ko` due to `NonMatchingSplitsSizesError`.\\r\\n\\r\\n## Steps to reproduce the bug\\r\\n\\r\\n```python\\r\\n>>> dataset = datasets.load_dataset('oscar', 'unshuffled_original_ko')\\r\\nNonMatchingSplitsSizesError: [{'expected': SplitInfo(name='train', num_bytes=25292102197, num_examples=7345075, dataset_name='oscar'), 'recorded': SplitInfo(name='train', num_bytes=25284578514, num_examples=7344907, dataset_name='oscar')}]\\r\\n```\\r\\n\\r\\n## Expected results\\r\\n\\r\\nLoading is successful.\\r\\n\\r\\n## Actual results\\r\\n\\r\\nLoading throws ab...  \n",
       "3  ## Describe the bug\\r\\nStandard process to download and load the wiki_bio dataset causes PermissionError in Windows 10 and 11.\\r\\n\\r\\n## Steps to reproduce the bug\\r\\n```python\\r\\nfrom datasets import load_dataset\\r\\nds = load_dataset('wiki_bio')\\r\\n```\\r\\n\\r\\n## Expected results\\r\\nIt is expected that the dataset downloads without any errors.\\r\\n\\r\\n## Actual results\\r\\nPermissionError see trace below:\\r\\n```\\r\\nUsing custom data configuration default\\r\\nDownloading and preparing dataset wiki_bio/default (download: 318.53 MiB, generated: 736.94 MiB, post-processed: Unknown size, total: 1....  \n",
       "4  To reproduce:\\r\\n```python\\r\\nimport datasets as ds\\r\\nimport weakref\\r\\nimport gc\\r\\n\\r\\nd = ds.load_dataset(\"mnist\", split=\"train\")\\r\\nref = weakref.ref(d._data.table)\\r\\ntfd = d.to_tf_dataset(\"image\", batch_size=1, shuffle=False, label_cols=\"label\")\\r\\ndel tfd, d\\r\\ngc.collect()\\r\\nassert ref() is None, \"Error: there is at least one reference left\"\\r\\n```\\r\\n\\r\\nThis causes issues because the table holds a reference to an open arrow file that should be closed. So on windows it's not possible to delete or move the arrow file afterwards.\\r\\n\\r\\nMoreover the CI test of the `to_tf_dataset` ...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issues_dataset_1.set_format(\"pandas\")\n",
    "df: pd.DataFrame = issues_dataset_1[:]\n",
    "# OR df = issues_dataset_1.to_pandas()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'Cool, I think we can do both :)'</span>\n",
       " <span style=\"color: #008000; text-decoration-color: #008000\">'@lhoestq now the 2 are implemented.\\r\\n\\r\\nPlease note that for the the second protection, finally I have chosen </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">to protect the master branch only from **merge commits** (see update comment above), so no need to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">disable/re-enable the protection on each release (direct commits, different from merge commits, can be pushed to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the remote master branch; and eventually reverted without messing up the repo history).'</span><span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[32m'Cool, I think we can do both :\u001b[0m\u001b[32m)\u001b[0m\u001b[32m'\u001b[0m\n",
       " \u001b[32m'@lhoestq now the 2 are implemented.\\r\\n\\r\\nPlease note that for the the second protection, finally I have chosen \u001b[0m\n",
       "\u001b[32mto protect the master branch only from **merge commits** \u001b[0m\u001b[32m(\u001b[0m\u001b[32msee update comment above\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, so no need to \u001b[0m\n",
       "\u001b[32mdisable/re-enable the protection on each release \u001b[0m\u001b[32m(\u001b[0m\u001b[32mdirect commits, different from merge commits, can be pushed to \u001b[0m\n",
       "\u001b[32mthe remote master branch; and eventually reverted without messing up the repo history\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.'\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(df[\"comments\"].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# There are two comments in this particular comment index\n",
    "print(len(df[\"comments\"].iloc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>html_url</th>\n",
       "      <th>title</th>\n",
       "      <th>comments</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues/2945</td>\n",
       "      <td>Protect master branch</td>\n",
       "      <td>Cool, I think we can do both :)</td>\n",
       "      <td>After accidental merge commit (91c55355b634d0dc73350a7ddee1a6776dbbdd69) into `datasets` master branch, all commits present in the feature branch were permanently added to `datasets` master branch history, as e.g.:\\r\\n- 00cc036fea7c7745cfe722360036ed306796a3f2\\r\\n- 13ae8c98602bbad8197de3b9b425f4c78f582af1\\r\\n- ...\\r\\n\\r\\nI propose to protect our master branch, so that we avoid we can accidentally make this kind of mistakes in the future:\\r\\n- [x] For Pull Requests using GitHub, allow only squash merging, so that only a single commit per Pull Request is merged into the master branch\\r\\n  - ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues/2945</td>\n",
       "      <td>Protect master branch</td>\n",
       "      <td>@lhoestq now the 2 are implemented.\\r\\n\\r\\nPlease note that for the the second protection, finally I have chosen to protect the master branch only from **merge commits** (see update comment above), so no need to disable/re-enable the protection on each release (direct commits, different from merge commits, can be pushed to the remote master branch; and eventually reverted without messing up the repo history).</td>\n",
       "      <td>After accidental merge commit (91c55355b634d0dc73350a7ddee1a6776dbbdd69) into `datasets` master branch, all commits present in the feature branch were permanently added to `datasets` master branch history, as e.g.:\\r\\n- 00cc036fea7c7745cfe722360036ed306796a3f2\\r\\n- 13ae8c98602bbad8197de3b9b425f4c78f582af1\\r\\n- ...\\r\\n\\r\\nI propose to protect our master branch, so that we avoid we can accidentally make this kind of mistakes in the future:\\r\\n- [x] For Pull Requests using GitHub, allow only squash merging, so that only a single commit per Pull Request is merged into the master branch\\r\\n  - ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues/2943</td>\n",
       "      <td>Backwards compatibility broken for cached datasets that use `.filter()`</td>\n",
       "      <td>Hi ! I guess the caching mechanism should have considered the new `filter` to be different from the old one, and don't use cached results from the old `filter`.\\r\\nTo avoid other users from having this issue we could make the caching differentiate the two, what do you think ?</td>\n",
       "      <td>## Describe the bug\\r\\nAfter upgrading to datasets `1.12.0`, some cached `.filter()` steps from `1.11.0` started failing with \\r\\n`ValueError: Keys mismatch: between {'indices': Value(dtype='uint64', id=None)} and {'file': Value(dtype='string', id=None), 'text': Value(dtype='string', id=None), 'speaker_id': Value(dtype='int64', id=None), 'chapter_id': Value(dtype='int64', id=None), 'id': Value(dtype='string', id=None)}`\\r\\n\\r\\nRelated feature: https://github.com/huggingface/datasets/pull/2836\\r\\n\\r\\n:question:  This is probably a `wontfix` bug, since it can be solved by simply cleaning the...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              html_url  \\\n",
       "0  https://github.com/huggingface/datasets/issues/2945   \n",
       "1  https://github.com/huggingface/datasets/issues/2945   \n",
       "2  https://github.com/huggingface/datasets/issues/2943   \n",
       "\n",
       "                                                                     title  \\\n",
       "0                                                    Protect master branch   \n",
       "1                                                    Protect master branch   \n",
       "2  Backwards compatibility broken for cached datasets that use `.filter()`   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                       comments  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                               Cool, I think we can do both :)   \n",
       "1  @lhoestq now the 2 are implemented.\\r\\n\\r\\nPlease note that for the the second protection, finally I have chosen to protect the master branch only from **merge commits** (see update comment above), so no need to disable/re-enable the protection on each release (direct commits, different from merge commits, can be pushed to the remote master branch; and eventually reverted without messing up the repo history).   \n",
       "2                                                                                                                                          Hi ! I guess the caching mechanism should have considered the new `filter` to be different from the old one, and don't use cached results from the old `filter`.\\r\\nTo avoid other users from having this issue we could make the caching differentiate the two, what do you think ?   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      body  \n",
       "0  After accidental merge commit (91c55355b634d0dc73350a7ddee1a6776dbbdd69) into `datasets` master branch, all commits present in the feature branch were permanently added to `datasets` master branch history, as e.g.:\\r\\n- 00cc036fea7c7745cfe722360036ed306796a3f2\\r\\n- 13ae8c98602bbad8197de3b9b425f4c78f582af1\\r\\n- ...\\r\\n\\r\\nI propose to protect our master branch, so that we avoid we can accidentally make this kind of mistakes in the future:\\r\\n- [x] For Pull Requests using GitHub, allow only squash merging, so that only a single commit per Pull Request is merged into the master branch\\r\\n  - ...  \n",
       "1  After accidental merge commit (91c55355b634d0dc73350a7ddee1a6776dbbdd69) into `datasets` master branch, all commits present in the feature branch were permanently added to `datasets` master branch history, as e.g.:\\r\\n- 00cc036fea7c7745cfe722360036ed306796a3f2\\r\\n- 13ae8c98602bbad8197de3b9b425f4c78f582af1\\r\\n- ...\\r\\n\\r\\nI propose to protect our master branch, so that we avoid we can accidentally make this kind of mistakes in the future:\\r\\n- [x] For Pull Requests using GitHub, allow only squash merging, so that only a single commit per Pull Request is merged into the master branch\\r\\n  - ...  \n",
       "2  ## Describe the bug\\r\\nAfter upgrading to datasets `1.12.0`, some cached `.filter()` steps from `1.11.0` started failing with \\r\\n`ValueError: Keys mismatch: between {'indices': Value(dtype='uint64', id=None)} and {'file': Value(dtype='string', id=None), 'text': Value(dtype='string', id=None), 'speaker_id': Value(dtype='int64', id=None), 'chapter_id': Value(dtype='int64', id=None), 'id': Value(dtype='string', id=None)}`\\r\\n\\r\\nRelated feature: https://github.com/huggingface/datasets/pull/2836\\r\\n\\r\\n:question:  This is probably a `wontfix` bug, since it can be solved by simply cleaning the...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the explode method to create a row for each comment in a given index.\n",
    "# i.e. index 0 with 2 comments creates 2 rows and index 1 with 6 comments creates 6 rows, etc.\n",
    "comments_df: pd.DataFrame = df.explode(\"comments\", ignore_index=True)\n",
    "comments_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'comments', 'body'],\n",
       "    num_rows: 2964\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert back to Dataset\n",
    "comments_dataset: Dataset = Dataset.from_pandas(comments_df)\n",
    "comments_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e05cda94c5754850a0a3c6ff20b87282",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2964 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'comments', 'body', 'comment_length'],\n",
       "    num_rows: 2964\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add n new columns\n",
    "comments_dataset = comments_dataset.map(\n",
    "    lambda x: {\"comment_length\": len(x.get(\"comments\").split())}\n",
    ")\n",
    "\n",
    "comments_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'comments': ['https://github.com/huggingface/datasets/blob/6c766f9115d686182d76b1b937cb27e099c45d68/src/datasets/builder.py#L179-L186',\n",
       "  'https://github.com/huggingface/datasets/blob/6c766f9115d686182d76b1b937cb27e099c45d68/src/datasets/builder.py#L179-L186',\n",
       "  '@albertvillanova ',\n",
       "  'Thanks!',\n",
       "  '#self-assign',\n",
       "  '#take',\n",
       "  '#take',\n",
       "  '#self-assign',\n",
       "  'Resolved',\n",
       "  'Ty!'],\n",
       " 'comment_length': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Short comments\n",
    "comments_dataset.sort(\"comment_length\").select_columns([\"comments\", \"comment_length\"])[\n",
    "    :10\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b22db812c754220b3fae9efebc382a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/2964 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'comments', 'body', 'comment_length'],\n",
       "    num_rows: 2175\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop short comments.\n",
    "# i.e comments like: 'Thanks!', '#self-assign', etc.\n",
    "comments_dataset = comments_dataset.filter(lambda x: x.get(\"comment_length\") > 15)\n",
    "comments_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce810079d1b343039eaab1c3941e83ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2175 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'comments', 'body', 'comment_length', 'text'],\n",
       "    num_rows: 2175\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate the issue title, description (body), and comments together in a new text column.\n",
    "def concat_data(example: dict[str, Any]) -> dict[str, Any]:\n",
    "    \"\"\"This is used to concatenate the title, body and comments together.\"\"\"\n",
    "    title: str = example.get(\"title\")\n",
    "    comments: str = example.get(\"comments\")\n",
    "    body: str = example.get(\"body\")\n",
    "    result = {\"text\": f\"{title} \\n {body} \\n {comments}\"}\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "comments_dataset = comments_dataset.map(concat_data)\n",
    "comments_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'html_url': 'https://github.com/huggingface/datasets/issues/2945',\n",
       " 'title': 'Protect master branch',\n",
       " 'comments': '@lhoestq now the 2 are implemented.\\r\\n\\r\\nPlease note that for the the second protection, finally I have chosen to protect the master branch only from **merge commits** (see update comment above), so no need to disable/re-enable the protection on each release (direct commits, different from merge commits, can be pushed to the remote master branch; and eventually reverted without messing up the repo history).',\n",
       " 'body': 'After accidental merge commit (91c55355b634d0dc73350a7ddee1a6776dbbdd69) into `datasets` master branch, all commits present in the feature branch were permanently added to `datasets` master branch history, as e.g.:\\r\\n- 00cc036fea7c7745cfe722360036ed306796a3f2\\r\\n- 13ae8c98602bbad8197de3b9b425f4c78f582af1\\r\\n- ...\\r\\n\\r\\nI propose to protect our master branch, so that we avoid we can accidentally make this kind of mistakes in the future:\\r\\n- [x] For Pull Requests using GitHub, allow only squash merging, so that only a single commit per Pull Request is merged into the master branch\\r\\n  - Currently, simple merge commits are already disabled\\r\\n  - I propose to disable rebase merging as well\\r\\n- ~~Protect the master branch from direct pushes (to avoid accidentally pushing of merge commits)~~\\r\\n  - ~~This protection would reject direct pushes to master branch~~\\r\\n  - ~~If so, for each release (when we need to commit directly to the master branch), we should previously disable the protection and re-enable it again after the release~~\\r\\n- [x] Protect the master branch only from direct pushing of **merge commits**\\r\\n  - GitHub offers the possibility to protect the master branch only from merge commits (which are the ones that introduce all the commits from the feature branch into the master branch).\\r\\n  - No need to disable/re-enable this protection on each release \\r\\n\\r\\nThis purpose of this Issue is to open a discussion about this problem and to agree in a solution.',\n",
       " 'comment_length': 64,\n",
       " 'text': 'Protect master branch \\n After accidental merge commit (91c55355b634d0dc73350a7ddee1a6776dbbdd69) into `datasets` master branch, all commits present in the feature branch were permanently added to `datasets` master branch history, as e.g.:\\r\\n- 00cc036fea7c7745cfe722360036ed306796a3f2\\r\\n- 13ae8c98602bbad8197de3b9b425f4c78f582af1\\r\\n- ...\\r\\n\\r\\nI propose to protect our master branch, so that we avoid we can accidentally make this kind of mistakes in the future:\\r\\n- [x] For Pull Requests using GitHub, allow only squash merging, so that only a single commit per Pull Request is merged into the master branch\\r\\n  - Currently, simple merge commits are already disabled\\r\\n  - I propose to disable rebase merging as well\\r\\n- ~~Protect the master branch from direct pushes (to avoid accidentally pushing of merge commits)~~\\r\\n  - ~~This protection would reject direct pushes to master branch~~\\r\\n  - ~~If so, for each release (when we need to commit directly to the master branch), we should previously disable the protection and re-enable it again after the release~~\\r\\n- [x] Protect the master branch only from direct pushing of **merge commits**\\r\\n  - GitHub offers the possibility to protect the master branch only from merge commits (which are the ones that introduce all the commits from the feature branch into the master branch).\\r\\n  - No need to disable/re-enable this protection on each release \\r\\n\\r\\nThis purpose of this Issue is to open a discussion about this problem and to agree in a solution. \\n @lhoestq now the 2 are implemented.\\r\\n\\r\\nPlease note that for the the second protection, finally I have chosen to protect the master branch only from **merge commits** (see update comment above), so no need to disable/re-enable the protection on each release (direct commits, different from merge commits, can be pushed to the remote master branch; and eventually reverted without messing up the repo history).'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Text Embeddings\n",
    "\n",
    "- This [table](https://www.sbert.net/docs/pretrained_models.html#model-overview) shows the model overview for open source semantic search.\n",
    "\n",
    "<br>\n",
    "\n",
    "[![image.png](https://i.postimg.cc/MZLLL8TS/image.png)](https://postimg.cc/c6QTK2w9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cbce3988ec94c0a9f7a56507c4fb282",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)okenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87aa59cda9f84a7f9e264597bd5a7b1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4080d535bafd4e72ac31db01d13e47a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8a4a7004d594fd6b188c2a29707ee16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)cial_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0c7cbb73e7445f08560504346a89841",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)lve/main/config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f977069414e47f8993f8c4611032585",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "\n",
    "checkpoint: str = \"sentence-transformers/multi-qa-mpnet-base-dot-v1\"\n",
    "\n",
    "# Instantiate tokenizer\n",
    "tokenizer: AutoTokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "# Instantiate model\n",
    "model: AutoModel = AutoModel.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# To speed up the processing, push to a GPU\n",
    "device_str: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device(device=device_str)\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MPNetModel(\n",
       "  (embeddings): MPNetEmbeddings(\n",
       "    (word_embeddings): Embedding(30527, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): MPNetEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x MPNetLayer(\n",
       "        (attention): MPNetAttention(\n",
       "          (attn): MPNetSelfAttention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (intermediate): MPNetIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): MPNetOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (relative_attention_bias): Embedding(32, 12)\n",
       "  )\n",
       "  (pooler): MPNetPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To represent each GitHub issue entry as a single vector, we need to pool or\n",
    "# average the token embeddings. One popular approach is CLS pooling, where the last\n",
    "# hidden state for the special [CLS] token is collected.\n",
    "\n",
    "\n",
    "def cls_pooling(model_output):\n",
    "    \"\"\"To represent each GitHub issue entry as a single vector, pool or average the token embeddings.\n",
    "    Using CLS pooling, where the last hidden state for the special [CLS] token is collected.\"\"\"\n",
    "    return model_output.last_hidden_state[:, 0]\n",
    "\n",
    "\n",
    "# Create a helper function that will tokenize a list of documents, place the tensors\n",
    "# on the GPU (if available), feed them to the model, and finally apply CLS pooling to the outputs:\n",
    "\n",
    "\n",
    "def get_embeddings(text_list: list):\n",
    "    \"\"\"This is used to tokenize a list of documents, place the tensors\n",
    "    on the GPU (if available), feed them to the model, and apply CLS pooling to the outputs.\"\"\"\n",
    "    encoded_input = tokenizer(\n",
    "        text_list, padding=True, truncation=True, return_tensors=\"pt\"\n",
    "    )\n",
    "    # Push to encoded input to the GPU (if available)\n",
    "    encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n",
    "    model_output = model(**encoded_input)\n",
    "    return cls_pooling(model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Protect master branch \n",
       " After accidental merge commit <span style=\"font-weight: bold\">(</span>91c55355b634d0dc73350a7ddee1a6776dbbdd69<span style=\"font-weight: bold\">)</span> into `datasets` master branch, all \n",
       "commits present in the feature branch were permanently added to `datasets` master branch history, as e.g.:\n",
       "- 00cc036fea7c7745cfe722360036ed306796a3f2\n",
       "- 13ae8c98602bbad8197de3b9b425f4c78f582af1\n",
       "- <span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "\n",
       "I propose to protect our master branch, so that we avoid we can accidentally make this kind of mistakes in the \n",
       "future:\n",
       "-  For Pull Requests using GitHub, allow only squash merging, so that only a single commit per Pull Request is \n",
       "merged into the master branch\n",
       "  - Currently, simple merge commits are already disabled\n",
       "  - I propose to disable rebase merging as well\n",
       "- ~~Protect the master branch from direct pushes <span style=\"font-weight: bold\">(</span>to avoid accidentally pushing of merge commits<span style=\"font-weight: bold\">)</span>~~\n",
       "  - ~~This protection would reject direct pushes to master branch~~\n",
       "  - ~~If so, for each release <span style=\"font-weight: bold\">(</span>when we need to commit directly to the master branch<span style=\"font-weight: bold\">)</span>, we should previously disable \n",
       "the protection and re-enable it again after the release~~\n",
       "-  Protect the master branch only from direct pushing of **merge commits**\n",
       "  - GitHub offers the possibility to protect the master branch only from merge commits <span style=\"font-weight: bold\">(</span>which are the ones that \n",
       "introduce all the commits from the feature branch into the master branch<span style=\"font-weight: bold\">)</span>.\n",
       "  - No need to disable/re-enable this protection on each release \n",
       "\n",
       "This purpose of this Issue is to open a discussion about this problem and to agree in a solution. \n",
       " @lhoestq now the <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> are implemented.\n",
       "\n",
       "Please note that for the the second protection, finally I have chosen to protect the master branch only from \n",
       "**merge commits** <span style=\"font-weight: bold\">(</span>see update comment above<span style=\"font-weight: bold\">)</span>, so no need to disable/re-enable the protection on each release \n",
       "<span style=\"font-weight: bold\">(</span>direct commits, different from merge commits, can be pushed to the remote master branch; and eventually reverted \n",
       "without messing up the repo history<span style=\"font-weight: bold\">)</span>.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Protect master branch \n",
       " After accidental merge commit \u001b[1m(\u001b[0m91c55355b634d0dc73350a7ddee1a6776dbbdd69\u001b[1m)\u001b[0m into `datasets` master branch, all \n",
       "commits present in the feature branch were permanently added to `datasets` master branch history, as e.g.:\n",
       "- 00cc036fea7c7745cfe722360036ed306796a3f2\n",
       "- 13ae8c98602bbad8197de3b9b425f4c78f582af1\n",
       "- \u001b[33m...\u001b[0m\n",
       "\n",
       "I propose to protect our master branch, so that we avoid we can accidentally make this kind of mistakes in the \n",
       "future:\n",
       "-  For Pull Requests using GitHub, allow only squash merging, so that only a single commit per Pull Request is \n",
       "merged into the master branch\n",
       "  - Currently, simple merge commits are already disabled\n",
       "  - I propose to disable rebase merging as well\n",
       "- ~~Protect the master branch from direct pushes \u001b[1m(\u001b[0mto avoid accidentally pushing of merge commits\u001b[1m)\u001b[0m~~\n",
       "  - ~~This protection would reject direct pushes to master branch~~\n",
       "  - ~~If so, for each release \u001b[1m(\u001b[0mwhen we need to commit directly to the master branch\u001b[1m)\u001b[0m, we should previously disable \n",
       "the protection and re-enable it again after the release~~\n",
       "-  Protect the master branch only from direct pushing of **merge commits**\n",
       "  - GitHub offers the possibility to protect the master branch only from merge commits \u001b[1m(\u001b[0mwhich are the ones that \n",
       "introduce all the commits from the feature branch into the master branch\u001b[1m)\u001b[0m.\n",
       "  - No need to disable/re-enable this protection on each release \n",
       "\n",
       "This purpose of this Issue is to open a discussion about this problem and to agree in a solution. \n",
       " @lhoestq now the \u001b[1;36m2\u001b[0m are implemented.\n",
       "\n",
       "Please note that for the the second protection, finally I have chosen to protect the master branch only from \n",
       "**merge commits** \u001b[1m(\u001b[0msee update comment above\u001b[1m)\u001b[0m, so no need to disable/re-enable the protection on each release \n",
       "\u001b[1m(\u001b[0mdirect commits, different from merge commits, can be pushed to the remote master branch; and eventually reverted \n",
       "without messing up the repo history\u001b[1m)\u001b[0m.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(comments_dataset[\"text\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test that the function works by feeding it the first text entry in our corpus\n",
    "# and inspecting the output shape;\n",
    "embedding = get_embeddings(comments_dataset[\"text\"][0])\n",
    "embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.5532e-01, -1.0023e-01, -7.0321e-02, -7.9816e-02, -1.0425e-01,\n",
       "         -1.8799e-01,  1.0403e-02,  2.7286e-01, -9.8491e-03, -8.4140e-02,\n",
       "          2.9261e-01, -7.7875e-02, -1.3246e-01,  2.1589e-01, -5.6229e-02,\n",
       "          1.7055e-01,  2.0032e-01, -4.0798e-02, -9.2333e-02,  2.7923e-02,\n",
       "          2.2609e-02, -6.8703e-02,  8.3200e-02,  4.3630e-02, -1.8378e-01,\n",
       "          5.4009e-03, -1.4811e-02,  1.5650e-01, -4.3977e-01, -4.6957e-01,\n",
       "          1.4525e-01,  2.3224e-01,  2.5106e-02,  5.7053e-01, -1.0436e-04,\n",
       "         -3.4580e-04,  1.6949e-01, -4.0601e-02, -1.4010e-01, -2.1546e-01,\n",
       "         -4.9011e-01, -4.4480e-01, -7.0063e-02, -8.3547e-03,  1.2881e-01,\n",
       "          5.2868e-02, -1.3250e-01,  2.3846e-02,  3.8485e-01,  1.0555e-01,\n",
       "          2.3861e-01, -1.0285e-01,  1.8124e-01,  1.3770e-02,  3.9201e-01,\n",
       "          5.4745e-01, -6.6635e-02,  3.0320e-01,  5.4018e-02,  6.7668e-04,\n",
       "          6.1106e-02,  1.7411e-01, -8.2373e-02, -2.8667e-01,  2.2702e-01,\n",
       "         -1.7799e-01,  5.1817e-01, -4.2435e-01, -2.4158e-01, -8.3117e-02,\n",
       "          5.2379e-02, -1.7815e-01, -4.0890e-01, -2.4194e-01, -1.4995e-01,\n",
       "          7.6336e-03,  6.7781e-02,  2.4686e-01,  3.9740e-02,  3.6495e-03,\n",
       "         -3.7737e-01, -2.7181e-01, -4.4080e-03,  4.9204e-02,  1.4194e-01,\n",
       "         -1.6263e-01,  6.6426e-02, -1.2704e-01,  2.9422e-01, -4.7328e-02,\n",
       "         -1.4698e-01, -3.6763e-01, -2.4133e-01, -1.2587e-02, -1.0261e-01,\n",
       "         -1.9575e-01, -2.4621e-01, -3.2483e-01,  4.2513e-01,  3.8897e-01,\n",
       "         -3.8327e-01,  1.3740e-02, -3.0489e-02, -8.1016e-02,  2.0718e-01,\n",
       "          1.1786e-01, -1.0490e-02,  5.8489e-02,  5.3071e-01,  3.1444e-01,\n",
       "          5.1579e-03,  3.2559e-01,  3.0157e-01,  3.0475e-03,  3.4447e-02,\n",
       "          5.7412e-01,  4.2894e-01, -2.6292e-01,  1.2765e-01,  1.9597e-01,\n",
       "         -3.4120e-01, -3.8451e-01,  7.1131e-02, -8.6172e-02,  1.1033e-01,\n",
       "          2.7133e-02, -1.1471e-01,  6.6802e-02,  4.5191e-02, -3.8086e-02,\n",
       "         -3.0790e-01, -1.4149e-01, -3.7440e-01, -6.8530e-02,  1.8604e-01,\n",
       "         -4.5710e-01,  1.4917e-01,  4.3252e-01, -6.8038e-02, -1.9929e-01,\n",
       "          4.4552e-02,  1.2923e-01,  5.5586e-02,  2.9612e-01, -1.5380e-01,\n",
       "         -4.5263e-01,  2.2360e-01,  1.3133e-01, -6.6899e-02,  1.9400e-01,\n",
       "         -1.5990e-01, -2.6162e-01, -1.0498e-01,  2.9777e-01, -1.4827e-01,\n",
       "          3.0716e-01, -5.2377e-01,  3.6373e-02, -4.4834e-03,  3.5067e-01,\n",
       "          4.9104e-01,  3.0255e-01,  3.2808e-01, -1.6859e-01,  6.1757e-02,\n",
       "          1.9035e-01,  3.2351e-01, -5.1687e-02,  6.2305e-02, -2.8774e-01,\n",
       "          1.0950e-01,  1.4507e-01, -3.1216e-01,  1.7557e-02, -5.2359e-03,\n",
       "         -1.3128e-01,  1.0777e-02, -7.1407e-02, -1.1199e-01, -9.8930e-02,\n",
       "         -3.6669e-01,  6.9402e-02, -4.0737e-02, -1.0078e-01,  2.0796e-01,\n",
       "         -3.2017e-01, -9.4794e-02,  2.0508e-01, -1.4812e-01,  2.0898e-02,\n",
       "         -2.9199e-01, -6.0614e-01, -1.6366e-01, -6.3439e-02,  1.2491e-01,\n",
       "          2.2756e-01,  3.6077e-01, -5.0310e-03, -2.8362e-02, -1.7140e-01,\n",
       "         -9.3979e-03,  1.1251e-01,  3.4214e-01, -1.4968e-01, -2.4154e-01,\n",
       "          1.3225e-01, -3.5459e-01,  9.1141e-02, -7.8513e-02, -9.7753e-03,\n",
       "          3.4449e-02, -3.1579e-01,  3.4128e-01, -4.4053e-02, -1.2080e-01,\n",
       "         -1.1415e-01,  2.4152e-01,  9.5951e-02, -1.7473e-01, -2.0392e-01,\n",
       "         -2.0741e-01,  2.6019e-01, -2.4083e-01,  3.0187e-01,  1.3001e-01,\n",
       "          1.0264e-01, -1.4118e-01, -3.6454e-02, -2.5096e-02,  3.6194e-01,\n",
       "          2.3630e-01, -8.9917e-02, -5.9159e-02,  1.6876e-01, -4.9063e-02,\n",
       "          2.0357e-01,  2.2469e-01,  4.2524e-01,  3.7362e-01, -1.0268e-01,\n",
       "          6.2601e-02, -9.1793e-02, -7.8725e-02,  2.4594e-02, -3.2913e-01,\n",
       "          3.2804e-01,  7.3694e-02, -1.3946e-01, -1.7403e-01, -1.0333e-01,\n",
       "         -1.7407e-01, -2.2027e-01, -1.7077e-01, -1.2951e-02, -1.6910e-01,\n",
       "          1.9436e-01, -3.3659e-01,  2.2237e-01, -2.7721e-01,  1.4858e-01,\n",
       "          1.5052e-02, -2.5161e-01, -1.0240e-01,  6.8298e-02, -5.9668e-03,\n",
       "         -1.3271e-01,  1.2417e-01,  3.7738e-01,  1.2787e-01,  1.7010e-01,\n",
       "          4.2740e-03,  2.0152e-02,  2.5728e-01, -1.1644e-01,  3.5333e-01,\n",
       "          1.4431e-01, -1.0544e-01,  5.4065e-02,  3.3646e-01,  1.8122e-01,\n",
       "          6.4332e-02,  3.4692e-01, -3.3246e-03,  3.5258e-02, -2.0841e-01,\n",
       "         -4.5985e-02, -2.6021e-01, -5.2834e-01,  2.1924e-01, -1.1064e-01,\n",
       "         -3.5522e-01, -8.5036e-03, -4.5366e-02,  4.7418e-02, -5.2487e-01,\n",
       "          1.3603e-01,  1.8358e-01,  2.9053e-01, -3.1780e-01, -1.1998e-01,\n",
       "          1.2632e-01, -2.2595e-02,  1.2344e-02,  1.2856e-01,  3.6316e-01,\n",
       "         -2.2668e-01,  4.5945e-01,  2.3898e-01, -1.6956e-01, -4.7264e-01,\n",
       "         -3.4341e-01,  1.9130e-02, -2.0394e-01, -1.1797e-01,  9.8188e-03,\n",
       "         -9.8931e-02,  9.2153e-02, -3.5854e-01, -3.2571e-01, -1.8774e-02,\n",
       "         -3.7300e-01, -1.6010e-01,  6.9595e-02,  6.2886e-02, -2.3462e-01,\n",
       "         -1.1404e-01, -3.8659e-01, -2.8215e-01,  4.7936e-01, -1.6433e-02,\n",
       "          1.1566e-01, -4.3174e-02, -2.9238e-01, -4.5918e-02,  8.7482e-02,\n",
       "          1.3664e-01, -4.4778e-05, -3.6093e-01,  3.2567e-02, -4.0660e-02,\n",
       "         -2.9347e-02,  3.5261e-01,  1.8549e-01, -4.3024e-01,  7.1251e-02,\n",
       "         -3.2176e-01, -1.5582e-01, -5.2085e-02,  4.8644e-01,  3.1412e-01,\n",
       "         -9.7374e-02,  7.1080e-02,  1.0355e-01, -1.9190e-01,  5.5089e-02,\n",
       "         -1.8161e-01,  1.4722e-01,  3.4549e-01,  2.8572e-01,  3.7900e-02,\n",
       "          1.4587e-01,  2.5497e-02,  7.1409e-01,  3.7785e-01, -5.9658e-02,\n",
       "          2.1034e-01,  3.1070e-01, -2.7719e-02,  3.9907e-03, -2.6016e-01,\n",
       "          6.5047e-02, -1.0124e-01, -2.0858e-05,  8.6704e-02, -1.2958e-01,\n",
       "         -1.2297e-01, -1.1211e-02, -4.7771e-01,  9.1553e-03, -4.1468e-01,\n",
       "         -2.3966e-01, -3.1818e-01,  1.8425e-01, -2.2526e-02, -4.8549e-01,\n",
       "          1.3138e-01,  5.3771e-02,  2.3058e-01,  2.1597e-01,  2.1421e-01,\n",
       "          1.8122e-01, -2.0259e-02, -5.1085e-02, -1.2007e-01, -6.7726e-02,\n",
       "         -4.3315e-03,  1.4525e-01,  9.0580e-02, -1.6704e-01,  3.1806e-02,\n",
       "          6.9550e-02,  3.2999e-01, -1.4868e-02,  1.4029e-01, -1.7081e-01,\n",
       "         -5.0948e-01,  6.0863e-01, -3.1859e-01,  1.3940e-01,  2.9910e-01,\n",
       "          5.1123e-01,  4.3976e-01, -2.9545e-01, -1.9532e-01,  2.0712e-01,\n",
       "          3.2556e-02,  1.9784e-01,  1.7735e-01,  1.4498e-01,  2.7831e-01,\n",
       "         -4.9038e-01,  2.8877e-01,  1.5483e-01, -2.5048e-01, -1.5518e-01,\n",
       "         -3.9057e-03, -2.7051e-02,  1.2646e-01, -2.6305e-02, -2.4540e-01,\n",
       "          9.1276e-03, -1.0944e-01,  1.7345e-01,  1.3292e-01,  6.9828e-02,\n",
       "          4.1522e-01,  5.9466e-01,  1.3493e-01, -3.8920e-01, -4.8202e-02,\n",
       "         -1.7507e-01, -7.4283e-02,  3.2714e-01,  1.9835e-01,  2.4823e-01,\n",
       "          2.0349e-02,  1.7769e-01, -1.2347e-01, -2.6624e-01, -2.0337e-01,\n",
       "         -3.8491e-02, -7.6742e-02,  7.5898e-03,  8.7330e-02, -4.5898e-02,\n",
       "         -9.0992e-02,  1.3048e-02,  1.4000e-01, -3.6610e-01,  1.6378e-01,\n",
       "          6.1805e-01,  9.8255e-01,  3.9757e-01,  8.9033e-02, -2.5466e-01,\n",
       "         -3.2738e-01,  3.2599e-01, -3.4011e-01, -1.6645e-01, -2.4702e-01,\n",
       "         -1.2359e-01, -1.8425e-01, -3.4031e-01, -3.8694e-02, -4.8985e-01,\n",
       "         -3.5286e-01,  1.3511e-01, -2.1806e-01,  5.2792e-01,  8.1777e-03,\n",
       "          1.2339e-01,  1.9102e-01, -1.2231e-01,  2.5515e-01,  1.9827e-01,\n",
       "          6.1811e-02,  1.0410e-01, -4.0280e-03, -4.6879e-01, -2.2320e-03,\n",
       "          2.0539e-02, -3.7727e-01,  8.9147e-02, -3.4378e-01,  3.8847e-02,\n",
       "         -2.0635e-01, -3.1605e-01,  1.1583e-01, -8.7048e-02,  6.0594e-01,\n",
       "          1.6663e-01, -7.9452e-02, -4.4621e-02,  2.9139e-01,  4.6452e-01,\n",
       "         -2.2885e-01, -2.1953e-01,  5.2237e-01, -2.1336e-01, -1.3022e-01,\n",
       "         -2.0476e-01, -8.0335e-02, -2.3709e-01, -2.6095e-01, -1.2804e-01,\n",
       "         -3.2823e-01,  7.9502e-02,  4.1714e-02,  2.6365e-01, -9.1070e-02,\n",
       "         -1.6761e-01,  1.8599e-01,  1.1549e-01, -1.0757e-01, -1.1858e-01,\n",
       "         -1.6952e-01, -2.6104e-01, -6.7816e-02,  2.0160e-01,  1.6161e-01,\n",
       "         -1.2061e-01, -7.6764e-02,  9.9658e-02, -2.4434e-01, -2.2079e-01,\n",
       "         -2.0729e-01,  1.0599e-01, -1.3947e-01,  9.2981e-02, -5.4868e-01,\n",
       "         -3.2993e-01,  5.1746e-02,  2.4619e-01,  2.7942e-01,  3.3050e-01,\n",
       "          1.1800e-01, -9.9771e-02, -2.2445e-01,  2.1864e-01, -3.2793e-01,\n",
       "          1.6865e-01,  2.3585e-02,  1.5881e-01, -2.6010e-02,  4.0431e-02,\n",
       "         -3.9060e-01,  1.6032e-01, -1.2548e-01, -2.1127e-02, -2.2552e-01,\n",
       "         -1.0175e-01, -6.3097e-04, -3.0819e-01,  3.2487e-02,  2.3434e-02,\n",
       "         -1.9576e-01, -2.8302e-01,  9.2669e-02,  6.5696e-02,  6.4789e-02,\n",
       "         -5.7014e-02, -5.7461e-02, -1.0816e-01,  8.2753e-02,  2.7203e-01,\n",
       "          2.3969e-01, -1.5849e-01,  1.0971e-01, -3.2225e-02,  1.7109e-01,\n",
       "          1.1058e-01, -2.4773e-02,  8.9242e-02,  1.4633e-02, -2.6580e-01,\n",
       "         -3.5375e-02,  2.6716e-01, -1.8175e-01, -2.1714e-01, -2.6854e-01,\n",
       "          1.8883e-01,  1.0563e-01,  5.8409e-01,  2.0288e-01,  1.4422e-01,\n",
       "         -4.0508e-01,  1.7541e-01, -5.3198e-02, -3.6357e-03, -1.6924e-01,\n",
       "         -9.6940e-02, -8.4962e-02,  2.7936e-01, -8.6720e-02, -3.0360e-01,\n",
       "          2.7138e-01,  1.8818e-01,  7.4830e-04,  1.0497e-01,  5.2807e-01,\n",
       "          1.2962e-01, -1.8965e-01,  4.2214e-01,  2.7124e-01, -8.7676e-02,\n",
       "          4.9490e-01,  4.7023e-01, -1.1318e-01, -2.0325e-02, -8.2736e-02,\n",
       "          1.1661e-01,  2.3069e-01, -2.5410e-01,  1.2847e-01,  2.9622e-01,\n",
       "          6.4630e-01, -1.6888e-01,  3.9356e-01, -3.3096e-02,  3.4374e-01,\n",
       "          1.9799e-01,  5.8740e-02, -2.4605e-01,  3.9271e-01,  2.1171e-01,\n",
       "          3.3015e-01, -6.1899e-01,  1.3637e-01,  4.9140e-01,  4.9283e-02,\n",
       "          1.9958e-01, -4.0011e-01, -4.9233e-02, -5.8476e-02, -4.8314e-02,\n",
       "         -7.8183e-02,  1.6350e-01,  1.2374e-01,  2.8297e-01,  6.5844e-03,\n",
       "         -8.8378e-02, -4.3441e-02,  1.8505e-01,  2.4156e-01, -1.1700e-01,\n",
       "         -1.8117e-01,  3.5776e-01, -2.7620e-01,  6.6493e-02,  2.1546e-01,\n",
       "          1.3507e-01, -7.4716e-02, -6.8634e-02,  1.8023e-01,  4.5039e-01,\n",
       "         -1.6993e-01,  2.2512e-02,  5.5749e-02, -1.6257e-01,  1.5772e-01,\n",
       "          8.0147e-02,  2.5803e-01, -5.3341e-02,  1.1280e-01,  3.5427e-02,\n",
       "         -4.1530e-01, -2.5322e-02,  4.3308e-01,  5.5646e-01, -3.3773e-01,\n",
       "          4.6014e-01,  1.1927e-01,  2.8470e-01, -3.2684e-02,  4.3744e-01,\n",
       "         -4.7126e-02,  1.1639e-01, -5.1538e-03,  1.4308e-01,  2.8089e-02,\n",
       "          2.5759e-01,  1.4150e-01, -3.3345e-02, -1.5929e-01,  3.0106e-02,\n",
       "         -1.2154e-01,  1.2744e-01,  2.9684e-01,  1.7307e-01,  4.1989e-01,\n",
       "         -2.9471e-01,  1.2757e-01,  1.2823e-01, -8.7569e-02, -3.4140e-01,\n",
       "          9.5221e-03,  9.4085e-02, -3.6053e-01,  4.2475e-03,  2.2248e-01,\n",
       "         -6.9440e-02, -2.4195e-02,  1.1893e-01,  4.0948e-01,  1.9279e-01,\n",
       "          1.1209e-01,  1.0513e-01, -4.9348e-02,  3.2094e-01, -3.9185e-01,\n",
       "          2.2496e-01,  1.8922e-01, -1.0293e-01, -2.2189e-01, -5.1098e-02,\n",
       "          7.3670e-03,  2.1281e-02, -4.3551e-01, -1.4018e-01, -1.6223e-01,\n",
       "          4.2098e-01, -1.3560e-01,  6.8298e-02, -3.7222e-01,  6.8767e-02,\n",
       "         -3.7135e-02,  2.6836e-01, -3.0010e-01,  8.6408e-02,  5.2548e-02,\n",
       "          1.7338e-02,  8.8371e-02,  1.3058e-01,  6.8434e-02,  2.5285e-01,\n",
       "         -3.5232e-01, -1.2489e-01,  3.5866e-01, -1.9469e-02,  9.1036e-02,\n",
       "         -2.1727e-01,  4.1820e-01,  4.6545e-01, -1.4808e-01, -3.3211e-01,\n",
       "         -2.6352e-01, -1.1156e-02, -2.7983e-01, -5.4209e-01,  2.9864e-01,\n",
       "          1.3787e-01,  1.9744e-01, -1.0587e-01,  3.6607e-02, -7.4327e-02,\n",
       "         -1.5276e-01,  6.1171e-02, -1.9879e-01]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "272de99e447e4ef3a90d808acee6412a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2175 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/neidu/Desktop/Projects/Personal/My_Projects/NLP-Tutorial/notebook/06_Transformers/05_semantic_search.ipynb Cell 30\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/neidu/Desktop/Projects/Personal/My_Projects/NLP-Tutorial/notebook/06_Transformers/05_semantic_search.ipynb#X55sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Get embeddings for the entire dataset\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/neidu/Desktop/Projects/Personal/My_Projects/NLP-Tutorial/notebook/06_Transformers/05_semantic_search.ipynb#X55sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m comments_dataset \u001b[39m=\u001b[39m comments_dataset\u001b[39m.\u001b[39;49mmap(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/neidu/Desktop/Projects/Personal/My_Projects/NLP-Tutorial/notebook/06_Transformers/05_semantic_search.ipynb#X55sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mlambda\u001b[39;49;00m x: {\u001b[39m\"\u001b[39;49m\u001b[39membeddings\u001b[39;49m\u001b[39m\"\u001b[39;49m: get_embeddings(x\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mtext\u001b[39;49m\u001b[39m\"\u001b[39;49m))\u001b[39m.\u001b[39;49mdetach()\u001b[39m.\u001b[39;49mcpu()\u001b[39m.\u001b[39;49mnumpy()[\u001b[39m0\u001b[39;49m]}\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/neidu/Desktop/Projects/Personal/My_Projects/NLP-Tutorial/notebook/06_Transformers/05_semantic_search.ipynb#X55sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m )\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/neidu/Desktop/Projects/Personal/My_Projects/NLP-Tutorial/notebook/06_Transformers/05_semantic_search.ipynb#X55sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m comments_dataset\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp_p310/lib/python3.10/site-packages/datasets/arrow_dataset.py:592\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    590\u001b[0m     \u001b[39mself\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mself\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    591\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 592\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    593\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[1;32m    594\u001b[0m \u001b[39mfor\u001b[39;00m dataset \u001b[39min\u001b[39;00m datasets:\n\u001b[1;32m    595\u001b[0m     \u001b[39m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp_p310/lib/python3.10/site-packages/datasets/arrow_dataset.py:557\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    550\u001b[0m self_format \u001b[39m=\u001b[39m {\n\u001b[1;32m    551\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_type,\n\u001b[1;32m    552\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mformat_kwargs\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_kwargs,\n\u001b[1;32m    553\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_columns,\n\u001b[1;32m    554\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39moutput_all_columns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_all_columns,\n\u001b[1;32m    555\u001b[0m }\n\u001b[1;32m    556\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 557\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    558\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[1;32m    559\u001b[0m \u001b[39m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp_p310/lib/python3.10/site-packages/datasets/arrow_dataset.py:3097\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3090\u001b[0m \u001b[39mif\u001b[39;00m transformed_dataset \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3091\u001b[0m     \u001b[39mwith\u001b[39;00m logging\u001b[39m.\u001b[39mtqdm(\n\u001b[1;32m   3092\u001b[0m         disable\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m logging\u001b[39m.\u001b[39mis_progress_bar_enabled(),\n\u001b[1;32m   3093\u001b[0m         unit\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m examples\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   3094\u001b[0m         total\u001b[39m=\u001b[39mpbar_total,\n\u001b[1;32m   3095\u001b[0m         desc\u001b[39m=\u001b[39mdesc \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mMap\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   3096\u001b[0m     ) \u001b[39mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3097\u001b[0m         \u001b[39mfor\u001b[39;00m rank, done, content \u001b[39min\u001b[39;00m Dataset\u001b[39m.\u001b[39m_map_single(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdataset_kwargs):\n\u001b[1;32m   3098\u001b[0m             \u001b[39mif\u001b[39;00m done:\n\u001b[1;32m   3099\u001b[0m                 shards_done \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp_p310/lib/python3.10/site-packages/datasets/arrow_dataset.py:3450\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3448\u001b[0m _time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m   3449\u001b[0m \u001b[39mfor\u001b[39;00m i, example \u001b[39min\u001b[39;00m shard_iterable:\n\u001b[0;32m-> 3450\u001b[0m     example \u001b[39m=\u001b[39m apply_function_on_filtered_inputs(example, i, offset\u001b[39m=\u001b[39;49moffset)\n\u001b[1;32m   3451\u001b[0m     \u001b[39mif\u001b[39;00m update_data:\n\u001b[1;32m   3452\u001b[0m         \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp_p310/lib/python3.10/site-packages/datasets/arrow_dataset.py:3353\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[0;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   3351\u001b[0m \u001b[39mif\u001b[39;00m with_rank:\n\u001b[1;32m   3352\u001b[0m     additional_args \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (rank,)\n\u001b[0;32m-> 3353\u001b[0m processed_inputs \u001b[39m=\u001b[39m function(\u001b[39m*\u001b[39;49mfn_args, \u001b[39m*\u001b[39;49madditional_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfn_kwargs)\n\u001b[1;32m   3354\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(processed_inputs, LazyDict):\n\u001b[1;32m   3355\u001b[0m     processed_inputs \u001b[39m=\u001b[39m {\n\u001b[1;32m   3356\u001b[0m         k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m processed_inputs\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m processed_inputs\u001b[39m.\u001b[39mkeys_to_format\n\u001b[1;32m   3357\u001b[0m     }\n",
      "\u001b[1;32m/Users/neidu/Desktop/Projects/Personal/My_Projects/NLP-Tutorial/notebook/06_Transformers/05_semantic_search.ipynb Cell 30\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/neidu/Desktop/Projects/Personal/My_Projects/NLP-Tutorial/notebook/06_Transformers/05_semantic_search.ipynb#X55sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Get embeddings for the entire dataset\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/neidu/Desktop/Projects/Personal/My_Projects/NLP-Tutorial/notebook/06_Transformers/05_semantic_search.ipynb#X55sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m comments_dataset \u001b[39m=\u001b[39m comments_dataset\u001b[39m.\u001b[39mmap(\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/neidu/Desktop/Projects/Personal/My_Projects/NLP-Tutorial/notebook/06_Transformers/05_semantic_search.ipynb#X55sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mlambda\u001b[39;00m x: {\u001b[39m\"\u001b[39m\u001b[39membeddings\u001b[39m\u001b[39m\"\u001b[39m: get_embeddings(x\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mtext\u001b[39;49m\u001b[39m\"\u001b[39;49m))\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()[\u001b[39m0\u001b[39m]}\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/neidu/Desktop/Projects/Personal/My_Projects/NLP-Tutorial/notebook/06_Transformers/05_semantic_search.ipynb#X55sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m )\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/neidu/Desktop/Projects/Personal/My_Projects/NLP-Tutorial/notebook/06_Transformers/05_semantic_search.ipynb#X55sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m comments_dataset\n",
      "\u001b[1;32m/Users/neidu/Desktop/Projects/Personal/My_Projects/NLP-Tutorial/notebook/06_Transformers/05_semantic_search.ipynb Cell 30\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/neidu/Desktop/Projects/Personal/My_Projects/NLP-Tutorial/notebook/06_Transformers/05_semantic_search.ipynb#X55sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m encoded_input \u001b[39m=\u001b[39m tokenizer(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/neidu/Desktop/Projects/Personal/My_Projects/NLP-Tutorial/notebook/06_Transformers/05_semantic_search.ipynb#X55sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     text_list, padding\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/neidu/Desktop/Projects/Personal/My_Projects/NLP-Tutorial/notebook/06_Transformers/05_semantic_search.ipynb#X55sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/neidu/Desktop/Projects/Personal/My_Projects/NLP-Tutorial/notebook/06_Transformers/05_semantic_search.ipynb#X55sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m encoded_input \u001b[39m=\u001b[39m {k: v\u001b[39m.\u001b[39mto(device) \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m encoded_input\u001b[39m.\u001b[39mitems()}\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/neidu/Desktop/Projects/Personal/My_Projects/NLP-Tutorial/notebook/06_Transformers/05_semantic_search.ipynb#X55sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m model_output \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mencoded_input)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/neidu/Desktop/Projects/Personal/My_Projects/NLP-Tutorial/notebook/06_Transformers/05_semantic_search.ipynb#X55sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mreturn\u001b[39;00m cls_pooling(model_output)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp_p310/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp_p310/lib/python3.10/site-packages/transformers/models/mpnet/modeling_mpnet.py:551\u001b[0m, in \u001b[0;36mMPNetModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m    549\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    550\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(input_ids\u001b[39m=\u001b[39minput_ids, position_ids\u001b[39m=\u001b[39mposition_ids, inputs_embeds\u001b[39m=\u001b[39minputs_embeds)\n\u001b[0;32m--> 551\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m    552\u001b[0m     embedding_output,\n\u001b[1;32m    553\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m    554\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    555\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    556\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    557\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    558\u001b[0m )\n\u001b[1;32m    559\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    560\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp_p310/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp_p310/lib/python3.10/site-packages/transformers/models/mpnet/modeling_mpnet.py:341\u001b[0m, in \u001b[0;36mMPNetEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[39mif\u001b[39;00m output_hidden_states:\n\u001b[1;32m    339\u001b[0m     all_hidden_states \u001b[39m=\u001b[39m all_hidden_states \u001b[39m+\u001b[39m (hidden_states,)\n\u001b[0;32m--> 341\u001b[0m layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    342\u001b[0m     hidden_states,\n\u001b[1;32m    343\u001b[0m     attention_mask,\n\u001b[1;32m    344\u001b[0m     head_mask[i],\n\u001b[1;32m    345\u001b[0m     position_bias,\n\u001b[1;32m    346\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    347\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    348\u001b[0m )\n\u001b[1;32m    349\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    351\u001b[0m \u001b[39mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp_p310/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp_p310/lib/python3.10/site-packages/transformers/models/mpnet/modeling_mpnet.py:311\u001b[0m, in \u001b[0;36mMPNetLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, position_bias, output_attentions, **kwargs)\u001b[0m\n\u001b[1;32m    308\u001b[0m outputs \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add self attentions if we output attention weights\u001b[39;00m\n\u001b[1;32m    310\u001b[0m intermediate_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintermediate(attention_output)\n\u001b[0;32m--> 311\u001b[0m layer_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput(intermediate_output, attention_output)\n\u001b[1;32m    312\u001b[0m outputs \u001b[39m=\u001b[39m (layer_output,) \u001b[39m+\u001b[39m outputs\n\u001b[1;32m    313\u001b[0m \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp_p310/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp_p310/lib/python3.10/site-packages/transformers/models/mpnet/modeling_mpnet.py:278\u001b[0m, in \u001b[0;36mMPNetOutput.forward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states: torch\u001b[39m.\u001b[39mTensor, input_tensor: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m--> 278\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdense(hidden_states)\n\u001b[1;32m    279\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(hidden_states)\n\u001b[1;32m    280\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mLayerNorm(hidden_states \u001b[39m+\u001b[39m input_tensor)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp_p310/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp_p310/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Get embeddings for the entire dataset\n",
    "comments_dataset = comments_dataset.map(\n",
    "    lambda x: {\"embeddings\": get_embeddings(x.get(\"text\")).detach().cpu().numpy()[0]}\n",
    ")\n",
    "\n",
    "comments_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dataset = comments_dataset.map(\n",
    "    lambda x: {\"embeddings\": get_embeddings(x[\"text\"]).detach().cpu().numpy()[0]}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_p310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
