{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chineidu/NLP-Tutorial/blob/main/notebook/06_Transformers/6b_types_of_tokenizers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjsgMP49KDxE"
      },
      "source": [
        "## [Byte-Pair Encoding Tokenization (BPE)](https://huggingface.co/learn/nlp-course/chapter6/5?fw=pt)\n",
        "\n",
        "```text\n",
        "- Byte-Pair Encoding (BPE) was initially developed as an algorithm to compress texts, and then used by OpenAI for tokenization when pretraining the GPT model.\n",
        "- It’s used by a lot of Transformer models, including GPT, GPT-2, RoBERTa, BART, and DeBERTa.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wgPR2fygKDxG",
        "outputId": "bbd2dcca-ac70-4469-8632-cca54627d14d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (13.6.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich) (0.1.2)\n",
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.34.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.12.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.1)\n",
            "Requirement already satisfied: torch!=1.12.0,>=1.10 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.1.0+cu118)\n",
            "Requirement already satisfied: accelerate>=0.20.3 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.23.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.20.3->transformers[torch]) (5.9.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers[torch]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers[torch]) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (2.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=1.12.0,>=1.10->transformers[torch]) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.12.0,>=1.10->transformers[torch]) (1.3.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.14.5)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.17.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.18.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.3.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install rich\n",
        "!pip install transformers[torch]\n",
        "!pip install torch datasets evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7yv7waD9KDxH"
      },
      "outputs": [],
      "source": [
        "# Built-in library\n",
        "import re\n",
        "import json\n",
        "from typing import Any, Dict, List, Optional, Union\n",
        "import logging\n",
        "import warnings\n",
        "\n",
        "# Standard imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from rich import print\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Pandas settings\n",
        "pd.options.display.max_rows = 1_000\n",
        "pd.options.display.max_columns = 1_000\n",
        "pd.options.display.max_colwidth = 600\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Black code formatter (Optional)\n",
        "# %load_ext lab_black\n",
        "\n",
        "# auto reload imports\n",
        "# %load_ext autoreload\n",
        "# %autoreload 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUSzyEXwKDxH"
      },
      "source": [
        "### Training algorithm\n",
        "\n",
        "```text\n",
        "- Byte-Pair Encoding Tokenization (BPE) training starts by computing the unique set of words used in the corpus (after the normalization and pre-tokenization steps are completed), then building the vocabulary by taking all the symbols used to write those words.\n",
        "- As a very simple example, let’s say our corpus uses these five words:\n",
        "  \"hug\", \"pug\", \"pun\", \"bun\", \"hugs\"\n",
        "\n",
        "- The base vocabulary will then be [\"b\", \"g\", \"h\", \"n\", \"p\", \"s\", \"u\"].\n",
        "- For real-world cases, that base vocabulary will contain all the ASCII characters, at the very least, and probably some Unicode characters as well.\n",
        "- If an example you are tokenizing uses a character that is not in the training corpus, that character will be converted to the unknown token.\n",
        "- That’s one reason why lots of NLP models are very bad at analyzing content with emojis, for instance.\n",
        "\n",
        "- The GPT-2 and RoBERTa tokenizers (which are pretty similar) have a clever way to deal with this: they don’t look at words as being written with Unicode characters, but with bytes.\n",
        "- This way the base vocabulary has a small size (256), but every character you can think of will still be included and not end up being converted to the unknown token.\n",
        "- This trick is called byte-level BPE.\n",
        "- After getting this base vocabulary, we add new tokens until the desired vocabulary size is reached by learning merges, which are rules to merge two elements of the existing vocabulary together into a new one.\n",
        "- So, at the beginning these merges will create tokens with two characters, and then, as training progresses, longer subwords.\n",
        "- At any step during the tokenizer training, the BPE algorithm will search for the most frequent pair of existing tokens (by “pair,” here we mean two consecutive tokens in a word).\n",
        "- That most frequent pair is the one that will be merged, and we rinse and repeat for the next step.\n",
        "\n",
        "- Going back to our previous example, let’s assume the words had the following frequencies:\n",
        "```\n",
        "\n",
        "```python\n",
        "(\"hug\", 10), (\"pug\", 5), (\"pun\", 12), (\"bun\", 4), (\"hugs\", 5)\n",
        "```\n",
        "\n",
        "```text\n",
        "meaning \"hug\" was present 10 times in the corpus, \"pug\" 5 times, \"pun\" 12 times, \"bun\" 4 times, and \"hugs\" 5 times. We start the training by splitting each word into characters (the ones that form our initial vocabulary) so we can see each word as a list of tokens:\n",
        "```\n",
        "\n",
        "```python\n",
        "(\"h\" \"u\" \"g\", 10), (\"p\" \"u\" \"g\", 5), (\"p\" \"u\" \"n\", 12), (\"b\" \"u\" \"n\", 4), (\"h\" \"u\" \"g\" \"s\", 5)\n",
        "```\n",
        "\n",
        "```text\n",
        "Then we look at pairs. The pair (\"h\", \"u\") is present in the words \"hug\" and \"hugs\", so 15 times total in the corpus. It’s not the most frequent pair, though: that honor belongs to (\"u\", \"g\"), which is present in \"hug\", \"pug\", and \"hugs\", for a grand total of 20 times in the vocabulary.\n",
        "\n",
        "Thus, the first merge rule learned by the tokenizer is (\"u\", \"g\") -> \"ug\", which means that \"ug\" will be added to the vocabulary, and the pair should be merged in all the words of the corpus. At the end of this stage, the vocabulary and corpus look like this:\n",
        "```\n",
        "\n",
        "```python\n",
        "Vocabulary: [\"b\", \"g\", \"h\", \"n\", \"p\", \"s\", \"u\", \"ug\"]\n",
        "Corpus: (\"h\" \"ug\", 10), (\"p\" \"ug\", 5), (\"p\" \"u\" \"n\", 12), (\"b\" \"u\" \"n\", 4), (\"h\" \"ug\" \"s\", 5)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2A5If4XQKDxI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMMM8WJ1KDxI"
      },
      "source": [
        "<br>\n",
        "\n",
        "```text\n",
        "- Now we have some pairs that result in a token longer than two characters: the pair (\"h\", \"ug\"), for instance (present 15 times in the corpus). The most frequent pair at this stage is (\"u\", \"n\"), however, present 16 times in the corpus, so the second merge rule learned is (\"u\", \"n\") -> \"un\".\n",
        "- Adding that to the vocabulary and merging all existing occurrences leads us to:\n",
        "```\n",
        "\n",
        "```python\n",
        "Vocabulary: [\"b\", \"g\", \"h\", \"n\", \"p\", \"s\", \"u\", \"ug\", \"un\"]\n",
        "Corpus: (\"h\" \"ug\", 10), (\"p\" \"ug\", 5), (\"p\" \"un\", 12), (\"b\" \"un\", 4), (\"h\" \"ug\" \"s\", 5)\n",
        "```\n",
        "\n",
        "```text\n",
        "Now the most frequent pair is (\"h\", \"ug\"), so we learn the merge rule (\"h\", \"ug\") -> \"hug\", which gives us our first three-letter token. After the merge, the corpus looks like this:\n",
        "```\n",
        "\n",
        "```python\n",
        "Vocabulary: [\"b\", \"g\", \"h\", \"n\", \"p\", \"s\", \"u\", \"ug\", \"un\", \"hug\"]\n",
        "Corpus: (\"hug\", 10), (\"p\" \"ug\", 5), (\"p\" \"un\", 12), (\"b\" \"un\", 4), (\"hug\" \"s\", 5)\n",
        "```\n",
        "\n",
        "<hr><br>\n",
        "\n",
        "### Tokenization algorithm\n",
        "Tokenization follows the training process closely, in the sense that new inputs are tokenized by applying the following steps:\n",
        "\n",
        "```text\n",
        "1. Normalization\n",
        "2. Pre-tokenization\n",
        "3. Splitting the words into individual characters\n",
        "4. Applying the merge rules learned in order on those splits\n",
        "\n",
        "Let’s take the example we used during training, with the three merge rules learned:\n",
        "```\n",
        "\n",
        "```python\n",
        "\n",
        "(\"u\", \"g\") -> \"ug\"\n",
        "(\"u\", \"n\") -> \"un\"\n",
        "(\"h\", \"ug\") -> \"hug\"\n",
        "```\n",
        "\n",
        "```text\n",
        "- The word \"bug\" will be tokenized as [\"b\", \"ug\"]. \"mug\", however, will be tokenized as [\"[UNK]\", \"ug\"] since the letter \"m\" was not in the base vocabulary.\n",
        "- Likewise, the word \"thug\" will be tokenized as [\"[UNK]\", \"hug\"]: the letter \"t\" is not in the base vocabulary, and applying the merge rules results first in \"u\" and \"g\" being merged and then \"hu\" and \"g\" being merged.\n",
        "```\n",
        "\n",
        "<br><br>\n",
        "\n",
        "#### Implementing Byte-Pair Encoding Tokenization (BPE)\n",
        "\n",
        "```text\n",
        "- Now let’s take a look at an implementation of the BPE algorithm.\n",
        "- This won’t be an optimized version you can actually use on a big corpus; we just want to show you the code so you can understand the algorithm a little bit better.\n",
        "- First we need a corpus, so let’s create a simple one with a few sentences:\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "UYnswAkVKDxI"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "\n",
        "tokenizer:AutoTokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Pre-tokenize the corpus into words. Since we are replicating a BPE tokenizer (like GPT-2), we will use\n",
        "# the gpt2 tokenizer for the pre-tokenization:\n",
        "corpus: list[str] = [\n",
        "    \"This is the Hugging Face Course.\",\n",
        "    \"This chapter is about tokenization.\",\n",
        "    \"This section shows several tokenizer algorithms.\",\n",
        "    \"Hopefully, you will be able to understand how they are trained and generate tokens.\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "z2NH3L3CKDxJ",
        "outputId": "ff4e69cf-c2da-4f89-b61e-eaf2681e5f26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;35mdefaultdict\u001b[0m\u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'int'\u001b[0m\u001b[1m>\u001b[0m, \u001b[1m{\u001b[0m\n",
              "    \u001b[32m'This'\u001b[0m: \u001b[1;36m3\u001b[0m,\n",
              "    \u001b[32m'Ġis'\u001b[0m: \u001b[1;36m2\u001b[0m,\n",
              "    \u001b[32m'Ġthe'\u001b[0m: \u001b[1;36m1\u001b[0m,\n",
              "    \u001b[32m'ĠHugging'\u001b[0m: \u001b[1;36m1\u001b[0m,\n",
              "    \u001b[32m'ĠFace'\u001b[0m: \u001b[1;36m1\u001b[0m,\n",
              "    \u001b[32m'ĠCourse'\u001b[0m: \u001b[1;36m1\u001b[0m,\n",
              "    \u001b[32m'.'\u001b[0m: \u001b[1;36m4\u001b[0m,\n",
              "    \u001b[32m'Ġchapter'\u001b[0m: \u001b[1;36m1\u001b[0m,\n",
              "    \u001b[32m'Ġabout'\u001b[0m: \u001b[1;36m1\u001b[0m,\n",
              "    \u001b[32m'Ġtokenization'\u001b[0m: \u001b[1;36m1\u001b[0m,\n",
              "    \u001b[32m'Ġsection'\u001b[0m: \u001b[1;36m1\u001b[0m,\n",
              "    \u001b[32m'Ġshows'\u001b[0m: \u001b[1;36m1\u001b[0m,\n",
              "    \u001b[32m'Ġseveral'\u001b[0m: \u001b[1;36m1\u001b[0m,\n",
              "    \u001b[32m'Ġtokenizer'\u001b[0m: \u001b[1;36m1\u001b[0m,\n",
              "    \u001b[32m'Ġalgorithms'\u001b[0m: \u001b[1;36m1\u001b[0m,\n",
              "    \u001b[32m'Hopefully'\u001b[0m: \u001b[1;36m1\u001b[0m,\n",
              "    \u001b[32m','\u001b[0m: \u001b[1;36m1\u001b[0m,\n",
              "    \u001b[32m'Ġyou'\u001b[0m: \u001b[1;36m1\u001b[0m,\n",
              "    \u001b[32m'Ġwill'\u001b[0m: \u001b[1;36m1\u001b[0m,\n",
              "    \u001b[32m'Ġbe'\u001b[0m: \u001b[1;36m1\u001b[0m,\n",
              "    \u001b[32m'Ġable'\u001b[0m: \u001b[1;36m1\u001b[0m,\n",
              "    \u001b[32m'Ġto'\u001b[0m: \u001b[1;36m1\u001b[0m,\n",
              "    \u001b[32m'Ġunderstand'\u001b[0m: \u001b[1;36m1\u001b[0m,\n",
              "    \u001b[32m'Ġhow'\u001b[0m: \u001b[1;36m1\u001b[0m,\n",
              "    \u001b[32m'Ġthey'\u001b[0m: \u001b[1;36m1\u001b[0m,\n",
              "    \u001b[32m'Ġare'\u001b[0m: \u001b[1;36m1\u001b[0m,\n",
              "    \u001b[32m'Ġtrained'\u001b[0m: \u001b[1;36m1\u001b[0m,\n",
              "    \u001b[32m'Ġand'\u001b[0m: \u001b[1;36m1\u001b[0m,\n",
              "    \u001b[32m'Ġgenerate'\u001b[0m: \u001b[1;36m1\u001b[0m,\n",
              "    \u001b[32m'Ġtokens'\u001b[0m: \u001b[1;36m1\u001b[0m\n",
              "\u001b[1m}\u001b[0m\u001b[1m)\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">defaultdict</span><span style=\"font-weight: bold\">(&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'int'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"font-weight: bold\">{</span>\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'This'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'Ġis'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'Ġthe'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'ĠHugging'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'ĠFace'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'ĠCourse'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'.'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'Ġchapter'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'Ġabout'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'Ġtokenization'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'Ġsection'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'Ġshows'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'Ġseveral'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'Ġtokenizer'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'Ġalgorithms'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'Hopefully'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">','</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'Ġyou'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'Ġwill'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'Ġbe'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'Ġable'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'Ġto'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'Ġunderstand'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'Ġhow'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'Ġthey'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'Ġare'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'Ġtrained'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'Ġand'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'Ġgenerate'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'Ġtokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>\n",
              "<span style=\"font-weight: bold\">})</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "# Compute the frequencies of each word in the corpus as we do the pre-tokenization:\n",
        "word_freqs: defaultdict = defaultdict(int)\n",
        "\n",
        "for text in corpus:\n",
        "    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
        "    new_words = [word for word, _ in words_with_offsets]\n",
        "    for word in new_words:\n",
        "        word_freqs[word] += 1\n",
        "\n",
        "print(word_freqs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(word_freqs)"
      ],
      "metadata": {
        "id": "6NvPqXWxM_Jj",
        "outputId": "5c444f5f-1f1a-40e0-f6ea-b0feff52e785",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "collections.defaultdict"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "P2NGlauwKDxJ",
        "outputId": "85643b0e-5eaf-4d3b-eaed-be5b119f91cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m[\u001b[0m\n",
              "    \u001b[32m','\u001b[0m,\n",
              "    \u001b[32m'.'\u001b[0m,\n",
              "    \u001b[32m'C'\u001b[0m,\n",
              "    \u001b[32m'F'\u001b[0m,\n",
              "    \u001b[32m'H'\u001b[0m,\n",
              "    \u001b[32m'T'\u001b[0m,\n",
              "    \u001b[32m'a'\u001b[0m,\n",
              "    \u001b[32m'b'\u001b[0m,\n",
              "    \u001b[32m'c'\u001b[0m,\n",
              "    \u001b[32m'd'\u001b[0m,\n",
              "    \u001b[32m'e'\u001b[0m,\n",
              "    \u001b[32m'f'\u001b[0m,\n",
              "    \u001b[32m'g'\u001b[0m,\n",
              "    \u001b[32m'h'\u001b[0m,\n",
              "    \u001b[32m'i'\u001b[0m,\n",
              "    \u001b[32m'k'\u001b[0m,\n",
              "    \u001b[32m'l'\u001b[0m,\n",
              "    \u001b[32m'm'\u001b[0m,\n",
              "    \u001b[32m'n'\u001b[0m,\n",
              "    \u001b[32m'o'\u001b[0m,\n",
              "    \u001b[32m'p'\u001b[0m,\n",
              "    \u001b[32m'r'\u001b[0m,\n",
              "    \u001b[32m's'\u001b[0m,\n",
              "    \u001b[32m't'\u001b[0m,\n",
              "    \u001b[32m'u'\u001b[0m,\n",
              "    \u001b[32m'v'\u001b[0m,\n",
              "    \u001b[32m'w'\u001b[0m,\n",
              "    \u001b[32m'y'\u001b[0m,\n",
              "    \u001b[32m'z'\u001b[0m,\n",
              "    \u001b[32m'Ġ'\u001b[0m\n",
              "\u001b[1m]\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">','</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'.'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'C'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'F'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'H'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'T'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'a'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'b'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'c'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'d'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'e'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'f'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'g'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'h'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'i'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'k'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'l'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'m'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'n'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'o'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'p'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'r'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'s'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'t'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'u'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'v'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'w'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'y'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'z'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'Ġ'</span>\n",
              "<span style=\"font-weight: bold\">]</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Compute the base vocabulary, formed by all the characters used in the corpus:\n",
        "alphabet: list[str] = []\n",
        "\n",
        "for word in word_freqs.keys():\n",
        "    for letter in word:\n",
        "        if letter not in alphabet:\n",
        "            alphabet.append(letter)\n",
        "alphabet.sort()\n",
        "\n",
        "print(alphabet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "UA4von5KKDxJ"
      },
      "outputs": [],
      "source": [
        "# Add the special tokens used by the model at the beginning of that vocabulary. In the case of GPT-2,\n",
        "# the only special token is \"<|endoftext|>\":\n",
        "\n",
        "vocab: list[str] = [\"<|endoftext|>\"] + alphabet.copy()\n",
        "\n",
        "# Split each word into individual characters, to be able to start training:\n",
        "splits: dict[str, Any] = {word: [c for c in word] for word in word_freqs.keys()}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "splits"
      ],
      "metadata": {
        "id": "YwxUFXyMN4ya",
        "outputId": "005e1b03-7fa6-418e-a638-cc613be14057",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'This': ['T', 'h', 'i', 's'],\n",
              " 'Ġis': ['Ġ', 'i', 's'],\n",
              " 'Ġthe': ['Ġ', 't', 'h', 'e'],\n",
              " 'ĠHugging': ['Ġ', 'H', 'u', 'g', 'g', 'i', 'n', 'g'],\n",
              " 'ĠFace': ['Ġ', 'F', 'a', 'c', 'e'],\n",
              " 'ĠCourse': ['Ġ', 'C', 'o', 'u', 'r', 's', 'e'],\n",
              " '.': ['.'],\n",
              " 'Ġchapter': ['Ġ', 'c', 'h', 'a', 'p', 't', 'e', 'r'],\n",
              " 'Ġabout': ['Ġ', 'a', 'b', 'o', 'u', 't'],\n",
              " 'Ġtokenization': ['Ġ',\n",
              "  't',\n",
              "  'o',\n",
              "  'k',\n",
              "  'e',\n",
              "  'n',\n",
              "  'i',\n",
              "  'z',\n",
              "  'a',\n",
              "  't',\n",
              "  'i',\n",
              "  'o',\n",
              "  'n'],\n",
              " 'Ġsection': ['Ġ', 's', 'e', 'c', 't', 'i', 'o', 'n'],\n",
              " 'Ġshows': ['Ġ', 's', 'h', 'o', 'w', 's'],\n",
              " 'Ġseveral': ['Ġ', 's', 'e', 'v', 'e', 'r', 'a', 'l'],\n",
              " 'Ġtokenizer': ['Ġ', 't', 'o', 'k', 'e', 'n', 'i', 'z', 'e', 'r'],\n",
              " 'Ġalgorithms': ['Ġ', 'a', 'l', 'g', 'o', 'r', 'i', 't', 'h', 'm', 's'],\n",
              " 'Hopefully': ['H', 'o', 'p', 'e', 'f', 'u', 'l', 'l', 'y'],\n",
              " ',': [','],\n",
              " 'Ġyou': ['Ġ', 'y', 'o', 'u'],\n",
              " 'Ġwill': ['Ġ', 'w', 'i', 'l', 'l'],\n",
              " 'Ġbe': ['Ġ', 'b', 'e'],\n",
              " 'Ġable': ['Ġ', 'a', 'b', 'l', 'e'],\n",
              " 'Ġto': ['Ġ', 't', 'o'],\n",
              " 'Ġunderstand': ['Ġ', 'u', 'n', 'd', 'e', 'r', 's', 't', 'a', 'n', 'd'],\n",
              " 'Ġhow': ['Ġ', 'h', 'o', 'w'],\n",
              " 'Ġthey': ['Ġ', 't', 'h', 'e', 'y'],\n",
              " 'Ġare': ['Ġ', 'a', 'r', 'e'],\n",
              " 'Ġtrained': ['Ġ', 't', 'r', 'a', 'i', 'n', 'e', 'd'],\n",
              " 'Ġand': ['Ġ', 'a', 'n', 'd'],\n",
              " 'Ġgenerate': ['Ġ', 'g', 'e', 'n', 'e', 'r', 'a', 't', 'e'],\n",
              " 'Ġtokens': ['Ġ', 't', 'o', 'k', 'e', 'n', 's']}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "0S_9FviHKDxJ"
      },
      "outputs": [],
      "source": [
        "def compute_pair_freqs(splits) -> defaultdict:\n",
        "    pair_freqs: defaultdict = defaultdict(int)\n",
        "    for word, freq in word_freqs.items():\n",
        "        split = splits[word]\n",
        "        if len(split) == 1:\n",
        "            continue\n",
        "        for i in range(len(split) - 1):\n",
        "            pair = (split[i], split[i + 1])\n",
        "            pair_freqs[pair] += freq\n",
        "    return pair_freqs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "5jazufhzKDxJ",
        "outputId": "2226febc-2825-4361-fba5-7b70fb01285c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 114
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m(\u001b[0m\u001b[32m'T'\u001b[0m, \u001b[32m'h'\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;36m3\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'T'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'h'</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m(\u001b[0m\u001b[32m'h'\u001b[0m, \u001b[32m'i'\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;36m3\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'h'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'i'</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m(\u001b[0m\u001b[32m'i'\u001b[0m, \u001b[32m's'\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;36m5\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'i'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'s'</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m(\u001b[0m\u001b[32m'Ġ'\u001b[0m, \u001b[32m'i'\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;36m2\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'Ġ'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'i'</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m(\u001b[0m\u001b[32m'Ġ'\u001b[0m, \u001b[32m't'\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;36m7\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'Ġ'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'t'</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m(\u001b[0m\u001b[32m't'\u001b[0m, \u001b[32m'h'\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;36m3\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'t'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'h'</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Let’s have a look at a part of this dictionary after the initial splits:\n",
        "pair_freqs: defaultdict = compute_pair_freqs(splits)\n",
        "\n",
        "for i, key in enumerate(pair_freqs.keys()):\n",
        "    print(f\"{key}: {pair_freqs[key]}\")\n",
        "    if i >= 5:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "dDaVKOwHKDxK",
        "outputId": "5dc79470-9f13-464c-a092-6cf0f7ee089f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m(\u001b[0m\u001b[32m'Ġ'\u001b[0m, \u001b[32m't'\u001b[0m\u001b[1m)\u001b[0m\n",
              "\u001b[1;36m7\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'Ġ'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'t'</span><span style=\"font-weight: bold\">)</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Now, finding the most frequent pair only takes a quick loop:\n",
        "best_pair: str = \"\"\n",
        "max_freq = None\n",
        "\n",
        "for pair, freq in pair_freqs.items():\n",
        "    if max_freq is None or max_freq < freq:\n",
        "        best_pair = pair\n",
        "        max_freq = freq\n",
        "\n",
        "print(best_pair, max_freq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "K0HAoqZ9KDxK"
      },
      "outputs": [],
      "source": [
        "# So the first merge to learn is ('Ġ', 't') -> 'Ġt', and we add 'Ġt' to the vocabulary:\n",
        "merges = {(\"Ġ\", \"t\"): \"Ġt\"}\n",
        "vocab.append(\"Ġt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "CezWw4eLKDxK"
      },
      "outputs": [],
      "source": [
        "# To continue, we need to apply that merge in our splits dictionary. Let’s write another function for this:\n",
        "def merge_pair(a, b, splits):\n",
        "    for word in word_freqs:\n",
        "        split = splits[word]\n",
        "        if len(split) == 1:\n",
        "            continue\n",
        "\n",
        "        i = 0\n",
        "        while i < len(split) - 1:\n",
        "            if split[i] == a and split[i + 1] == b:\n",
        "                split = split[:i] + [a + b] + split[i + 2 :]\n",
        "            else:\n",
        "                i += 1\n",
        "        splits[word] = split\n",
        "    return splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "770Y5QtYKDxK",
        "outputId": "c51145c8-799f-47cb-caae-0d0b7ce3f7b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m[\u001b[0m\u001b[32m'Ġt'\u001b[0m, \u001b[32m'r'\u001b[0m, \u001b[32m'a'\u001b[0m, \u001b[32m'i'\u001b[0m, \u001b[32m'n'\u001b[0m, \u001b[32m'e'\u001b[0m, \u001b[32m'd'\u001b[0m\u001b[1m]\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'Ġt'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'r'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'a'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'i'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'n'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'e'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'d'</span><span style=\"font-weight: bold\">]</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# And we can have a look at the result of the first merge:\n",
        "splits = merge_pair(\"Ġ\", \"t\", splits)\n",
        "print(splits[\"Ġtrained\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "f13N_Rn4KDxK"
      },
      "outputs": [],
      "source": [
        "\n",
        "vocab_size: int = 50\n",
        "\n",
        "while len(vocab) < vocab_size:\n",
        "    pair_freqs = compute_pair_freqs(splits)\n",
        "    best_pair = \"\"\n",
        "    max_freq = None\n",
        "    for pair, freq in pair_freqs.items():\n",
        "        if max_freq is None or max_freq < freq:\n",
        "            best_pair = pair\n",
        "            max_freq = freq\n",
        "    splits = merge_pair(*best_pair, splits)\n",
        "    merges[best_pair] = best_pair[0] + best_pair[1]\n",
        "    vocab.append(best_pair[0] + best_pair[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "yItsesy9KDxL",
        "outputId": "8e66e04b-80c7-4941-dd5a-063039c30051",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m{\u001b[0m\n",
              "    \u001b[1m(\u001b[0m\u001b[32m'Ġ'\u001b[0m, \u001b[32m't'\u001b[0m\u001b[1m)\u001b[0m: \u001b[32m'Ġt'\u001b[0m,\n",
              "    \u001b[1m(\u001b[0m\u001b[32m'i'\u001b[0m, \u001b[32m's'\u001b[0m\u001b[1m)\u001b[0m: \u001b[32m'is'\u001b[0m,\n",
              "    \u001b[1m(\u001b[0m\u001b[32m'e'\u001b[0m, \u001b[32m'r'\u001b[0m\u001b[1m)\u001b[0m: \u001b[32m'er'\u001b[0m,\n",
              "    \u001b[1m(\u001b[0m\u001b[32m'Ġ'\u001b[0m, \u001b[32m'a'\u001b[0m\u001b[1m)\u001b[0m: \u001b[32m'Ġa'\u001b[0m,\n",
              "    \u001b[1m(\u001b[0m\u001b[32m'Ġt'\u001b[0m, \u001b[32m'o'\u001b[0m\u001b[1m)\u001b[0m: \u001b[32m'Ġto'\u001b[0m,\n",
              "    \u001b[1m(\u001b[0m\u001b[32m'e'\u001b[0m, \u001b[32m'n'\u001b[0m\u001b[1m)\u001b[0m: \u001b[32m'en'\u001b[0m,\n",
              "    \u001b[1m(\u001b[0m\u001b[32m'T'\u001b[0m, \u001b[32m'h'\u001b[0m\u001b[1m)\u001b[0m: \u001b[32m'Th'\u001b[0m,\n",
              "    \u001b[1m(\u001b[0m\u001b[32m'Th'\u001b[0m, \u001b[32m'is'\u001b[0m\u001b[1m)\u001b[0m: \u001b[32m'This'\u001b[0m,\n",
              "    \u001b[1m(\u001b[0m\u001b[32m'o'\u001b[0m, \u001b[32m'u'\u001b[0m\u001b[1m)\u001b[0m: \u001b[32m'ou'\u001b[0m,\n",
              "    \u001b[1m(\u001b[0m\u001b[32m's'\u001b[0m, \u001b[32m'e'\u001b[0m\u001b[1m)\u001b[0m: \u001b[32m'se'\u001b[0m,\n",
              "    \u001b[1m(\u001b[0m\u001b[32m'Ġto'\u001b[0m, \u001b[32m'k'\u001b[0m\u001b[1m)\u001b[0m: \u001b[32m'Ġtok'\u001b[0m,\n",
              "    \u001b[1m(\u001b[0m\u001b[32m'Ġtok'\u001b[0m, \u001b[32m'en'\u001b[0m\u001b[1m)\u001b[0m: \u001b[32m'Ġtoken'\u001b[0m,\n",
              "    \u001b[1m(\u001b[0m\u001b[32m'n'\u001b[0m, \u001b[32m'd'\u001b[0m\u001b[1m)\u001b[0m: \u001b[32m'nd'\u001b[0m,\n",
              "    \u001b[1m(\u001b[0m\u001b[32m'Ġ'\u001b[0m, \u001b[32m'is'\u001b[0m\u001b[1m)\u001b[0m: \u001b[32m'Ġis'\u001b[0m,\n",
              "    \u001b[1m(\u001b[0m\u001b[32m'Ġt'\u001b[0m, \u001b[32m'h'\u001b[0m\u001b[1m)\u001b[0m: \u001b[32m'Ġth'\u001b[0m,\n",
              "    \u001b[1m(\u001b[0m\u001b[32m'Ġth'\u001b[0m, \u001b[32m'e'\u001b[0m\u001b[1m)\u001b[0m: \u001b[32m'Ġthe'\u001b[0m,\n",
              "    \u001b[1m(\u001b[0m\u001b[32m'i'\u001b[0m, \u001b[32m'n'\u001b[0m\u001b[1m)\u001b[0m: \u001b[32m'in'\u001b[0m,\n",
              "    \u001b[1m(\u001b[0m\u001b[32m'Ġa'\u001b[0m, \u001b[32m'b'\u001b[0m\u001b[1m)\u001b[0m: \u001b[32m'Ġab'\u001b[0m,\n",
              "    \u001b[1m(\u001b[0m\u001b[32m'Ġtoken'\u001b[0m, \u001b[32m'i'\u001b[0m\u001b[1m)\u001b[0m: \u001b[32m'Ġtokeni'\u001b[0m\n",
              "\u001b[1m}\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
              "    <span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'Ġ'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'t'</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Ġt'</span>,\n",
              "    <span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'i'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'s'</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'is'</span>,\n",
              "    <span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'e'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'r'</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'er'</span>,\n",
              "    <span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'Ġ'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'a'</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Ġa'</span>,\n",
              "    <span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'Ġt'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'o'</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Ġto'</span>,\n",
              "    <span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'e'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'n'</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'en'</span>,\n",
              "    <span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'T'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'h'</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Th'</span>,\n",
              "    <span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'Th'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'is'</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'This'</span>,\n",
              "    <span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'o'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'u'</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'ou'</span>,\n",
              "    <span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'s'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'e'</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'se'</span>,\n",
              "    <span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'Ġto'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'k'</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Ġtok'</span>,\n",
              "    <span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'Ġtok'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'en'</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Ġtoken'</span>,\n",
              "    <span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'n'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'d'</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'nd'</span>,\n",
              "    <span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'Ġ'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'is'</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Ġis'</span>,\n",
              "    <span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'Ġt'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'h'</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Ġth'</span>,\n",
              "    <span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'Ġth'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'e'</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Ġthe'</span>,\n",
              "    <span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'i'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'n'</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'in'</span>,\n",
              "    <span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'Ġa'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'b'</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Ġab'</span>,\n",
              "    <span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'Ġtoken'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'i'</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Ġtokeni'</span>\n",
              "<span style=\"font-weight: bold\">}</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# As a result, we’ve learned 19 merge rules (the initial vocabulary had a size of 31 — 30 characters in\n",
        "# the alphabet, plus the special token):\n",
        "print(merges)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "UP15zisvKDxL",
        "outputId": "8899d841-cf2a-4fca-e036-56d0b6d8417a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 862
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m[\u001b[0m\n",
              "    \u001b[32m'\u001b[0m\u001b[32m<\u001b[0m\u001b[32m|endoftext|\u001b[0m\u001b[32m>\u001b[0m\u001b[32m'\u001b[0m,\n",
              "    \u001b[32m','\u001b[0m,\n",
              "    \u001b[32m'.'\u001b[0m,\n",
              "    \u001b[32m'C'\u001b[0m,\n",
              "    \u001b[32m'F'\u001b[0m,\n",
              "    \u001b[32m'H'\u001b[0m,\n",
              "    \u001b[32m'T'\u001b[0m,\n",
              "    \u001b[32m'a'\u001b[0m,\n",
              "    \u001b[32m'b'\u001b[0m,\n",
              "    \u001b[32m'c'\u001b[0m,\n",
              "    \u001b[32m'd'\u001b[0m,\n",
              "    \u001b[32m'e'\u001b[0m,\n",
              "    \u001b[32m'f'\u001b[0m,\n",
              "    \u001b[32m'g'\u001b[0m,\n",
              "    \u001b[32m'h'\u001b[0m,\n",
              "    \u001b[32m'i'\u001b[0m,\n",
              "    \u001b[32m'k'\u001b[0m,\n",
              "    \u001b[32m'l'\u001b[0m,\n",
              "    \u001b[32m'm'\u001b[0m,\n",
              "    \u001b[32m'n'\u001b[0m,\n",
              "    \u001b[32m'o'\u001b[0m,\n",
              "    \u001b[32m'p'\u001b[0m,\n",
              "    \u001b[32m'r'\u001b[0m,\n",
              "    \u001b[32m's'\u001b[0m,\n",
              "    \u001b[32m't'\u001b[0m,\n",
              "    \u001b[32m'u'\u001b[0m,\n",
              "    \u001b[32m'v'\u001b[0m,\n",
              "    \u001b[32m'w'\u001b[0m,\n",
              "    \u001b[32m'y'\u001b[0m,\n",
              "    \u001b[32m'z'\u001b[0m,\n",
              "    \u001b[32m'Ġ'\u001b[0m,\n",
              "    \u001b[32m'Ġt'\u001b[0m,\n",
              "    \u001b[32m'is'\u001b[0m,\n",
              "    \u001b[32m'er'\u001b[0m,\n",
              "    \u001b[32m'Ġa'\u001b[0m,\n",
              "    \u001b[32m'Ġto'\u001b[0m,\n",
              "    \u001b[32m'en'\u001b[0m,\n",
              "    \u001b[32m'Th'\u001b[0m,\n",
              "    \u001b[32m'This'\u001b[0m,\n",
              "    \u001b[32m'ou'\u001b[0m,\n",
              "    \u001b[32m'se'\u001b[0m,\n",
              "    \u001b[32m'Ġtok'\u001b[0m,\n",
              "    \u001b[32m'Ġtoken'\u001b[0m,\n",
              "    \u001b[32m'nd'\u001b[0m,\n",
              "    \u001b[32m'Ġis'\u001b[0m,\n",
              "    \u001b[32m'Ġth'\u001b[0m,\n",
              "    \u001b[32m'Ġthe'\u001b[0m,\n",
              "    \u001b[32m'in'\u001b[0m,\n",
              "    \u001b[32m'Ġab'\u001b[0m,\n",
              "    \u001b[32m'Ġtokeni'\u001b[0m\n",
              "\u001b[1m]\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'&lt;|endoftext|&gt;'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">','</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'.'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'C'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'F'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'H'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'T'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'a'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'b'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'c'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'d'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'e'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'f'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'g'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'h'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'i'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'k'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'l'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'m'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'n'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'o'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'p'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'r'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'s'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'t'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'u'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'v'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'w'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'y'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'z'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'Ġ'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'Ġt'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'is'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'er'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'Ġa'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'Ġto'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'en'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'Th'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'This'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'ou'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'se'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'Ġtok'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'Ġtoken'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'nd'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'Ġis'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'Ġth'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'Ġthe'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'in'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'Ġab'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'Ġtokeni'</span>\n",
              "<span style=\"font-weight: bold\">]</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "print(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "K465dtG-KDxL"
      },
      "outputs": [],
      "source": [
        "# To tokenize a new text, we pre-tokenize it, split it, then apply all the merge rules learned:\n",
        "def tokenize(text):\n",
        "    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
        "    pre_tokenized_text = [word for word, offset in pre_tokenize_result]\n",
        "    splits = [[l for l in word] for word in pre_tokenized_text]\n",
        "    for pair, merge in merges.items():\n",
        "        for idx, split in enumerate(splits):\n",
        "            i = 0\n",
        "            while i < len(split) - 1:\n",
        "                if split[i] == pair[0] and split[i + 1] == pair[1]:\n",
        "                    split = split[:i] + [merge] + split[i + 2 :]\n",
        "                else:\n",
        "                    i += 1\n",
        "            splits[idx] = split\n",
        "\n",
        "    return sum(splits, [])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Zzq53WzhKDxL",
        "outputId": "2d30c02c-27ac-4dba-a003-e6130ac85ed4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This', 'Ġis', 'Ġ', 'n', 'o', 't', 'Ġa', 'Ġtoken', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "# We can try this on any text composed of characters in the alphabet:\n",
        "tokenize(\"This is not a token.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "K_k6YKnnKDxL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [WordPiece Tokenization](https://huggingface.co/learn/nlp-course/chapter6/6?fw=pt)\n",
        "\n",
        "```text\n",
        "- WordPiece is the tokenization algorithm Google developed to pretrain BERT.\n",
        "- It has since been reused in quite a few Transformer models based on BERT, such as DistilBERT, MobileBERT, Funnel Transformers, and MPNET.\n",
        "- It’s very similar to BPE in terms of the training, but the actual tokenization is done differently.\n",
        "```\n",
        "\n",
        "### Training algorithm\n",
        "\n",
        "```text\n",
        "⚠️ Google never open-sourced its implementation of the training algorithm of WordPiece, so what follows is our best guess based on the published literature. It may not be 100% accurate.\n",
        "\n",
        "Like BPE, WordPiece starts from a small vocabulary including the special tokens used by the model and the initial alphabet.\n",
        "Since it identifies subwords by adding a prefix (like ## for BERT), each word is initially split by adding that prefix to all the characters inside the word. So, for instance, \"word\" gets split like this:\n",
        "```\n",
        "\n",
        "```python\n",
        "w ##o ##r ##d\n",
        "```\n",
        "\n",
        "```text\n",
        "Thus, the initial alphabet contains all the characters present at the beginning of a word and the characters present inside a word preceded by the WordPiece prefix.\n",
        "\n",
        "Let’s look at the same vocabulary we used in the BPE training example:\n",
        "```\n",
        "\n",
        "```python\n",
        "(\"hug\", 10), (\"pug\", 5), (\"pun\", 12), (\"bun\", 4), (\"hugs\", 5)\n",
        "\n",
        "# The splits here will be:\n",
        "\n",
        "(\"h\" \"##u\" \"##g\", 10), (\"p\" \"##u\" \"##g\", 5), (\"p\" \"##u\" \"##n\", 12), (\"b\" \"##u\" \"##n\", 4), (\"h\" \"##u\" \"##g\" \"##s\", 5)\n",
        "```\n",
        "\n",
        "```text\n",
        "so the initial vocabulary will be [\"b\", \"h\", \"p\", \"##g\", \"##n\", \"##s\", \"##u\"] (if we forget about special tokens for now). The most frequent pair is (\"##u\", \"##g\") (present 20 times), but the individual frequency of \"##u\" is very high, so its score is not the highest (it’s 1 / 36). All pairs with a \"##u\" actually have that same score (1 / 36), so the best score goes to the pair (\"##g\", \"##s\") — the only one without a \"##u\" — at 1 / 20, and the first merge learned is (\"##g\", \"##s\") -> (\"##gs\").\n",
        "\n",
        "Note that when we merge, we remove the ## between the two tokens, so we add \"##gs\" to the vocabulary and apply the merge in the words of the corpus:\n",
        "```\n",
        "\n",
        "```python\n",
        "Vocabulary: [\"b\", \"h\", \"p\", \"##g\", \"##n\", \"##s\", \"##u\", \"##gs\"]\n",
        "Corpus: (\"h\" \"##u\" \"##g\", 10), (\"p\" \"##u\" \"##g\", 5), (\"p\" \"##u\" \"##n\", 12), (\"b\" \"##u\" \"##n\", 4), (\"h\" \"##u\" \"##gs\", 5)\n",
        "```\n",
        "\n",
        "```text\n",
        "At this point, \"##u\" is in all the possible pairs, so they all end up with the same score. Let’s say that in this case, the first pair is merged, so (\"h\", \"##u\") -> \"hu\". This takes us to:\n",
        "```\n",
        "\n",
        "```python\n",
        "Vocabulary: [\"b\", \"h\", \"p\", \"##g\", \"##n\", \"##s\", \"##u\", \"##gs\", \"hu\"]\n",
        "Corpus: (\"hu\" \"##g\", 10), (\"p\" \"##u\" \"##g\", 5), (\"p\" \"##u\" \"##n\", 12), (\"b\" \"##u\" \"##n\", 4), (\"hu\" \"##gs\", 5)\n",
        "```\n",
        "\n",
        "```text\n",
        "Then the next best score is shared by (\"hu\", \"##g\") and (\"hu\", \"##gs\") (with 1/15, compared to 1/21 for all the other pairs), so the first pair with the biggest score is merged:\n",
        "```\n",
        "\n",
        "```python\n",
        "Vocabulary: [\"b\", \"h\", \"p\", \"##g\", \"##n\", \"##s\", \"##u\", \"##gs\", \"hu\", \"hug\"]\n",
        "Corpus: (\"hug\", 10), (\"p\" \"##u\" \"##g\", 5), (\"p\" \"##u\" \"##n\", 12), (\"b\" \"##u\" \"##n\", 4), (\"hu\" \"##gs\", 5)\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "TZRhn-zNKDxL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "IYKgAshdKDxL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenization Algorithm\n",
        "\n",
        "```\n",
        "- Tokenization differs in WordPiece and BPE in that WordPiece only saves the final vocabulary, not the merge rules learned.\n",
        "- Starting from the word to tokenize, WordPiece finds the longest subword that is in the vocabulary, then splits on it.\n",
        "- For instance, if we use the vocabulary learned in the example above, for the word \"hugs\" the longest subword starting from the beginning that is inside the vocabulary is \"hug\", so we split there and get [\"hug\", \"##s\"].\n",
        "- We then continue with \"##s\", which is in the vocabulary, so the tokenization of \"hugs\" is [\"hug\", \"##s\"].\n",
        "\n",
        "- With BPE, we would have applied the merges learned in order and tokenized this as [\"hu\", \"##gs\"], so the encoding is different.\n",
        "\n",
        "- As another example, let’s see how the word \"bugs\" would be tokenized. \"b\" is the longest subword starting at the beginning of the word that is in the vocabulary, so we split there and get [\"b\", \"##ugs\"].\n",
        "- Then \"##u\" is the longest subword starting at the beginning of \"##ugs\" that is in the vocabulary, so we split there and get [\"b\", \"##u, \"##gs\"].\n",
        "- Finally, \"##gs\" is in the vocabulary, so this last list is the tokenization of \"bugs\".\n",
        "\n",
        "- When the tokenization gets to a stage where it’s not possible to find a subword in the vocabulary, the whole word is tokenized as unknown — so, for instance, \"mug\" would be tokenized as [\"[UNK]\"], as would \"bum\" (even if we can begin with \"b\" and \"##u\", \"##m\" is not the vocabulary, and the resulting tokenization will just be [\"[UNK]\"], not [\"b\", \"##u\", \"[UNK]\"]).\n",
        "- This is another difference from BPE, which would only classify the individual characters not in the vocabulary as unknown.\n",
        "```\n",
        "\n",
        "<br>\n",
        "\n",
        "### Implementing WordPiece\n",
        "\n",
        "```text\n",
        "- Now let’s take a look at an implementation of the WordPiece algorithm. Like with BPE, this is just pedagogical, and you won’t able to use this on a big corpus.\n",
        "\n",
        "- We will use the same corpus as in the BPE example:\n",
        "```"
      ],
      "metadata": {
        "id": "Pd8yMzMBKDxL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "WyTrypjuKDxM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Jm3kZ0OUKDxM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "hKOvWLtMKDxM"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}