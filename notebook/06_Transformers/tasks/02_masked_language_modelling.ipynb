{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Masked Language Modelling (MLM)](https://huggingface.co/learn/nlp-course/chapter7/3?fw=pt)\n",
    "\n",
    "- For NLP tasks with Transformer models, you can use pretrained models from Hugging Face and fine-tune them on your data. \n",
    "- Transfer learning works well if the pretraining and fine-tuning corpora are similar. \n",
    "- However, in cases like `legal` or `scientific text`, domain-specific words may be treated as `rare` tokens. \n",
    "- Fine-tuning the language model on in-domain data can improve downstream task performance. \n",
    "- This process is called d`omain adaptation`, popularized by ULMFiT in 2018. \n",
    "- We'll perform a similar process with `Transformers` instead of LSTMs.\n",
    "\n",
    "<br>\n",
    "\n",
    "## Benefits of MLM \n",
    "\n",
    "- **Improved Generalization**: By exposing the model to various masking patterns, MLM enhances its ability to generalize to unseen data and perform well on downstream tasks.\n",
    "\n",
    "- **Effective Pre-training for Diverse Tasks**: MLM has shown to be effective in pre-training language models for a wide range of NLP tasks, including text generation, machine translation, and question answering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install rich\n",
    "!pip install transformers[torch]\n",
    "!pip install torch datasets evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in library\n",
    "import re\n",
    "import json\n",
    "from typing import Any, Optional, Union\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rich import print\n",
    "import torch\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Pandas settings\n",
    "pd.options.display.max_rows = 1_000\n",
    "pd.options.display.max_columns = 1_000\n",
    "pd.options.display.max_colwidth = 600\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# # Black code formatter (Optional)\n",
    "# %load_ext lab_black\n",
    "\n",
    "# # auto reload imports\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9476a3153d3c49a983f286246dfcc9e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e688dff2aaf34592b75e765dd4a492bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForMaskedLM\n",
    "\n",
    "\n",
    "model_checkpoint: str = \"distilbert-base-uncased\"\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">'&gt;&gt;&gt; DistilBERT number of parameters: 67M'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m'>>> DistilBERT number of parameters: 67M'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">'&gt;&gt;&gt; BERT number of parameters: 110M'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m'>>> BERT number of parameters: 110M'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "distilbert_num_parameters: float = model.num_parameters() / 1_000_000\n",
    "print(f\"'>>> DistilBERT number of parameters: {round(distilbert_num_parameters)}M'\")\n",
    "print(f\"'>>> BERT number of parameters: 110M'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let’s see what kinds of tokens this model predicts:\n",
    "text: str = \"This is a great [MASK].\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment\n",
    "\n",
    "- As humans, we can imagine many possibilities for the [MASK] token, such as “day”, “ride”, or “painting”. \n",
    "- For pretrained models, the predictions depend on the corpus the model was trained on, since it learns to pick up the statistical patterns present in the data. \n",
    "- Like BERT, DistilBERT was pretrained on the [English Wikipedia](https://huggingface.co/datasets/wikipedia) and [BookCorpus datasets](https://huggingface.co/datasets/bookcorpus), so we expect the predictions for [MASK] to reflect these domains. \n",
    "- To predict the mask we need DistilBERT’s tokenizer to produce the inputs for the model, so let’s download that from the Hub as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db39dff2b5e74905bf546b33adb9106f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9741f506bd54fd7baf2b91209ba16da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74d1e5681b7a4d4b8054dd00f8ff866b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ -5.5882,  -5.5868,  -5.5958,  ...,  -4.9448,  -4.8174,  -2.9905],\n",
       "         [-11.9031, -11.8872, -12.0623,  ..., -10.9570, -10.6464,  -8.6324],\n",
       "         [-11.9604, -12.1520, -12.1279,  ..., -10.0218,  -8.6074,  -8.0971],\n",
       "         ...,\n",
       "         [ -4.8228,  -4.6268,  -5.1041,  ...,  -4.2771,  -5.0184,  -3.9428],\n",
       "         [-11.2945, -11.2388, -11.3857,  ...,  -9.2063,  -9.3411,  -6.1505],\n",
       "         [ -9.5213,  -9.4632,  -9.5022,  ...,  -8.6561,  -8.4908,  -4.6903]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "inputs: dict[str, Any] = tokenizer(text, return_tensors=\"pt\")\n",
    "token_logits: torch.Tensor = model(**inputs).logits\n",
    "token_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">input_ids: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">101</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2023</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2003</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1037</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2307</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">103</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1012</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">102</span><span style=\"font-weight: bold\">]])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "input_ids: \u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m101\u001b[0m, \u001b[1;36m2023\u001b[0m, \u001b[1;36m2003\u001b[0m, \u001b[1;36m1037\u001b[0m, \u001b[1;36m2307\u001b[0m,  \u001b[1;36m103\u001b[0m, \u001b[1;36m1012\u001b[0m,  \u001b[1;36m102\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">mask_token_id: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">103</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "mask_token_id: \u001b[1;36m103\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>, <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>, <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>, <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>, <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>,  <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>, <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>, <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[3;91mFalse\u001b[0m, \u001b[3;91mFalse\u001b[0m, \u001b[3;91mFalse\u001b[0m, \u001b[3;91mFalse\u001b[0m, \u001b[3;91mFalse\u001b[0m,  \u001b[3;92mTrue\u001b[0m, \u001b[3;91mFalse\u001b[0m, \u001b[3;91mFalse\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f'input_ids: {inputs[\"input_ids\"]}')\n",
    "print(f\"mask_token_id: {tokenizer.mask_token_id}\")\n",
    "\n",
    "print(inputs[\"input_ids\"].flatten() == tokenizer.mask_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'this'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'is'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'a'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'great'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'[MASK]'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'.'</span><span style=\"font-weight: bold\">]</span>\n",
       "<span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">101</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2023</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2003</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1037</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2307</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">103</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1012</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">102</span><span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[32m'this'\u001b[0m, \u001b[32m'is'\u001b[0m, \u001b[32m'a'\u001b[0m, \u001b[32m'great'\u001b[0m, \u001b[32m'\u001b[0m\u001b[32m[\u001b[0m\u001b[32mMASK\u001b[0m\u001b[32m]\u001b[0m\u001b[32m'\u001b[0m, \u001b[32m'.'\u001b[0m\u001b[1m]\u001b[0m\n",
       "\u001b[1m[\u001b[0m\u001b[1;36m101\u001b[0m, \u001b[1;36m2023\u001b[0m, \u001b[1;36m2003\u001b[0m, \u001b[1;36m1037\u001b[0m, \u001b[1;36m2307\u001b[0m, \u001b[1;36m103\u001b[0m, \u001b[1;36m1012\u001b[0m, \u001b[1;36m102\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_tokens: list[str] = tokenizer.tokenize(text)\n",
    "token_ids: list[int] = tokenizer(text).get(\"input_ids\")\n",
    "\n",
    "print(raw_tokens, token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch.Size</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">30522</span><span style=\"font-weight: bold\">])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m8\u001b[0m, \u001b[1;36m30522\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ -5.5882,  -5.5868,  -5.5958,  ...,  -4.9448,  -4.8174,  -2.9905],\n",
       "         [-11.9031, -11.8872, -12.0623,  ..., -10.9570, -10.6464,  -8.6324],\n",
       "         [-11.9604, -12.1520, -12.1279,  ..., -10.0218,  -8.6074,  -8.0971],\n",
       "         ...,\n",
       "         [ -4.8228,  -4.6268,  -5.1041,  ...,  -4.2771,  -5.0184,  -3.9428],\n",
       "         [-11.2945, -11.2388, -11.3857,  ...,  -9.2063,  -9.3411,  -6.1505],\n",
       "         [ -9.5213,  -9.4632,  -9.5022,  ...,  -8.6561,  -8.4908,  -4.6903]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(token_logits.shape)\n",
    "\n",
    "token_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-4.8228, -4.6268, -5.1041,  ..., -4.2771, -5.0184, -3.9428],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_logits[0, 5, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">mask_token_index: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"font-weight: bold\">])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "mask_token_index: \u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-4.8228, -4.6268, -5.1041,  ..., -4.2771, -5.0184, -3.9428]],\n",
       "       grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the location of [MASK] and extract its logits\n",
    "mask_token_index: torch.Tensor = torch.where(\n",
    "    inputs[\"input_ids\"].flatten() == tokenizer.mask_token_id\n",
    ")[0]\n",
    "mask_token_logits: torch.Tensor = token_logits[0, mask_token_index, :]\n",
    "\n",
    "print(f\"mask_token_index: {mask_token_index}\")\n",
    "\n",
    "mask_token_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">'&gt;&gt;&gt; This is a great deal.'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m'>>> This is a great deal.'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">'&gt;&gt;&gt; This is a great success.'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m'>>> This is a great success.'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">'&gt;&gt;&gt; This is a great adventure.'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m'>>> This is a great adventure.'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">'&gt;&gt;&gt; This is a great idea.'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m'>>> This is a great idea.'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">'&gt;&gt;&gt; This is a great feat.'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m'>>> This is a great feat.'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Pick the [MASK] candidates with the highest logits\n",
    "k: int = 5\n",
    "top_5_tokens = torch.topk(mask_token_logits, k, dim=1).indices[0].tolist()\n",
    "\n",
    "for token in top_5_tokens:\n",
    "    print(f\"'>>> {text.replace(tokenizer.mask_token, tokenizer.decode([token]))}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load The dataset\n",
    "\n",
    "- To showcase domain adaptation, we’ll use the famous [Large Movie Review Dataset (or IMDb for short)](https://huggingface.co/datasets/imdb), which is a corpus of movie reviews that is often used to benchmark sentiment analysis models. \n",
    "- By fine-tuning DistilBERT on this corpus, we expect the language model will adapt its vocabulary from the factual data of Wikipedia that it was pretrained on to the more subjective elements of movie reviews. \n",
    "- We can get the data from the Hugging Face Hub with the load_dataset() function from 🤗 Datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "\n",
    "\n",
    "data_path: str = \"imdb\"\n",
    "imdb_dataset: DatasetDict = load_dataset(data_path)\n",
    "\n",
    "imdb_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">&gt;&gt;&gt; Review: There is no relation at all between Fortier and Profiler but the fact that both are police series about\n",
       "violent crimes. Profiler looks crispy, Fortier looks classic. Profiler plots are quite simple. Fortier's plot are \n",
       "far more complicated<span style=\"color: #808000; text-decoration-color: #808000\">...</span> Fortier looks more like Prime Suspect, if we have to spot similarities<span style=\"color: #808000; text-decoration-color: #808000\">...</span> The main \n",
       "character is weak and weirdo, but have <span style=\"color: #008000; text-decoration-color: #008000\">\"clairvoyance\"</span>. People like to compare, to judge, to evaluate. How about \n",
       "just enjoying? Funny thing too, people writing Fortier looks American but, on the other hand, arguing they prefer \n",
       "American series <span style=\"font-weight: bold\">(</span>!!!<span style=\"font-weight: bold\">)</span>. Maybe it's the language, or the spirit, but I think this series is more English than \n",
       "American. By the way, the actors are really good and funny. The acting is not superficial at all<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       ">>> Review: There is no relation at all between Fortier and Profiler but the fact that both are police series about\n",
       "violent crimes. Profiler looks crispy, Fortier looks classic. Profiler plots are quite simple. Fortier's plot are \n",
       "far more complicated\u001b[33m...\u001b[0m Fortier looks more like Prime Suspect, if we have to spot similarities\u001b[33m...\u001b[0m The main \n",
       "character is weak and weirdo, but have \u001b[32m\"clairvoyance\"\u001b[0m. People like to compare, to judge, to evaluate. How about \n",
       "just enjoying? Funny thing too, people writing Fortier looks American but, on the other hand, arguing they prefer \n",
       "American series \u001b[1m(\u001b[0m!!!\u001b[1m)\u001b[0m. Maybe it's the language, or the spirit, but I think this series is more English than \n",
       "American. By the way, the actors are really good and funny. The acting is not superficial at all\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">&gt;&gt;&gt; Label: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       ">>> Label: \u001b[1;36m1\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">&gt;&gt;&gt; Review: This movie is a great. The plot is very true to the book which is a classic written by Mark Twain. The \n",
       "movie starts of with a scene where Hank sings a song with a bunch of kids called <span style=\"color: #008000; text-decoration-color: #008000\">\"when you stub your toe on the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">moon\"</span> It reminds me of Sinatra's song High Hopes, it is fun and inspirational. The Music is great throughout and my\n",
       "favorite song is sung by the King, Hank <span style=\"font-weight: bold\">(</span>bing Crosby<span style=\"font-weight: bold\">)</span> and Sir <span style=\"color: #008000; text-decoration-color: #008000\">\"Saggy\"</span> Sagamore. OVerall a great family movie or \n",
       "even a great Date movie. This is a movie you can watch over and over again. The princess played by Rhonda Fleming \n",
       "is gorgeous. I love this movie!! If you liked Danny Kaye in the Court Jester then you will definitely like this \n",
       "movie.\n",
       "</pre>\n"
      ],
      "text/plain": [
       ">>> Review: This movie is a great. The plot is very true to the book which is a classic written by Mark Twain. The \n",
       "movie starts of with a scene where Hank sings a song with a bunch of kids called \u001b[32m\"when you stub your toe on the \u001b[0m\n",
       "\u001b[32mmoon\"\u001b[0m It reminds me of Sinatra's song High Hopes, it is fun and inspirational. The Music is great throughout and my\n",
       "favorite song is sung by the King, Hank \u001b[1m(\u001b[0mbing Crosby\u001b[1m)\u001b[0m and Sir \u001b[32m\"Saggy\"\u001b[0m Sagamore. OVerall a great family movie or \n",
       "even a great Date movie. This is a movie you can watch over and over again. The princess played by Rhonda Fleming \n",
       "is gorgeous. I love this movie!! If you liked Danny Kaye in the Court Jester then you will definitely like this \n",
       "movie.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">&gt;&gt;&gt; Label: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       ">>> Label: \u001b[1;36m1\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">&gt;&gt;&gt; Review: George P. Cosmatos' <span style=\"color: #008000; text-decoration-color: #008000\">\"Rambo: First Blood Part II\"</span> is pure wish-fulfillment. The United States clearly \n",
       "didn't win the war in Vietnam. They caused damage to this country beyond the imaginable and this movie continues \n",
       "the fairy story of the oh-so innocent soldiers. The only bad guys were the leaders of the nation, who made this war\n",
       "happen. The character of Rambo is perfect to notice this. He is extremely patriotic, bemoans that US-Americans \n",
       "didn't appreciate and celebrate the achievements of the single soldier, but has nothing but distrust for leading \n",
       "officers and politicians. Like every film that defends the war <span style=\"font-weight: bold\">(</span>e.g. <span style=\"color: #008000; text-decoration-color: #008000\">\"We Were Soldiers\"</span><span style=\"font-weight: bold\">)</span> also this one avoids the \n",
       "need to give a comprehensible reason for the engagement in South Asia. And for that matter also the reason for \n",
       "every single US-American soldier that was there. Instead, Rambo gets to take revenge for the wounds of a whole \n",
       "nation. It would have been better to work on how to deal with the memories, rather than suppressing them. <span style=\"color: #008000; text-decoration-color: #008000\">\"Do we </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">get to win this time?\"</span> Yes, you do.\n",
       "</pre>\n"
      ],
      "text/plain": [
       ">>> Review: George P. Cosmatos' \u001b[32m\"Rambo: First Blood Part II\"\u001b[0m is pure wish-fulfillment. The United States clearly \n",
       "didn't win the war in Vietnam. They caused damage to this country beyond the imaginable and this movie continues \n",
       "the fairy story of the oh-so innocent soldiers. The only bad guys were the leaders of the nation, who made this war\n",
       "happen. The character of Rambo is perfect to notice this. He is extremely patriotic, bemoans that US-Americans \n",
       "didn't appreciate and celebrate the achievements of the single soldier, but has nothing but distrust for leading \n",
       "officers and politicians. Like every film that defends the war \u001b[1m(\u001b[0me.g. \u001b[32m\"We Were Soldiers\"\u001b[0m\u001b[1m)\u001b[0m also this one avoids the \n",
       "need to give a comprehensible reason for the engagement in South Asia. And for that matter also the reason for \n",
       "every single US-American soldier that was there. Instead, Rambo gets to take revenge for the wounds of a whole \n",
       "nation. It would have been better to work on how to deal with the memories, rather than suppressing them. \u001b[32m\"Do we \u001b[0m\n",
       "\u001b[32mget to win this time?\"\u001b[0m Yes, you do.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">&gt;&gt;&gt; Label: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       ">>> Label: \u001b[1;36m0\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Preview a small sample\n",
    "RANDOM_STATE: int = 42\n",
    "sample = imdb_dataset.get(\"train\").shuffle(seed=RANDOM_STATE).select(range(3))\n",
    "\n",
    "for row in sample:\n",
    "    print(f\">>> Review: {row.get('text')}\")\n",
    "    print(f\">>> Label: {row.get('label')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the number of unique labels\n",
    "imdb_dataset.get(\"train\").unique(\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">&gt;&gt;&gt; Review: If you've seen the classic Roger Corman version starring Vincent Price it's hard to put it out of your \n",
       "head, but you probably should do because this one is totally different. Subtlety has been abandoned in favour of \n",
       "gross-out horror - nudity, gore and all-round unpleasantness. OK it's ridiculous, trashy, sensationalised and \n",
       "historically dubious <span style=\"font-weight: bold\">(</span>did any members of the Inquisition really wear horn-rimmed glasses?<span style=\"font-weight: bold\">)</span>, but despite all this it\n",
       "is strangely compelling. I literally couldn't tear myself away from the screen until the end of the movie. If \n",
       "there's a bigger compliment you can pay to a film I don't know what it is.\n",
       "</pre>\n"
      ],
      "text/plain": [
       ">>> Review: If you've seen the classic Roger Corman version starring Vincent Price it's hard to put it out of your \n",
       "head, but you probably should do because this one is totally different. Subtlety has been abandoned in favour of \n",
       "gross-out horror - nudity, gore and all-round unpleasantness. OK it's ridiculous, trashy, sensationalised and \n",
       "historically dubious \u001b[1m(\u001b[0mdid any members of the Inquisition really wear horn-rimmed glasses?\u001b[1m)\u001b[0m, but despite all this it\n",
       "is strangely compelling. I literally couldn't tear myself away from the screen until the end of the movie. If \n",
       "there's a bigger compliment you can pay to a film I don't know what it is.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">&gt;&gt;&gt; Label: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       ">>> Label: \u001b[1;36m-1\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">&gt;&gt;&gt; Review: For me, this was the most moving film of the decade. Samira Makhmalbaf shows pure bravery and vision in\n",
       "the making. She has an intelligence and gift for speaking to the people, regardless of their nationality or \n",
       "beliefs. I am inspired and touched by her humanity and can only hope that she has touched many people the same way.\n",
       "Her message in this film is strong, simple and pure. The human soul can survive the most unheard of cruelties and \n",
       "repression, yet still have the capability to hope and dream even the biggest dreams. Under the most incredible \n",
       "circumstances, the most unexpected people rise up to be heroes. This young girl who has recently regained her \n",
       "voice, yet is still afraid to use her new found freedom, is our hero. She daydreams of becoming president of war \n",
       "torn Afghanistan, the only vision of power that she can imagine that could truly change her current situation. We \n",
       "catch a glimpse of her spirit while witnessing her hardships. In the end, we are left with hope, hope that when her\n",
       "young voice does eventually speak out, it speaks loud and clear for all to hear- sounding a message that transcends\n",
       "borders, nationality and religion. The true epitome of the phoenix rising from the ashes. Hats off to the simple \n",
       "tale of the complex truth.\n",
       "</pre>\n"
      ],
      "text/plain": [
       ">>> Review: For me, this was the most moving film of the decade. Samira Makhmalbaf shows pure bravery and vision in\n",
       "the making. She has an intelligence and gift for speaking to the people, regardless of their nationality or \n",
       "beliefs. I am inspired and touched by her humanity and can only hope that she has touched many people the same way.\n",
       "Her message in this film is strong, simple and pure. The human soul can survive the most unheard of cruelties and \n",
       "repression, yet still have the capability to hope and dream even the biggest dreams. Under the most incredible \n",
       "circumstances, the most unexpected people rise up to be heroes. This young girl who has recently regained her \n",
       "voice, yet is still afraid to use her new found freedom, is our hero. She daydreams of becoming president of war \n",
       "torn Afghanistan, the only vision of power that she can imagine that could truly change her current situation. We \n",
       "catch a glimpse of her spirit while witnessing her hardships. In the end, we are left with hope, hope that when her\n",
       "young voice does eventually speak out, it speaks loud and clear for all to hear- sounding a message that transcends\n",
       "borders, nationality and religion. The true epitome of the phoenix rising from the ashes. Hats off to the simple \n",
       "tale of the complex truth.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">&gt;&gt;&gt; Label: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       ">>> Label: \u001b[1;36m-1\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">&gt;&gt;&gt; Review: There really isn't much to say about this <span style=\"color: #008000; text-decoration-color: #008000\">\"film\"</span>. It has the odd smile or chuckle moment, but on the \n",
       "whole it's bland, predictable and generally pretty dull.<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">br</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;&lt;br </span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;The only reason I gave it three out of ten was </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">for the annoyingly catchy jingle </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">which I hope I will forget soon</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"color: #000000; text-decoration-color: #000000\">.please God!</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">. Otherwise its junk. Or mostly </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">junk, interspersed with adverts for Smirnoff Ice.&lt;br </span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;&lt;br </span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;The lead characters give OK performances, but they </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">really don't have anything much to work with.&lt;br </span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;&lt;br </span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"font-weight: bold\">&gt;</span>Best advice: Avoid it like a dentist's appointment. Or \n",
       "better yet, make a dentist's appointment instead of watching it.\n",
       "</pre>\n"
      ],
      "text/plain": [
       ">>> Review: There really isn't much to say about this \u001b[32m\"film\"\u001b[0m. It has the odd smile or chuckle moment, but on the \n",
       "whole it's bland, predictable and generally pretty dull.\u001b[1m<\u001b[0m\u001b[1;95mbr\u001b[0m\u001b[39m \u001b[0m\u001b[35m/\u001b[0m\u001b[39m><br \u001b[0m\u001b[35m/\u001b[0m\u001b[39m>The only reason I gave it three out of ten was \u001b[0m\n",
       "\u001b[39mfor the annoyingly catchy jingle \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mwhich I hope I will forget soon\u001b[0m\u001b[33m...\u001b[0m\u001b[39m.please God!\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m. Otherwise its junk. Or mostly \u001b[0m\n",
       "\u001b[39mjunk, interspersed with adverts for Smirnoff Ice.<br \u001b[0m\u001b[35m/\u001b[0m\u001b[39m><br \u001b[0m\u001b[35m/\u001b[0m\u001b[39m>The lead characters give OK performances, but they \u001b[0m\n",
       "\u001b[39mreally don't have anything much to work with.<br \u001b[0m\u001b[35m/\u001b[0m\u001b[39m><br \u001b[0m\u001b[35m/\u001b[0m\u001b[1m>\u001b[0mBest advice: Avoid it like a dentist's appointment. Or \n",
       "better yet, make a dentist's appointment instead of watching it.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">&gt;&gt;&gt; Label: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       ">>> Label: \u001b[1;36m-1\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Preview a small sample of the unsupervised/unlabelled data\n",
    "sample = imdb_dataset.get(\"unsupervised\").shuffle(seed=RANDOM_STATE).select(range(3))\n",
    "\n",
    "for row in sample:\n",
    "    print(f\">>> Review: {row.get('text')}\")\n",
    "    print(f\">>> Label: {row.get('label')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the number of unique labels\n",
    "imdb_dataset.get(\"unsupervised\").unique(\"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment\n",
    "\n",
    "- For both `auto-regressive` and `masked language modeling`, a common preprocessing step is to `concatenate` all the examples and then split the whole corpus into chunks of equal size. \n",
    "- This is quite different from our usual approach, where we simply tokenize individual examples. Why concatenate everything together? \n",
    "- The reason is that individual examples might get truncated if they’re too long, and that would result in losing information that might be useful for the language modeling task!\n",
    "\n",
    "- To get started, we’ll first tokenize our corpus as usual, but without setting the `truncation=True` option in our tokenizer. \n",
    "- We’ll also grab the word IDs if they are available (which they will be if we’re using a fast tokenizer, as described in Chapter 6), as we will need them later on to do whole word masking. \n",
    "- We’ll wrap this in a simple function, and while we’re at it we’ll remove the `text` and `label` columns since we don’t need them any longer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples: dict[str, Any]) -> dict[str, Any]:\n",
    "    \"\"\"This is used to tokenize the texts.\"\"\"\n",
    "    result: dict[str, Any] = tokenizer(examples.get(\"text\"))\n",
    "\n",
    "    if tokenizer.is_fast:\n",
    "        result[\"word_ids\"] = [\n",
    "            result.word_ids(idx) for idx in range(len(result[\"input_ids\"]))\n",
    "        ]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fbb5a6cfe72437dafc0c83e3b3ef43c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (720 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "642add4b6f35499c980695c2d0ca6f65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "446a21d4b22245b7bf928dad97f0c5c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use batched=True to activate fast multithreading!\n",
    "tokenized_datasets: DatasetDict = imdb_dataset.map(\n",
    "    tokenize_function, batched=True, remove_columns=[\"text\", \"label\"]\n",
    ")\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input_ids Vs. Word_ids\n",
    "\n",
    "[![image.png](https://i.postimg.cc/MptyPP3S/image.png)](https://postimg.cc/MnMMXD8P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment\n",
    "\n",
    "- Now that we’ve tokenized our movie reviews, the next step is to group them all together and split the result into chunks. \n",
    "- But how big should these chunks be? This will ultimately be determined by the amount of GPU memory that you have available, but a good starting point is to see what the model’s maximum context size is. \n",
    "- This can be inferred by inspecting the model_max_length attribute of the tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model's context size\n",
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">'&gt;&gt;&gt; Review 0 length: 363'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m'>>> Review 0 length: 363'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">'&gt;&gt;&gt; Review 1 length: 304'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m'>>> Review 1 length: 304'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">'&gt;&gt;&gt; Review 2 length: 133'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m'>>> Review 2 length: 133'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# To run our experiments on GPUs like those found on Google Colab, choose a smaller size that can fit in memory:\n",
    "chunk_size: int = 128\n",
    "\n",
    "# Slicing produces a list of lists for each feature\n",
    "tokenized_samples: Dataset = tokenized_datasets[\"train\"][:3]\n",
    "\n",
    "for idx, sample in enumerate(tokenized_samples[\"input_ids\"]):\n",
    "    print(f\"'>>> Review {idx} length: {len(sample)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'word_ids'])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_samples.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 4, 5]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate lists\n",
    "sum([[10, 4, 5]], [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">'&gt;&gt;&gt; Concatenated reviews length: 800'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m'>>> Concatenated reviews length: 800'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Concatenate all these examples with a simple dictionary comprehension:\n",
    "concatenated_examples: dict[str, Any] = {\n",
    "    k: sum(tokenized_samples[k], []) for k in tokenized_samples.keys()\n",
    "}\n",
    "total_length: int = len(concatenated_examples[\"input_ids\"])\n",
    "print(f\"'>>> Concatenated reviews length: {total_length}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'word_ids'])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concatenated_examples.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">'&gt;&gt;&gt; Chunk length: 128'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m'>>> Chunk length: 128'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">'&gt;&gt;&gt; Chunk length: 128'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m'>>> Chunk length: 128'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">'&gt;&gt;&gt; Chunk length: 128'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m'>>> Chunk length: 128'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">'&gt;&gt;&gt; Chunk length: 128'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m'>>> Chunk length: 128'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">'&gt;&gt;&gt; Chunk length: 128'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m'>>> Chunk length: 128'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">'&gt;&gt;&gt; Chunk length: 128'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m'>>> Chunk length: 128'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">'&gt;&gt;&gt; Chunk length: 32'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m'>>> Chunk length: 32'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Chunk the data\n",
    "chunks: dict[str, Any] = {\n",
    "    key: [tokens[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "    for key, tokens in concatenated_examples.items()\n",
    "}\n",
    "\n",
    "for chunk in chunks[\"input_ids\"]:\n",
    "    print(f\"'>>> Chunk length: {len(chunk)}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment\n",
    "\n",
    "- As you can see the last chunk will generally be smaller than the maximum chunk size. \n",
    "- There are two main strategies for dealing with this:\n",
    "  - Drop the last chunk if it’s smaller than chunk_size.\n",
    "  - Pad the last chunk until its length equals chunk_size.\n",
    "- We’ll take the first approach here, so let’s wrap all of the above logic in a single function that we can apply to our tokenized datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk the data\n",
    "chunks: dict[str, Any] = {\n",
    "    key: [tokens[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "    for key, tokens in concatenated_examples.items()\n",
    "}\n",
    "\n",
    "for chunk in chunks[\"input_ids\"]:\n",
    "    print(f\"'>>> Chunk length: {len(chunk)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_texts(examples: dict[str, Any]) -> dict[str, Any]:\n",
    "    \"\"\"This is used to concatenate the input_ids and chunk the data.\"\"\"\n",
    "    # Concatenate all texts\n",
    "    concatenated_examples = {key: sum(examples[key], []) for key in examples.keys()}\n",
    "    # Compute length of concatenated texts\n",
    "    # Select the 1st item in the list and calculate the length\n",
    "    total_length: int = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the last chunk if it's smaller than chunk_size\n",
    "    total_length: int = (total_length // chunk_size) * chunk_size\n",
    "    # Chunk the data\n",
    "    chunks: dict[str, Any] = {\n",
    "        key: [tokens[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "        for key, tokens in concatenated_examples.items()\n",
    "    }\n",
    "\n",
    "    # Create a new labels column\n",
    "    chunks[\"labels\"] = chunks[\"input_ids\"].copy()\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment\n",
    "\n",
    "- Note that in the last step of group_texts() we create a new `labels` column which is a copy of the input_ids one. \n",
    "- As we’ll see shortly, that’s because in masked language modeling the objective is to predict randomly masked tokens in the input batch, and by creating a labels column we provide the ground truth for our language model to learn from.\n",
    "- Let’s now apply group_texts() to our tokenized datasets using `Dataset.map()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b0b20386ac749d3905c3d416c56542f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48489d5f6d6c4d0ebd538eaa126ed1b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89495d34c55d42b2bdeaa3757404461e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 61291\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 59904\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 122957\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_datasets:DatasetDict = tokenized_datasets.map(group_texts, batched=True)\n",
    "lm_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment\n",
    "\n",
    "- Grouping and then chunking the texts has produced many more examples than the original 25,000 for the train and test splits. \n",
    "- That’s because we now have examples involving contiguous tokens that span across multiple examples from the original corpus. \n",
    "- You can see this explicitly by looking for the special `[SEP]` and `[CLS]` tokens in one of the chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">arguably their answer to good old boy john ford, had sex scenes in his films. <span style=\"font-weight: bold\">&lt;</span><span style=\"color: #000000; text-decoration-color: #000000\"> br </span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #000000; text-decoration-color: #000000\"> &gt; &lt; br </span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"font-weight: bold\">&gt;</span> i do commend the \n",
       "filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock \n",
       "people and make money to be shown in pornographic theaters in america. i am curious - yellow is a good film for \n",
       "anyone wanting to study the meat and potatoes <span style=\"font-weight: bold\">(</span> no pun intended <span style=\"font-weight: bold\">)</span> of swedish cinema. but really, this film doesn't \n",
       "have much of a plot. <span style=\"font-weight: bold\">[</span>SEP<span style=\"font-weight: bold\">]</span> <span style=\"font-weight: bold\">[</span>CLS<span style=\"font-weight: bold\">]</span> <span style=\"color: #008000; text-decoration-color: #008000\">\" i am curious : yellow \"</span> is a risible and pretentious steaming pile. it doesn\n",
       "</pre>\n"
      ],
      "text/plain": [
       "arguably their answer to good old boy john ford, had sex scenes in his films. \u001b[1m<\u001b[0m\u001b[39m br \u001b[0m\u001b[35m/\u001b[0m\u001b[39m > < br \u001b[0m\u001b[35m/\u001b[0m\u001b[39m \u001b[0m\u001b[1m>\u001b[0m i do commend the \n",
       "filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock \n",
       "people and make money to be shown in pornographic theaters in america. i am curious - yellow is a good film for \n",
       "anyone wanting to study the meat and potatoes \u001b[1m(\u001b[0m no pun intended \u001b[1m)\u001b[0m of swedish cinema. but really, this film doesn't \n",
       "have much of a plot. \u001b[1m[\u001b[0mSEP\u001b[1m]\u001b[0m \u001b[1m[\u001b[0mCLS\u001b[1m]\u001b[0m \u001b[32m\" i am curious : yellow \"\u001b[0m is a risible and pretentious steaming pile. it doesn\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Decode the tokenized texts\n",
    "print(tokenizer.decode(lm_datasets[\"train\"][2][\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">arguably their answer to good old boy john ford, had sex scenes in his films. <span style=\"font-weight: bold\">&lt;</span><span style=\"color: #000000; text-decoration-color: #000000\"> br </span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #000000; text-decoration-color: #000000\"> &gt; &lt; br </span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"font-weight: bold\">&gt;</span> i do commend the \n",
       "filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock \n",
       "people and make money to be shown in pornographic theaters in america. i am curious - yellow is a good film for \n",
       "anyone wanting to study the meat and potatoes <span style=\"font-weight: bold\">(</span> no pun intended <span style=\"font-weight: bold\">)</span> of swedish cinema. but really, this film doesn't \n",
       "have much of a plot. <span style=\"font-weight: bold\">[</span>SEP<span style=\"font-weight: bold\">]</span> <span style=\"font-weight: bold\">[</span>CLS<span style=\"font-weight: bold\">]</span> <span style=\"color: #008000; text-decoration-color: #008000\">\" i am curious : yellow \"</span> is a risible and pretentious steaming pile. it doesn\n",
       "</pre>\n"
      ],
      "text/plain": [
       "arguably their answer to good old boy john ford, had sex scenes in his films. \u001b[1m<\u001b[0m\u001b[39m br \u001b[0m\u001b[35m/\u001b[0m\u001b[39m > < br \u001b[0m\u001b[35m/\u001b[0m\u001b[39m \u001b[0m\u001b[1m>\u001b[0m i do commend the \n",
       "filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock \n",
       "people and make money to be shown in pornographic theaters in america. i am curious - yellow is a good film for \n",
       "anyone wanting to study the meat and potatoes \u001b[1m(\u001b[0m no pun intended \u001b[1m)\u001b[0m of swedish cinema. but really, this film doesn't \n",
       "have much of a plot. \u001b[1m[\u001b[0mSEP\u001b[1m]\u001b[0m \u001b[1m[\u001b[0mCLS\u001b[1m]\u001b[0m \u001b[32m\" i am curious : yellow \"\u001b[0m is a risible and pretentious steaming pile. it doesn\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Decode the tokenized labels\n",
    "print(tokenizer.decode(lm_datasets[\"train\"][2][\"labels\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment\n",
    "\n",
    "- As expected from our `group_texts()` function above, this looks identical to the decoded input_ids — but then how can our model possibly learn anything? \n",
    "- We’re missing a key step: inserting [MASK] tokens at random positions in the inputs! \n",
    "- Let’s see how we can do this on the fly during fine-tuning using a special data collator.\n",
    "\n",
    "<br><hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning DistilBERT with the Trainer API\n",
    "\n",
    "- Fine-tuning a masked language model is almost identical to fine-tuning a sequence classification model.\n",
    "- The only difference is that we need a special data collator that can randomly mask some of the tokens in each batch of texts. \n",
    "- Fortunately, 🤗 Transformers comes prepared with a dedicated DataCollatorForLanguageModeling for just this task. \n",
    "- We just have to pass it the tokenizer and an `mlm_probability` argument that specifies what fraction of the tokens to mask. \n",
    "- We’ll use `15%`, which is the amount used for `BERT` and a common choice in the literature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "\n",
    "data_collator: DataCollatorForLanguageModeling = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'&gt;&gt;&gt; [CLS] i rented i am curious - yellow from my video store because of all the controversy that surrounded it </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">when it was [MASK] released in 1967. i also heard that at first it was seized by [MASK]. s. customs if it ever </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">tried [MASK] [MASK] this country, therefore being a fan of [MASK] considered \" controversial \" i really had to see </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">this for myself. &lt; br / &gt; &lt; br [MASK] &gt; the [MASK] is [MASK] [MASK] a young swedish drama student named lena who </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">wantsands learn everything [MASK] can about life. in particular she wants to focus her attentions to making some </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">sort of documentary on what the average swede simply [MASK] certain political issues such'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[32m'>>> \u001b[0m\u001b[32m[\u001b[0m\u001b[32mCLS\u001b[0m\u001b[32m]\u001b[0m\u001b[32m i rented i am curious - yellow from my video store because of all the controversy that surrounded it \u001b[0m\n",
       "\u001b[32mwhen it was \u001b[0m\u001b[32m[\u001b[0m\u001b[32mMASK\u001b[0m\u001b[32m]\u001b[0m\u001b[32m released in 1967. i also heard that at first it was seized by \u001b[0m\u001b[32m[\u001b[0m\u001b[32mMASK\u001b[0m\u001b[32m]\u001b[0m\u001b[32m. s. customs if it ever \u001b[0m\n",
       "\u001b[32mtried \u001b[0m\u001b[32m[\u001b[0m\u001b[32mMASK\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \u001b[0m\u001b[32m[\u001b[0m\u001b[32mMASK\u001b[0m\u001b[32m]\u001b[0m\u001b[32m this country, therefore being a fan of \u001b[0m\u001b[32m[\u001b[0m\u001b[32mMASK\u001b[0m\u001b[32m]\u001b[0m\u001b[32m considered \" controversial \" i really had to see \u001b[0m\n",
       "\u001b[32mthis for myself. \u001b[0m\u001b[32m<\u001b[0m\u001b[32m br / > < br \u001b[0m\u001b[32m[\u001b[0m\u001b[32mMASK\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \u001b[0m\u001b[32m>\u001b[0m\u001b[32m the \u001b[0m\u001b[32m[\u001b[0m\u001b[32mMASK\u001b[0m\u001b[32m]\u001b[0m\u001b[32m is \u001b[0m\u001b[32m[\u001b[0m\u001b[32mMASK\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \u001b[0m\u001b[32m[\u001b[0m\u001b[32mMASK\u001b[0m\u001b[32m]\u001b[0m\u001b[32m a young swedish drama student named lena who \u001b[0m\n",
       "\u001b[32mwantsands learn everything \u001b[0m\u001b[32m[\u001b[0m\u001b[32mMASK\u001b[0m\u001b[32m]\u001b[0m\u001b[32m can about life. in particular she wants to focus her attentions to making some \u001b[0m\n",
       "\u001b[32msort of documentary on what the average swede simply \u001b[0m\u001b[32m[\u001b[0m\u001b[32mMASK\u001b[0m\u001b[32m]\u001b[0m\u001b[32m certain political issues such'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'&gt;&gt;&gt; as the vietnam war and race issues in the united states. in between asking politicians and ordinary denizens </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">of月 about [MASK] opinions on politics, she has sex with her drama teacher, classmates, and married men. &lt; br / &gt; &lt;</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">br / &gt; what kills me about i [MASK] curious [MASK] yellow [MASK] that 40 years ago, [MASK] was considered [MASK]. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">really [MASK] the sex and [MASK]dity scenes are few [MASK] [MASK] between, even then [MASK]'</span>s not shot like some \n",
       "cheaply made porno. while <span style=\"font-weight: bold\">[</span>MASK<span style=\"font-weight: bold\">]</span> countrymen <span style=\"font-weight: bold\">[</span>MASK<span style=\"font-weight: bold\">]</span> find <span style=\"font-weight: bold\">[</span>MASK<span style=\"font-weight: bold\">]</span> shocking, in <span style=\"font-weight: bold\">[</span>MASK<span style=\"font-weight: bold\">]</span> sex and nudity are a major \n",
       "staple in swedish cinema. even ingmar bergman,'\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[32m'>>> as the vietnam war and race issues in the united states. in between asking politicians and ordinary denizens \u001b[0m\n",
       "\u001b[32mof月 about \u001b[0m\u001b[32m[\u001b[0m\u001b[32mMASK\u001b[0m\u001b[32m]\u001b[0m\u001b[32m opinions on politics, she has sex with her drama teacher, classmates, and married men. \u001b[0m\u001b[32m<\u001b[0m\u001b[32m br / > <\u001b[0m\n",
       "\u001b[32mbr / \u001b[0m\u001b[32m>\u001b[0m\u001b[32m what kills me about i \u001b[0m\u001b[32m[\u001b[0m\u001b[32mMASK\u001b[0m\u001b[32m]\u001b[0m\u001b[32m curious \u001b[0m\u001b[32m[\u001b[0m\u001b[32mMASK\u001b[0m\u001b[32m]\u001b[0m\u001b[32m yellow \u001b[0m\u001b[32m[\u001b[0m\u001b[32mMASK\u001b[0m\u001b[32m]\u001b[0m\u001b[32m that 40 years ago, \u001b[0m\u001b[32m[\u001b[0m\u001b[32mMASK\u001b[0m\u001b[32m]\u001b[0m\u001b[32m was considered \u001b[0m\u001b[32m[\u001b[0m\u001b[32mMASK\u001b[0m\u001b[32m]\u001b[0m\u001b[32m. \u001b[0m\n",
       "\u001b[32mreally \u001b[0m\u001b[32m[\u001b[0m\u001b[32mMASK\u001b[0m\u001b[32m]\u001b[0m\u001b[32m the sex and \u001b[0m\u001b[32m[\u001b[0m\u001b[32mMASK\u001b[0m\u001b[32m]\u001b[0m\u001b[32mdity scenes are few \u001b[0m\u001b[32m[\u001b[0m\u001b[32mMASK\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \u001b[0m\u001b[32m[\u001b[0m\u001b[32mMASK\u001b[0m\u001b[32m]\u001b[0m\u001b[32m between, even then \u001b[0m\u001b[32m[\u001b[0m\u001b[32mMASK\u001b[0m\u001b[32m]\u001b[0m\u001b[32m'\u001b[0ms not shot like some \n",
       "cheaply made porno. while \u001b[1m[\u001b[0mMASK\u001b[1m]\u001b[0m countrymen \u001b[1m[\u001b[0mMASK\u001b[1m]\u001b[0m find \u001b[1m[\u001b[0mMASK\u001b[1m]\u001b[0m shocking, in \u001b[1m[\u001b[0mMASK\u001b[1m]\u001b[0m sex and nudity are a major \n",
       "staple in swedish cinema. even ingmar bergman,'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# To see how the random masking works, let’s feed a few examples to the data collator.\n",
    "# Since it expects a list of dicts, where each dict represents a single chunk of contiguous text,\n",
    "# we first iterate over the dataset before feeding the batch to the collator. We remove the \"word_ids\" key\n",
    "# for this data collator as it does not expect it:\n",
    "samples: list[dict[str, Any]] = [lm_datasets[\"train\"][i] for i in range(2)]\n",
    "\n",
    "for sample in samples:\n",
    "    _ = sample.pop(\"word_ids\")\n",
    "\n",
    "for chunk in data_collator(samples)[\"input_ids\"]:\n",
    "    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment\n",
    "\n",
    "- Nice, it worked! We can see that the [MASK] token has been randomly inserted at various locations in our text. \n",
    "- These will be the tokens which our model will have to predict during training — and the beauty of the data collator is that it will randomize the [MASK] insertion with every batch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'&gt;&gt;&gt; ['</span><span style=\"font-weight: bold\">[</span>CLS<span style=\"font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\">', '</span>i', <span style=\"color: #008000; text-decoration-color: #008000\">'rented'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'i'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'am'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'curious'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'-'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'yellow'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'from'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'my'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'video'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'store'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'because'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'of'</span>,\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'all'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'the'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'[MASK]'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'that'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'surrounded'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'it'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'when'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'it'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'was'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'first'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'released'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'in'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'1967'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'.'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'[MASK]'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'also'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'[MASK]'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'smash'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'at'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'first'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'it'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'was'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'seized'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'by'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'u'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'.'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'s'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'.'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'customs'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'if'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'it'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'ever'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'tried'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'to'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'enter'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'this'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'[MASK]'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">','</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'therefore'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'being'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'a'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'fan'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'of'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'films'</span>,\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'considered'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'\"'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'controversial'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'\"'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'i'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'really'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'had'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'to'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'see'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'this'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'for'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'myself'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'.'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'&lt;'</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'br'</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'/'</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'&gt;'</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'tanker'</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'[MASK]'</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'/'</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'&gt;'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'the'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'plot'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'[MASK]'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'centered'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'around'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'a'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'young'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'swedish'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'drama'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'student'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'named'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'lena'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'who'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'[MASK]'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'to'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'[MASK]'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'everything'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'[MASK]'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'can'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'about'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'life'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'.'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'in'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'particular'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'she'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'wants'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'to'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'focus'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'her'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'attention'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'##s'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'to'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'making'</span>,\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'some'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'sort'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'of'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'documentary'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'[MASK]'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'what'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'the'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'[MASK]'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'sw'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'##ede'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'thought'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'about'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'certain'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'political'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'issues'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'such'</span><span style=\"font-weight: bold\">]</span>'\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[32m'>>> \u001b[0m\u001b[32m[\u001b[0m\u001b[32m'\u001b[0m\u001b[1m[\u001b[0mCLS\u001b[1m]\u001b[0m\u001b[32m', '\u001b[0mi', \u001b[32m'rented'\u001b[0m, \u001b[32m'i'\u001b[0m, \u001b[32m'am'\u001b[0m, \u001b[32m'curious'\u001b[0m, \u001b[32m'-'\u001b[0m, \u001b[32m'yellow'\u001b[0m, \u001b[32m'from'\u001b[0m, \u001b[32m'my'\u001b[0m, \u001b[32m'video'\u001b[0m, \u001b[32m'store'\u001b[0m, \u001b[32m'because'\u001b[0m, \u001b[32m'of'\u001b[0m,\n",
       "\u001b[32m'all'\u001b[0m, \u001b[32m'the'\u001b[0m, \u001b[32m'\u001b[0m\u001b[32m[\u001b[0m\u001b[32mMASK\u001b[0m\u001b[32m]\u001b[0m\u001b[32m'\u001b[0m, \u001b[32m'that'\u001b[0m, \u001b[32m'surrounded'\u001b[0m, \u001b[32m'it'\u001b[0m, \u001b[32m'when'\u001b[0m, \u001b[32m'it'\u001b[0m, \u001b[32m'was'\u001b[0m, \u001b[32m'first'\u001b[0m, \u001b[32m'released'\u001b[0m, \u001b[32m'in'\u001b[0m, \u001b[32m'1967'\u001b[0m, \u001b[32m'.'\u001b[0m, \n",
       "\u001b[32m'\u001b[0m\u001b[32m[\u001b[0m\u001b[32mMASK\u001b[0m\u001b[32m]\u001b[0m\u001b[32m'\u001b[0m, \u001b[32m'also'\u001b[0m, \u001b[32m'\u001b[0m\u001b[32m[\u001b[0m\u001b[32mMASK\u001b[0m\u001b[32m]\u001b[0m\u001b[32m'\u001b[0m, \u001b[32m'smash'\u001b[0m, \u001b[32m'at'\u001b[0m, \u001b[32m'first'\u001b[0m, \u001b[32m'it'\u001b[0m, \u001b[32m'was'\u001b[0m, \u001b[32m'seized'\u001b[0m, \u001b[32m'by'\u001b[0m, \u001b[32m'u'\u001b[0m, \u001b[32m'.'\u001b[0m, \u001b[32m's'\u001b[0m, \u001b[32m'.'\u001b[0m, \u001b[32m'customs'\u001b[0m, \n",
       "\u001b[32m'if'\u001b[0m, \u001b[32m'it'\u001b[0m, \u001b[32m'ever'\u001b[0m, \u001b[32m'tried'\u001b[0m, \u001b[32m'to'\u001b[0m, \u001b[32m'enter'\u001b[0m, \u001b[32m'this'\u001b[0m, \u001b[32m'\u001b[0m\u001b[32m[\u001b[0m\u001b[32mMASK\u001b[0m\u001b[32m]\u001b[0m\u001b[32m'\u001b[0m, \u001b[32m','\u001b[0m, \u001b[32m'therefore'\u001b[0m, \u001b[32m'being'\u001b[0m, \u001b[32m'a'\u001b[0m, \u001b[32m'fan'\u001b[0m, \u001b[32m'of'\u001b[0m, \u001b[32m'films'\u001b[0m,\n",
       "\u001b[32m'considered'\u001b[0m, \u001b[32m'\"'\u001b[0m, \u001b[32m'controversial'\u001b[0m, \u001b[32m'\"'\u001b[0m, \u001b[32m'i'\u001b[0m, \u001b[32m'really'\u001b[0m, \u001b[32m'had'\u001b[0m, \u001b[32m'to'\u001b[0m, \u001b[32m'see'\u001b[0m, \u001b[32m'this'\u001b[0m, \u001b[32m'for'\u001b[0m, \u001b[32m'myself'\u001b[0m, \u001b[32m'.'\u001b[0m, \u001b[32m'\u001b[0m\u001b[32m<\u001b[0m\u001b[32m'\u001b[0m\u001b[39m, \u001b[0m\n",
       "\u001b[32m'br'\u001b[0m\u001b[39m, \u001b[0m\u001b[32m'/'\u001b[0m\u001b[39m, \u001b[0m\u001b[32m'>'\u001b[0m\u001b[39m, \u001b[0m\u001b[32m'tanker'\u001b[0m\u001b[39m, \u001b[0m\u001b[32m'\u001b[0m\u001b[32m[\u001b[0m\u001b[32mMASK\u001b[0m\u001b[32m]\u001b[0m\u001b[32m'\u001b[0m\u001b[39m, \u001b[0m\u001b[32m'/'\u001b[0m\u001b[39m, \u001b[0m\u001b[32m'\u001b[0m\u001b[32m>\u001b[0m\u001b[32m'\u001b[0m, \u001b[32m'the'\u001b[0m, \u001b[32m'plot'\u001b[0m, \u001b[32m'\u001b[0m\u001b[32m[\u001b[0m\u001b[32mMASK\u001b[0m\u001b[32m]\u001b[0m\u001b[32m'\u001b[0m, \u001b[32m'centered'\u001b[0m, \u001b[32m'around'\u001b[0m, \u001b[32m'a'\u001b[0m, \u001b[32m'young'\u001b[0m, \n",
       "\u001b[32m'swedish'\u001b[0m, \u001b[32m'drama'\u001b[0m, \u001b[32m'student'\u001b[0m, \u001b[32m'named'\u001b[0m, \u001b[32m'lena'\u001b[0m, \u001b[32m'who'\u001b[0m, \u001b[32m'\u001b[0m\u001b[32m[\u001b[0m\u001b[32mMASK\u001b[0m\u001b[32m]\u001b[0m\u001b[32m'\u001b[0m, \u001b[32m'to'\u001b[0m, \u001b[32m'\u001b[0m\u001b[32m[\u001b[0m\u001b[32mMASK\u001b[0m\u001b[32m]\u001b[0m\u001b[32m'\u001b[0m, \u001b[32m'everything'\u001b[0m, \u001b[32m'\u001b[0m\u001b[32m[\u001b[0m\u001b[32mMASK\u001b[0m\u001b[32m]\u001b[0m\u001b[32m'\u001b[0m, \u001b[32m'can'\u001b[0m, \n",
       "\u001b[32m'about'\u001b[0m, \u001b[32m'life'\u001b[0m, \u001b[32m'.'\u001b[0m, \u001b[32m'in'\u001b[0m, \u001b[32m'particular'\u001b[0m, \u001b[32m'she'\u001b[0m, \u001b[32m'wants'\u001b[0m, \u001b[32m'to'\u001b[0m, \u001b[32m'focus'\u001b[0m, \u001b[32m'her'\u001b[0m, \u001b[32m'attention'\u001b[0m, \u001b[32m'##s'\u001b[0m, \u001b[32m'to'\u001b[0m, \u001b[32m'making'\u001b[0m,\n",
       "\u001b[32m'some'\u001b[0m, \u001b[32m'sort'\u001b[0m, \u001b[32m'of'\u001b[0m, \u001b[32m'documentary'\u001b[0m, \u001b[32m'\u001b[0m\u001b[32m[\u001b[0m\u001b[32mMASK\u001b[0m\u001b[32m]\u001b[0m\u001b[32m'\u001b[0m, \u001b[32m'what'\u001b[0m, \u001b[32m'the'\u001b[0m, \u001b[32m'\u001b[0m\u001b[32m[\u001b[0m\u001b[32mMASK\u001b[0m\u001b[32m]\u001b[0m\u001b[32m'\u001b[0m, \u001b[32m'sw'\u001b[0m, \u001b[32m'##ede'\u001b[0m, \u001b[32m'thought'\u001b[0m, \u001b[32m'about'\u001b[0m, \n",
       "\u001b[32m'certain'\u001b[0m, \u001b[32m'political'\u001b[0m, \u001b[32m'issues'\u001b[0m, \u001b[32m'such'\u001b[0m\u001b[1m]\u001b[0m'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'&gt;&gt;&gt; ['</span>as', <span style=\"color: #008000; text-decoration-color: #008000\">'the'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'vietnam'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'war'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'and'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'race'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'issues'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'in'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'the'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'united'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'states'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'.'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'in'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'between'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'asking'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'politicians'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'and'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'ordinary'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'den'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'[MASK]'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'[MASK]'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'of'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'stockholm'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'about'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'their'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'[MASK]'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'on'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'politics'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">','</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'she'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'skipper'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'sex'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'[MASK]'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'[MASK]'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'drama'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'teacher'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">','</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'classmates'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">','</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'and'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'married'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'men'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'.'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'&lt;'</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'br'</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'/'</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'&gt;'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'&lt;'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'br'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'/'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'[MASK]'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'what'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'kills'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'remainder'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'about'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'i'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'am'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'curious'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'-'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'yellow'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'is'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'[MASK]'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'40'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'years'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'ago'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">','</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'reggie'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'was'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'considered'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'pornographic'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'.'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'really'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">','</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'the'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'[MASK]'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'entrepreneurs'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'nu'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'##dity'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'scenes'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'are'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'few'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'and'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'far'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'between'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'[MASK]'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'even'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'then'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'it'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">\"'\"</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'quintet'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'not'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'shot'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'[MASK]'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'some'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'cheap'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'##ly'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'made'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'porn'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'##o'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'.'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'while'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'my'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'country'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'##men'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'mind'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'find'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'formal'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'shocking'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">','</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'[MASK]'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'reality'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'sex'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'and'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'nu'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'##dity'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'are'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'a'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'major'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'staple'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'in'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'swedish'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'cinema'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'.'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'even'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'ing'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'##mar'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'bergman'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">','</span><span style=\"font-weight: bold\">]</span>'\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[32m'>>> \u001b[0m\u001b[32m[\u001b[0m\u001b[32m'\u001b[0mas', \u001b[32m'the'\u001b[0m, \u001b[32m'vietnam'\u001b[0m, \u001b[32m'war'\u001b[0m, \u001b[32m'and'\u001b[0m, \u001b[32m'race'\u001b[0m, \u001b[32m'issues'\u001b[0m, \u001b[32m'in'\u001b[0m, \u001b[32m'the'\u001b[0m, \u001b[32m'united'\u001b[0m, \u001b[32m'states'\u001b[0m, \u001b[32m'.'\u001b[0m, \u001b[32m'in'\u001b[0m, \n",
       "\u001b[32m'between'\u001b[0m, \u001b[32m'asking'\u001b[0m, \u001b[32m'politicians'\u001b[0m, \u001b[32m'and'\u001b[0m, \u001b[32m'ordinary'\u001b[0m, \u001b[32m'den'\u001b[0m, \u001b[32m'\u001b[0m\u001b[32m[\u001b[0m\u001b[32mMASK\u001b[0m\u001b[32m]\u001b[0m\u001b[32m'\u001b[0m, \u001b[32m'\u001b[0m\u001b[32m[\u001b[0m\u001b[32mMASK\u001b[0m\u001b[32m]\u001b[0m\u001b[32m'\u001b[0m, \u001b[32m'of'\u001b[0m, \u001b[32m'stockholm'\u001b[0m, \u001b[32m'about'\u001b[0m, \n",
       "\u001b[32m'their'\u001b[0m, \u001b[32m'\u001b[0m\u001b[32m[\u001b[0m\u001b[32mMASK\u001b[0m\u001b[32m]\u001b[0m\u001b[32m'\u001b[0m, \u001b[32m'on'\u001b[0m, \u001b[32m'politics'\u001b[0m, \u001b[32m','\u001b[0m, \u001b[32m'she'\u001b[0m, \u001b[32m'skipper'\u001b[0m, \u001b[32m'sex'\u001b[0m, \u001b[32m'\u001b[0m\u001b[32m[\u001b[0m\u001b[32mMASK\u001b[0m\u001b[32m]\u001b[0m\u001b[32m'\u001b[0m, \u001b[32m'\u001b[0m\u001b[32m[\u001b[0m\u001b[32mMASK\u001b[0m\u001b[32m]\u001b[0m\u001b[32m'\u001b[0m, \u001b[32m'drama'\u001b[0m, \u001b[32m'teacher'\u001b[0m, \u001b[32m','\u001b[0m, \n",
       "\u001b[32m'classmates'\u001b[0m, \u001b[32m','\u001b[0m, \u001b[32m'and'\u001b[0m, \u001b[32m'married'\u001b[0m, \u001b[32m'men'\u001b[0m, \u001b[32m'.'\u001b[0m, \u001b[32m'\u001b[0m\u001b[32m<\u001b[0m\u001b[32m'\u001b[0m\u001b[39m, \u001b[0m\u001b[32m'br'\u001b[0m\u001b[39m, \u001b[0m\u001b[32m'/'\u001b[0m\u001b[39m, \u001b[0m\u001b[32m'\u001b[0m\u001b[32m>\u001b[0m\u001b[32m'\u001b[0m, \u001b[32m'<'\u001b[0m, \u001b[32m'br'\u001b[0m, \u001b[32m'/'\u001b[0m, \u001b[32m'\u001b[0m\u001b[32m[\u001b[0m\u001b[32mMASK\u001b[0m\u001b[32m]\u001b[0m\u001b[32m'\u001b[0m, \u001b[32m'what'\u001b[0m, \u001b[32m'kills'\u001b[0m, \n",
       "\u001b[32m'remainder'\u001b[0m, \u001b[32m'about'\u001b[0m, \u001b[32m'i'\u001b[0m, \u001b[32m'am'\u001b[0m, \u001b[32m'curious'\u001b[0m, \u001b[32m'-'\u001b[0m, \u001b[32m'yellow'\u001b[0m, \u001b[32m'is'\u001b[0m, \u001b[32m'\u001b[0m\u001b[32m[\u001b[0m\u001b[32mMASK\u001b[0m\u001b[32m]\u001b[0m\u001b[32m'\u001b[0m, \u001b[32m'40'\u001b[0m, \u001b[32m'years'\u001b[0m, \u001b[32m'ago'\u001b[0m, \u001b[32m','\u001b[0m, \u001b[32m'reggie'\u001b[0m, \n",
       "\u001b[32m'was'\u001b[0m, \u001b[32m'considered'\u001b[0m, \u001b[32m'pornographic'\u001b[0m, \u001b[32m'.'\u001b[0m, \u001b[32m'really'\u001b[0m, \u001b[32m','\u001b[0m, \u001b[32m'the'\u001b[0m, \u001b[32m'\u001b[0m\u001b[32m[\u001b[0m\u001b[32mMASK\u001b[0m\u001b[32m]\u001b[0m\u001b[32m'\u001b[0m, \u001b[32m'entrepreneurs'\u001b[0m, \u001b[32m'nu'\u001b[0m, \u001b[32m'##dity'\u001b[0m, \n",
       "\u001b[32m'scenes'\u001b[0m, \u001b[32m'are'\u001b[0m, \u001b[32m'few'\u001b[0m, \u001b[32m'and'\u001b[0m, \u001b[32m'far'\u001b[0m, \u001b[32m'between'\u001b[0m, \u001b[32m'\u001b[0m\u001b[32m[\u001b[0m\u001b[32mMASK\u001b[0m\u001b[32m]\u001b[0m\u001b[32m'\u001b[0m, \u001b[32m'even'\u001b[0m, \u001b[32m'then'\u001b[0m, \u001b[32m'it'\u001b[0m, \u001b[32m\"'\"\u001b[0m, \u001b[32m'quintet'\u001b[0m, \u001b[32m'not'\u001b[0m, \u001b[32m'shot'\u001b[0m, \n",
       "\u001b[32m'\u001b[0m\u001b[32m[\u001b[0m\u001b[32mMASK\u001b[0m\u001b[32m]\u001b[0m\u001b[32m'\u001b[0m, \u001b[32m'some'\u001b[0m, \u001b[32m'cheap'\u001b[0m, \u001b[32m'##ly'\u001b[0m, \u001b[32m'made'\u001b[0m, \u001b[32m'porn'\u001b[0m, \u001b[32m'##o'\u001b[0m, \u001b[32m'.'\u001b[0m, \u001b[32m'while'\u001b[0m, \u001b[32m'my'\u001b[0m, \u001b[32m'country'\u001b[0m, \u001b[32m'##men'\u001b[0m, \u001b[32m'mind'\u001b[0m, \u001b[32m'find'\u001b[0m, \n",
       "\u001b[32m'formal'\u001b[0m, \u001b[32m'shocking'\u001b[0m, \u001b[32m','\u001b[0m, \u001b[32m'\u001b[0m\u001b[32m[\u001b[0m\u001b[32mMASK\u001b[0m\u001b[32m]\u001b[0m\u001b[32m'\u001b[0m, \u001b[32m'reality'\u001b[0m, \u001b[32m'sex'\u001b[0m, \u001b[32m'and'\u001b[0m, \u001b[32m'nu'\u001b[0m, \u001b[32m'##dity'\u001b[0m, \u001b[32m'are'\u001b[0m, \u001b[32m'a'\u001b[0m, \u001b[32m'major'\u001b[0m, \u001b[32m'staple'\u001b[0m, \u001b[32m'in'\u001b[0m, \n",
       "\u001b[32m'swedish'\u001b[0m, \u001b[32m'cinema'\u001b[0m, \u001b[32m'.'\u001b[0m, \u001b[32m'even'\u001b[0m, \u001b[32m'ing'\u001b[0m, \u001b[32m'##mar'\u001b[0m, \u001b[32m'bergman'\u001b[0m, \u001b[32m','\u001b[0m\u001b[1m]\u001b[0m'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Replace the tokenizer.decode() method with tokenizer.convert_ids_to_tokens() to see that \n",
    "# sometimes a single token from a given word is masked, and not the others.\n",
    "\n",
    "for chunk in data_collator(samples)[\"input_ids\"]:\n",
    "    print(f\"\\n'>>> {tokenizer.convert_ids_to_tokens(chunk)}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment\n",
    "\n",
    "- One side effect of random masking is that our evaluation metrics will not be deterministic when using the Trainer, since we use the same data collator for the training and test sets. \n",
    "- We’ll see later, when we look at fine-tuning with 🤗 Accelerate, how we can use the flexibility of a custom evaluation loop to freeze the randomness.\n",
    "- When training models for masked language modeling, one technique that can be used is to `mask whole words together`, not just individual tokens. \n",
    "- This approach is called `whole word masking`. If we want to use whole word masking, we will need to build a data collator ourselves. \n",
    "- A data collator is just a function that takes a list of samples and converts them into a batch, so let’s do this now! \n",
    "- We’ll use the word IDs computed earlier to make a map between word indices and the corresponding tokens, then randomly decide which words to mask and apply that mask on the inputs. \n",
    "- Note that the labels are all `-100` except for the ones corresponding to mask words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from transformers import default_data_collator\n",
    "\n",
    "\n",
    "wwm_probability: float = 0.2\n",
    "\n",
    "\n",
    "def whole_word_masking_data_collator(features: list[Any]) -> dict[str, Any]:\n",
    "    \"\"\"This is used for whole word masking.\"\"\"\n",
    "    for feature in features:\n",
    "        word_ids: list[int] = feature.pop(\"word_ids\")\n",
    "\n",
    "        # Create a map between words and corresponding token indices\n",
    "        mapping = collections.defaultdict(list)\n",
    "        current_word_index: int = -1\n",
    "        current_word: Union[int, None] = None\n",
    "        for idx, word_id in enumerate(word_ids):\n",
    "            if word_id is not None:\n",
    "                if word_id != current_word:\n",
    "                    current_word: int = word_id\n",
    "                    current_word_index += 1\n",
    "                mapping[current_word_index].append(idx)\n",
    "\n",
    "        # Randomly mask words\n",
    "        mask: np.ndarray = np.random.binomial(1, wwm_probability, (len(mapping),))\n",
    "        input_ids: list[Union[int, None]] = feature[\"input_ids\"]\n",
    "        labels: list[int] = feature[\"labels\"]\n",
    "        new_labels: list[int] = [-100] * len(labels)\n",
    "        for word_id in np.where(mask)[0]:\n",
    "            word_id = word_id.item()\n",
    "            for idx in mapping[word_id]:\n",
    "                new_labels[idx] = labels[idx]\n",
    "                input_ids[idx] = tokenizer.mask_token_id\n",
    "        feature[\"labels\"] = new_labels\n",
    "\n",
    "    return default_data_collator(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'&gt;&gt;&gt; [CLS] i [MASK] i am curious - [MASK] from my video [MASK] because [MASK] all [MASK] controversy that </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">surrounded [MASK] when it was first released in 1967. i also heard that [MASK] first it [MASK] seized by u [MASK] </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">[MASK]. customs if it [MASK] tried to enter this [MASK], therefore being a fan [MASK] films considered \" </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">controversial [MASK] i really had to see this for myself. &lt; [MASK] [MASK] &gt; &lt; br / [MASK] the plot [MASK] centered </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">around [MASK] young swedish drama student named [MASK] who wants to learn everything she can about life [MASK] in </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">[MASK] she wants to focus her attentions to making some sort [MASK] documentary on what the average swede thought </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">about certain political issues [MASK]'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[32m'>>> \u001b[0m\u001b[32m[\u001b[0m\u001b[32mCLS\u001b[0m\u001b[32m]\u001b[0m\u001b[32m i \u001b[0m\u001b[32m[\u001b[0m\u001b[32mMASK\u001b[0m\u001b[32m]\u001b[0m\u001b[32m i am curious - \u001b[0m\u001b[32m[\u001b[0m\u001b[32mMASK\u001b[0m\u001b[32m]\u001b[0m\u001b[32m from my video \u001b[0m\u001b[32m[\u001b[0m\u001b[32mMASK\u001b[0m\u001b[32m]\u001b[0m\u001b[32m because \u001b[0m\u001b[32m[\u001b[0m\u001b[32mMASK\u001b[0m\u001b[32m]\u001b[0m\u001b[32m all \u001b[0m\u001b[32m[\u001b[0m\u001b[32mMASK\u001b[0m\u001b[32m]\u001b[0m\u001b[32m controversy that \u001b[0m\n",
       "\u001b[32msurrounded \u001b[0m\u001b[32m[\u001b[0m\u001b[32mMASK\u001b[0m\u001b[32m]\u001b[0m\u001b[32m when it was first released in 1967. i also heard that \u001b[0m\u001b[32m[\u001b[0m\u001b[32mMASK\u001b[0m\u001b[32m]\u001b[0m\u001b[32m first it \u001b[0m\u001b[32m[\u001b[0m\u001b[32mMASK\u001b[0m\u001b[32m]\u001b[0m\u001b[32m seized by u \u001b[0m\u001b[32m[\u001b[0m\u001b[32mMASK\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \u001b[0m\n",
       "\u001b[32m[\u001b[0m\u001b[32mMASK\u001b[0m\u001b[32m]\u001b[0m\u001b[32m. customs if it \u001b[0m\u001b[32m[\u001b[0m\u001b[32mMASK\u001b[0m\u001b[32m]\u001b[0m\u001b[32m tried to enter this \u001b[0m\u001b[32m[\u001b[0m\u001b[32mMASK\u001b[0m\u001b[32m]\u001b[0m\u001b[32m, therefore being a fan \u001b[0m\u001b[32m[\u001b[0m\u001b[32mMASK\u001b[0m\u001b[32m]\u001b[0m\u001b[32m films considered \" \u001b[0m\n",
       "\u001b[32mcontroversial \u001b[0m\u001b[32m[\u001b[0m\u001b[32mMASK\u001b[0m\u001b[32m]\u001b[0m\u001b[32m i really had to see this for myself. \u001b[0m\u001b[32m<\u001b[0m\u001b[32m \u001b[0m\u001b[32m[\u001b[0m\u001b[32mMASK\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \u001b[0m\u001b[32m[\u001b[0m\u001b[32mMASK\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \u001b[0m\u001b[32m>\u001b[0m\u001b[32m < br / \u001b[0m\u001b[32m[\u001b[0m\u001b[32mMASK\u001b[0m\u001b[32m]\u001b[0m\u001b[32m the plot \u001b[0m\u001b[32m[\u001b[0m\u001b[32mMASK\u001b[0m\u001b[32m]\u001b[0m\u001b[32m centered \u001b[0m\n",
       "\u001b[32maround \u001b[0m\u001b[32m[\u001b[0m\u001b[32mMASK\u001b[0m\u001b[32m]\u001b[0m\u001b[32m young swedish drama student named \u001b[0m\u001b[32m[\u001b[0m\u001b[32mMASK\u001b[0m\u001b[32m]\u001b[0m\u001b[32m who wants to learn everything she can about life \u001b[0m\u001b[32m[\u001b[0m\u001b[32mMASK\u001b[0m\u001b[32m]\u001b[0m\u001b[32m in \u001b[0m\n",
       "\u001b[32m[\u001b[0m\u001b[32mMASK\u001b[0m\u001b[32m]\u001b[0m\u001b[32m she wants to focus her attentions to making some sort \u001b[0m\u001b[32m[\u001b[0m\u001b[32mMASK\u001b[0m\u001b[32m]\u001b[0m\u001b[32m documentary on what the average swede thought \u001b[0m\n",
       "\u001b[32mabout certain political issues \u001b[0m\u001b[32m[\u001b[0m\u001b[32mMASK\u001b[0m\u001b[32m]\u001b[0m\u001b[32m'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'&gt;&gt;&gt; as the vietnam war and race issues in the united states. in between asking [MASK] and [MASK] denizens [MASK] </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">stockholm about their opinions [MASK] [MASK], she [MASK] sex with her drama teacher, classmates, and [MASK] men. &lt; </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">br / &gt; &lt; br / &gt; what kills [MASK] about i [MASK] [MASK] - yellow is [MASK] [MASK] years ago [MASK] this was [MASK] </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">pornographic. really, the sex [MASK] [MASK] [MASK] scenes are [MASK] [MASK] far between, even then it [MASK] s </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">[MASK] [MASK] like some cheaply made porno. while [MASK] countrymen mind find it shocking, in reality sex and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">nudity are a major [MASK] in swedish cinema. even [MASK] [MASK] bergman,'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[32m'>>> as the vietnam war and race issues in the united states. in between asking \u001b[0m\u001b[32m[\u001b[0m\u001b[32mMASK\u001b[0m\u001b[32m]\u001b[0m\u001b[32m and \u001b[0m\u001b[32m[\u001b[0m\u001b[32mMASK\u001b[0m\u001b[32m]\u001b[0m\u001b[32m denizens \u001b[0m\u001b[32m[\u001b[0m\u001b[32mMASK\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \u001b[0m\n",
       "\u001b[32mstockholm about their opinions \u001b[0m\u001b[32m[\u001b[0m\u001b[32mMASK\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \u001b[0m\u001b[32m[\u001b[0m\u001b[32mMASK\u001b[0m\u001b[32m]\u001b[0m\u001b[32m, she \u001b[0m\u001b[32m[\u001b[0m\u001b[32mMASK\u001b[0m\u001b[32m]\u001b[0m\u001b[32m sex with her drama teacher, classmates, and \u001b[0m\u001b[32m[\u001b[0m\u001b[32mMASK\u001b[0m\u001b[32m]\u001b[0m\u001b[32m men. \u001b[0m\u001b[32m<\u001b[0m\u001b[32m \u001b[0m\n",
       "\u001b[32mbr / > < br / \u001b[0m\u001b[32m>\u001b[0m\u001b[32m what kills \u001b[0m\u001b[32m[\u001b[0m\u001b[32mMASK\u001b[0m\u001b[32m]\u001b[0m\u001b[32m about i \u001b[0m\u001b[32m[\u001b[0m\u001b[32mMASK\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \u001b[0m\u001b[32m[\u001b[0m\u001b[32mMASK\u001b[0m\u001b[32m]\u001b[0m\u001b[32m - yellow is \u001b[0m\u001b[32m[\u001b[0m\u001b[32mMASK\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \u001b[0m\u001b[32m[\u001b[0m\u001b[32mMASK\u001b[0m\u001b[32m]\u001b[0m\u001b[32m years ago \u001b[0m\u001b[32m[\u001b[0m\u001b[32mMASK\u001b[0m\u001b[32m]\u001b[0m\u001b[32m this was \u001b[0m\u001b[32m[\u001b[0m\u001b[32mMASK\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \u001b[0m\n",
       "\u001b[32mpornographic. really, the sex \u001b[0m\u001b[32m[\u001b[0m\u001b[32mMASK\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \u001b[0m\u001b[32m[\u001b[0m\u001b[32mMASK\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \u001b[0m\u001b[32m[\u001b[0m\u001b[32mMASK\u001b[0m\u001b[32m]\u001b[0m\u001b[32m scenes are \u001b[0m\u001b[32m[\u001b[0m\u001b[32mMASK\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \u001b[0m\u001b[32m[\u001b[0m\u001b[32mMASK\u001b[0m\u001b[32m]\u001b[0m\u001b[32m far between, even then it \u001b[0m\u001b[32m[\u001b[0m\u001b[32mMASK\u001b[0m\u001b[32m]\u001b[0m\u001b[32m s \u001b[0m\n",
       "\u001b[32m[\u001b[0m\u001b[32mMASK\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \u001b[0m\u001b[32m[\u001b[0m\u001b[32mMASK\u001b[0m\u001b[32m]\u001b[0m\u001b[32m like some cheaply made porno. while \u001b[0m\u001b[32m[\u001b[0m\u001b[32mMASK\u001b[0m\u001b[32m]\u001b[0m\u001b[32m countrymen mind find it shocking, in reality sex and \u001b[0m\n",
       "\u001b[32mnudity are a major \u001b[0m\u001b[32m[\u001b[0m\u001b[32mMASK\u001b[0m\u001b[32m]\u001b[0m\u001b[32m in swedish cinema. even \u001b[0m\u001b[32m[\u001b[0m\u001b[32mMASK\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \u001b[0m\u001b[32m[\u001b[0m\u001b[32mMASK\u001b[0m\u001b[32m]\u001b[0m\u001b[32m bergman,'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Next, we can try it on the same samples as before:\n",
    "samples: list[Any] = [lm_datasets[\"train\"][i] for i in range(2)]\n",
    "batch = whole_word_masking_data_collator(samples)\n",
    "\n",
    "for chunk in batch[\"input_ids\"]:\n",
    "    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment\n",
    "\n",
    "- Now that we have two data collators, the rest of the fine-tuning steps are standard. \n",
    "- Training can take a while on Google Colab if you’re not lucky enough to score a mythical P100 GPU 😭, so we’ll first downsample the size of the training set to a few thousand examples. \n",
    "- Don’t worry, we’ll still get a pretty decent language model! \n",
    "- A quick way to downsample a dataset in 🤗 Datasets is via the `Dataset.train_test_split()` function that we saw in Chapter 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size: int = 10_000\n",
    "test_size: int = int(0.1 * train_size)\n",
    "\n",
    "downsampled_dataset: DatasetDict = lm_datasets[\"train\"].train_test_split(\n",
    "    train_size=train_size, test_size=test_size, seed=RANDOM_STATE\n",
    ")\n",
    "downsampled_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "FP16 Mixed precision training with AMP or APEX (`--fp16`) and FP16 half precision evaluation (`--fp16_full_eval`) can only be used on CUDA or NPU devices.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/neidu/Desktop/Projects/Personal/My_Projects/NLP-Tutorial/notebook/06_Transformers/tasks/02_masked_language_modelling.ipynb Cell 53\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/neidu/Desktop/Projects/Personal/My_Projects/NLP-Tutorial/notebook/06_Transformers/tasks/02_masked_language_modelling.ipynb#Y143sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m learning_rate: \u001b[39mfloat\u001b[39m \u001b[39m=\u001b[39m \u001b[39m2e-5\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/neidu/Desktop/Projects/Personal/My_Projects/NLP-Tutorial/notebook/06_Transformers/tasks/02_masked_language_modelling.ipynb#Y143sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m weight_decay: \u001b[39mfloat\u001b[39m \u001b[39m=\u001b[39m \u001b[39m0.01\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/neidu/Desktop/Projects/Personal/My_Projects/NLP-Tutorial/notebook/06_Transformers/tasks/02_masked_language_modelling.ipynb#Y143sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m training_args \u001b[39m=\u001b[39m TrainingArguments(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/neidu/Desktop/Projects/Personal/My_Projects/NLP-Tutorial/notebook/06_Transformers/tasks/02_masked_language_modelling.ipynb#Y143sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     output_dir\u001b[39m=\u001b[39;49m\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00mmodel_name\u001b[39m}\u001b[39;49;00m\u001b[39m-finetuned-imdb\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/neidu/Desktop/Projects/Personal/My_Projects/NLP-Tutorial/notebook/06_Transformers/tasks/02_masked_language_modelling.ipynb#Y143sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     overwrite_output_dir\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/neidu/Desktop/Projects/Personal/My_Projects/NLP-Tutorial/notebook/06_Transformers/tasks/02_masked_language_modelling.ipynb#Y143sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     evaluation_strategy\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mepoch\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/neidu/Desktop/Projects/Personal/My_Projects/NLP-Tutorial/notebook/06_Transformers/tasks/02_masked_language_modelling.ipynb#Y143sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     learning_rate\u001b[39m=\u001b[39;49mlearning_rate,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/neidu/Desktop/Projects/Personal/My_Projects/NLP-Tutorial/notebook/06_Transformers/tasks/02_masked_language_modelling.ipynb#Y143sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/neidu/Desktop/Projects/Personal/My_Projects/NLP-Tutorial/notebook/06_Transformers/tasks/02_masked_language_modelling.ipynb#Y143sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     per_device_train_batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/neidu/Desktop/Projects/Personal/My_Projects/NLP-Tutorial/notebook/06_Transformers/tasks/02_masked_language_modelling.ipynb#Y143sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     per_device_eval_batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/neidu/Desktop/Projects/Personal/My_Projects/NLP-Tutorial/notebook/06_Transformers/tasks/02_masked_language_modelling.ipynb#Y143sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     push_to_hub\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/neidu/Desktop/Projects/Personal/My_Projects/NLP-Tutorial/notebook/06_Transformers/tasks/02_masked_language_modelling.ipynb#Y143sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     fp16\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/neidu/Desktop/Projects/Personal/My_Projects/NLP-Tutorial/notebook/06_Transformers/tasks/02_masked_language_modelling.ipynb#Y143sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     logging_steps\u001b[39m=\u001b[39;49mlogging_steps,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/neidu/Desktop/Projects/Personal/My_Projects/NLP-Tutorial/notebook/06_Transformers/tasks/02_masked_language_modelling.ipynb#Y143sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m )\n",
      "File \u001b[0;32m<string>:114\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, evaluation_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, sharded_ddp, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, gradient_checkpointing, include_inputs_for_metrics, fp16_backend, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, dispatch_batches)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp_p310/lib/python3.10/site-packages/transformers/training_args.py:1405\u001b[0m, in \u001b[0;36mTrainingArguments.__post_init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1395\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m--optim adamw_torch_fused with --fp16 requires PyTorch>2.0\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1397\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1398\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframework \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1399\u001b[0m     \u001b[39mand\u001b[39;00m is_torch_available()\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1403\u001b[0m     \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp16 \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp16_full_eval)\n\u001b[1;32m   1404\u001b[0m ):\n\u001b[0;32m-> 1405\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1406\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFP16 Mixed precision training with AMP or APEX (`--fp16`) and FP16 half precision evaluation\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1407\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m (`--fp16_full_eval`) can only be used on CUDA or NPU devices.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1408\u001b[0m     )\n\u001b[1;32m   1410\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1411\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframework \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1412\u001b[0m     \u001b[39mand\u001b[39;00m is_torch_available()\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1417\u001b[0m     \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbf16 \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbf16_full_eval)\n\u001b[1;32m   1418\u001b[0m ):\n\u001b[1;32m   1419\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1420\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mBF16 Mixed precision training with AMP (`--bf16`) and BF16 half precision evaluation\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1421\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m (`--bf16_full_eval`) can only be used on CUDA or CPU/TPU/NeuronCore devices.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1422\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: FP16 Mixed precision training with AMP or APEX (`--fp16`) and FP16 half precision evaluation (`--fp16_full_eval`) can only be used on CUDA or NPU devices."
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "\n",
    "batch_size: int = 64\n",
    "# Show the training loss with every epoch\n",
    "logging_steps: int = len(downsampled_dataset[\"train\"]) // batch_size\n",
    "model_name: str = model_checkpoint.split(\"/\")[-1]\n",
    "learning_rate: float = 2e-5\n",
    "weight_decay: float = 0.01\n",
    "\n",
    "training_args: TrainingArguments = TrainingArguments(\n",
    "    output_dir=f\"{model_name}-finetuned-imdb\",\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    push_to_hub=True,\n",
    "    fp16=True,\n",
    "    logging_steps=logging_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "\n",
    "trainer:Trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=downsampled_dataset[\"train\"],\n",
    "    eval_dataset=downsampled_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metric\n",
    "\n",
    "### Perplexity\n",
    "\n",
    "- We can calculate the `perplexity` of our pretrained model by using the `Trainer.evaluate()` function to compute the cross-entropy loss on the test set and then taking the exponential of the result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Code Block\n",
    "\n",
    "```python\n",
    "import math\n",
    "\n",
    "# Perplexity before training\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Perplexity after training\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning DistilBERT with 🤗 Accelerate\n",
    "\n",
    "- Fine-tuning a masked language model is very similar to the text classification example from Chapter 3. In fact, the only subtlety is the use of a special data collator, and we’ve already covered that earlier in this section!\n",
    "- However, we saw that `DataCollatorForLanguageModeling` also applies random masking with each evaluation, so we’ll see some fluctuations in our perplexity scores with each training run. \n",
    "- One way to eliminate this source of randomness is to apply the masking once on the whole test set, and then use the default data collator in 🤗 Transformers to collect the batches during evaluation. \n",
    "- To see how this works, let’s implement a simple function that applies the masking on a batch, similar to our first encounter with DataCollatorForLanguageModeling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_random_mask(batch) -> dict[str, Any]:\n",
    "    \"\"\"This is used to insert masks randomly to the test set. It uses the default data collator \n",
    "    in 🤗 Transformers to collect the batches during evaluation.\"\"\"\n",
    "\n",
    "    features: list[dict[Any, Any]] = [dict(zip(batch, t)) for t in zip(*batch.values())]\n",
    "    masked_inputs = data_collator(features)\n",
    "    # Create a new \"masked\" column for each column in the dataset\n",
    "    return {\"masked_\" + k: v.numpy() for k, v in masked_inputs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4c0a824152d4902aa6ed415d5608663",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "downsampled_dataset: DatasetDict = downsampled_dataset.remove_columns([\"word_ids\"])\n",
    "\n",
    "# Apply random masking\n",
    "eval_dataset: Dataset = downsampled_dataset[\"test\"].map(\n",
    "    insert_random_mask,\n",
    "    batched=True,\n",
    "    remove_columns=downsampled_dataset[\"test\"].column_names,\n",
    ")\n",
    "eval_dataset = eval_dataset.rename_columns(\n",
    "    {\n",
    "        \"masked_input_ids\": \"input_ids\",\n",
    "        \"masked_attention_mask\": \"attention_mask\",\n",
    "        \"masked_labels\": \"labels\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the dataloaders\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import default_data_collator\n",
    "\n",
    "\n",
    "batch_size: int = 64\n",
    "train_dataloader: DataLoader = DataLoader(\n",
    "    dataset=downsampled_dataset[\"train\"],\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "eval_dataloader: DataLoader = DataLoader(\n",
    "    dataset=eval_dataset, batch_size=batch_size, collate_fn=default_data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "from torch.optim import AdamW\n",
    "\n",
    "\n",
    "# Load a fresh version of the pretrained model\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)\n",
    "\n",
    "# Specify the learning rate\n",
    "learning_rate: float = 5e-5\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Prepare everything for training with the Accelerator object\n",
    "accelerator: Accelerator = Accelerator()\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "# Specify the learning rate scheduler\n",
    "num_train_epochs: int = 3\n",
    "num_update_steps_per_epoch: int = len(train_dataloader)\n",
    "num_training_steps: int = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chineidu/distilbert-base-uncased-finetuned-imdb-accelerate'"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import get_full_repo_name\n",
    "\n",
    "\n",
    "# Ceate a model repository on the Hugging Face Hub! Use the 🤗 Hub library to first\n",
    "# generate the full name of our repo\n",
    "model_name: str = \"distilbert-base-uncased-finetuned-imdb-accelerate\"\n",
    "repo_name: str = get_full_repo_name(model_name)\n",
    "repo_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import Repository\n",
    "\n",
    "\n",
    "# Create and clone the repository using the Repository class from 🤗 Hub:\n",
    "output_dir: str = model_name\n",
    "repo: Repository = Repository(output_dir, clone_from=repo_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FUll Training And Evaluation Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import math\n",
    "\n",
    "progress_bar:tqdm = tqdm(range(num_training_steps))\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "\n",
    "    # Training\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        # Forward prop\n",
    "        outputs: dict[str, Any] = model(**batch)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Backprop\n",
    "        accelerator.backward(loss)\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    losses: list[float] = []\n",
    "\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            # Forward prop\n",
    "            outputs: dict[str, Any] = model(**batch)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        losses.append(accelerator.gather(loss.repeat(batch_size)))\n",
    "\n",
    "    losses: torch.Tensor = torch.cat(losses)\n",
    "    losses = losses[: len(eval_dataset)]\n",
    "    \n",
    "    try:\n",
    "        perplexity = math.exp(torch.mean(losses))\n",
    "    except OverflowError:\n",
    "        perplexity = float(\"inf\")\n",
    "\n",
    "    print(f\">>> Epoch {epoch}: Perplexity: {perplexity}\")\n",
    "\n",
    "    # Save and upload\n",
    "    accelerator.wait_for_everyone()\n",
    "    unwrapped_model = accelerator.unwrap_model(model)\n",
    "    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
    "    if accelerator.is_main_process:\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "        repo.push_to_hub(\n",
    "            commit_message=f\"Training in progress epoch {epoch}\", blocking=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_p310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
