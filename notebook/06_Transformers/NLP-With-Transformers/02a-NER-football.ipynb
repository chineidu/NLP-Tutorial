{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The watermark extension is already loaded. To reload it, use:\n",
      "  %reload_ext watermark\n",
      "Python implementation: CPython\n",
      "Python version       : 3.11.8\n",
      "IPython version      : 8.22.2\n",
      "\n",
      "numpy       : 1.26.4\n",
      "pandas      : 2.2.1\n",
      "polars      : 0.20.18\n",
      "mlxtend     : 0.23.1\n",
      "transformers: 4.39.3\n",
      "\n",
      "conda environment: torch_p11\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -v -p numpy,pandas,polars,mlxtend,transformers --conda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lab_black extension is already loaded. To reload it, use:\n",
      "  %reload_ext lab_black\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Built-in library\n",
    "from pathlib import Path\n",
    "import re\n",
    "import json\n",
    "from typing import Any, Optional, Union\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "# Standard imports\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from rich.console import Console\n",
    "from rich.theme import Theme\n",
    "\n",
    "custom_theme = Theme(\n",
    "    {\n",
    "        \"info\": \"#76FF7B\",\n",
    "        \"warning\": \"#FBDDFE\",\n",
    "        \"error\": \"#FF0000\",\n",
    "    }\n",
    ")\n",
    "console = Console(theme=custom_theme)\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# NumPy settings\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "# Pandas settings\n",
    "pd.options.display.max_rows = 1_000\n",
    "pd.options.display.max_columns = 1_000\n",
    "pd.options.display.max_colwidth = 600\n",
    "\n",
    "# Polars settings\n",
    "pl.Config.set_fmt_str_lengths(1_000)\n",
    "pl.Config.set_tbl_cols(n=1_000)\n",
    "pl.Config.set_tbl_rows(n=200)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# Black code formatter (Optional)\n",
    "%load_ext lab_black\n",
    "\n",
    "# auto reload imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fp: str = \"../../../data/ner_data.jsonl\"\n",
    "\n",
    "# with open(fp, \"r\") as f:\n",
    "#     json_data = [json.loads(line) for line in f]\n",
    "\n",
    "# print(len(json_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['document_id', 'sentences'],\n",
       "        num_rows: 1940\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['document_id', 'sentences'],\n",
       "        num_rows: 222\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['document_id', 'sentences'],\n",
       "        num_rows: 222\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict, load_dataset, load_from_disk\n",
    "from datasets.features.features import ClassLabel, Sequence\n",
    "\n",
    "# data: DatasetDict = load_dataset(\"ontonotes/conll2012_ontonotesv5\", \"english_v4\")\n",
    "# data.save_to_disk(\"../../../data/conll2012_ontonotesv5\")\n",
    "\n",
    "fp: str = \"../../../data/conll2012_ontonotesv5\"\n",
    "ds_dict: DatasetDict = load_from_disk(dataset_path=fp)\n",
    "ds_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'part_id': Value(dtype='int32', id=None),\n",
       " 'words': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
       " 'pos_tags': Sequence(feature=ClassLabel(names=['XX', '``', '$', \"''\", ',', '-LRB-', '-RRB-', '.', ':', 'ADD', 'AFX', 'CC', 'CD', 'DT', 'EX', 'FW', 'HYPH', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NFP', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB'], id=None), length=-1, id=None),\n",
       " 'parse_tree': Value(dtype='string', id=None),\n",
       " 'predicate_lemmas': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
       " 'predicate_framenet_ids': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
       " 'word_senses': Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None),\n",
       " 'speaker': Value(dtype='string', id=None),\n",
       " 'named_entities': Sequence(feature=ClassLabel(names=['O', 'B-PERSON', 'I-PERSON', 'B-NORP', 'I-NORP', 'B-FAC', 'I-FAC', 'B-ORG', 'I-ORG', 'B-GPE', 'I-GPE', 'B-LOC', 'I-LOC', 'B-PRODUCT', 'I-PRODUCT', 'B-DATE', 'I-DATE', 'B-TIME', 'I-TIME', 'B-PERCENT', 'I-PERCENT', 'B-MONEY', 'I-MONEY', 'B-QUANTITY', 'I-QUANTITY', 'B-ORDINAL', 'I-ORDINAL', 'B-CARDINAL', 'I-CARDINAL', 'B-EVENT', 'I-EVENT', 'B-WORK_OF_ART', 'I-WORK_OF_ART', 'B-LAW', 'I-LAW', 'B-LANGUAGE', 'I-LANGUAGE'], id=None), length=-1, id=None),\n",
       " 'srl_frames': [{'verb': Value(dtype='string', id=None),\n",
       "   'frames': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}],\n",
       " 'coref_spans': Sequence(feature=Sequence(feature=Value(dtype='int32', id=None), length=3, id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_dict[\"train\"].features[\"sentences\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClassLabel(names=['O', 'B-PERSON', 'I-PERSON', 'B-NORP', 'I-NORP', 'B-FAC', 'I-FAC', 'B-ORG', 'I-ORG', 'B-GPE', 'I-GPE', 'B-LOC', 'I-LOC', 'B-PRODUCT', 'I-PRODUCT', 'B-DATE', 'I-DATE', 'B-TIME', 'I-TIME', 'B-PERCENT', 'I-PERCENT', 'B-MONEY', 'I-MONEY', 'B-QUANTITY', 'I-QUANTITY', 'B-ORDINAL', 'I-ORDINAL', 'B-CARDINAL', 'I-CARDINAL', 'B-EVENT', 'I-EVENT', 'B-WORK_OF_ART', 'I-WORK_OF_ART', 'B-LAW', 'I-LAW', 'B-LANGUAGE', 'I-LANGUAGE'], id=None)\n"
     ]
    }
   ],
   "source": [
    "tags: ClassLabel = ds_dict[\"train\"].features[\"sentences\"][0][\"named_entities\"].feature\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'O', 1: 'B-PERSON', 2: 'I-PERSON', 3: 'B-NORP', 4: 'I-NORP', 5: 'B-FAC', 6: 'I-FAC', 7: 'B-ORG', 8: 'I-ORG', 9: 'B-GPE', 10: 'I-GPE', 11: 'B-LOC', 12: 'I-LOC', 13: 'B-PRODUCT', 14: 'I-PRODUCT', 15: 'B-DATE', 16: 'I-DATE', 17: 'B-TIME', 18: 'I-TIME', 19: 'B-PERCENT', 20: 'I-PERCENT', 21: 'B-MONEY', 22: 'I-MONEY', 23: 'B-QUANTITY', 24: 'I-QUANTITY', 25: 'B-ORDINAL', 26: 'I-ORDINAL', 27: 'B-CARDINAL', 28: 'I-CARDINAL', 29: 'B-EVENT', 30: 'I-EVENT', 31: 'B-WORK_OF_ART', 32: 'I-WORK_OF_ART', 33: 'B-LAW', 34: 'I-LAW', 35: 'B-LANGUAGE', 36: 'I-LANGUAGE'}\n"
     ]
    }
   ],
   "source": [
    "index2tag: dict[int, str] = {tags.str2int(tag): tag for tag in tags.names}\n",
    "tag2index: dict[str, int] = {tag: tags.str2int(tag) for tag in tags.names}\n",
    "\n",
    "print(index2tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment\n",
    "\n",
    "- Drop irrelevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tokens_and_labels(example: dict[str, Any]) -> dict[str, Any]:\n",
    "    tokens = example[\"sentences\"][0][\"words\"]\n",
    "    labels = example[\"sentences\"][0][\"named_entities\"]\n",
    "\n",
    "    return {\"tokens\": tokens, \"labels\": labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': Dataset({\n",
       "     features: ['document_id', 'tokens', 'labels', 'ner_labels'],\n",
       "     num_rows: 1940\n",
       " }),\n",
       " 'validation': Dataset({\n",
       "     features: ['document_id', 'tokens', 'labels', 'ner_labels'],\n",
       "     num_rows: 222\n",
       " }),\n",
       " 'test': Dataset({\n",
       "     features: ['document_id', 'tokens', 'labels', 'ner_labels'],\n",
       "     num_rows: 222\n",
       " })}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ClassLabel object\n",
    "class_label = ClassLabel(num_classes=len(tags.names), names=tags.names)\n",
    "\n",
    "# Create a copy\n",
    "ds_dict: DatasetDict = ds_dict.copy()\n",
    "\n",
    "for split in ds_dict.keys():\n",
    "    ds_dict[split] = ds_dict[split].map(extract_tokens_and_labels)\n",
    "    ds_dict[split] = ds_dict[split].remove_columns([\"sentences\"])\n",
    "    # Update the 'labels' column to use ClassLabel\n",
    "    ds_dict[split] = ds_dict[split].cast_column(\n",
    "        column=\"labels\", feature=Sequence(class_label)\n",
    "    )\n",
    "    ds_dict[split] = ds_dict[split].map(\n",
    "        lambda x: {\"ner_labels\": [index2tag[t] for t in x[\"labels\"]]}\n",
    "    )\n",
    "\n",
    "ds_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'document_id': 'bc/cnn/00/cnn_0007',\n",
       " 'tokens': ['Journalists', 'sources', 'and', 'jail', '/.'],\n",
       " 'labels': [0, 0, 0, 0, 0],\n",
       " 'ner_labels': ['O', 'O', 'O', 'O', 'O']}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_dict[\"train\"][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 5)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>document_id</th><th>tokens</th><th>labels</th><th>ner_labels</th><th>token_length</th></tr><tr><td>str</td><td>list[str]</td><td>list[i64]</td><td>list[str]</td><td>i64</td></tr></thead><tbody><tr><td>&quot;bc/cctv/00/cctv_0001&quot;</td><td>[&quot;What&quot;, &quot;kind&quot;, … &quot;?&quot;]</td><td>[0, 0, … 0]</td><td>[&quot;O&quot;, &quot;O&quot;, … &quot;O&quot;]</td><td>5</td></tr><tr><td>&quot;bc/cctv/00/cctv_0002&quot;</td><td>[&quot;Abramov&quot;, &quot;had&quot;, … &quot;.&quot;]</td><td>[1, 0, … 0]</td><td>[&quot;B-PERSON&quot;, &quot;O&quot;, … &quot;O&quot;]</td><td>14</td></tr><tr><td>&quot;bc/cctv/00/cctv_0003&quot;</td><td>[&quot;Hello&quot;, &quot;,&quot;, … &quot;.&quot;]</td><td>[0, 0, … 0]</td><td>[&quot;O&quot;, &quot;O&quot;, … &quot;O&quot;]</td><td>5</td></tr><tr><td>&quot;bc/cctv/00/cctv_0004&quot;</td><td>[&quot;There&quot;, &quot;will&quot;, … &quot;.&quot;]</td><td>[0, 0, … 0]</td><td>[&quot;O&quot;, &quot;O&quot;, … &quot;O&quot;]</td><td>13</td></tr><tr><td>&quot;bc/cnn/00/cnn_0001&quot;</td><td>[&quot;Sunday&quot;, &quot;the&quot;, … &quot;/.&quot;]</td><td>[15, 0, … 0]</td><td>[&quot;B-DATE&quot;, &quot;O&quot;, … &quot;O&quot;]</td><td>7</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 5)\n",
       "┌──────────────────────┬──────────────────────┬──────────────┬──────────────────────┬──────────────┐\n",
       "│ document_id          ┆ tokens               ┆ labels       ┆ ner_labels           ┆ token_length │\n",
       "│ ---                  ┆ ---                  ┆ ---          ┆ ---                  ┆ ---          │\n",
       "│ str                  ┆ list[str]            ┆ list[i64]    ┆ list[str]            ┆ i64          │\n",
       "╞══════════════════════╪══════════════════════╪══════════════╪══════════════════════╪══════════════╡\n",
       "│ bc/cctv/00/cctv_0001 ┆ [\"What\", \"kind\", …   ┆ [0, 0, … 0]  ┆ [\"O\", \"O\", … \"O\"]    ┆ 5            │\n",
       "│                      ┆ \"?\"]                 ┆              ┆                      ┆              │\n",
       "│ bc/cctv/00/cctv_0002 ┆ [\"Abramov\", \"had\", … ┆ [1, 0, … 0]  ┆ [\"B-PERSON\", \"O\", …  ┆ 14           │\n",
       "│                      ┆ \".\"]                 ┆              ┆ \"O\"]                 ┆              │\n",
       "│ bc/cctv/00/cctv_0003 ┆ [\"Hello\", \",\", …     ┆ [0, 0, … 0]  ┆ [\"O\", \"O\", … \"O\"]    ┆ 5            │\n",
       "│                      ┆ \".\"]                 ┆              ┆                      ┆              │\n",
       "│ bc/cctv/00/cctv_0004 ┆ [\"There\", \"will\", …  ┆ [0, 0, … 0]  ┆ [\"O\", \"O\", … \"O\"]    ┆ 13           │\n",
       "│                      ┆ \".\"]                 ┆              ┆                      ┆              │\n",
       "│ bc/cnn/00/cnn_0001   ┆ [\"Sunday\", \"the\", …  ┆ [15, 0, … 0] ┆ [\"B-DATE\", \"O\", …    ┆ 7            │\n",
       "│                      ┆ \"/.\"]                ┆              ┆ \"O\"]                 ┆              │\n",
       "└──────────────────────┴──────────────────────┴──────────────┴──────────────────────┴──────────────┘"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df: pl.DataFrame = ds_dict[\"train\"].to_polars()\n",
    "df = df.with_columns(token_length=pl.col(\"tokens\").map_elements(lambda x: len(x)))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train</th>\n",
       "      <th>validation</th>\n",
       "      <th>test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'PERSON': 774, 'GPE': 1069, 'TIME': 88, 'CARDINAL': 243, 'DATE': 842, 'LOC': 109, 'ORG': 1005, 'NORP': 432, 'PRODUCT': 77, 'WORK_OF_ART': 56, 'ORDINAL': 44, 'FAC': 47, 'MONEY': 119, 'EVENT': 26, 'PERCENT': 47, 'QUANTITY': 11, 'LAW': 13, 'LANGUAGE': 6}</td>\n",
       "      <td>{'DATE': 105, 'GPE': 142, 'WORK_OF_ART': 4, 'PERSON': 100, 'ORDINAL': 12, 'ORG': 125, 'PRODUCT': 9, 'CARDINAL': 38, 'FAC': 4, 'TIME': 10, 'NORP': 46, 'EVENT': 6, 'LOC': 4, 'QUANTITY': 1, 'MONEY': 16, 'PERCENT': 7}</td>\n",
       "      <td>{'GPE': 128, 'DATE': 103, 'PERSON': 81, 'LOC': 12, 'TIME': 13, 'NORP': 47, 'MONEY': 8, 'ORG': 118, 'EVENT': 5, 'CARDINAL': 33, 'WORK_OF_ART': 9, 'PRODUCT': 9, 'ORDINAL': 9, 'FAC': 2, 'QUANTITY': 2, 'PERCENT': 6, 'LANGUAGE': 1}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                          train  \\\n",
       "0  {'PERSON': 774, 'GPE': 1069, 'TIME': 88, 'CARDINAL': 243, 'DATE': 842, 'LOC': 109, 'ORG': 1005, 'NORP': 432, 'PRODUCT': 77, 'WORK_OF_ART': 56, 'ORDINAL': 44, 'FAC': 47, 'MONEY': 119, 'EVENT': 26, 'PERCENT': 47, 'QUANTITY': 11, 'LAW': 13, 'LANGUAGE': 6}   \n",
       "\n",
       "                                                                                                                                                                                                              validation  \\\n",
       "0  {'DATE': 105, 'GPE': 142, 'WORK_OF_ART': 4, 'PERSON': 100, 'ORDINAL': 12, 'ORG': 125, 'PRODUCT': 9, 'CARDINAL': 38, 'FAC': 4, 'TIME': 10, 'NORP': 46, 'EVENT': 6, 'LOC': 4, 'QUANTITY': 1, 'MONEY': 16, 'PERCENT': 7}   \n",
       "\n",
       "                                                                                                                                                                                                                                 test  \n",
       "0  {'GPE': 128, 'DATE': 103, 'PERSON': 81, 'LOC': 12, 'TIME': 13, 'NORP': 47, 'MONEY': 8, 'ORG': 118, 'EVENT': 5, 'CARDINAL': 33, 'WORK_OF_ART': 9, 'PRODUCT': 9, 'ORDINAL': 9, 'FAC': 2, 'QUANTITY': 2, 'PERCENT': 6, 'LANGUAGE': 1}  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the distribution of all the tags\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "\n",
    "splits_freq: defaultdict = defaultdict(Counter)\n",
    "\n",
    "for split, ds in ds_dict.items():\n",
    "    for row in ds[\"ner_labels\"]:\n",
    "        for tag in row:\n",
    "            # Focus on the `beginning` tags\n",
    "            if tag.startswith(\"B\"):\n",
    "                tag_type: str = tag.split(\"-\")[1]\n",
    "                splits_freq[split][tag_type] += 1\n",
    "\n",
    "\n",
    "# The tags are roughly equally distributed\n",
    "pl.DataFrame(splits_freq).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "chkpoint: str = \"dslim/distilbert-NER\"  # \"dslim/bert-base-NER\"\n",
    "\n",
    "tokenizer: AutoTokenizer = AutoTokenizer.from_pretrained(chkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'Note',\n",
       " ':',\n",
       " 'There',\n",
       " \"'\",\n",
       " 's',\n",
       " 'a',\n",
       " 'piece',\n",
       " 'by',\n",
       " 'George',\n",
       " 'Pack',\n",
       " '##er',\n",
       " 'in',\n",
       " 'The',\n",
       " 'New',\n",
       " 'Yorker',\n",
       " '-',\n",
       " 'L',\n",
       " '##RB',\n",
       " '-',\n",
       " \"'\",\n",
       " \"'\",\n",
       " 'Bet',\n",
       " '##ray',\n",
       " '##ed',\n",
       " ':',\n",
       " 'the',\n",
       " 'Iraqi',\n",
       " '##s',\n",
       " 'who',\n",
       " 'trusted',\n",
       " 'America',\n",
       " 'the',\n",
       " 'most',\n",
       " \"'\",\n",
       " \"'\",\n",
       " '-',\n",
       " 'R',\n",
       " '##RB',\n",
       " '-',\n",
       " 'about',\n",
       " 'all',\n",
       " 'this',\n",
       " ',',\n",
       " 'but',\n",
       " 'it',\n",
       " \"'\",\n",
       " 's',\n",
       " 'way',\n",
       " 'too',\n",
       " 'long',\n",
       " 'to',\n",
       " 'blog',\n",
       " 'here',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text: str = \" \".join(ds_dict[\"train\"][\"tokens\"][-3])\n",
    "inp_labels: list[int] = ds_dict[\"train\"][\"labels\"][-3]\n",
    "tokens: list[str] = tokenizer(text).tokens()  # tokenizer.tokenize(text)\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 56, 768])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "\n",
    "bert_model = AutoModel.from_pretrained(chkpoint)\n",
    "bert_model(**inputs)[\"last_hidden_state\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization Pipeline\n",
    "\n",
    "<br>\n",
    "\n",
    "[![image.png](https://i.postimg.cc/zvJ0SrZz/image.png)](https://postimg.cc/dkx2wgVp)\n",
    "\n",
    "<br>\n",
    "\n",
    "- `Normalization`: it involves cleaning raw text by removing whitespace, accents, and standardizing Unicode characters. It also includes lowercasing to reduce vocabulary size. After normalization, our example string becomes \"jack sparrow loves new york!\".\n",
    "\n",
    "- `Pretokenization`: it splits text into words for easier tokenization. For English and similar languages, this is simple. For languages like Chinese, it's more complex and might require language-specific libraries.\n",
    "\n",
    "- `Tokenizer model`: it splits words into subwords to reduce vocabulary size and out-of-vocabulary tokens. This is done using algorithms like BPE, Unigram, and WordPiece. For example, \"jack sparrow\" might become \"[jack, spa, rrow]\".\n",
    "\n",
    "- `Postprocessing`: it's the final step in tokenization, where additional tokens (like [CLS] and [SEP]) are added to the beginning and end of the token sequence to prepare it for input into a model like BERT.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition\n",
    "\n",
    "[![image.png](https://i.postimg.cc/26YwrD5Q/image.png)](https://postimg.cc/rdhWN7xs)\n",
    "\n",
    "<br>\n",
    "\n",
    "- In `token classification`, assign the label (e.g., B-PER) to the first subword (\"Chr\") and ignore subsequent subwords (\"##ista\"). This convention follows the BERT paper and maintains the IOB2 format. Postprocessing can propagate the label to all subwords.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Classification Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from transformers import BertConfig, BertModel\n",
    "from transformers.models.bert.modeling_bert import BertPreTrainedModel\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "\n",
    "\n",
    "class BertForTokenClassification(BertPreTrainedModel):\n",
    "    class_config = BertConfig\n",
    "\n",
    "    def __init__(self, config) -> None:\n",
    "        super().__init__(config)\n",
    "\n",
    "        # Load model body\n",
    "        self.model = BertModel(config)\n",
    "\n",
    "        # Setup classifiaction head\n",
    "        self.num_labels = config.num_labels\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "        # Load pretrained weights\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Tensor | None = None,\n",
    "        attention_mask: Tensor | None = None,\n",
    "        token_type_ids: Tensor | None = None,\n",
    "        labels: Tensor | None = None,\n",
    "        **kwargs\n",
    "    ) -> TokenClassifierOutput:\n",
    "        # Get the encoder representations using the body\n",
    "        outputs: dict = self.model(input_ids, attention_mask, token_type_ids, **kwargs)\n",
    "\n",
    "        # Apply classifier to encoder representation\n",
    "        sequence_output: Tensor = self.dropout(outputs[\"last_hidden_state\"])\n",
    "        logits: Tensor = self.classifier(sequence_output)\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss: Tensor | None = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        return TokenClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassLabel(names=['O', 'B-PERSON', 'I-PERSON', 'B-NORP', 'I-NORP', 'B-FAC', 'I-FAC', 'B-ORG', 'I-ORG', 'B-GPE', 'I-GPE', 'B-LOC', 'I-LOC', 'B-PRODUCT', 'I-PRODUCT', 'B-DATE', 'I-DATE', 'B-TIME', 'I-TIME', 'B-PERCENT', 'I-PERCENT', 'B-MONEY', 'I-MONEY', 'B-QUANTITY', 'I-QUANTITY', 'B-ORDINAL', 'I-ORDINAL', 'B-CARDINAL', 'I-CARDINAL', 'B-EVENT', 'I-EVENT', 'B-WORK_OF_ART', 'I-WORK_OF_ART', 'B-LAW', 'I-LAW', 'B-LANGUAGE', 'I-LANGUAGE'], id=None)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"_name_or_path\": \"distilbert-base-cased\",\n",
       "  \"activation\": \"gelu\",\n",
       "  \"architectures\": [\n",
       "    \"DistilBertForTokenClassification\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.1,\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"dim\": 768,\n",
       "  \"dropout\": 0.1,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dim\": 3072,\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"O\",\n",
       "    \"1\": \"B-PERSON\",\n",
       "    \"2\": \"I-PERSON\",\n",
       "    \"3\": \"B-NORP\",\n",
       "    \"4\": \"I-NORP\",\n",
       "    \"5\": \"B-FAC\",\n",
       "    \"6\": \"I-FAC\",\n",
       "    \"7\": \"B-ORG\",\n",
       "    \"8\": \"I-ORG\",\n",
       "    \"9\": \"B-GPE\",\n",
       "    \"10\": \"I-GPE\",\n",
       "    \"11\": \"B-LOC\",\n",
       "    \"12\": \"I-LOC\",\n",
       "    \"13\": \"B-PRODUCT\",\n",
       "    \"14\": \"I-PRODUCT\",\n",
       "    \"15\": \"B-DATE\",\n",
       "    \"16\": \"I-DATE\",\n",
       "    \"17\": \"B-TIME\",\n",
       "    \"18\": \"I-TIME\",\n",
       "    \"19\": \"B-PERCENT\",\n",
       "    \"20\": \"I-PERCENT\",\n",
       "    \"21\": \"B-MONEY\",\n",
       "    \"22\": \"I-MONEY\",\n",
       "    \"23\": \"B-QUANTITY\",\n",
       "    \"24\": \"I-QUANTITY\",\n",
       "    \"25\": \"B-ORDINAL\",\n",
       "    \"26\": \"I-ORDINAL\",\n",
       "    \"27\": \"B-CARDINAL\",\n",
       "    \"28\": \"I-CARDINAL\",\n",
       "    \"29\": \"B-EVENT\",\n",
       "    \"30\": \"I-EVENT\",\n",
       "    \"31\": \"B-WORK_OF_ART\",\n",
       "    \"32\": \"I-WORK_OF_ART\",\n",
       "    \"33\": \"B-LAW\",\n",
       "    \"34\": \"I-LAW\",\n",
       "    \"35\": \"B-LANGUAGE\",\n",
       "    \"36\": \"I-LANGUAGE\"\n",
       "  },\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"label2id\": {\n",
       "    \"B-CARDINAL\": 27,\n",
       "    \"B-DATE\": 15,\n",
       "    \"B-EVENT\": 29,\n",
       "    \"B-FAC\": 5,\n",
       "    \"B-GPE\": 9,\n",
       "    \"B-LANGUAGE\": 35,\n",
       "    \"B-LAW\": 33,\n",
       "    \"B-LOC\": 11,\n",
       "    \"B-MONEY\": 21,\n",
       "    \"B-NORP\": 3,\n",
       "    \"B-ORDINAL\": 25,\n",
       "    \"B-ORG\": 7,\n",
       "    \"B-PERCENT\": 19,\n",
       "    \"B-PERSON\": 1,\n",
       "    \"B-PRODUCT\": 13,\n",
       "    \"B-QUANTITY\": 23,\n",
       "    \"B-TIME\": 17,\n",
       "    \"B-WORK_OF_ART\": 31,\n",
       "    \"I-CARDINAL\": 28,\n",
       "    \"I-DATE\": 16,\n",
       "    \"I-EVENT\": 30,\n",
       "    \"I-FAC\": 6,\n",
       "    \"I-GPE\": 10,\n",
       "    \"I-LANGUAGE\": 36,\n",
       "    \"I-LAW\": 34,\n",
       "    \"I-LOC\": 12,\n",
       "    \"I-MONEY\": 22,\n",
       "    \"I-NORP\": 4,\n",
       "    \"I-ORDINAL\": 26,\n",
       "    \"I-ORG\": 8,\n",
       "    \"I-PERCENT\": 20,\n",
       "    \"I-PERSON\": 2,\n",
       "    \"I-PRODUCT\": 14,\n",
       "    \"I-QUANTITY\": 24,\n",
       "    \"I-TIME\": 18,\n",
       "    \"I-WORK_OF_ART\": 32,\n",
       "    \"O\": 0\n",
       "  },\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"n_heads\": 12,\n",
       "  \"n_layers\": 6,\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"output_past\": true,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"qa_dropout\": 0.1,\n",
       "  \"seq_classif_dropout\": 0.2,\n",
       "  \"sinusoidal_pos_embds\": false,\n",
       "  \"tie_weights_\": true,\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.39.3\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 28996\n",
       "}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = BertConfig.from_pretrained(\n",
    "    chkpoint, num_labels=len(tags.names), id2label=index2tag, label2id=tag2index\n",
    ")\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'[CLS]': 101, 'Note': 5322, ':': 131, 'There': 1247, \"'\": 112, 's': 188, 'a': 170, 'piece': 2727, 'by': 1118, 'George': 1667, 'Pack': 14667, '##er': 1200, 'in': 1107, 'The': 1109, 'New': 1203, 'Yorker': 20998, '-': 118, 'L': 149, '##RB': 22672, 'Bet': 26615, '##ray': 6447, '##ed': 1174, 'the': 1103, 'Iraqi': 8612, '##s': 1116, 'who': 1150, 'trusted': 9373, 'America': 1738, 'most': 1211, 'R': 155, 'about': 1164, 'all': 1155, 'this': 1142, ',': 117, 'but': 1133, 'it': 1122, 'way': 1236, 'too': 1315, 'long': 1263, 'to': 1106, 'blog': 10679, 'here': 1303, '.': 119, '[SEP]': 102}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1, 44)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>[CLS]</th><th>Note</th><th>:</th><th>There</th><th>&#x27;</th><th>s</th><th>a</th><th>piece</th><th>by</th><th>George</th><th>Pack</th><th>##er</th><th>in</th><th>The</th><th>New</th><th>Yorker</th><th>-</th><th>L</th><th>##RB</th><th>Bet</th><th>##ray</th><th>##ed</th><th>the</th><th>Iraqi</th><th>##s</th><th>who</th><th>trusted</th><th>America</th><th>most</th><th>R</th><th>about</th><th>all</th><th>this</th><th>,</th><th>but</th><th>it</th><th>way</th><th>too</th><th>long</th><th>to</th><th>blog</th><th>here</th><th>.</th><th>[SEP]</th></tr><tr><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td></tr></thead><tbody><tr><td>101</td><td>5322</td><td>131</td><td>1247</td><td>112</td><td>188</td><td>170</td><td>2727</td><td>1118</td><td>1667</td><td>14667</td><td>1200</td><td>1107</td><td>1109</td><td>1203</td><td>20998</td><td>118</td><td>149</td><td>22672</td><td>26615</td><td>6447</td><td>1174</td><td>1103</td><td>8612</td><td>1116</td><td>1150</td><td>9373</td><td>1738</td><td>1211</td><td>155</td><td>1164</td><td>1155</td><td>1142</td><td>117</td><td>1133</td><td>1122</td><td>1236</td><td>1315</td><td>1263</td><td>1106</td><td>10679</td><td>1303</td><td>119</td><td>102</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1, 44)\n",
       "┌─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┐\n",
       "│ [CL ┆ Not ┆ :   ┆ The ┆ '   ┆ s   ┆ a   ┆ pie ┆ by  ┆ Geo ┆ Pac ┆ ##e ┆ in  ┆ The ┆ New ┆ Yor ┆ -   ┆ L   ┆ ##R ┆ Bet ┆ ##r ┆ ##e ┆ the ┆ Ira ┆ ##s ┆ who ┆ tru ┆ Ame ┆ mos ┆ R   ┆ abo ┆ all ┆ thi ┆ ,   ┆ but ┆ it  ┆ way ┆ too ┆ lon ┆ to  ┆ blo ┆ her ┆ .   ┆ [SE │\n",
       "│ S]  ┆ e   ┆ --- ┆ re  ┆ --- ┆ --- ┆ --- ┆ ce  ┆ --- ┆ rge ┆ k   ┆ r   ┆ --- ┆ --- ┆ --- ┆ ker ┆ --- ┆ --- ┆ B   ┆ --- ┆ ay  ┆ d   ┆ --- ┆ qi  ┆ --- ┆ --- ┆ ste ┆ ric ┆ t   ┆ --- ┆ ut  ┆ --- ┆ s   ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ g   ┆ --- ┆ g   ┆ e   ┆ --- ┆ P]  │\n",
       "│ --- ┆ --- ┆ i64 ┆ --- ┆ i64 ┆ i64 ┆ i64 ┆ --- ┆ i64 ┆ --- ┆ --- ┆ --- ┆ i64 ┆ i64 ┆ i64 ┆ --- ┆ i64 ┆ i64 ┆ --- ┆ i64 ┆ --- ┆ --- ┆ i64 ┆ --- ┆ i64 ┆ i64 ┆ d   ┆ a   ┆ --- ┆ i64 ┆ --- ┆ i64 ┆ --- ┆ i64 ┆ i64 ┆ i64 ┆ i64 ┆ i64 ┆ --- ┆ i64 ┆ --- ┆ --- ┆ i64 ┆ --- │\n",
       "│ i64 ┆ i64 ┆     ┆ i64 ┆     ┆     ┆     ┆ i64 ┆     ┆ i64 ┆ i64 ┆ i64 ┆     ┆     ┆     ┆ i64 ┆     ┆     ┆ i64 ┆     ┆ i64 ┆ i64 ┆     ┆ i64 ┆     ┆     ┆ --- ┆ --- ┆ i64 ┆     ┆ i64 ┆     ┆ i64 ┆     ┆     ┆     ┆     ┆     ┆ i64 ┆     ┆ i64 ┆ i64 ┆     ┆ i64 │\n",
       "│     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ i64 ┆ i64 ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     │\n",
       "╞═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╡\n",
       "│ 101 ┆ 532 ┆ 131 ┆ 124 ┆ 112 ┆ 188 ┆ 170 ┆ 272 ┆ 111 ┆ 166 ┆ 146 ┆ 120 ┆ 110 ┆ 110 ┆ 120 ┆ 209 ┆ 118 ┆ 149 ┆ 226 ┆ 266 ┆ 644 ┆ 117 ┆ 110 ┆ 861 ┆ 111 ┆ 115 ┆ 937 ┆ 173 ┆ 121 ┆ 155 ┆ 116 ┆ 115 ┆ 114 ┆ 117 ┆ 113 ┆ 112 ┆ 123 ┆ 131 ┆ 126 ┆ 110 ┆ 106 ┆ 130 ┆ 119 ┆ 102 │\n",
       "│     ┆ 2   ┆     ┆ 7   ┆     ┆     ┆     ┆ 7   ┆ 8   ┆ 7   ┆ 67  ┆ 0   ┆ 7   ┆ 9   ┆ 3   ┆ 98  ┆     ┆     ┆ 72  ┆ 15  ┆ 7   ┆ 4   ┆ 3   ┆ 2   ┆ 6   ┆ 0   ┆ 3   ┆ 8   ┆ 1   ┆     ┆ 4   ┆ 5   ┆ 2   ┆     ┆ 3   ┆ 2   ┆ 6   ┆ 5   ┆ 3   ┆ 6   ┆ 79  ┆ 3   ┆     ┆     │\n",
       "└─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┘"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = BertForTokenClassification(config=config).to(device)\n",
    "\n",
    "\n",
    "# Check that the tokenizer and model were properly initialized\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "data: dict[str, int] = {\n",
    "    col: int(val) for col, val in zip(tokens, inputs.input_ids.flatten())\n",
    "}\n",
    "print(data)\n",
    "\n",
    "pl.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  5322,   131,  1247,   112,   188,   170,  2727,  1118,  1667,\n",
       "          14667,  1200,  1107,  1109,  1203, 20998,   118,   149, 22672,   118,\n",
       "            112,   112, 26615,  6447,  1174,   131,  1103,  8612,  1116,  1150,\n",
       "           9373,  1738,  1103,  1211,   112,   112,   118,   155, 22672,   118,\n",
       "           1164,  1155,  1142,   117,  1133,  1122,   112,   188,  1236,  1315,\n",
       "           1263,  1106, 10679,  1303,   119,   102]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_device: dict[str, Tensor] = {\n",
    "    k: torch.tensor(v).to(device) for k, v in inputs.items()\n",
    "}\n",
    "inputs_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  5322,   131,  1247,   112,   188,   170,  2727,  1118,  1667,\n",
       "          14667,  1200,  1107,  1109,  1203, 20998,   118,   149, 22672,   118,\n",
       "            112,   112, 26615,  6447,  1174,   131,  1103,  8612,  1116,  1150,\n",
       "           9373,  1738,  1103,  1211,   112,   112,   118,   155, 22672,   118,\n",
       "           1164,  1155,  1142,   117,  1133,  1122,   112,   188,  1236,  1315,\n",
       "           1263,  1106, 10679,  1303,   119,   102]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in sequence: 56\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1, 44)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>[CLS]</th><th>Note</th><th>:</th><th>There</th><th>&#x27;</th><th>s</th><th>a</th><th>piece</th><th>by</th><th>George</th><th>Pack</th><th>##er</th><th>in</th><th>The</th><th>New</th><th>Yorker</th><th>-</th><th>L</th><th>##RB</th><th>Bet</th><th>##ray</th><th>##ed</th><th>the</th><th>Iraqi</th><th>##s</th><th>who</th><th>trusted</th><th>America</th><th>most</th><th>R</th><th>about</th><th>all</th><th>this</th><th>,</th><th>but</th><th>it</th><th>way</th><th>too</th><th>long</th><th>to</th><th>blog</th><th>here</th><th>.</th><th>[SEP]</th></tr><tr><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;I-LOC&quot;</td><td>&quot;I-LOC&quot;</td><td>&quot;I-MONEY&quot;</td><td>&quot;B-MONEY&quot;</td><td>&quot;I-MONEY&quot;</td><td>&quot;B-GPE&quot;</td><td>&quot;B-GPE&quot;</td><td>&quot;I-ORDINAL&quot;</td><td>&quot;I-MONEY&quot;</td><td>&quot;B-PRODUCT&quot;</td><td>&quot;B-GPE&quot;</td><td>&quot;I-MONEY&quot;</td><td>&quot;I-FAC&quot;</td><td>&quot;I-LOC&quot;</td><td>&quot;I-MONEY&quot;</td><td>&quot;I-MONEY&quot;</td><td>&quot;B-GPE&quot;</td><td>&quot;B-LANGUAGE&quot;</td><td>&quot;B-GPE&quot;</td><td>&quot;I-MONEY&quot;</td><td>&quot;I-MONEY&quot;</td><td>&quot;I-MONEY&quot;</td><td>&quot;B-LOC&quot;</td><td>&quot;B-PRODUCT&quot;</td><td>&quot;I-ORG&quot;</td><td>&quot;I-GPE&quot;</td><td>&quot;I-MONEY&quot;</td><td>&quot;I-ORDINAL&quot;</td><td>&quot;I-PERCENT&quot;</td><td>&quot;B-PRODUCT&quot;</td><td>&quot;B-MONEY&quot;</td><td>&quot;B-LANGUAGE&quot;</td><td>&quot;B-MONEY&quot;</td><td>&quot;I-LOC&quot;</td><td>&quot;B-MONEY&quot;</td><td>&quot;B-PERCENT&quot;</td><td>&quot;I-ORG&quot;</td><td>&quot;B-MONEY&quot;</td><td>&quot;B-LANGUAGE&quot;</td><td>&quot;I-MONEY&quot;</td><td>&quot;B-MONEY&quot;</td><td>&quot;B-LANGUAGE&quot;</td><td>&quot;I-LOC&quot;</td><td>&quot;B-PERCENT&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1, 44)\n",
       "┌─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┐\n",
       "│ [CL ┆ Not ┆ :   ┆ The ┆ '   ┆ s   ┆ a   ┆ pie ┆ by  ┆ Geo ┆ Pac ┆ ##e ┆ in  ┆ The ┆ New ┆ Yor ┆ -   ┆ L   ┆ ##R ┆ Bet ┆ ##r ┆ ##e ┆ the ┆ Ira ┆ ##s ┆ who ┆ tru ┆ Ame ┆ mos ┆ R   ┆ abo ┆ all ┆ thi ┆ ,   ┆ but ┆ it  ┆ way ┆ too ┆ lon ┆ to  ┆ blo ┆ her ┆ .   ┆ [SE │\n",
       "│ S]  ┆ e   ┆ --- ┆ re  ┆ --- ┆ --- ┆ --- ┆ ce  ┆ --- ┆ rge ┆ k   ┆ r   ┆ --- ┆ --- ┆ --- ┆ ker ┆ --- ┆ --- ┆ B   ┆ --- ┆ ay  ┆ d   ┆ --- ┆ qi  ┆ --- ┆ --- ┆ ste ┆ ric ┆ t   ┆ --- ┆ ut  ┆ --- ┆ s   ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ g   ┆ --- ┆ g   ┆ e   ┆ --- ┆ P]  │\n",
       "│ --- ┆ --- ┆ str ┆ --- ┆ str ┆ str ┆ str ┆ --- ┆ str ┆ --- ┆ --- ┆ --- ┆ str ┆ str ┆ str ┆ --- ┆ str ┆ str ┆ --- ┆ str ┆ --- ┆ --- ┆ str ┆ --- ┆ str ┆ str ┆ d   ┆ a   ┆ --- ┆ str ┆ --- ┆ str ┆ --- ┆ str ┆ str ┆ str ┆ str ┆ str ┆ --- ┆ str ┆ --- ┆ --- ┆ str ┆ --- │\n",
       "│ str ┆ str ┆     ┆ str ┆     ┆     ┆     ┆ str ┆     ┆ str ┆ str ┆ str ┆     ┆     ┆     ┆ str ┆     ┆     ┆ str ┆     ┆ str ┆ str ┆     ┆ str ┆     ┆     ┆ --- ┆ --- ┆ str ┆     ┆ str ┆     ┆ str ┆     ┆     ┆     ┆     ┆     ┆ str ┆     ┆ str ┆ str ┆     ┆ str │\n",
       "│     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ str ┆ str ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     │\n",
       "╞═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╪═════╡\n",
       "│ I-L ┆ I-L ┆ I-M ┆ B-M ┆ I-M ┆ B-G ┆ B-G ┆ I-O ┆ I-M ┆ B-P ┆ B-G ┆ I-M ┆ I-F ┆ I-L ┆ I-M ┆ I-M ┆ B-G ┆ B-L ┆ B-G ┆ I-M ┆ I-M ┆ I-M ┆ B-L ┆ B-P ┆ I-O ┆ I-G ┆ I-M ┆ I-O ┆ I-P ┆ B-P ┆ B-M ┆ B-L ┆ B-M ┆ I-L ┆ B-M ┆ B-P ┆ I-O ┆ B-M ┆ B-L ┆ I-M ┆ B-M ┆ B-L ┆ I-L ┆ B-P │\n",
       "│ OC  ┆ OC  ┆ ONE ┆ ONE ┆ ONE ┆ PE  ┆ PE  ┆ RDI ┆ ONE ┆ ROD ┆ PE  ┆ ONE ┆ AC  ┆ OC  ┆ ONE ┆ ONE ┆ PE  ┆ ANG ┆ PE  ┆ ONE ┆ ONE ┆ ONE ┆ OC  ┆ ROD ┆ RG  ┆ PE  ┆ ONE ┆ RDI ┆ ERC ┆ ROD ┆ ONE ┆ ANG ┆ ONE ┆ OC  ┆ ONE ┆ ERC ┆ RG  ┆ ONE ┆ ANG ┆ ONE ┆ ONE ┆ ANG ┆ OC  ┆ ERC │\n",
       "│     ┆     ┆ Y   ┆ Y   ┆ Y   ┆     ┆     ┆ NAL ┆ Y   ┆ UCT ┆     ┆ Y   ┆     ┆     ┆ Y   ┆ Y   ┆     ┆ UAG ┆     ┆ Y   ┆ Y   ┆ Y   ┆     ┆ UCT ┆     ┆     ┆ Y   ┆ NAL ┆ ENT ┆ UCT ┆ Y   ┆ UAG ┆ Y   ┆     ┆ Y   ┆ ENT ┆     ┆ Y   ┆ UAG ┆ Y   ┆ Y   ┆ UAG ┆     ┆ ENT │\n",
       "│     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ E   ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆     ┆ E   ┆     ┆     ┆     ┆     ┆     ┆     ┆ E   ┆     ┆     ┆ E   ┆     ┆     │\n",
       "└─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┘"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pass the input to the model and extract the predictions\n",
    "inputs_device: dict[str, Tensor] = {\n",
    "    k: torch.tensor(v).to(device) for k, v in inputs.items()\n",
    "}\n",
    "outputs = model(**inputs_device).logits\n",
    "predictions: Tensor = torch.argmax(outputs, dim=-1)\n",
    "print(f\"Number of tokens in sequence: {len(tokens)}\")\n",
    "\n",
    "data: dict[str, str] = {\n",
    "    col: tags.names[p] for col, p in zip(tokens, predictions.flatten())\n",
    "}\n",
    "\n",
    "pl.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text.split()\n",
    "# pattern: str = r\"[,.:;?!\\s]+\"\n",
    "# tokens: list[str] = re.compile(pattern).split(text)\n",
    "# tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 5)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = ds_dict[\"train\"][10]\n",
    "\n",
    "words, inp_labels = sample[\"tokens\"], sample[\"labels\"]\n",
    "\n",
    "len(words), len(inp_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'Journalists', 'sources', 'and', 'jail', '/', '.', '[SEP]']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize each word and specify that the input sequence has already been split into words.\n",
    "tokenized_input = tokenizer(words, is_split_into_words=True)\n",
    "tokens: list[str] = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, 0, 1, 2, 3, 4, 4, None]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_ids = tokenized_input.word_ids()\n",
    "word_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tokens</th>\n",
       "      <td>[CLS]</td>\n",
       "      <td>Journalists</td>\n",
       "      <td>sources</td>\n",
       "      <td>and</td>\n",
       "      <td>jail</td>\n",
       "      <td>/</td>\n",
       "      <td>.</td>\n",
       "      <td>[SEP]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Word IDs</th>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label IDs</th>\n",
       "      <td>-100</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Labels</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0            1        2    3     4     5     6      7\n",
       "Tokens     [CLS]  Journalists  sources  and  jail     /     .  [SEP]\n",
       "Word IDs    None            0        1    2     3     4     4   None\n",
       "label IDs   -100            0        0    0     0     0  -100   -100\n",
       "Labels         0            0        0    0     0  None  None   None"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Align the labels\n",
    "prev_word_idx: int | None = None\n",
    "label_ids: list[int] = []\n",
    "\n",
    "for word_idx in word_ids:\n",
    "    if word_idx is None or word_idx == prev_word_idx:\n",
    "        label_ids.append(-100)\n",
    "    else:\n",
    "        label_ids.append(inp_labels[word_idx])\n",
    "    prev_word_idx = word_idx\n",
    "\n",
    "\n",
    "# Update the labels\n",
    "labels = [index2tag[l] if l != -100 else \"IGN\" for l in label_ids]\n",
    "index = [\"Tokens\", \"Word IDs\", \"label IDs\", \"Labels\"]\n",
    "\n",
    "pd.DataFrame([tokens, word_ids, label_ids, inp_labels], index=index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment\n",
    "\n",
    "- The ID `-100` is used to mask subword representations in PyTorch's cross-entropy loss to ignore them during training, preventing their influence on the model's learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting it together\n",
    "import os\n",
    "\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "\n",
    "\n",
    "def tokenize_and_align_labels(examples: dict[str, list]) -> dict[str, list]:\n",
    "    labels: list[list[int]] = []\n",
    "\n",
    "    # Tokenize each word and specify that the input sequence has\n",
    "    # already been split into words.\n",
    "    tokenized_inputs: dict[str, list] = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True,\n",
    "        max_length=config.max_position_embeddings,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    for idx, label in enumerate(examples[\"labels\"]):\n",
    "        word_ids: list[int | None] = tokenized_inputs.word_ids(batch_index=idx)\n",
    "        prev_word_idx: int | None = None\n",
    "        label_ids: list[int] = []\n",
    "\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None or word_idx == prev_word_idx:\n",
    "                label_ids.append(-100)\n",
    "            else:\n",
    "                label_ids.append(label[word_idx])\n",
    "            prev_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    # Add the labels to the inputs\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "\n",
    "    return tokenized_inputs\n",
    "\n",
    "\n",
    "def encode_dataset(corpus: DatasetDict) -> DatasetDict:\n",
    "    encoded_corpus = DatasetDict()\n",
    "    for split, dataset in corpus.items():\n",
    "        encoded_corpus[split] = dataset.map(\n",
    "            tokenize_and_align_labels,\n",
    "            batched=True,\n",
    "            remove_columns=[\"tokens\", \"ner_labels\"],\n",
    "            desc=f\"Running tokenizer on {split} split\",\n",
    "        )\n",
    "    return encoded_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87937f1a9f4c4a9aae2a1b298e2a7d5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on validation split:   0%|          | 0/222 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['document_id', 'labels', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 1940\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['document_id', 'labels', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 222\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['document_id', 'labels', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 222\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_dict_encoded: DatasetDict = encode_dataset(ds_dict)\n",
    "ds_dict_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Performance Metrics\n",
    "\n",
    "- Evaluating NER models involves precision, recall, and F1-score.\n",
    "- All words of an entity must be predicted correctly.\n",
    "- The `seqeval` library can compute these metrics using the `classification_report()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import classification_report\n",
    "\n",
    "\n",
    "def align_predictions(\n",
    "    predictions: np.ndarray, label_ids: np.ndarray\n",
    ") -> tuple[list[list[str]], list[list[str]]]:\n",
    "    \"\"\"\n",
    "    Align predictions with label IDs and convert them to tag strings. It's\n",
    "    required to convert the model outputs to the format `seqeval` expects.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    predictions : np.ndarray\n",
    "        The prediction tensor of shape (batch_size, seq_len, num_classes).\n",
    "    label_ids : np.ndarray\n",
    "        The label IDs tensor of shape (batch_size, seq_len).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple[list[list[str]], list[list[str]]]\n",
    "        A tuple containing two lists:\n",
    "        - preds_list: List of lists containing predicted tags for each sequence.\n",
    "        - labels_list: List of lists containing true tags for each sequence.\n",
    "    \"\"\"\n",
    "    preds: np.ndarray = np.argmax(predictions, axis=2)\n",
    "    batch_size, seq_len = preds.shape\n",
    "    labels_list: list[list[str]] = []\n",
    "    preds_list: list[list[str]] = []\n",
    "\n",
    "    for batch_idx in range(batch_size):\n",
    "        example_labels: list[str] = []\n",
    "        example_preds: list[str] = []\n",
    "        for seq_idx in range(seq_len):\n",
    "            # Skip the special tokens (label IDs = -100)\n",
    "            if label_ids[batch_idx, seq_idx] != -100:\n",
    "                example_labels.append(index2tag[label_ids[batch_idx][seq_idx]])\n",
    "                example_preds.append(index2tag[preds[batch_idx][seq_idx]])\n",
    "\n",
    "        labels_list.append(example_labels)\n",
    "        preds_list.append(example_preds)\n",
    "\n",
    "    return preds_list, labels_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /Users/neidu/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "\n",
    "# Load the .env file\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "\n",
    "# Access an environment variable\n",
    "HF_NOTEBOOK_TOKEN = os.getenv(\"HF_NOTEBOOK_TOKEN\")\n",
    "login(token=HF_NOTEBOOK_TOKEN, add_to_git_credential=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "\n",
    "# Define the training arguments\n",
    "num_epochs: int = 3\n",
    "batch_size: int = 24  # (4 because data is small)\n",
    "logging_steps: int = len(ds_dict_encoded[\"train\"]) // batch_size\n",
    "model_name: str = f\"{chkpoint}-finetuned-bert-tiny\"\n",
    "\n",
    "# The model's predictions are evaluated on the validation set after each epoch.\n",
    "# Weight decay is adjusted, and checkpointing is disabled to speed up training.\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_name,\n",
    "    log_level=\"error\",\n",
    "    num_train_epochs=num_epochs,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_steps=1e6,\n",
    "    weight_decay=0.01,\n",
    "    disable_tqdm=False,\n",
    "    logging_steps=logging_steps,\n",
    "    # push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import accuracy_score, f1_score\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred: tuple[Any, Any]) -> dict[str, float]:\n",
    "    \"\"\"\n",
    "    Compute F1 score for the predictions.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    eval_pred :tuple[Any, Any]\n",
    "        A tuple containing model predictions and true labels.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict[str, float]\n",
    "        A dictionary containing the F1 score.\n",
    "    \"\"\"\n",
    "    y_pred, y_true = align_predictions(*eval_pred)\n",
    "    return {\"f1\": f1_score(y_true, y_pred)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment\n",
    "\n",
    "- The labels in sequence classification are padded with `-100` to avoid affecting the loss.\n",
    "- A `model_init()` method is used to load an untrained model for training to avoid creating a new model for each Trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init() -> BertForTokenClassification:\n",
    "    \"\"\"\n",
    "    Initialize and return a BertForTokenClassification model.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    BertForTokenClassification\n",
    "        A pre-trained BERT model for token classification, loaded with the specified\n",
    "        checkpoint and configuration, and moved to the specified device.\n",
    "    \"\"\"\n",
    "    return BertForTokenClassification.from_pretrained(\n",
    "        chkpoint, config=config, ignore_mismatched_sizes=True\n",
    "    ).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3edf1e2a02a4e5abff09132da073665",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/243 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[75], line 16\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# A model_init() method is used to load an untrained model for\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# training to avoid creating a new model for each Trainer.\u001b[39;00m\n\u001b[1;32m      6\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m      7\u001b[0m     model_init\u001b[38;5;241m=\u001b[39mmodel_init,\n\u001b[1;32m      8\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[1;32m     14\u001b[0m )\n\u001b[0;32m---> 16\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_p11/lib/python3.11/site-packages/transformers/trainer.py:1780\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1778\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1780\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1781\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1782\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1783\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1784\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1785\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_p11/lib/python3.11/site-packages/transformers/trainer.py:2118\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2115\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2117\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2118\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2121\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2122\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2123\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2124\u001b[0m ):\n\u001b[1;32m   2125\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2126\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_p11/lib/python3.11/site-packages/transformers/trainer.py:3045\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   3043\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   3044\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3045\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3047\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_p11/lib/python3.11/site-packages/accelerate/accelerator.py:2159\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2157\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[1;32m   2158\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2159\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_p11/lib/python3.11/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_p11/lib/python3.11/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "\n",
    "# A model_init() method is used to load an untrained model for\n",
    "# training to avoid creating a new model for each Trainer.\n",
    "trainer = Trainer(\n",
    "    model_init=model_init,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=ds_dict_encoded[\"train\"],\n",
    "    eval_dataset=ds_dict_encoded[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_text(\n",
    "    text: str,\n",
    "    tags: ClassLabel,\n",
    "    model: BertModel,\n",
    "    tokenizer: AutoTokenizer,\n",
    ") -> pl.DataFrame:\n",
    "    # Get tokens with special characters\n",
    "    tokens: list[str] = tokenizer(text).tokens()\n",
    "\n",
    "    # Encode\n",
    "    inputs: Tensor = tokenizer(\n",
    "        text, padding=True, truncation=True, max_length=512, return_tensors=\"pt\"\n",
    "    )\n",
    "    inputs = {k: torch.tensor(v).to(device) for k, v in inputs.items()}\n",
    "\n",
    "    # Get predictions\n",
    "    outputs: Tensor = model(**inputs).logits\n",
    "\n",
    "    # Get predictions\n",
    "    predictions: Tensor = torch.argmax(outputs, dim=-1)\n",
    "\n",
    "    data: dict[str, str] = {\n",
    "        col: tags.names[p]\n",
    "        for col, p in zip(tokens, predictions.flatten().cpu().numpy())\n",
    "    }\n",
    "\n",
    "    return pl.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text: str = \"tomorrow\"\n",
    "\n",
    "tag_text(text, tags, trainer.model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_dict[\"test\"][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_p11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
