{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Building A Tokenizer, Block By Block](https://huggingface.co/learn/nlp-course/chapter6/8?fw=pt)\n",
    "\n",
    "```text\n",
    "As we’ve seen in the previous sections, tokenization comprises several steps:\n",
    "  1. Normalization (any cleanup of the text that is deemed necessary, such as removing spaces or accents, Unicode normalization, etc.)\n",
    "  2. Pre-tokenization (splitting the input into words)\n",
    "  3. Running the input through the model (using the pre-tokenized words to produce a sequence of tokens)\n",
    "  4. Post-processing (adding the special tokens of the tokenizer, generating the attention mask and token type IDs)\n",
    "  \n",
    "As a reminder, here’s another look at the overall process:\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "[![image.png](https://i.postimg.cc/Qxkc6N43/image.png)](https://postimg.cc/dL373FcH)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!pip install rich\n",
    "!pip install transformers[torch]\n",
    "!pip install torch datasets evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in library\n",
    "import re\n",
    "import json\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rich import print\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Pandas settings\n",
    "pd.options.display.max_rows = 1_000\n",
    "pd.options.display.max_columns = 1_000\n",
    "pd.options.display.max_colwidth = 600\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Black code formatter (Optional)\n",
    "%load_ext lab_black\n",
    "\n",
    "# auto reload imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note\n",
    "\n",
    "- The 🤗 Tokenizers library has been built to provide several options for each of those steps, which you can mix and match together. \n",
    "- In this section we’ll see how we can build a tokenizer from scratch, as opposed to training a new tokenizer from an old one as we did in [section 2](https://huggingface.co/course/chapter6/2).\n",
    "-  You’ll then be able to build any kind of tokenizer you can think of!\n",
    "\n",
    "More precisely, the library is built around a central Tokenizer class with the building blocks regrouped in submodules:\n",
    "\n",
    "- **normalizers** contains all the possible types of Normalizer you can use (complete list [here](https://huggingface.co/docs/tokenizers/python/latest/api/reference.html#module-tokenizers.normalizers)).\n",
    "- **pre_tokenizers** contains all the possible types of PreTokenizer you can use (complete list [here](https://huggingface.co/docs/tokenizers/python/latest/api/reference.html#module-tokenizers.pre_tokenizers)).\n",
    "- **models** contains the various types of Model you can use, like BPE, WordPiece, and Unigram (complete list [here](https://huggingface.co/docs/tokenizers/python/latest/api/reference.html#module-tokenizers.models)).\n",
    "trainers contains all the different types of Trainer you can use to train your model on a corpus (one per type of model; complete list [here](https://huggingface.co/docs/tokenizers/python/latest/api/reference.html#module-tokenizers.trainers)).\n",
    "- **post_processors** contains the various types of PostProcessor you can use (complete list [here](https://huggingface.co/docs/tokenizers/python/latest/api/reference.html#module-tokenizers.processors)).\n",
    "- **decoders** contains the various types of Decoder you can use to decode the outputs of tokenization (complete list [here](https://huggingface.co/docs/tokenizers/python/latest/components.html#decoders)).\n",
    "You can find the whole list of building blocks [here](https://huggingface.co/docs/tokenizers/python/latest/components.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "name: str = \"wikitext-2-raw-v1\"  # wikitext-103-v1 (~191 MB)\n",
    "split: str = \"train\"\n",
    "dataset: Dataset = load_dataset(path=\"wikitext\", name=name, split=split)\n",
    "\n",
    "\n",
    "def get_training_corpus() -> str:\n",
    "    \"\"\"This is used to load the dataset in batches.\"\"\"\n",
    "    for i in range(0, len(dataset), 1000):\n",
    "        yield dataset[i : i + 1000][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 36718\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ' = Valkyria Chronicles III = \\n'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```text\n",
    "The function get_training_corpus() is a generator that will yield batches of 1,000 texts, which we will use to train the tokenizer.\n",
    "\n",
    "🤗 Tokenizers can also be trained on text files directly. Here’s how we can generate/save a text file containing all the texts/inputs from WikiText-2 that we can use locally:\n",
    "```\n",
    "\n",
    "```python\n",
    "with open(\"wikitext-2.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for i in range(len(dataset)):\n",
    "        f.write(dataset[i][\"text\"] + \"\\n\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data\n",
    "fp: str = \"./my_data/wikitext-2.txt\"\n",
    "\n",
    "with open(file=fp, mode=\"w\") as f:\n",
    "    for idx in range(len(dataset)):\n",
    "        f.write(dataset[idx].get(\"text\") + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build A Custom BERT, GPT-2, and XLNet Tokenizer\n",
    "\n",
    "```text\n",
    "- Build a custom tokenizer block by block. \n",
    "- Using the three main tokenization algorithms: \n",
    "  - WordPiece\n",
    "  - BPE \n",
    "  - Unigram\n",
    "```\n",
    "\n",
    "<br><hr>\n",
    "\n",
    "### Building A WordPiece Tokenizer From Sratch\n",
    "\n",
    "```text\n",
    "- To build a tokenizer with the 🤗 Tokenizers library, we start by instantiating a Tokenizer object with a model, then set its normalizer, pre_tokenizer, post_processor, and decoder attributes to the values we want.\n",
    "\n",
    "- For this example, we’ll create a Tokenizer with a WordPiece model:\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tokenizers.Tokenizer at 0x7fcb22943600>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    ")\n",
    "\n",
    "# To ensure proper tokenization, we need to define the unk_token, which represents unknown characters.\n",
    "tokenizer: Tokenizer = Tokenizer(model=models.WordPiece(unk_token=\"[UNK]\"))\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional arguments include vocab (unnecessary for training) and max_input_chars_per_word to set a maximum\n",
    "# word length for splitting. For normalization, the BertNormalizer can be used with options like lowercase,\n",
    "# strip_accents, clean_text, and handle_chinese_chars for replicating the bert-base-uncased tokenizer.\n",
    "tokenizer.normalizer = normalizers.BertNormalizer(lowercase=True)\n",
    "\n",
    "\n",
    "# OR\n",
    "# The library provides a Lowercase normalizer and a StripAccents normalizer, and you can compose several\n",
    "# normalizers using a Sequence:\n",
    "tokenizer.normalizer = normalizers.Sequence(\n",
    "    [normalizers.NFD(), normalizers.Lowercase(), normalizers.StripAccents()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">hello how are u?\n",
       "</pre>\n"
      ],
      "text/plain": [
       "hello how are u?\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We’re also using an NFD Unicode normalizer, as otherwise the StripAccents normalizer won’t properly\n",
    "# recognize the accented characters and thus won’t strip them out. As we’ve seen before, we can use\n",
    "# the normalize_str() method of the normalizer to check out the effects it has on a given text:\n",
    "print(tokenizer.normalizer.normalize_str(\"Héllò hôw are ü?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next is the pre-tokenization step. Again, there is a prebuilt BertPreTokenizer that we can use:\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or we can build it from scratch:\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Let', (0, 3)),\n",
       " (\"'\", (3, 4)),\n",
       " ('s', (4, 5)),\n",
       " ('test', (6, 10)),\n",
       " ('my', (11, 13)),\n",
       " ('pre', (14, 17)),\n",
       " ('-', (17, 18)),\n",
       " ('tokenizer', (18, 27)),\n",
       " ('.', (27, 28))]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note that the Whitespace pre-tokenizer splits on whitespace and all characters that are not letters,\n",
    "# digits, or the underscore character, so it technically splits on whitespace and punctuation:\n",
    "tokenizer.pre_tokenizer.pre_tokenize_str(\"Let's test my pre-tokenizer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"Let's\", (0, 5)),\n",
       " ('test', (6, 10)),\n",
       " ('my', (11, 13)),\n",
       " ('pre-tokenizer.', (14, 28))]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you only want to split on whitespace, you should use the WhitespaceSplit pre-tokenizer instead:\n",
    "pre_tokenizer = pre_tokenizers.WhitespaceSplit()\n",
    "pre_tokenizer.pre_tokenize_str(\"Let's test my pre-tokenizer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Let', (0, 3)),\n",
       " (\"'\", (3, 4)),\n",
       " ('s', (4, 5)),\n",
       " ('test', (6, 10)),\n",
       " ('my', (11, 13)),\n",
       " ('pre', (14, 17)),\n",
       " ('-', (17, 18)),\n",
       " ('tokenizer', (18, 27)),\n",
       " ('.', (27, 28))]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Like with normalizers, you can use a Sequence to compose several pre-tokenizers:\n",
    "pre_tokenizer = pre_tokenizers.Sequence(\n",
    "    [pre_tokenizers.WhitespaceSplit(), pre_tokenizers.Punctuation()]\n",
    ")\n",
    "pre_tokenizer.pre_tokenize_str(\"Let's test my pre-tokenizer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The next step in the tokenization pipeline is running the inputs through the model. We already specified our model\n",
    "# in the initialization, but we still need to train it, which will require a WordPieceTrainer. The main thing to\n",
    "# remember when instantiating a trainer in 🤗 Tokenizers is that you need to pass it all the special tokens you\n",
    "# intend to use — otherwise it won’t add them to the vocabulary, since they are not in the training corpus:\n",
    "special_tokens: list[str] = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "trainer: trainers = trainers.WordPieceTrainer(\n",
    "    vocab_size=25000, special_tokens=special_tokens\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# As well as specifying the vocab_size and special_tokens, we can set the min_frequency\n",
    "# (the number of times a token must appear to be included in the vocabulary) or change the\n",
    "# continuing_subword_prefix (if we want to use something different from ##).\n",
    "# To train our model using the iterator we defined earlier, we just have to execute this command:\n",
    "tokenizer.train_from_iterator(iterator=get_training_corpus(), trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We can also use text files to train our tokenizer, which would look like this (we reinitialize the model with\n",
    "# an empty WordPiece beforehand):\n",
    "fp: str = \"./my_data/wikitext-2.txt\"\n",
    "tokenizer.model = models.WordPiece(unk_token=\"[UNK]\")\n",
    "tokenizer.train([fp], trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'let'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">\"'\"</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'s'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'test'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'this'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'tok'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'##eni'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'##zer'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'.'</span><span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[32m'let'\u001b[0m, \u001b[32m\"'\"\u001b[0m, \u001b[32m's'\u001b[0m, \u001b[32m'test'\u001b[0m, \u001b[32m'this'\u001b[0m, \u001b[32m'tok'\u001b[0m, \u001b[32m'##eni'\u001b[0m, \u001b[32m'##zer'\u001b[0m, \u001b[32m'.'\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# In both cases, we can then test the tokenizer on a text by calling the encode() method:\n",
    "encoding = tokenizer.encode(\"Let's test this tokenizer.\")\n",
    "print(encoding.tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```text\n",
    "- The encoding generated by the tokenizer includes attributes such as ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, and overflowing. \n",
    "\n",
    "- The final step is post-processing, which involves adding the [CLS] token at the beginning and the [SEP] token at the end using a TemplateProcessor, with the IDs of these tokens obtained from the vocabulary.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2\u001b[0m \u001b[1;36m3\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cls_token_id: int = tokenizer.token_to_id(\"[CLS]\")\n",
    "sep_token_id: int = tokenizer.token_to_id(\"[SEP]\")\n",
    "print(cls_token_id, sep_token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```text\n",
    "- To write the template for the TemplateProcessor, we have to specify how to treat a single sentence and a pair of sentences. \n",
    "- For both, we write the special tokens we want to use; the first (or single) sentence is represented by $A, while the second sentence (if encoding a pair) is represented by $B. \n",
    "- For each of these (special tokens and sentences), we also specify the corresponding token type ID after a colon.\n",
    "\n",
    "- The classic BERT template is thus defined as follows:\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.post_processor = processors.TemplateProcessing(\n",
    "    single=f\"[CLS]:0 $A:0 [SEP]:0\",\n",
    "    pair=f\"[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1\",\n",
    "    special_tokens=[(\"[CLS]\", cls_token_id), (\"[SEP]\", sep_token_id)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'[CLS]'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'let'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">\"'\"</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'s'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'test'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'this'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'tok'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'##eni'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'##zer'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'.'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'[SEP]'</span><span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[32m'\u001b[0m\u001b[32m[\u001b[0m\u001b[32mCLS\u001b[0m\u001b[32m]\u001b[0m\u001b[32m'\u001b[0m, \u001b[32m'let'\u001b[0m, \u001b[32m\"'\"\u001b[0m, \u001b[32m's'\u001b[0m, \u001b[32m'test'\u001b[0m, \u001b[32m'this'\u001b[0m, \u001b[32m'tok'\u001b[0m, \u001b[32m'##eni'\u001b[0m, \u001b[32m'##zer'\u001b[0m, \u001b[32m'.'\u001b[0m, \u001b[32m'\u001b[0m\u001b[32m[\u001b[0m\u001b[32mSEP\u001b[0m\u001b[32m]\u001b[0m\u001b[32m'\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Note that we need to pass along the IDs of the special tokens, so the tokenizer can properly convert them to their IDs.\n",
    "# Once this is added, going back to our previous example will give:\n",
    "encoding = tokenizer.encode(\"Let's test this tokenizer.\")\n",
    "print(encoding.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, 0, 1, 2, 3, 4, 5, 5, 5, 6, None]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dir(encoding)\n",
    "\n",
    "encoding.word_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'[CLS]'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'let'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"'\"</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'s'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'test'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'this'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'tok'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'##eni'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'##zer'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'...'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'[SEP]'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'on'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'a'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'pair'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'of'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'sentences'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'.'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'[SEP]'</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[32m'\u001b[0m\u001b[32m[\u001b[0m\u001b[32mCLS\u001b[0m\u001b[32m]\u001b[0m\u001b[32m'\u001b[0m,\n",
       "    \u001b[32m'let'\u001b[0m,\n",
       "    \u001b[32m\"'\"\u001b[0m,\n",
       "    \u001b[32m's'\u001b[0m,\n",
       "    \u001b[32m'test'\u001b[0m,\n",
       "    \u001b[32m'this'\u001b[0m,\n",
       "    \u001b[32m'tok'\u001b[0m,\n",
       "    \u001b[32m'##eni'\u001b[0m,\n",
       "    \u001b[32m'##zer'\u001b[0m,\n",
       "    \u001b[32m'...'\u001b[0m,\n",
       "    \u001b[32m'\u001b[0m\u001b[32m[\u001b[0m\u001b[32mSEP\u001b[0m\u001b[32m]\u001b[0m\u001b[32m'\u001b[0m,\n",
       "    \u001b[32m'on'\u001b[0m,\n",
       "    \u001b[32m'a'\u001b[0m,\n",
       "    \u001b[32m'pair'\u001b[0m,\n",
       "    \u001b[32m'of'\u001b[0m,\n",
       "    \u001b[32m'sentences'\u001b[0m,\n",
       "    \u001b[32m'.'\u001b[0m,\n",
       "    \u001b[32m'\u001b[0m\u001b[32m[\u001b[0m\u001b[32mSEP\u001b[0m\u001b[32m]\u001b[0m\u001b[32m'\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# And on a pair of sentences, we get the proper result:\n",
    "encoding = tokenizer.encode(\"Let's test this tokenizer...\", \"on a pair of sentences.\")\n",
    "\n",
    "print(encoding.tokens)\n",
    "print(encoding.type_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We’ve almost finished building this tokenizer from scratch — the last step is to include a decoder:\n",
    "tokenizer.decoder = decoders.WordPiece(prefix=\"##\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3019</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">61</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3611</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1519</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">24319</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">18903</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6614</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2382</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1369</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">43</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4154</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1345</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9452</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">18</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m3019\u001b[0m, \u001b[1;36m11\u001b[0m, \u001b[1;36m61\u001b[0m, \u001b[1;36m3611\u001b[0m, \u001b[1;36m1519\u001b[0m, \u001b[1;36m24319\u001b[0m, \u001b[1;36m18903\u001b[0m, \u001b[1;36m6614\u001b[0m, \u001b[1;36m2382\u001b[0m, \u001b[1;36m3\u001b[0m, \u001b[1;36m1369\u001b[0m, \u001b[1;36m43\u001b[0m, \u001b[1;36m4154\u001b[0m, \u001b[1;36m1345\u001b[0m, \u001b[1;36m9452\u001b[0m, \u001b[1;36m18\u001b[0m, \u001b[1;36m3\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(encoding.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"let ' s test this tokenizer... on a pair of sentences.\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let’s test it on our previous encoding:\n",
    "tokenizer.decode(encoding.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Tokenizer (As JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Great! We can save our tokenizer in a single JSON file like this:\n",
    "fp: str = \"./tokenizers/tokenizer.json\"\n",
    "tokenizer.save(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Tokenizer (From A File)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can then reload that file in a Tokenizer object with the from_file() method:\n",
    "new_tokenizer: Tokenizer = Tokenizer.from_file(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```text\n",
    "- To use the tokenizer in 🤗 Transformers, it needs to be wrapped in a PreTrainedTokenizerFast. \n",
    "- This can be done by either passing the tokenizer object or the saved tokenizer file. \n",
    "- It's important to manually set the special tokens such as the mask token and the [CLS] token.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "\n",
    "wrapped_tokenizer: PreTrainedTokenizerFast = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=tokenizer,\n",
    "    # tokenizer_file=\"tokenizer.json\", # You can load from the tokenizer file, alternatively\n",
    "    unk_token=\"[UNK]\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    cls_token=\"[CLS]\",\n",
    "    sep_token=\"[SEP]\",\n",
    "    mask_token=\"[MASK]\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "\n",
    "\n",
    "# If you are using a specific tokenizer class (like BertTokenizerFast), you will only need to specify the special tokens\n",
    "# that are different from the default ones (here, none):\n",
    "wrapped_tokenizer = BertTokenizerFast(tokenizer_object=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```text\n",
    "- You can then use this tokenizer like any other 🤗 Transformers tokenizer. \n",
    "- You can save it with the save_pretrained() method, or upload it to the Hub with the push_to_hub() method.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><hr><br>\n",
    "\n",
    "## Building A BPE Tokenizer From Scratch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a GPT-2 tokenizer by initializing a Tokenizer with a BPE model:\n",
    "tokenizer: Tokenizer = Tokenizer(models.BPE())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For GPT-2, initializing the model with a vocabulary is not necessary as we will train from scratch.\n",
    "# The unk_token is not required as GPT-2 uses byte-level BPE. The pre-tokenization step follows the omission of the normalizer.\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Let', (0, 3)),\n",
       " (\"'s\", (3, 5)),\n",
       " ('Ġtest', (5, 10)),\n",
       " ('Ġpre', (10, 14)),\n",
       " ('-', (14, 15)),\n",
       " ('tokenization', (15, 27)),\n",
       " ('!', (27, 28))]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The option we added to ByteLevel here is to not add a space at the beginning of a sentence (which is the default otherwise).\n",
    "# We can have a look at the pre-tokenization of an example text like before:\n",
    "tokenizer.pre_tokenizer.pre_tokenize_str(\"Let's test pre-tokenization!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the model. For GPT-2, the only special token is the end-of-text token:\n",
    "trainer = trainers.BpeTrainer(vocab_size=25000, special_tokens=[\"<|endoftext|>\"])\n",
    "tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Like with the WordPieceTrainer, as well as the vocab_size and special_tokens, we can specify the min_frequency if we want to,\n",
    "# or if we have an end-of-word suffix (like </w>), we can set it with end_of_word_suffix.\n",
    "# This tokenizer can also be trained on text files:\n",
    "tokenizer.model = models.BPE()\n",
    "tokenizer.train([fp], trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'L'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'et'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">\"'\"</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'s'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Ġ'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'test'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Ġ'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'th'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'is'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Ġ'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'token'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'izer'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'.'</span><span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[32m'L'\u001b[0m, \u001b[32m'et'\u001b[0m, \u001b[32m\"'\"\u001b[0m, \u001b[32m's'\u001b[0m, \u001b[32m'Ġ'\u001b[0m, \u001b[32m'test'\u001b[0m, \u001b[32m'Ġ'\u001b[0m, \u001b[32m'th'\u001b[0m, \u001b[32m'is'\u001b[0m, \u001b[32m'Ġ'\u001b[0m, \u001b[32m'token'\u001b[0m, \u001b[32m'izer'\u001b[0m, \u001b[32m'.'\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenization of a sample text:\n",
    "encoding = tokenizer.encode(\"Let's test this tokenizer.\")\n",
    "print(encoding.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We apply the byte-level post-processing for the GPT-2 tokenizer as follows:\n",
    "tokenizer.post_processor = processors.ByteLevel(trim_offsets=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' '"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# By setting trim_offsets = False, the post-processor preserves the offsets of tokens starting with 'Ġ',\n",
    "# ensuring that the offsets indicate the space before the word. This is demonstrated in the encoded text example.\n",
    "sentence: str = \"Let's test this tokenizer.\"\n",
    "encoding = tokenizer.encode(sentence)\n",
    "start, end = encoding.offsets[4]\n",
    "sentence[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Let's test this tokenizer.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Let's test this tokenizer.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Add a byte-level decoder:\n",
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "\n",
    "# Double-check it works properly:\n",
    "print(tokenizer.decode(encoding.ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save BPE Tokenizer\n",
    "\n",
    "```text\n",
    "- Wrap it in a PreTrainedTokenizerFast or GPT2TokenizerFast if we want to use it in 🤗 Transformers:\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast, GPT2TokenizerFast\n",
    "\n",
    "\n",
    "wrapped_tokenizer: PreTrainedTokenizerFast = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=tokenizer,\n",
    "    bos_token=\"<|endoftext|>\",\n",
    "    eos_token=\"<|endoftext|>\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OR\n",
    "wrapped_tokenizer = GPT2TokenizerFast(tokenizer_object=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><hr>\n",
    "\n",
    "## Building A Unigram Tokenizer From Scratch\n",
    "\n",
    "```text\n",
    "Let’s now build an XLNet tokenizer. Like for the previous tokenizers, we start by initializing a Tokenizer with a Unigram model\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Regex\n",
    "\n",
    "\n",
    "# You can initialize the model with a vocabulary if a vocab is available.\n",
    "tokenizer: Tokenizer = Tokenizer(models.Unigram())\n",
    "\n",
    "\n",
    "# For the normalization, XLNet uses a few replacements (which come from SentencePiece)\n",
    "tokenizer.normalizer = normalizers.Sequence(\n",
    "    [\n",
    "        normalizers.Replace(\"``\", '\"'),\n",
    "        normalizers.Replace(\"''\", '\"'),\n",
    "        normalizers.NFKD(),\n",
    "        normalizers.StripAccents(),\n",
    "        normalizers.Replace(Regex(\" {2,}\"), \" \"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"▁Let's\", (0, 5)),\n",
       " ('▁test', (5, 10)),\n",
       " ('▁the', (10, 14)),\n",
       " ('▁pre-tokenizer!', (14, 29))]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The pre-tokenizer to use for any SentencePiece tokenizer is Metaspace:\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Metaspace()\n",
    "\n",
    "# Pre-tokenization of an example text:\n",
    "tokenizer.pre_tokenizer.pre_tokenize_str(\"Let's test the pre-tokenizer!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  Add some special tokens and train the model.\n",
    "special_tokens = [\"<cls>\", \"<sep>\", \"<unk>\", \"<pad>\", \"<mask>\", \"<s>\", \"</s>\"]\n",
    "trainer = trainers.UnigramTrainer(\n",
    "    vocab_size=25000, special_tokens=special_tokens, unk_token=\"<unk>\"\n",
    ")\n",
    "tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train The Model [on text files]\n",
    "\n",
    "```python\n",
    "# This tokenizer can also be trained on text files\n",
    "tokenizer.model = models.Unigram()\n",
    "tokenizer.train([\"wikitext-2.txt\"], trainer=trainer)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'▁Let'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">\"'\"</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'s'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'▁test'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'▁this'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'▁to'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'ken'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'izer'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'.'</span><span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[32m'▁Let'\u001b[0m, \u001b[32m\"'\"\u001b[0m, \u001b[32m's'\u001b[0m, \u001b[32m'▁test'\u001b[0m, \u001b[32m'▁this'\u001b[0m, \u001b[32m'▁to'\u001b[0m, \u001b[32m'ken'\u001b[0m, \u001b[32m'izer'\u001b[0m, \u001b[32m'.'\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenization of a sample text:\n",
    "encoding = tokenizer.encode(\"Let's test this tokenizer.\")\n",
    "print(encoding.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m0\u001b[0m \u001b[1;36m1\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# XLNet is unique in that it places the <cls> token at the end of the sentence, with a type ID of 2\n",
    "# to distinguish it from other tokens. This results in left-padding. We can use a template to handle\n",
    "# all special tokens and token type IDs, but we must first obtain the IDs of the <cls> and <sep> tokens.\n",
    "cls_token_id = tokenizer.token_to_id(\"<cls>\")\n",
    "sep_token_id = tokenizer.token_to_id(\"<sep>\")\n",
    "print(cls_token_id, sep_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The template looks like this:\n",
    "tokenizer.post_processor = processors.TemplateProcessing(\n",
    "    single=\"$A:0 <sep>:0 <cls>:2\",\n",
    "    pair=\"$A:0 <sep>:0 $B:1 <sep>:1 <cls>:2\",\n",
    "    special_tokens=[(\"<sep>\", sep_token_id), (\"<cls>\", cls_token_id)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'▁Let'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"'\"</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'s'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'▁test'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'▁this'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'▁to'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'ken'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'izer'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'.'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'.'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'.'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'&lt;sep&gt;'</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'▁'</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'on'</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'▁'</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'a'</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'▁pair'</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'▁of'</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'▁sentence'</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'s'</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'!'</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'&lt;sep&gt;'</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'&lt;cls&gt;'</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[32m'▁Let'\u001b[0m,\n",
       "    \u001b[32m\"'\"\u001b[0m,\n",
       "    \u001b[32m's'\u001b[0m,\n",
       "    \u001b[32m'▁test'\u001b[0m,\n",
       "    \u001b[32m'▁this'\u001b[0m,\n",
       "    \u001b[32m'▁to'\u001b[0m,\n",
       "    \u001b[32m'ken'\u001b[0m,\n",
       "    \u001b[32m'izer'\u001b[0m,\n",
       "    \u001b[32m'.'\u001b[0m,\n",
       "    \u001b[32m'.'\u001b[0m,\n",
       "    \u001b[32m'.'\u001b[0m,\n",
       "    \u001b[32m'\u001b[0m\u001b[32m<\u001b[0m\u001b[32msep\u001b[0m\u001b[32m>'\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m    \u001b[0m\u001b[32m'▁'\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m    \u001b[0m\u001b[32m'on'\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m    \u001b[0m\u001b[32m'▁'\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m    \u001b[0m\u001b[32m'a'\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m    \u001b[0m\u001b[32m'▁pair'\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m    \u001b[0m\u001b[32m'▁of'\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m    \u001b[0m\u001b[32m'▁sentence'\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m    \u001b[0m\u001b[32m's'\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m    \u001b[0m\u001b[32m'!'\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m    \u001b[0m\u001b[32m'<sep>'\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m    \u001b[0m\u001b[32m'<cls\u001b[0m\u001b[32m>\u001b[0m\u001b[32m'\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test the tokenizer by encoding a pair of sentences:\n",
    "encoding = tokenizer.encode(\"Let's test this tokenizer...\", \"on a pair of sentences!\")\n",
    "print(encoding.tokens)\n",
    "print(encoding.type_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a Metaspace decoder\n",
    "tokenizer.decoder = decoders.Metaspace()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Tokenizer\n",
    "\n",
    "```text\n",
    "- The tokenizer is now complete! We can save it and use it in 🤗 Transformers by wrapping it in PreTrainedTokenizerFast or XLNetTokenizerFast. \n",
    "\n",
    "- We need to tell the 🤗 Transformers library to pad on the left when using PreTrainedTokenizerFast.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "\n",
    "wrapped_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=tokenizer,\n",
    "    bos_token=\"<s>\",\n",
    "    eos_token=\"</s>\",\n",
    "    unk_token=\"<unk>\",\n",
    "    pad_token=\"<pad>\",\n",
    "    cls_token=\"<cls>\",\n",
    "    sep_token=\"<sep>\",\n",
    "    mask_token=\"<mask>\",\n",
    "    padding_side=\"left\",  # tell the 🤗 Transformers library to pad on the left\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OR\n",
    "from transformers import XLNetTokenizerFast\n",
    "\n",
    "\n",
    "wrapped_tokenizer = XLNetTokenizerFast(tokenizer_object=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_p310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
