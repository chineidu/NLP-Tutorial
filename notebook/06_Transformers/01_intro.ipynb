{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction To HuggingFace Transformers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in library\n",
    "import re\n",
    "import json\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "# Standard imports\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Pandas settings\n",
    "pd.options.display.max_rows = 1_000\n",
    "pd.options.display.max_columns = 1_000\n",
    "pd.options.display.max_colwidth = 600\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Black code formatter (Optional)\n",
    "%load_ext lab_black\n",
    "# auto reload imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-05 17:21:52.840615: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9598051905632019},\n",
       " {'label': 'NEGATIVE', 'score': 0.9994558691978455}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "classifier = pipeline(task=\"sentiment-analysis\")\n",
    "classifier(\n",
    "    [\n",
    "        \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "        \"I hate this so much!\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing With a Tokenizer\n",
    "\n",
    "```text\n",
    "- Transformer models can‚Äôt process raw text directly, so the first step of our pipeline is to convert the text inputs into numbers that the model can make sense of. \n",
    "- To do this we use a tokenizer, which will be responsible for:\n",
    "  - Splitting the input into words, subwords, or symbols (like punctuation) that are called tokens\n",
    "  - Mapping each token to an integer\n",
    "  - Adding additional inputs that may be useful to the model\n",
    "\n",
    "- All this preprocessing needs to be done in exactly the same way as when the model was pretrained.\n",
    "- The `AutoTokenizer` class and its `from_pretrained()` method are used to download and cache the data associated with the model's tokenizer. \n",
    "- This is done automatically using the checkpoint name of the model. \n",
    "- The data is only downloaded the first time the code is run.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "# The default checkpoint of the sentiment-analysis pipeline is:\n",
    "# distilbert-base-uncased-finetuned-sst-2-english\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]]),\n",
      " 'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
      "          2607,  2026,  2878,  2166,  1012,   102],\n",
      "        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0]])}\n"
     ]
    }
   ],
   "source": [
    "# Transformer models only accept tensors as input.\n",
    "# To specify the type of tensors we want to get back (PyTorch, TensorFlow, or plain NumPy), use the return_tensors argument:\n",
    "raw_inputs = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"I hate this so much!\",\n",
    "]\n",
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "pprint(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Going Through The Model\n",
    "\n",
    "```text\n",
    "- Download the pretrained model the same way just like the tokenizer. \n",
    "- ü§ó Transformers provides an AutoModel class which also has a from_pretrained() method:\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModel.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High-dimensional Vector\n",
    "\n",
    "```text\n",
    "- The vector output by the Transformer module is usually large. \n",
    "- It generally has three dimensions:\n",
    "  - Batch size: The number of sequences processed at a time (2 in our example).\n",
    "  - Sequence length: The length of the numerical representation of the sequence (16 in our example).\n",
    "  - Hidden size: The vector dimension of each model input.\n",
    "  \n",
    "- It's said to be ‚Äúhigh dimensional‚Äù because of the last value. \n",
    "- The hidden size can be very large (768 is common for smaller models, and in larger models this can reach 3072 or more).\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 16, 768])\n"
     ]
    }
   ],
   "source": [
    "outputs = model(**inputs)\n",
    "print(outputs.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Model Heads](https://huggingface.co/learn/nlp-course/chapter2/2?fw=pt)\n",
    "\n",
    "```text\n",
    "\n",
    "- The model heads take the high-dimensional vector of hidden states as input and project them onto a different dimension. \n",
    "- They are usually composed of one or a few linear layers.\n",
    "- The output of the Transformer model is sent directly to the model head to be processed.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "\n",
    "# In this example, we'll need a model with a sequence classification head (to be able to classify the sentences as positive or negative).\n",
    "# So, we won‚Äôt actually use the AutoModel class, but AutoModelForSequenceClassification:\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "# Since we have just two sentences and two labels, the result we get from our model is of shape 2 x 2.\n",
    "print(outputs.logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Postprocessing the output\n",
    "\n",
    "```text\n",
    "- The model provided logits [-1.5607, 1.6123] for the first sentence and [4.1692, -3.3464] for the second sentence. \n",
    "- Logits are raw scores that require conversion to probabilities using a SoftMax layer.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.5607,  1.6123],\n",
      "        [ 4.1692, -3.3464]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# The values we get as output from our model don‚Äôt necessarily make sense by themselves. Let‚Äôs take a look:\n",
    "print(outputs.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.0195e-02, 9.5981e-01],\n",
      "        [9.9946e-01, 5.4418e-04]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "# The model predicted [-1.5607, 1.6123] for the first sentence and [4.1692, -3.3464] for the second one.\n",
    "# These are logits, not probabilities. To convert them to probabilities, they need to go through a SoftMax layer.\n",
    "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'NEGATIVE', 1: 'POSITIVE'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The model predicted [0.0402, 0.9598] for the first sentence and [0.9995, 0.0005] for the second one. These are probability scores.\n",
    "# The labels corresponding to each position can be found by inspecting the id2label attribute of the model config.\n",
    "model.config.id2label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><hr>\n",
    "\n",
    "### Ex 1:\n",
    "\n",
    "```text\n",
    "‚úèÔ∏è Choose two (or more) texts of your own and run them through the sentiment-analysis pipeline. \n",
    "- Replicate the steps you saw here yourself and check that you obtain the same results!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9985768795013428},\n",
       " {'label': 'POSITIVE', 'score': 0.9977651834487915},\n",
       " {'label': 'POSITIVE', 'score': 0.9936624765396118},\n",
       " {'label': 'POSITIVE', 'score': 0.9965687990188599},\n",
       " {'label': 'NEGATIVE', 'score': 0.9996651411056519}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Method 1: Using the pipeline\n",
    "raw_inputs = [\n",
    "    \"Yesterday's football match was not the greatest.\",\n",
    "    \"I'm looking forward to starting my consultancy firm.\",\n",
    "    \"One of my favourite quoutes is 'All you have is all you need!'\",\n",
    "    \"Another favourite of mine is 'If you have never failed at anything, you have never tried anything new!'\",\n",
    "    \"We've been making very poor choices as a nation for a long time.\",\n",
    "]\n",
    "task = \"sentiment-analysis\"\n",
    "clf = pipeline(task=task)\n",
    "clf(raw_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m     \n",
      "\u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtext_pair\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtext_target\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtext_pair_target\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpadding\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPaddingStrategy\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtruncation\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenization_utils_base\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTruncationStrategy\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmax_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mstride\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mis_split_into_words\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpad_to_multiple_of\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mreturn_token_type_ids\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mreturn_attention_mask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mreturn_overflowing_tokens\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mreturn_special_tokens_mask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mreturn_offsets_mapping\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mreturn_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenization_utils_base\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBatchEncoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mType:\u001b[0m           DistilBertTokenizerFast\n",
      "\u001b[0;31mString form:\u001b[0m    DistilBertTokenizerFast(name_or_path='distilbert-base-uncased-finetuned-sst-2-english', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)\n",
      "\u001b[0;31mLength:\u001b[0m         30522\n",
      "\u001b[0;31mFile:\u001b[0m           ~/miniconda3/envs/nlp_p310/lib/python3.10/site-packages/transformers/models/distilbert/tokenization_distilbert_fast.py\n",
      "\u001b[0;31mSource:\u001b[0m        \n",
      "\u001b[0;32mclass\u001b[0m \u001b[0mDistilBertTokenizerFast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPreTrainedTokenizerFast\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34mr\"\"\"\u001b[0m\n",
      "\u001b[0;34m    Construct a \"fast\" DistilBERT tokenizer (backed by HuggingFace's *tokenizers* library). Based on WordPiece.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    This tokenizer inherits from [`PreTrainedTokenizerFast`] which contains most of the main methods. Users should\u001b[0m\n",
      "\u001b[0;34m    refer to this superclass for more information regarding those methods.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Args:\u001b[0m\n",
      "\u001b[0;34m        vocab_file (`str`):\u001b[0m\n",
      "\u001b[0;34m            File containing the vocabulary.\u001b[0m\n",
      "\u001b[0;34m        do_lower_case (`bool`, *optional*, defaults to `True`):\u001b[0m\n",
      "\u001b[0;34m            Whether or not to lowercase the input when tokenizing.\u001b[0m\n",
      "\u001b[0;34m        unk_token (`str`, *optional*, defaults to `\"[UNK]\"`):\u001b[0m\n",
      "\u001b[0;34m            The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this\u001b[0m\n",
      "\u001b[0;34m            token instead.\u001b[0m\n",
      "\u001b[0;34m        sep_token (`str`, *optional*, defaults to `\"[SEP]\"`):\u001b[0m\n",
      "\u001b[0;34m            The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for\u001b[0m\n",
      "\u001b[0;34m            sequence classification or for a text and a question for question answering. It is also used as the last\u001b[0m\n",
      "\u001b[0;34m            token of a sequence built with special tokens.\u001b[0m\n",
      "\u001b[0;34m        pad_token (`str`, *optional*, defaults to `\"[PAD]\"`):\u001b[0m\n",
      "\u001b[0;34m            The token used for padding, for example when batching sequences of different lengths.\u001b[0m\n",
      "\u001b[0;34m        cls_token (`str`, *optional*, defaults to `\"[CLS]\"`):\u001b[0m\n",
      "\u001b[0;34m            The classifier token which is used when doing sequence classification (classification of the whole sequence\u001b[0m\n",
      "\u001b[0;34m            instead of per-token classification). It is the first token of the sequence when built with special tokens.\u001b[0m\n",
      "\u001b[0;34m        mask_token (`str`, *optional*, defaults to `\"[MASK]\"`):\u001b[0m\n",
      "\u001b[0;34m            The token used for masking values. This is the token used when training this model with masked language\u001b[0m\n",
      "\u001b[0;34m            modeling. This is the token which the model will try to predict.\u001b[0m\n",
      "\u001b[0;34m        clean_text (`bool`, *optional*, defaults to `True`):\u001b[0m\n",
      "\u001b[0;34m            Whether or not to clean the text before tokenization by removing any control characters and replacing all\u001b[0m\n",
      "\u001b[0;34m            whitespaces by the classic one.\u001b[0m\n",
      "\u001b[0;34m        tokenize_chinese_chars (`bool`, *optional*, defaults to `True`):\u001b[0m\n",
      "\u001b[0;34m            Whether or not to tokenize Chinese characters. This should likely be deactivated for Japanese (see [this\u001b[0m\n",
      "\u001b[0;34m            issue](https://github.com/huggingface/transformers/issues/328)).\u001b[0m\n",
      "\u001b[0;34m        strip_accents (`bool`, *optional*):\u001b[0m\n",
      "\u001b[0;34m            Whether or not to strip all accents. If this option is not specified, then it will be determined by the\u001b[0m\n",
      "\u001b[0;34m            value for `lowercase` (as in the original BERT).\u001b[0m\n",
      "\u001b[0;34m        wordpieces_prefix (`str`, *optional*, defaults to `\"##\"`):\u001b[0m\n",
      "\u001b[0;34m            The prefix for subwords.\u001b[0m\n",
      "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mvocab_files_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVOCAB_FILES_NAMES\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpretrained_vocab_files_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPRETRAINED_VOCAB_FILES_MAP\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmax_model_input_sizes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPRETRAINED_POSITIONAL_EMBEDDINGS_SIZES\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpretrained_init_configuration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPRETRAINED_INIT_CONFIGURATION\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmodel_input_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"attention_mask\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mslow_tokenizer_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDistilBertTokenizer\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mvocab_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtokenizer_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mdo_lower_case\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0munk_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"[UNK]\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0msep_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"[SEP]\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mpad_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"[PAD]\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mcls_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"[CLS]\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmask_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"[MASK]\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtokenize_chinese_chars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mstrip_accents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mvocab_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mtokenizer_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mdo_lower_case\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdo_lower_case\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0munk_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0munk_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0msep_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msep_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mpad_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpad_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mcls_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcls_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mmask_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mtokenize_chinese_chars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenize_chinese_chars\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mstrip_accents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrip_accents\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mnormalizer_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mnormalizer_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"lowercase\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_lower_case\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mdo_lower_case\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mor\u001b[0m \u001b[0mnormalizer_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"strip_accents\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mor\u001b[0m \u001b[0mnormalizer_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"handle_chinese_chars\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenize_chinese_chars\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtokenize_chinese_chars\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mnormalizer_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalizers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalizer_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"type\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mnormalizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"lowercase\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdo_lower_case\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mnormalizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"strip_accents\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mnormalizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"handle_chinese_chars\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenize_chinese_chars\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalizer_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mnormalizer_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_lower_case\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdo_lower_case\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# Copied from transformers.models.bert.tokenization_bert_fast.BertTokenizerFast.build_inputs_with_special_tokens\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mbuild_inputs_with_special_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_ids_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_ids_1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"\u001b[0m\n",
      "\u001b[0;34m        Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and\u001b[0m\n",
      "\u001b[0;34m        adding special tokens. A BERT sequence has the following format:\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        - single sequence: `[CLS] X [SEP]`\u001b[0m\n",
      "\u001b[0;34m        - pair of sequences: `[CLS] A [SEP] B [SEP]`\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Args:\u001b[0m\n",
      "\u001b[0;34m            token_ids_0 (`List[int]`):\u001b[0m\n",
      "\u001b[0;34m                List of IDs to which the special tokens will be added.\u001b[0m\n",
      "\u001b[0;34m            token_ids_1 (`List[int]`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                Optional second list of IDs for sequence pairs.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Returns:\u001b[0m\n",
      "\u001b[0;34m            `List[int]`: List of [input IDs](../glossary#input-ids) with the appropriate special tokens.\u001b[0m\n",
      "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcls_token_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtoken_ids_0\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msep_token_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mtoken_ids_1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtoken_ids_1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msep_token_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# Copied from transformers.models.bert.tokenization_bert_fast.BertTokenizerFast.create_token_type_ids_from_sequences\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mcreate_token_type_ids_from_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_ids_0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_ids_1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"\u001b[0m\n",
      "\u001b[0;34m        Create a mask from the two sequences passed to be used in a sequence-pair classification task. A BERT sequence\u001b[0m\n",
      "\u001b[0;34m        pair mask has the following format:\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        ```\u001b[0m\n",
      "\u001b[0;34m        0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\u001b[0m\n",
      "\u001b[0;34m        | first sequence    | second sequence |\u001b[0m\n",
      "\u001b[0;34m        ```\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        If `token_ids_1` is `None`, this method only returns the first portion of the mask (0s).\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Args:\u001b[0m\n",
      "\u001b[0;34m            token_ids_0 (`List[int]`):\u001b[0m\n",
      "\u001b[0;34m                List of IDs.\u001b[0m\n",
      "\u001b[0;34m            token_ids_1 (`List[int]`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                Optional second list of IDs for sequence pairs.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Returns:\u001b[0m\n",
      "\u001b[0;34m            `List[int]`: List of [token type IDs](../glossary#token-type-ids) according to the given sequence(s).\u001b[0m\n",
      "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msep_token_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcls_token_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mtoken_ids_1\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtoken_ids_0\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtoken_ids_0\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_ids_1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# Copied from transformers.models.bert.tokenization_bert_fast.BertTokenizerFast.save_vocabulary\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0msave_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_directory\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename_prefix\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_directory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilename_prefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCall docstring:\u001b[0m\n",
      "Main method to tokenize and prepare for the model one or several sequence(s) or one or several pair(s) of\n",
      "sequences.\n",
      "\n",
      "Args:\n",
      "    text (`str`, `List[str]`, `List[List[str]]`, *optional*):\n",
      "        The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n",
      "        (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n",
      "        `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n",
      "    text_pair (`str`, `List[str]`, `List[List[str]]`, *optional*):\n",
      "        The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n",
      "        (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n",
      "        `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n",
      "    text_target (`str`, `List[str]`, `List[List[str]]`, *optional*):\n",
      "        The sequence or batch of sequences to be encoded as target texts. Each sequence can be a string or a\n",
      "        list of strings (pretokenized string). If the sequences are provided as list of strings (pretokenized),\n",
      "        you must set `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n",
      "    text_pair_target (`str`, `List[str]`, `List[List[str]]`, *optional*):\n",
      "        The sequence or batch of sequences to be encoded as target texts. Each sequence can be a string or a\n",
      "        list of strings (pretokenized string). If the sequences are provided as list of strings (pretokenized),\n",
      "        you must set `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n",
      "\n",
      "    add_special_tokens (`bool`, *optional*, defaults to `True`):\n",
      "        Whether or not to encode the sequences with the special tokens relative to their model.\n",
      "    padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `False`):\n",
      "        Activates and controls padding. Accepts the following values:\n",
      "\n",
      "        - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
      "          sequence if provided).\n",
      "        - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n",
      "          acceptable input length for the model if that argument is not provided.\n",
      "        - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different\n",
      "          lengths).\n",
      "    truncation (`bool`, `str` or [`~tokenization_utils_base.TruncationStrategy`], *optional*, defaults to `False`):\n",
      "        Activates and controls truncation. Accepts the following values:\n",
      "\n",
      "        - `True` or `'longest_first'`: Truncate to a maximum length specified with the argument `max_length` or\n",
      "          to the maximum acceptable input length for the model if that argument is not provided. This will\n",
      "          truncate token by token, removing a token from the longest sequence in the pair if a pair of\n",
      "          sequences (or a batch of pairs) is provided.\n",
      "        - `'only_first'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
      "          maximum acceptable input length for the model if that argument is not provided. This will only\n",
      "          truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      "        - `'only_second'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
      "          maximum acceptable input length for the model if that argument is not provided. This will only\n",
      "          truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      "        - `False` or `'do_not_truncate'` (default): No truncation (i.e., can output batch with sequence lengths\n",
      "          greater than the model maximum admissible input size).\n",
      "    max_length (`int`, *optional*):\n",
      "        Controls the maximum length to use by one of the truncation/padding parameters.\n",
      "\n",
      "        If left unset or set to `None`, this will use the predefined model maximum length if a maximum length\n",
      "        is required by one of the truncation/padding parameters. If the model has no specific maximum input\n",
      "        length (like XLNet) truncation/padding to a maximum length will be deactivated.\n",
      "    stride (`int`, *optional*, defaults to 0):\n",
      "        If set to a number along with `max_length`, the overflowing tokens returned when\n",
      "        `return_overflowing_tokens=True` will contain some tokens from the end of the truncated sequence\n",
      "        returned to provide some overlap between truncated and overflowing sequences. The value of this\n",
      "        argument defines the number of overlapping tokens.\n",
      "    is_split_into_words (`bool`, *optional*, defaults to `False`):\n",
      "        Whether or not the input is already pre-tokenized (e.g., split into words). If set to `True`, the\n",
      "        tokenizer assumes the input is already split into words (for instance, by splitting it on whitespace)\n",
      "        which it will tokenize. This is useful for NER or token classification.\n",
      "    pad_to_multiple_of (`int`, *optional*):\n",
      "        If set will pad the sequence to a multiple of the provided value. Requires `padding` to be activated.\n",
      "        This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability\n",
      "        `>= 7.5` (Volta).\n",
      "    return_tensors (`str` or [`~utils.TensorType`], *optional*):\n",
      "        If set, will return tensors instead of list of python integers. Acceptable values are:\n",
      "\n",
      "        - `'tf'`: Return TensorFlow `tf.constant` objects.\n",
      "        - `'pt'`: Return PyTorch `torch.Tensor` objects.\n",
      "        - `'np'`: Return Numpy `np.ndarray` objects.\n",
      "\n",
      "    return_token_type_ids (`bool`, *optional*):\n",
      "        Whether to return token type IDs. If left to the default, will return the token type IDs according to\n",
      "        the specific tokenizer's default, defined by the `return_outputs` attribute.\n",
      "\n",
      "        [What are token type IDs?](../glossary#token-type-ids)\n",
      "    return_attention_mask (`bool`, *optional*):\n",
      "        Whether to return the attention mask. If left to the default, will return the attention mask according\n",
      "        to the specific tokenizer's default, defined by the `return_outputs` attribute.\n",
      "\n",
      "        [What are attention masks?](../glossary#attention-mask)\n",
      "    return_overflowing_tokens (`bool`, *optional*, defaults to `False`):\n",
      "        Whether or not to return overflowing token sequences. If a pair of sequences of input ids (or a batch\n",
      "        of pairs) is provided with `truncation_strategy = longest_first` or `True`, an error is raised instead\n",
      "        of returning overflowing tokens.\n",
      "    return_special_tokens_mask (`bool`, *optional*, defaults to `False`):\n",
      "        Whether or not to return special tokens mask information.\n",
      "    return_offsets_mapping (`bool`, *optional*, defaults to `False`):\n",
      "        Whether or not to return `(char_start, char_end)` for each token.\n",
      "\n",
      "        This is only available on fast tokenizers inheriting from [`PreTrainedTokenizerFast`], if using\n",
      "        Python's tokenizer, this method will raise `NotImplementedError`.\n",
      "    return_length  (`bool`, *optional*, defaults to `False`):\n",
      "        Whether or not to return the lengths of the encoded inputs.\n",
      "    verbose (`bool`, *optional*, defaults to `True`):\n",
      "        Whether or not to print more information and warnings.\n",
      "    **kwargs: passed to the `self.tokenize()` method\n",
      "\n",
      "Return:\n",
      "    [`BatchEncoding`]: A [`BatchEncoding`] with the following fields:\n",
      "\n",
      "    - **input_ids** -- List of token ids to be fed to a model.\n",
      "\n",
      "      [What are input IDs?](../glossary#input-ids)\n",
      "\n",
      "    - **token_type_ids** -- List of token type ids to be fed to a model (when `return_token_type_ids=True` or\n",
      "      if *\"token_type_ids\"* is in `self.model_input_names`).\n",
      "\n",
      "      [What are token type IDs?](../glossary#token-type-ids)\n",
      "\n",
      "    - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n",
      "      `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names`).\n",
      "\n",
      "      [What are attention masks?](../glossary#attention-mask)\n",
      "\n",
      "    - **overflowing_tokens** -- List of overflowing tokens sequences (when a `max_length` is specified and\n",
      "      `return_overflowing_tokens=True`).\n",
      "    - **num_truncated_tokens** -- Number of tokens truncated (when a `max_length` is specified and\n",
      "      `return_overflowing_tokens=True`).\n",
      "    - **special_tokens_mask** -- List of 0s and 1s, with 1 specifying added special tokens and 0 specifying\n",
      "      regular sequence tokens (when `add_special_tokens=True` and `return_special_tokens_mask=True`).\n",
      "    - **length** -- The length of the inputs (when `return_length=True`)"
     ]
    }
   ],
   "source": [
    "tokenizer??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'input_ids'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([[</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">101</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7483</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1005</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1055</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2374</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2674</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2001</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1996</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4602</span>,\n",
       "          <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1012</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">102</span>,     <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,     <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,     <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,     <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,     <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,     <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,     <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,     <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
       "             <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,     <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,     <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,     <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"font-weight: bold\">[</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">101</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1045</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1005</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1049</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2559</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2830</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2000</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3225</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2026</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">24853</span>,\n",
       "          <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3813</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1012</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">102</span>,     <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,     <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,     <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,     <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,     <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,     <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,     <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
       "             <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,     <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,     <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,     <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"font-weight: bold\">[</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">101</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2028</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1997</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2026</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8837</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">22035</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10421</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2015</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2003</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1005</span>,\n",
       "          <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2035</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2017</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2031</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2003</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2035</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2017</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2342</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">999</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1005</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">102</span>,\n",
       "             <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,     <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,     <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,     <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"font-weight: bold\">[</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">101</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2178</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8837</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1997</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3067</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2003</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1005</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2065</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2017</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2031</span>,\n",
       "          <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2196</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3478</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2012</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2505</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1010</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2017</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2031</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2196</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2699</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2505</span>,\n",
       "          <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2047</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">999</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1005</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">102</span><span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"font-weight: bold\">[</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">101</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2057</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1005</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2310</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2042</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2437</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2200</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3532</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9804</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2004</span>,\n",
       "          <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1037</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3842</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2005</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1037</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2146</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2051</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1012</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">102</span>,     <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,     <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
       "             <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,     <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,     <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,     <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">]])</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'attention_mask'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">]])</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'input_ids'\u001b[0m: \u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m  \u001b[1;36m101\u001b[0m,  \u001b[1;36m7483\u001b[0m,  \u001b[1;36m1005\u001b[0m,  \u001b[1;36m1055\u001b[0m,  \u001b[1;36m2374\u001b[0m,  \u001b[1;36m2674\u001b[0m,  \u001b[1;36m2001\u001b[0m,  \u001b[1;36m2025\u001b[0m,  \u001b[1;36m1996\u001b[0m,  \u001b[1;36m4602\u001b[0m,\n",
       "          \u001b[1;36m1012\u001b[0m,   \u001b[1;36m102\u001b[0m,     \u001b[1;36m0\u001b[0m,     \u001b[1;36m0\u001b[0m,     \u001b[1;36m0\u001b[0m,     \u001b[1;36m0\u001b[0m,     \u001b[1;36m0\u001b[0m,     \u001b[1;36m0\u001b[0m,     \u001b[1;36m0\u001b[0m,     \u001b[1;36m0\u001b[0m,\n",
       "             \u001b[1;36m0\u001b[0m,     \u001b[1;36m0\u001b[0m,     \u001b[1;36m0\u001b[0m,     \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m  \u001b[1;36m101\u001b[0m,  \u001b[1;36m1045\u001b[0m,  \u001b[1;36m1005\u001b[0m,  \u001b[1;36m1049\u001b[0m,  \u001b[1;36m2559\u001b[0m,  \u001b[1;36m2830\u001b[0m,  \u001b[1;36m2000\u001b[0m,  \u001b[1;36m3225\u001b[0m,  \u001b[1;36m2026\u001b[0m, \u001b[1;36m24853\u001b[0m,\n",
       "          \u001b[1;36m3813\u001b[0m,  \u001b[1;36m1012\u001b[0m,   \u001b[1;36m102\u001b[0m,     \u001b[1;36m0\u001b[0m,     \u001b[1;36m0\u001b[0m,     \u001b[1;36m0\u001b[0m,     \u001b[1;36m0\u001b[0m,     \u001b[1;36m0\u001b[0m,     \u001b[1;36m0\u001b[0m,     \u001b[1;36m0\u001b[0m,\n",
       "             \u001b[1;36m0\u001b[0m,     \u001b[1;36m0\u001b[0m,     \u001b[1;36m0\u001b[0m,     \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m  \u001b[1;36m101\u001b[0m,  \u001b[1;36m2028\u001b[0m,  \u001b[1;36m1997\u001b[0m,  \u001b[1;36m2026\u001b[0m,  \u001b[1;36m8837\u001b[0m, \u001b[1;36m22035\u001b[0m, \u001b[1;36m10421\u001b[0m,  \u001b[1;36m2015\u001b[0m,  \u001b[1;36m2003\u001b[0m,  \u001b[1;36m1005\u001b[0m,\n",
       "          \u001b[1;36m2035\u001b[0m,  \u001b[1;36m2017\u001b[0m,  \u001b[1;36m2031\u001b[0m,  \u001b[1;36m2003\u001b[0m,  \u001b[1;36m2035\u001b[0m,  \u001b[1;36m2017\u001b[0m,  \u001b[1;36m2342\u001b[0m,   \u001b[1;36m999\u001b[0m,  \u001b[1;36m1005\u001b[0m,   \u001b[1;36m102\u001b[0m,\n",
       "             \u001b[1;36m0\u001b[0m,     \u001b[1;36m0\u001b[0m,     \u001b[1;36m0\u001b[0m,     \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m  \u001b[1;36m101\u001b[0m,  \u001b[1;36m2178\u001b[0m,  \u001b[1;36m8837\u001b[0m,  \u001b[1;36m1997\u001b[0m,  \u001b[1;36m3067\u001b[0m,  \u001b[1;36m2003\u001b[0m,  \u001b[1;36m1005\u001b[0m,  \u001b[1;36m2065\u001b[0m,  \u001b[1;36m2017\u001b[0m,  \u001b[1;36m2031\u001b[0m,\n",
       "          \u001b[1;36m2196\u001b[0m,  \u001b[1;36m3478\u001b[0m,  \u001b[1;36m2012\u001b[0m,  \u001b[1;36m2505\u001b[0m,  \u001b[1;36m1010\u001b[0m,  \u001b[1;36m2017\u001b[0m,  \u001b[1;36m2031\u001b[0m,  \u001b[1;36m2196\u001b[0m,  \u001b[1;36m2699\u001b[0m,  \u001b[1;36m2505\u001b[0m,\n",
       "          \u001b[1;36m2047\u001b[0m,   \u001b[1;36m999\u001b[0m,  \u001b[1;36m1005\u001b[0m,   \u001b[1;36m102\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m  \u001b[1;36m101\u001b[0m,  \u001b[1;36m2057\u001b[0m,  \u001b[1;36m1005\u001b[0m,  \u001b[1;36m2310\u001b[0m,  \u001b[1;36m2042\u001b[0m,  \u001b[1;36m2437\u001b[0m,  \u001b[1;36m2200\u001b[0m,  \u001b[1;36m3532\u001b[0m,  \u001b[1;36m9804\u001b[0m,  \u001b[1;36m2004\u001b[0m,\n",
       "          \u001b[1;36m1037\u001b[0m,  \u001b[1;36m3842\u001b[0m,  \u001b[1;36m2005\u001b[0m,  \u001b[1;36m1037\u001b[0m,  \u001b[1;36m2146\u001b[0m,  \u001b[1;36m2051\u001b[0m,  \u001b[1;36m1012\u001b[0m,   \u001b[1;36m102\u001b[0m,     \u001b[1;36m0\u001b[0m,     \u001b[1;36m0\u001b[0m,\n",
       "             \u001b[1;36m0\u001b[0m,     \u001b[1;36m0\u001b[0m,     \u001b[1;36m0\u001b[0m,     \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[32m'attention_mask'\u001b[0m: \u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rich import print\n",
    "\n",
    "\n",
    "# Method 2: Manual Approach\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "input = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "print(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">SequenceClassifierOutput</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">loss</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">logits</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.5847</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-2.9688</span><span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-2.9588</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.1426</span><span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-2.4516</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.6034</span><span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-2.7536</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.9178</span><span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.4035</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-3.5980</span><span style=\"font-weight: bold\">]]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">grad_fn</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">AddmmBackward0</span><span style=\"font-weight: bold\">&gt;)</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">hidden_states</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">attentions</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mSequenceClassifierOutput\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mloss\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "    \u001b[33mlogits\u001b[0m=\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m3.5847\u001b[0m, \u001b[1;36m-2.9688\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m\u001b[1;36m-2.9588\u001b[0m,  \u001b[1;36m3.1426\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m\u001b[1;36m-2.4516\u001b[0m,  \u001b[1;36m2.6034\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m\u001b[1;36m-2.7536\u001b[0m,  \u001b[1;36m2.9178\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m \u001b[1;36m4.4035\u001b[0m, \u001b[1;36m-3.5980\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mgrad_fn\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mAddmmBackward0\u001b[0m\u001b[1m>\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[33mhidden_states\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "    \u001b[33mattentions\u001b[0m=\u001b[3;35mNone\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clf_model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "output = clf_model(**input)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9.9858e-01</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.4231e-03</span><span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.2348e-03</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9.9777e-01</span><span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6.3375e-03</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9.9366e-01</span><span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.4311e-03</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9.9657e-01</span><span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9.9967e-01</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.3486e-04</span><span style=\"font-weight: bold\">]]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">grad_fn</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">SoftmaxBackward0</span><span style=\"font-weight: bold\">&gt;)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m9.9858e-01\u001b[0m, \u001b[1;36m1.4231e-03\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m\u001b[1;36m2.2348e-03\u001b[0m, \u001b[1;36m9.9777e-01\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m\u001b[1;36m6.3375e-03\u001b[0m, \u001b[1;36m9.9366e-01\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m\u001b[1;36m3.4311e-03\u001b[0m, \u001b[1;36m9.9657e-01\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m\u001b[1;36m9.9967e-01\u001b[0m, \u001b[1;36m3.3486e-04\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mgrad_fn\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mSoftmaxBackward0\u001b[0m\u001b[1m>\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "logits_ = output.logits\n",
    "prob = F.softmax(logits_, dim=1)\n",
    "print(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'NEGATIVE', 1: 'POSITIVE'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Label names\n",
    "clf_model.config.id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "```text\n",
    "\n",
    "- The `AutoModel` class is a simple wrapper over the wide variety of models available in the library. \n",
    "- It can automatically guess the appropriate model architecture for your checkpoint, and then instantiates a model with this architecture.\n",
    "- If you know the type of model you want to use, you can use the class that defines its architecture directly. \n",
    "- For example, to use a BERT model, you can use the BertModel class.\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "### Creating a Transformer\n",
    "\n",
    "```text\n",
    "\n",
    "The first thing we‚Äôll need to do to initialize a BERT model is load a configuration object:\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, BertModel\n",
    "\n",
    "\n",
    "# Building the config\n",
    "config = BertConfig()\n",
    "\n",
    "# Building the model from the config\n",
    "# Weights are initialized randomly\n",
    "model = BertModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">BertConfig <span style=\"font-weight: bold\">{</span>\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"attention_probs_dropout_prob\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.1</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"classifier_dropout\"</span>: null,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"hidden_act\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"gelu\"</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"hidden_dropout_prob\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.1</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"hidden_size\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">768</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"initializer_range\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.02</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"intermediate_size\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3072</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"layer_norm_eps\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1e-12</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"max_position_embeddings\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">512</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"model_type\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"bert\"</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"num_attention_heads\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"num_hidden_layers\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"pad_token_id\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"position_embedding_type\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"absolute\"</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"transformers_version\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"4.32.0\"</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"type_vocab_size\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"use_cache\"</span>: true,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"vocab_size\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">30522</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "BertConfig \u001b[1m{\u001b[0m\n",
       "  \u001b[32m\"attention_probs_dropout_prob\"\u001b[0m: \u001b[1;36m0.1\u001b[0m,\n",
       "  \u001b[32m\"classifier_dropout\"\u001b[0m: null,\n",
       "  \u001b[32m\"hidden_act\"\u001b[0m: \u001b[32m\"gelu\"\u001b[0m,\n",
       "  \u001b[32m\"hidden_dropout_prob\"\u001b[0m: \u001b[1;36m0.1\u001b[0m,\n",
       "  \u001b[32m\"hidden_size\"\u001b[0m: \u001b[1;36m768\u001b[0m,\n",
       "  \u001b[32m\"initializer_range\"\u001b[0m: \u001b[1;36m0.02\u001b[0m,\n",
       "  \u001b[32m\"intermediate_size\"\u001b[0m: \u001b[1;36m3072\u001b[0m,\n",
       "  \u001b[32m\"layer_norm_eps\"\u001b[0m: \u001b[1;36m1e-12\u001b[0m,\n",
       "  \u001b[32m\"max_position_embeddings\"\u001b[0m: \u001b[1;36m512\u001b[0m,\n",
       "  \u001b[32m\"model_type\"\u001b[0m: \u001b[32m\"bert\"\u001b[0m,\n",
       "  \u001b[32m\"num_attention_heads\"\u001b[0m: \u001b[1;36m12\u001b[0m,\n",
       "  \u001b[32m\"num_hidden_layers\"\u001b[0m: \u001b[1;36m12\u001b[0m,\n",
       "  \u001b[32m\"pad_token_id\"\u001b[0m: \u001b[1;36m0\u001b[0m,\n",
       "  \u001b[32m\"position_embedding_type\"\u001b[0m: \u001b[32m\"absolute\"\u001b[0m,\n",
       "  \u001b[32m\"transformers_version\"\u001b[0m: \u001b[32m\"4.32.0\"\u001b[0m,\n",
       "  \u001b[32m\"type_vocab_size\"\u001b[0m: \u001b[1;36m2\u001b[0m,\n",
       "  \u001b[32m\"use_cache\"\u001b[0m: true,\n",
       "  \u001b[32m\"vocab_size\"\u001b[0m: \u001b[1;36m30522\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The configuration contains many attributes that are used to build the model:\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different loading methods\n",
    "```text\n",
    "\n",
    "Creating a model from the default configuration initializes it with random values:\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, BertModel\n",
    "from transformers import BertModel\n",
    "\n",
    "\n",
    "# Load the pretrained model weights\n",
    "model = BertModel.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The AutoModel class is a checkpoint-agnostic wrapper over the wide variety of models available in the library. This means that if your code works for one checkpoint, it should work seamlessly with another, even if the architecture is different.\n",
    "\n",
    "\n",
    "- In the code sample above we didn‚Äôt use BertConfig, and instead loaded a pretrained model via the bert-base-cased identifier. This is a model checkpoint that was trained by the authors of BERT themselves; you can find more details about it in its [model card](https://huggingface.co/bert-base-cased).\n",
    "\n",
    "- This model is now initialized with all the weights of the checkpoint. It can be used directly for inference on the tasks it was trained on, and it can also be fine-tuned on a new task. By training with pretrained weights rather than from scratch, we can quickly achieve good results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Saving methods\n",
    "\n",
    "```text\n",
    "-  `save_pretrained()` method is used for saving the model.\n",
    "```\n",
    "<br>\n",
    "\n",
    "```python\n",
    "model.save_pretrained(\"directory_on_my_computer\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Using a Transformer Model For Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">101</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7592</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">999</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">102</span><span style=\"font-weight: bold\">]</span>, <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">101</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4658</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1012</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">102</span><span style=\"font-weight: bold\">]</span>, <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">101</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3835</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">999</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">102</span><span style=\"font-weight: bold\">]]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m101\u001b[0m, \u001b[1;36m7592\u001b[0m, \u001b[1;36m999\u001b[0m, \u001b[1;36m102\u001b[0m\u001b[1m]\u001b[0m, \u001b[1m[\u001b[0m\u001b[1;36m101\u001b[0m, \u001b[1;36m4658\u001b[0m, \u001b[1;36m1012\u001b[0m, \u001b[1;36m102\u001b[0m\u001b[1m]\u001b[0m, \u001b[1m[\u001b[0m\u001b[1;36m101\u001b[0m, \u001b[1;36m3835\u001b[0m, \u001b[1;36m999\u001b[0m, \u001b[1;36m102\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sequences = [\"Hello!\", \"Cool.\", \"Nice!\"]\n",
    "\n",
    "encoded_sequences = tokenizer(sequences).get(\"input_ids\")\n",
    "print(encoded_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the tensors as inputs to the model\n",
    "\n",
    "```text\n",
    "\n",
    "- Making use of the tensors with the model is extremely simple.\n",
    "‚Äî We just call the model with the inputs:\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">BaseModelOutputWithPoolingAndCrossAttentions</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">last_hidden_state</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([[[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.4496e-01</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.8276e-01</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.7797e-01</span>,  <span style=\"color: #808000; text-decoration-color: #808000\">...</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-5.4033e-02</span>,\n",
       "           <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.9394e-01</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-9.4770e-02</span><span style=\"font-weight: bold\">]</span>,\n",
       "         <span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.4943e-01</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-4.4093e-01</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8.1772e-01</span>,  <span style=\"color: #808000; text-decoration-color: #808000\">...</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-3.1917e-01</span>,\n",
       "           <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.2992e-01</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-4.1172e-02</span><span style=\"font-weight: bold\">]</span>,\n",
       "         <span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.3668e-01</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.2518e-01</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.4502e-01</span>,  <span style=\"color: #808000; text-decoration-color: #808000\">...</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-4.6914e-02</span>,\n",
       "           <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.8224e-01</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7.5566e-02</span><span style=\"font-weight: bold\">]</span>,\n",
       "         <span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.1789e+00</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.6738e-01</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.8187e-01</span>,  <span style=\"color: #808000; text-decoration-color: #808000\">...</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.4671e-01</span>,\n",
       "           <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0441e+00</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-6.1966e-03</span><span style=\"font-weight: bold\">]]</span>,\n",
       "\n",
       "        <span style=\"font-weight: bold\">[[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.6436e-01</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.2464e-02</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.0258e-01</span>,  <span style=\"color: #808000; text-decoration-color: #808000\">...</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6.0111e-02</span>,\n",
       "           <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.2451e-01</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-2.0996e-02</span><span style=\"font-weight: bold\">]</span>,\n",
       "         <span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7.1866e-01</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-4.8725e-01</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5.1740e-01</span>,  <span style=\"color: #808000; text-decoration-color: #808000\">...</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-4.4012e-01</span>,\n",
       "           <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.4553e-01</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-3.7545e-02</span><span style=\"font-weight: bold\">]</span>,\n",
       "         <span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.3223e-01</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-2.3271e-01</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9.4876e-02</span>,  <span style=\"color: #808000; text-decoration-color: #808000\">...</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-2.5268e-01</span>,\n",
       "           <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.2172e-01</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8.1097e-04</span><span style=\"font-weight: bold\">]</span>,\n",
       "         <span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.2523e+00</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.5754e-01</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-5.1320e-02</span>,  <span style=\"color: #808000; text-decoration-color: #808000\">...</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-3.7840e-01</span>,\n",
       "           <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0526e+00</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-5.6255e-01</span><span style=\"font-weight: bold\">]]</span>,\n",
       "\n",
       "        <span style=\"font-weight: bold\">[[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.4042e-01</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.4718e-01</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.2110e-01</span>,  <span style=\"color: #808000; text-decoration-color: #808000\">...</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7.6062e-02</span>,\n",
       "           <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.3564e-01</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.8262e-01</span><span style=\"font-weight: bold\">]</span>,\n",
       "         <span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6.5701e-01</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-3.2787e-01</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.4968e-01</span>,  <span style=\"color: #808000; text-decoration-color: #808000\">...</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-2.5920e-01</span>,\n",
       "           <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.0175e-01</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.3275e-01</span><span style=\"font-weight: bold\">]</span>,\n",
       "         <span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.0160e-01</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.5783e-01</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9.8970e-03</span>,  <span style=\"color: #808000; text-decoration-color: #808000\">...</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-3.8850e-01</span>,\n",
       "           <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.1307e-01</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.9732e-01</span><span style=\"font-weight: bold\">]</span>,\n",
       "         <span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0175e+00</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6.4387e-01</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-7.8147e-01</span>,  <span style=\"color: #808000; text-decoration-color: #808000\">...</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-4.2109e-01</span>,\n",
       "           <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0925e+00</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-4.8456e-02</span><span style=\"font-weight: bold\">]]]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">grad_fn</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">NativeLayerNormBackward0</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    </span><span style=\"color: #808000; text-decoration-color: #808000\">pooler_output</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">([[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.6856</span><span style=\"color: #000000; text-decoration-color: #000000\">,  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.5262</span><span style=\"color: #000000; text-decoration-color: #000000\">,  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0000</span><span style=\"color: #000000; text-decoration-color: #000000\">,  </span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"color: #000000; text-decoration-color: #000000\">,  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0000</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.6112</span><span style=\"color: #000000; text-decoration-color: #000000\">,  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9971</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">]</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">        </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.6055</span><span style=\"color: #000000; text-decoration-color: #000000\">,  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.4997</span><span style=\"color: #000000; text-decoration-color: #000000\">,  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9998</span><span style=\"color: #000000; text-decoration-color: #000000\">,  </span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"color: #000000; text-decoration-color: #000000\">,  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9999</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.6753</span><span style=\"color: #000000; text-decoration-color: #000000\">,  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9769</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">]</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">        </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.7702</span><span style=\"color: #000000; text-decoration-color: #000000\">,  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.5447</span><span style=\"color: #000000; text-decoration-color: #000000\">,  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9999</span><span style=\"color: #000000; text-decoration-color: #000000\">,  </span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"color: #000000; text-decoration-color: #000000\">,  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0000</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.4655</span><span style=\"color: #000000; text-decoration-color: #000000\">,  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9894</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">]]</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">       </span><span style=\"color: #808000; text-decoration-color: #808000\">grad_fn</span><span style=\"color: #000000; text-decoration-color: #000000\">=&lt;TanhBackward0</span><span style=\"font-weight: bold\">&gt;)</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">hidden_states</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">past_key_values</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">attentions</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">cross_attentions</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mBaseModelOutputWithPoolingAndCrossAttentions\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mlast_hidden_state\u001b[0m=\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m4.4496e-01\u001b[0m,  \u001b[1;36m4.8276e-01\u001b[0m,  \u001b[1;36m2.7797e-01\u001b[0m,  \u001b[33m...\u001b[0m, \u001b[1;36m-5.4033e-02\u001b[0m,\n",
       "           \u001b[1;36m3.9394e-01\u001b[0m, \u001b[1;36m-9.4770e-02\u001b[0m\u001b[1m]\u001b[0m,\n",
       "         \u001b[1m[\u001b[0m \u001b[1;36m2.4943e-01\u001b[0m, \u001b[1;36m-4.4093e-01\u001b[0m,  \u001b[1;36m8.1772e-01\u001b[0m,  \u001b[33m...\u001b[0m, \u001b[1;36m-3.1917e-01\u001b[0m,\n",
       "           \u001b[1;36m2.2992e-01\u001b[0m, \u001b[1;36m-4.1172e-02\u001b[0m\u001b[1m]\u001b[0m,\n",
       "         \u001b[1m[\u001b[0m \u001b[1;36m1.3668e-01\u001b[0m,  \u001b[1;36m2.2518e-01\u001b[0m,  \u001b[1;36m1.4502e-01\u001b[0m,  \u001b[33m...\u001b[0m, \u001b[1;36m-4.6914e-02\u001b[0m,\n",
       "           \u001b[1;36m2.8224e-01\u001b[0m,  \u001b[1;36m7.5566e-02\u001b[0m\u001b[1m]\u001b[0m,\n",
       "         \u001b[1m[\u001b[0m \u001b[1;36m1.1789e+00\u001b[0m,  \u001b[1;36m1.6738e-01\u001b[0m, \u001b[1;36m-1.8187e-01\u001b[0m,  \u001b[33m...\u001b[0m,  \u001b[1;36m2.4671e-01\u001b[0m,\n",
       "           \u001b[1;36m1.0441e+00\u001b[0m, \u001b[1;36m-6.1966e-03\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\n",
       "        \u001b[1m[\u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m3.6436e-01\u001b[0m,  \u001b[1;36m3.2464e-02\u001b[0m,  \u001b[1;36m2.0258e-01\u001b[0m,  \u001b[33m...\u001b[0m,  \u001b[1;36m6.0111e-02\u001b[0m,\n",
       "           \u001b[1;36m3.2451e-01\u001b[0m, \u001b[1;36m-2.0996e-02\u001b[0m\u001b[1m]\u001b[0m,\n",
       "         \u001b[1m[\u001b[0m \u001b[1;36m7.1866e-01\u001b[0m, \u001b[1;36m-4.8725e-01\u001b[0m,  \u001b[1;36m5.1740e-01\u001b[0m,  \u001b[33m...\u001b[0m, \u001b[1;36m-4.4012e-01\u001b[0m,\n",
       "           \u001b[1;36m1.4553e-01\u001b[0m, \u001b[1;36m-3.7545e-02\u001b[0m\u001b[1m]\u001b[0m,\n",
       "         \u001b[1m[\u001b[0m \u001b[1;36m3.3223e-01\u001b[0m, \u001b[1;36m-2.3271e-01\u001b[0m,  \u001b[1;36m9.4876e-02\u001b[0m,  \u001b[33m...\u001b[0m, \u001b[1;36m-2.5268e-01\u001b[0m,\n",
       "           \u001b[1;36m3.2172e-01\u001b[0m,  \u001b[1;36m8.1097e-04\u001b[0m\u001b[1m]\u001b[0m,\n",
       "         \u001b[1m[\u001b[0m \u001b[1;36m1.2523e+00\u001b[0m,  \u001b[1;36m3.5754e-01\u001b[0m, \u001b[1;36m-5.1320e-02\u001b[0m,  \u001b[33m...\u001b[0m, \u001b[1;36m-3.7840e-01\u001b[0m,\n",
       "           \u001b[1;36m1.0526e+00\u001b[0m, \u001b[1;36m-5.6255e-01\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\n",
       "        \u001b[1m[\u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m2.4042e-01\u001b[0m,  \u001b[1;36m1.4718e-01\u001b[0m,  \u001b[1;36m1.2110e-01\u001b[0m,  \u001b[33m...\u001b[0m,  \u001b[1;36m7.6062e-02\u001b[0m,\n",
       "           \u001b[1;36m3.3564e-01\u001b[0m,  \u001b[1;36m2.8262e-01\u001b[0m\u001b[1m]\u001b[0m,\n",
       "         \u001b[1m[\u001b[0m \u001b[1;36m6.5701e-01\u001b[0m, \u001b[1;36m-3.2787e-01\u001b[0m,  \u001b[1;36m2.4968e-01\u001b[0m,  \u001b[33m...\u001b[0m, \u001b[1;36m-2.5920e-01\u001b[0m,\n",
       "           \u001b[1;36m2.0175e-01\u001b[0m,  \u001b[1;36m3.3275e-01\u001b[0m\u001b[1m]\u001b[0m,\n",
       "         \u001b[1m[\u001b[0m \u001b[1;36m2.0160e-01\u001b[0m,  \u001b[1;36m1.5783e-01\u001b[0m,  \u001b[1;36m9.8970e-03\u001b[0m,  \u001b[33m...\u001b[0m, \u001b[1;36m-3.8850e-01\u001b[0m,\n",
       "           \u001b[1;36m4.1307e-01\u001b[0m,  \u001b[1;36m3.9732e-01\u001b[0m\u001b[1m]\u001b[0m,\n",
       "         \u001b[1m[\u001b[0m \u001b[1;36m1.0175e+00\u001b[0m,  \u001b[1;36m6.4387e-01\u001b[0m, \u001b[1;36m-7.8147e-01\u001b[0m,  \u001b[33m...\u001b[0m, \u001b[1;36m-4.2109e-01\u001b[0m,\n",
       "           \u001b[1;36m1.0925e+00\u001b[0m, \u001b[1;36m-4.8456e-02\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mgrad_fn\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mNativeLayerNormBackward0\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m    \u001b[0m\u001b[33mpooler_output\u001b[0m\u001b[39m=\u001b[0m\u001b[1;35mtensor\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m[\u001b[0m\u001b[1;39m[\u001b[0m\u001b[1;36m-0.6856\u001b[0m\u001b[39m,  \u001b[0m\u001b[1;36m0.5262\u001b[0m\u001b[39m,  \u001b[0m\u001b[1;36m1.0000\u001b[0m\u001b[39m,  \u001b[0m\u001b[33m...\u001b[0m\u001b[39m,  \u001b[0m\u001b[1;36m1.0000\u001b[0m\u001b[39m, \u001b[0m\u001b[1;36m-0.6112\u001b[0m\u001b[39m,  \u001b[0m\u001b[1;36m0.9971\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m        \u001b[0m\u001b[1;39m[\u001b[0m\u001b[1;36m-0.6055\u001b[0m\u001b[39m,  \u001b[0m\u001b[1;36m0.4997\u001b[0m\u001b[39m,  \u001b[0m\u001b[1;36m0.9998\u001b[0m\u001b[39m,  \u001b[0m\u001b[33m...\u001b[0m\u001b[39m,  \u001b[0m\u001b[1;36m0.9999\u001b[0m\u001b[39m, \u001b[0m\u001b[1;36m-0.6753\u001b[0m\u001b[39m,  \u001b[0m\u001b[1;36m0.9769\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m        \u001b[0m\u001b[1;39m[\u001b[0m\u001b[1;36m-0.7702\u001b[0m\u001b[39m,  \u001b[0m\u001b[1;36m0.5447\u001b[0m\u001b[39m,  \u001b[0m\u001b[1;36m0.9999\u001b[0m\u001b[39m,  \u001b[0m\u001b[33m...\u001b[0m\u001b[39m,  \u001b[0m\u001b[1;36m1.0000\u001b[0m\u001b[39m, \u001b[0m\u001b[1;36m-0.4655\u001b[0m\u001b[39m,  \u001b[0m\u001b[1;36m0.9894\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m       \u001b[0m\u001b[33mgrad_fn\u001b[0m\u001b[39m=<TanhBackward0\u001b[0m\u001b[1m>\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[33mhidden_states\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "    \u001b[33mpast_key_values\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "    \u001b[33mattentions\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "    \u001b[33mcross_attentions\u001b[0m=\u001b[3;35mNone\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input = torch.tensor(encoded_sequences)\n",
    "output = model(input)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizers\n",
    "\n",
    "```text\n",
    "- Tokenizers are one of the core components of the NLP pipeline. \n",
    "- They're used to translate text into data that can be processed by the model. \n",
    "- Since models can only process numbers, tokenizers are required to convert the text inputs to numerical data. \n",
    "\n",
    "- In NLP tasks, the data that is generally processed is raw text. Here‚Äôs an example of such text:\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Word-based Tokenizers\n",
    "\n",
    "[![image.png](https://i.postimg.cc/hvHrwMbk/image.png)](https://postimg.cc/YLzYL6PR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'Jim'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Henson'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'was'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'a'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'puppeteer'</span><span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[32m'Jim'\u001b[0m, \u001b[32m'Henson'\u001b[0m, \u001b[32m'was'\u001b[0m, \u001b[32m'a'\u001b[0m, \u001b[32m'puppeteer'\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_text = \"Jim Henson was a puppeteer\".split()\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```text\n",
    "\n",
    "- Word tokenizers can have additional rules for handling punctuation, resulting in larger vocabularies. \n",
    "- A vocabulary refers to the total number of distinct tokens in a corpus.\n",
    "\n",
    "- Word-based tokenization assigns a unique ID to each word, starting from 0 and going up to the size of the vocabulary. \n",
    "- However, covering an entire language with word-based tokenization requires a massive number of tokens. \n",
    "- e.g., the English language alone has over 500,000 words, necessitating the tracking of that many IDs. \n",
    "- Moreover, variations of words, such as \"dog\" and \"dogs\" or \"run\" and \"running,\" are considered unrelated by the model initially, lacking the understanding of their similarity.\n",
    "\n",
    "- To handle words not in the vocabulary, a custom token called the \"unknown\" token is used, typically denoted as \"[UNK]\" or \"\". \n",
    "- If the tokenizer generates many unknown tokens, it indicates a loss of information and a suboptimal representation of words.\n",
    "- The aim when creating the vocabulary is to minimize the number of words tokenized into the unknown token. \n",
    "- One approach to achieve this is by using a character-based tokenizer, which delves deeper into the structure of words.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Character-based\n",
    "\n",
    "```text\n",
    "- Character-based tokenizers split the text into characters, rather than words. This has two primary benefits:\n",
    "  - The vocabulary is much smaller.\n",
    "  - There are much fewer out-of-vocabulary (unknown) tokens, since every word can be built from characters.\n",
    "\n",
    "Limitations\n",
    "-----------\n",
    "- Using a character-based tokenizer has its limitations. \n",
    "- While it captures more information in languages like Chinese, where characters carry meaning, it may be less meaningful in languages using Latin characters. \n",
    "- Additionally, this approach results in a larger number of tokens for the model to process. \n",
    "- A single word token in a word-based tokenizer can transform into 10 or more tokens in a character-based tokenizer.\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "[![image.png](https://i.postimg.cc/HLd9MrLC/image.png)](https://postimg.cc/68bZJ5RH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### Subword tokenization\n",
    "\n",
    "```text\n",
    "- Subword tokenization algorithms rely on the principle that frequently used words should not be split into smaller subwords, but rare words should be decomposed into meaningful subwords.\n",
    "- For instance, ‚Äúannoyingly‚Äù might be considered a rare word and could be decomposed into ‚Äúannoying‚Äù and ‚Äúly‚Äù. \n",
    "- These are both likely to appear more frequently as standalone subwords, while at the same time the meaning of ‚Äúannoyingly‚Äù is kept by the composite meaning of ‚Äúannoying‚Äù and ‚Äúly‚Äù.\n",
    "```\n",
    "<br>\n",
    "\n",
    "> Here is an example showing how a subword tokenization algorithm would tokenize the sequence `Let‚Äôs do tokenization!`:\n",
    "\n",
    "\n",
    "```text\n",
    "Let‚Äôs</w> | do | token | ization |  !</w>\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "```text\n",
    "- Subword tokenization provides semantic meaning by splitting words into smaller units, enabling efficient representation and good coverage with minimal unknown tokens.\n",
    "- E.g. in the example above ‚Äútokenization‚Äù was split into ‚Äútoken‚Äù and ‚Äúization‚Äù, two tokens that have a semantic meaning while being space-efficient (only two tokens are needed to represent a long word).\n",
    "- This approach is especially useful in agglutinative languages such as Turkish, where you can form (almost) arbitrarily long complex words by stringing together subwords.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### Other Types of Subword Tokenizers\n",
    "\n",
    "```text\n",
    "\n",
    "- There are many more techniques out there. To name a few:\n",
    "  - Byte-level BPE, as used in GPT-2\n",
    "  - WordPiece, as used in BERT\n",
    "  - SentencePiece or Unigram, as used in several multilingual models\n",
    "  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and Saving\n",
    "\n",
    "```text\n",
    "Loading and saving tokenizers is based on the same two methods: `from_pretrained()` and s`ave_pretrained()`. \n",
    "- These methods will load or save the algorithm used by the tokenizer (a bit like the architecture of the model) as well as its vocabulary (a bit like the weights of the model).\n",
    "- Loading the BERT tokenizer trained with the same checkpoint as BERT is done the same way as loading the model, except we use the BertTokenizer class:\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 7993, 170, 13809, 23763, 2443, 1110, 3014, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "# Similar to AutoModel, the AutoTokenizer class will grab the proper tokenizer class in the library based\n",
    "# on the checkpoint name, and can be used directly with any checkpoint:\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "tokenizer(\"Using a Transformer network is simple\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Saving a tokenizer is identical to saving a model:\n",
    "  \n",
    "```python\n",
    "tokenizer.save_pretrained(\"directory_on_my_computer\")\n",
    "```\n",
    "\n",
    "<br><hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding\n",
    "\n",
    "```text\n",
    "- Translating text to numbers is known as encoding. \n",
    "- Encoding is done in a two-step process: the tokenization, followed by the conversion to input IDs.\n",
    "\n",
    "- As we‚Äôve seen, the first step is to split the text into words (or parts of words, punctuation symbols, etc.), usually called tokens. \n",
    "- There are multiple rules that can govern that process, which is why we need to instantiate the tokenizer using the name of the model, to make sure we use the same rules that were used when the model was pretrained.\n",
    "\n",
    "- The second step is to convert those tokens into numbers, so we can build a tensor out of them and feed them to the model. To do this, the tokenizer has a vocabulary, which is the part we download when we instantiate it with the from_pretrained() method. \n",
    "- Again, we need to use the same vocabulary used when the model was pretrained.\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "[![image.png](https://i.postimg.cc/t456dZBQ/image.png)](https://postimg.cc/k22Dq4rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Tokenization\n",
    "\n",
    "```text\n",
    "- The tokenization process is done by the tokenize() method of the tokenizer:\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'Using'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'a'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Trans'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'##former'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'network'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'is'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'simple'</span><span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[32m'Using'\u001b[0m, \u001b[32m'a'\u001b[0m, \u001b[32m'Trans'\u001b[0m, \u001b[32m'##former'\u001b[0m, \u001b[32m'network'\u001b[0m, \u001b[32m'is'\u001b[0m, \u001b[32m'simple'\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "sequence = \"Using a Transformer network is simple\"\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Using', 'a', 'transform', '##er', 'network', 'is', 'simple']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This tokenizer is a subword tokenizer: it splits the words until it obtains tokens that\n",
    "# can be represented by its vocabulary. That‚Äôs the case here with transformer, which\n",
    "# is split into two tokens: transform and ##er.\n",
    "\n",
    "[\"Using\", \"a\", \"transform\", \"##er\", \"network\", \"is\", \"simple\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From tokens to input IDs\n",
    "\n",
    "```text\n",
    "The conversion to input IDs is handled by the convert_tokens_to_ids() tokenizer method:\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7993</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">170</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13809</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">23763</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2443</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1110</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3014</span><span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[1;36m7993\u001b[0m, \u001b[1;36m170\u001b[0m, \u001b[1;36m13809\u001b[0m, \u001b[1;36m23763\u001b[0m, \u001b[1;36m2443\u001b[0m, \u001b[1;36m1110\u001b[0m, \u001b[1;36m3014\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 2:\n",
    "\n",
    "```text \n",
    "- Replicate the two last steps (tokenization and conversion to input IDs) on the input sentences we used in section 2. \n",
    "- i.e. (‚ÄúI‚Äôve been waiting for a HuggingFace course my whole life.‚Äù and ‚ÄúI hate this so much!‚Äù). \n",
    "- Check that you get the same input IDs we got earlier!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'i'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"'\"</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'ve'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'been'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'waiting'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'for'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'a'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'hugging'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'##face'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'course'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'my'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'whole'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'life'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'.'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'i'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'hate'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'this'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'so'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'much'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'!'</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[32m'i'\u001b[0m,\n",
       "    \u001b[32m\"'\"\u001b[0m,\n",
       "    \u001b[32m've'\u001b[0m,\n",
       "    \u001b[32m'been'\u001b[0m,\n",
       "    \u001b[32m'waiting'\u001b[0m,\n",
       "    \u001b[32m'for'\u001b[0m,\n",
       "    \u001b[32m'a'\u001b[0m,\n",
       "    \u001b[32m'hugging'\u001b[0m,\n",
       "    \u001b[32m'##face'\u001b[0m,\n",
       "    \u001b[32m'course'\u001b[0m,\n",
       "    \u001b[32m'my'\u001b[0m,\n",
       "    \u001b[32m'whole'\u001b[0m,\n",
       "    \u001b[32m'life'\u001b[0m,\n",
       "    \u001b[32m'.'\u001b[0m,\n",
       "    \u001b[32m'i'\u001b[0m,\n",
       "    \u001b[32m'hate'\u001b[0m,\n",
       "    \u001b[32m'this'\u001b[0m,\n",
       "    \u001b[32m'so'\u001b[0m,\n",
       "    \u001b[32m'much'\u001b[0m,\n",
       "    \u001b[32m'!'\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "# checkpoint = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "raw_inputs = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"I hate this so much!\",\n",
    "]\n",
    "tokens = tokenizer.tokenize(raw_inputs)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1045</span>,\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1005</span>,\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2310</span>,\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2042</span>,\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3403</span>,\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2005</span>,\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1037</span>,\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17662</span>,\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12172</span>,\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2607</span>,\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2026</span>,\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2878</span>,\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2166</span>,\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1012</span>,\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1045</span>,\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5223</span>,\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2023</span>,\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2061</span>,\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2172</span>,\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">999</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[1;36m1045\u001b[0m,\n",
       "    \u001b[1;36m1005\u001b[0m,\n",
       "    \u001b[1;36m2310\u001b[0m,\n",
       "    \u001b[1;36m2042\u001b[0m,\n",
       "    \u001b[1;36m3403\u001b[0m,\n",
       "    \u001b[1;36m2005\u001b[0m,\n",
       "    \u001b[1;36m1037\u001b[0m,\n",
       "    \u001b[1;36m17662\u001b[0m,\n",
       "    \u001b[1;36m12172\u001b[0m,\n",
       "    \u001b[1;36m2607\u001b[0m,\n",
       "    \u001b[1;36m2026\u001b[0m,\n",
       "    \u001b[1;36m2878\u001b[0m,\n",
       "    \u001b[1;36m2166\u001b[0m,\n",
       "    \u001b[1;36m1012\u001b[0m,\n",
       "    \u001b[1;36m1045\u001b[0m,\n",
       "    \u001b[1;36m5223\u001b[0m,\n",
       "    \u001b[1;36m2023\u001b[0m,\n",
       "    \u001b[1;36m2061\u001b[0m,\n",
       "    \u001b[1;36m2172\u001b[0m,\n",
       "    \u001b[1;36m999\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "# The only difference between this and the prev. result is the lack of start and end tokens.\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prev Result\n",
    "\n",
    "```python\n",
    "\n",
    "{\n",
    "  'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]]),\n",
    "  'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
    "          2607,  2026,  2878,  2166,  1012,   102],\n",
    "        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0]])\n",
    "}\n",
    "```\n",
    "\n",
    "<hr><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding\n",
    "\n",
    "```text\n",
    "- Decoding is going the other way around: from vocabulary indices, we want to get a string. \n",
    "- This can be done with the decode() method as follows:\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1848</span>  needing —Ö included …ë degree\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m1848\u001b[0m  needing —Ö included …ë degree\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "decoded_string = tokenizer.decode([7993, 170, 11303, 1200, 2443, 1110, 3014])\n",
    "print(decoded_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note \n",
    "\n",
    "```text\n",
    "- Note that the decode method not only converts the indices back to tokens, but also groups together the tokens that were part of the same words to produce a readable sentence. \n",
    "- This behavior will be extremely useful when we use models that predict new text (either text generated from a prompt, or for sequence-to-sequence problems like translation or summarization).\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Handling Multiple Sequences](https://huggingface.co/learn/nlp-course/chapter2/5?fw=pt)\n",
    "\n",
    "```text\n",
    "- In the previous section, we explored the simplest of use cases: doing inference on a single sequence of a small length. \n",
    "- However, some questions emerge already:\n",
    "\n",
    "  - How do we handle multiple sequences?\n",
    "  - How do we handle multiple sequences of different lengths?\n",
    "  - Are vocabulary indices the only inputs that allow a model to work well?\n",
    "  - Is there such a thing as too long a sequence?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models expect a batch of inputs\n",
    "\n",
    "\n",
    "```text\n",
    "- In the previous exercise you saw how sequences get translated into lists of numbers. \n",
    "- Let‚Äôs convert this list of numbers to a tensor and send it to the model:\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Logits: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-2.7276</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.8789</span><span style=\"font-weight: bold\">]]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">grad_fn</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">AddmmBackward0</span><span style=\"font-weight: bold\">&gt;)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Logits: \u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m-2.7276\u001b[0m,  \u001b[1;36m2.8789\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mgrad_fn\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mAddmmBackward0\u001b[0m\u001b[1m>\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "# Convert to a 2D Tensor\n",
    "input_ids = torch.tensor(ids).view(1, -1)\n",
    "\n",
    "output = model(input_ids)\n",
    "\n",
    "print(\"Logits:\", output.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch.Size</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14</span><span style=\"font-weight: bold\">])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m14\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-2.7276,  2.8789],\n",
       "        [-2.7276,  2.8789]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batching is the act of sending multiple sentences through the model, all at once.\n",
    "# If you only have one sentence, you can just build a batch with a single sequence:\n",
    "batched_ids = [ids, ids]\n",
    "input = torch.tensor(batched_ids)\n",
    "print(input.shape)\n",
    "\n",
    "model(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Longer sequences\n",
    "\n",
    "```text\n",
    "- With Transformer models, there is a limit to the lengths of the sequences we can pass the models. \n",
    "- Most models handle sequences of up to 512 or 1024 tokens, and will crash when asked to process longer sequences. \n",
    "- There are two solutions to this problem:\n",
    "  - Use a model with a longer supported sequence length.\n",
    "  - Truncate your sequences.\n",
    "\n",
    "- Models have different supported sequence lengths, and some specialize in handling very long sequences. \n",
    "- Longformer is one example, and another is LED. \n",
    "- If you‚Äôre working on a task that requires very long sequences, it's recommended that you take a look at those models.\n",
    "- Otherwise, truncate your sequences by specifying the max_sequence_length parameter:\n",
    "```\n",
    "\n",
    "\n",
    "```python\n",
    "sequence = sequence[:max_sequence_length]\n",
    "```\n",
    "<br>\n",
    "\n",
    "- [Longformer](https://huggingface.co/transformers/model_doc/longformer.html)\n",
    "- [LED](https://huggingface.co/transformers/model_doc/led.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting It All Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "\n",
    "model_inputs = tokenizer(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a single sequence\n",
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "\n",
    "model_inputs = tokenizer(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple sequencea\n",
    "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
    "\n",
    "model_inputs = tokenizer(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add padding\n",
    "# Will pad the sequences up to the maximum sequence length\n",
    "model_inputs = tokenizer(sequences, padding=\"longest\")\n",
    "\n",
    "# Will pad the sequences up to the model max length\n",
    "# (512 for BERT or DistilBERT)\n",
    "model_inputs = tokenizer(sequences, padding=\"max_length\")\n",
    "\n",
    "# Will pad the sequences up to the specified max length\n",
    "model_inputs = tokenizer(sequences, padding=\"max_length\", max_length=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Truncate\n",
    "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
    "\n",
    "# Will truncate the sequences that are longer than the model max length\n",
    "# (512 for BERT or DistilBERT)\n",
    "model_inputs = tokenizer(sequences, truncation=True)\n",
    "\n",
    "# Will truncate the sequences that are longer than the specified max length\n",
    "model_inputs = tokenizer(sequences, max_length=8, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return Tensors for different frameworks\n",
    "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
    "\n",
    "# Returns PyTorch tensors\n",
    "model_inputs = tokenizer(sequences, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "# Returns TensorFlow tensors\n",
    "model_inputs = tokenizer(sequences, padding=True, return_tensors=\"tf\")\n",
    "\n",
    "# Returns NumPy arrays\n",
    "model_inputs = tokenizer(sequences, padding=True, return_tensors=\"np\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Special tokens\n",
    "```\n",
    "If we take a look at the input IDs returned by the tokenizer, we will see they are a tiny bit different from what we had earlier:\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">101</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1045</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1005</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2310</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2042</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3403</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2005</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1037</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17662</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12172</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2607</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2026</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2878</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2166</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1012</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">102</span><span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[1;36m101\u001b[0m, \u001b[1;36m1045\u001b[0m, \u001b[1;36m1005\u001b[0m, \u001b[1;36m2310\u001b[0m, \u001b[1;36m2042\u001b[0m, \u001b[1;36m3403\u001b[0m, \u001b[1;36m2005\u001b[0m, \u001b[1;36m1037\u001b[0m, \u001b[1;36m17662\u001b[0m, \u001b[1;36m12172\u001b[0m, \u001b[1;36m2607\u001b[0m, \u001b[1;36m2026\u001b[0m, \u001b[1;36m2878\u001b[0m, \u001b[1;36m2166\u001b[0m, \u001b[1;36m1012\u001b[0m, \u001b[1;36m102\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1045</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1005</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2310</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2042</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3403</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2005</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1037</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17662</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12172</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2607</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2026</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2878</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2166</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1012</span><span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[1;36m1045\u001b[0m, \u001b[1;36m1005\u001b[0m, \u001b[1;36m2310\u001b[0m, \u001b[1;36m2042\u001b[0m, \u001b[1;36m3403\u001b[0m, \u001b[1;36m2005\u001b[0m, \u001b[1;36m1037\u001b[0m, \u001b[1;36m17662\u001b[0m, \u001b[1;36m12172\u001b[0m, \u001b[1;36m2607\u001b[0m, \u001b[1;36m2026\u001b[0m, \u001b[1;36m2878\u001b[0m, \u001b[1;36m2166\u001b[0m, \u001b[1;36m1012\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "\n",
    "model_inputs = tokenizer(sequence)\n",
    "print(model_inputs[\"input_ids\"])\n",
    "\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>CLS<span style=\"font-weight: bold\">]</span> i've been waiting for a huggingface course my whole life. <span style=\"font-weight: bold\">[</span>SEP<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0mCLS\u001b[1m]\u001b[0m i've been waiting for a huggingface course my whole life. \u001b[1m[\u001b[0mSEP\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">i've been waiting for a huggingface course my whole life.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "i've been waiting for a huggingface course my whole life.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# One token ID was added at the beginning, and one at the end.\n",
    "# Let‚Äôs decode the two sequences of IDs above to see what this is about:\n",
    "print(tokenizer.decode(model_inputs[\"input_ids\"]))\n",
    "print(tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note\n",
    "\n",
    "```text\n",
    "- The tokenizer adds [CLS] at the start and [SEP] at the end to match the model's pretraining. \n",
    "- Different models may have different special words, but the tokenizer handles this automatically.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><hr>\n",
    "\n",
    "### Wrapping Up: From Tokenizer To Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">SequenceClassifierOutput</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">loss</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">logits</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.5607</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.6123</span><span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-3.6183</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.9137</span><span style=\"font-weight: bold\">]]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">grad_fn</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">AddmmBackward0</span><span style=\"font-weight: bold\">&gt;)</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">hidden_states</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">attentions</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mSequenceClassifierOutput\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mloss\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "    \u001b[33mlogits\u001b[0m=\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m-1.5607\u001b[0m,  \u001b[1;36m1.6123\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m\u001b[1;36m-3.6183\u001b[0m,  \u001b[1;36m3.9137\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mgrad_fn\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mAddmmBackward0\u001b[0m\u001b[1m>\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[33mhidden_states\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "    \u001b[33mattentions\u001b[0m=\u001b[3;35mNone\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
    "\n",
    "tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "output = model(**tokens)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_p310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
